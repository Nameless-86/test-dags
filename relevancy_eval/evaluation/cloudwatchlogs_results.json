{
    "test_cases_lookup_map": {
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $1000, indicating a potential cost spike in our S3 service?\", \"retrieval_context\": [\" - This pattern captures S3 Glacier costs using unblended pricing. It identifies the actual costs charged for Glacier storage, retrieval, and data transfer.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\", \" - This pattern captures detailed S3 costs broken down by usage type. It identifies specific charges for storage classes, data transfer, and API requests for cost optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6053452278543201,
                        "reason": "The description clearly explains that the metric captures S3 storage costs using unblended pricing and details the types of charges included, which aligns with the question about UnblendedCost for S3. However, it lacks explicit mention of units (e.g., dollars), does not directly address the $1000 threshold or cost spike implication, and does not explicitly state how to interpret exceeding that amount, limiting its utility for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the description to ensure it is easily understandable and concise given the input question.\",\n    \"Assess the coverage of the description by verifying it explains what the metric measures, its units, context, and implications relevant to the question.\",\n    \"Determine the utility by checking if the description provides sufficient information to effectively answer the monitoring question.\",\n    \"Compare the description (retrieval context) with the input question to ensure alignment and relevance between what is described and what is asked.\"\n] \n \nRubric:\nNone \n \nScore: 0.6053452278543201"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the description to ensure it is easily understandable and concise given the input question.",
                            "Assess the coverage of the description by verifying it explains what the metric measures, its units, context, and implications relevant to the question.",
                            "Determine the utility by checking if the description provides sufficient information to effectively answer the monitoring question.",
                            "Compare the description (retrieval context) with the input question to ensure alignment and relevance between what is described and what is asked."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"RDS\\\", \\\"MetricName\\\": \\\"AmortizedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in AmortizedCost exceeding $100, indicating potential issues with reserved instance utilization?\", \"retrieval_context\": [\" - This pattern captures EC2 compute costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term, providing a more accurate monthly cost view.\", \" - This pattern captures EC2 compute costs using unblended pricing. It identifies the actual costs charged for EC2 instances, including on-demand, reserved instance, and spot instance charges.\", \" - This pattern captures RDS database costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3517433309168356,
                        "reason": "The Retrieval Context explains amortized pricing for EC2 and RDS costs and mentions reserved instances, which relates to the Input question about spikes in AmortizedCost and reserved instance utilization. However, it lacks specific details on what is measured (e.g., cost spikes), units (dollar amounts), or how to identify spikes exceeding $100, limiting clarity and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.\",\n    \"Evaluate the clarity of the Retrieval Context, ensuring it is understandable and concise enough to interpret the Input correctly.\",\n    \"Check the coverage of the Retrieval Context to confirm it includes what is measured, units, context, and implications relevant to the Input question.\",\n    \"Determine the utility by verifying if the Retrieval Context enables a user to confidently answer the Input monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3517433309168356"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.",
                            "Evaluate the clarity of the Retrieval Context, ensuring it is understandable and concise enough to interpret the Input correctly.",
                            "Check the coverage of the Retrieval Context to confirm it includes what is measured, units, context, and implications relevant to the Input question.",
                            "Determine the utility by verifying if the Retrieval Context enables a user to confidently answer the Input monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"LAMBDA\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in our Lambda service?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Lambda function costs using unblended pricing. It identifies the actual costs charged for Lambda function invocations, duration, and requests.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7622459338205512,
                        "reason": "The description directly addresses the monitoring question by focusing on UnblendedCost related to Lambda service costs, aligning with the retrieval context that details Lambda function costs using unblended pricing. It is clear and concise, using terminology consistent with the context. However, it could improve coverage by explicitly stating the units (dollars) and clarifying the implications of exceeding $1000 as a cost spike, enhancing utility for decision-making.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description to the retrieval context ensuring the description directly addresses the monitoring question posed.\",\n    \"Evaluate clarity by checking if the description is understandable and concise in relation to the terminology used in the retrieval context.\",\n    \"Assess coverage by verifying if the description explains what is measured, including units, context, and implications relevant to the retrieval context and question.\",\n    \"Determine utility by judging if the description helps the user effectively answer the monitoring question given the retrieval context; if not, suggest targeted improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.7622459338205512"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description to the retrieval context ensuring the description directly addresses the monitoring question posed.",
                            "Evaluate clarity by checking if the description is understandable and concise in relation to the terminology used in the retrieval context.",
                            "Assess coverage by verifying if the description explains what is measured, including units, context, and implications relevant to the retrieval context and question.",
                            "Determine utility by judging if the description helps the user effectively answer the monitoring question given the retrieval context; if not, suggest targeted improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2-OTHER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3077496168789852,
                        "reason": "The retrieval context mentions unblended costs for specific services but does not directly address whether the UnblendedCost metric exceeds $1000 or indicate how to detect a cost spike. It provides some clarity on what is measured (actual costs charged) but lacks units, thresholds, or implications related to the input question, limiting its practical utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable and concise when read in conjunction with the Input, ensuring no ambiguity remains.\",\n    \"Assess Coverage by verifying the description explains what is measured, including units, context, and implications relevant to the Input question.\",\n    \"Determine Utility by judging whether the Retrieval Context provides sufficient information to confidently answer the Input question, indicating practical usefulness.\"\n] \n \nRubric:\nNone \n \nScore: 0.3077496168789852"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable and concise when read in conjunction with the Input, ensuring no ambiguity remains.",
                            "Assess Coverage by verifying the description explains what is measured, including units, context, and implications relevant to the Input question.",
                            "Determine Utility by judging whether the Retrieval Context provides sufficient information to confidently answer the Input question, indicating practical usefulness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"LAMBDA\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric's UsageQuantity exceed 10 million, indicating a potential Lambda function overload?\", \"retrieval_context\": [\" - This pattern captures CloudFront usage quantities. It identifies the raw usage numbers for CloudFront distributions, including data transfer GB and requests for capacity planning.\", \" - This pattern captures Lambda function costs using unblended pricing. It identifies the actual costs charged for Lambda function invocations, duration, and requests.\", \" - This pattern captures Lambda usage quantities. It identifies the raw usage numbers for Lambda functions, including invocation counts and duration for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.695785401551211,
                        "reason": "The description directly addresses Lambda usage quantities relevant to the question about UsageQuantity exceeding 10 million, mentioning raw usage numbers and invocation counts. It is clear and concise, providing context for capacity planning. However, it lacks explicit units or thresholds (e.g., what constitutes overload) and does not explicitly link UsageQuantity to potential overload implications, limiting full utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Retrieval Context) with the monitoring question (Input) to determine if the description directly addresses the question's focus.\",\n    \"Evaluate the clarity of the description to ensure it is understandable and concise relative to the complexity of the question asked.\",\n    \"Assess coverage by checking if the description includes what is measured, units, context, and possible implications that are relevant to the question.\",\n    \"Determine utility by verifying if the description enables a user to confidently answer the monitoring question based on the provided information.\"\n] \n \nRubric:\nNone \n \nScore: 0.695785401551211"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Retrieval Context) with the monitoring question (Input) to determine if the description directly addresses the question's focus.",
                            "Evaluate the clarity of the description to ensure it is understandable and concise relative to the complexity of the question asked.",
                            "Assess coverage by checking if the description includes what is measured, units, context, and possible implications that are relevant to the question.",
                            "Determine utility by verifying if the description enables a user to confidently answer the monitoring question based on the provided information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric indicate an unexpected spike in EC2 usage quantities?\", \"retrieval_context\": [\" - This pattern captures CloudFront usage quantities. It identifies the raw usage numbers for CloudFront distributions, including data transfer GB and requests for capacity planning.\", \" - This pattern captures EC2 compute costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term, providing a more accurate monthly cost view.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.519142177640589,
                        "reason": "The description addresses EC2 usage quantities and what is measured (raw usage numbers, instance hours), which relates to the question about EC2 usage spikes. However, it lacks clarity on whether it indicates unexpected spikes specifically, and does not mention units explicitly or implications for detecting anomalies. Thus, it provides partial coverage and moderate utility but is not fully aligned with the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question asked.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise in relation to the question\u2019s needs.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to the question.\",\n    \"Judge Utility by determining if the description provides enough information to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.519142177640589"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question asked.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise in relation to the question\u2019s needs.",
                            "Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to the question.",
                            "Judge Utility by determining if the description provides enough information to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2851952809486198,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not directly address whether UnblendedCost exceeds $1000 or indicate how to detect a cost spike. It lacks clarity on measurement units, thresholds, or implications related to the input question, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent.\",\n    \"Evaluate clarity by checking if the retrieval context is easy to understand and concise, avoiding ambiguity that would hinder answering the input question.\",\n    \"Assess coverage by verifying the description includes what is measured, units, relevant context, and implications that relate specifically to the input question.\",\n    \"Determine utility by testing if the retrieval context provides sufficient information for a user to confidently answer the input question; if not, identify gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.2851952809486198"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent.",
                            "Evaluate clarity by checking if the retrieval context is easy to understand and concise, avoiding ambiguity that would hinder answering the input question.",
                            "Assess coverage by verifying the description includes what is measured, units, relevant context, and implications that relate specifically to the input question.",
                            "Determine utility by testing if the retrieval context provides sufficient information for a user to confidently answer the input question; if not, identify gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2\\\", \\\"MetricName\\\": \\\"AmortizedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does AmortizedCost exceed $1000, indicating potential overspending on EC2 Reserved Instances?\", \"retrieval_context\": [\" - This pattern captures EC2 compute costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term, providing a more accurate monthly cost view.\", \" - This pattern captures EC2 compute costs using unblended pricing. It identifies the actual costs charged for EC2 instances, including on-demand, reserved instance, and spot instance charges.\", \" - This pattern captures RDS database costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6962839416889974,
                        "reason": "The description relates to the question by explaining amortized pricing for EC2 Reserved Instances, clarifying what is measured (EC2 compute costs with upfront fees spread over time). However, it lacks explicit mention of the cost threshold ($1000) or direct implications of overspending, and the unit of measurement is implied but not clearly stated. The description is clear and concise but could better address the monitoring question's specific focus on cost limits and potential overspending.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to ensure the description directly addresses or relates to the question.\",\n    \"Assess the clarity of the description to confirm it is understandable and concise, avoiding ambiguity in explaining the metric.\",\n    \"Evaluate the coverage of the description by checking if it includes what is measured, the unit of measurement, relevant context, and possible implications.\",\n    \"Determine the utility by judging whether the description provides sufficient information to help answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.6962839416889974"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to ensure the description directly addresses or relates to the question.",
                            "Assess the clarity of the description to confirm it is understandable and concise, avoiding ambiguity in explaining the metric.",
                            "Evaluate the coverage of the description by checking if it includes what is measured, the unit of measurement, relevant context, and possible implications.",
                            "Determine the utility by judging whether the description provides sufficient information to help answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\", \\\"UsageType\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do spikes in UnblendedCost exceed 20% of the average monthly spend, indicating a potential cost anomaly?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Shield DDoS protection costs using unblended pricing. It identifies the actual costs charged for Shield Advanced protection and data transfer.\", \" - This pattern captures Single Sign-On costs using unblended pricing. It identifies the actual costs charged for SSO user management and application access.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2767997463133452,
                        "reason": "The description partially addresses the monitoring question by focusing on unblended costs and potential anomalies but lacks clarity and detail about the specific services or time frames in the retrieval context. It does not specify units or how the 20% threshold relates to the average monthly spend, limiting coverage and utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Input) against the Retrieval Context to assess if the description addresses the monitoring question fully and accurately.\",\n    \"Evaluate Clarity by determining whether the description is easily understandable and concise in relation to the Retrieval Context and the question's focus.\",\n    \"Assess Coverage by verifying that the description explains what is measured, including units, relevant context, and potential implications relevant to the Retrieval Context and question.\",\n    \"Judge Utility by confirming that the description provides sufficient information to effectively answer the monitoring question based on the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.2767997463133452"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Input) against the Retrieval Context to assess if the description addresses the monitoring question fully and accurately.",
                            "Evaluate Clarity by determining whether the description is easily understandable and concise in relation to the Retrieval Context and the question's focus.",
                            "Assess Coverage by verifying that the description explains what is measured, including units, relevant context, and potential implications relevant to the Retrieval Context and question.",
                            "Judge Utility by confirming that the description provides sufficient information to effectively answer the monitoring question based on the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EBS\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Is the UsageQuantity exceeding 50% of total provisioned capacity, indicating potential resource constraints?\", \"retrieval_context\": [\" - This pattern captures Athena usage quantities. It identifies the raw usage numbers for Athena queries and data scanned TB for capacity planning.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures Route 53 usage quantities. It identifies the raw usage numbers for Route 53 queries and health checks for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.20758581810658705,
                        "reason": "The retrieval context mentions capturing raw usage quantities for various services but does not address 'UsageQuantity' relative to total provisioned capacity or indicate how to determine if usage exceeds 50%, which is the core of the input question. The context is somewhat clear and concise but lacks coverage of measurement units, comparison metrics, or implications related to resource constraints, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to assess if the description clearly addresses the specific topic raised by the question.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it is understandable and concise relative to the information requested in the Input.\",\n    \"Assess Coverage by verifying whether the Retrieval Context explains what is measured, units, context, and implications relevant to the Input.\",\n    \"Determine Utility by judging if the Retrieval Context provides enough information for the user to confidently answer the Input question, ensuring alignment between both.\"\n] \n \nRubric:\nNone \n \nScore: 0.20758581810658705"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to assess if the description clearly addresses the specific topic raised by the question.",
                            "Evaluate the clarity of the Retrieval Context by checking if it is understandable and concise relative to the information requested in the Input.",
                            "Assess Coverage by verifying whether the Retrieval Context explains what is measured, units, context, and implications relevant to the Input.",
                            "Determine Utility by judging if the Retrieval Context provides enough information for the user to confidently answer the Input question, ensuring alignment between both."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this S3 UsageQuantity exceed 10% of total allocated capacity?\", \"retrieval_context\": [\" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures S3 usage quantities. It identifies the raw usage numbers for S3 storage, including GB-months and API requests for capacity planning.\", \" - This pattern captures SNS usage quantities. It identifies the raw usage numbers for SNS messages, including publish requests and deliveries for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.32689414155105034,
                        "reason": "The retrieval context identifies that S3 UsageQuantity captures raw usage numbers including GB-months and API requests for capacity planning, which partially addresses the question about exceeding 10% of total allocated capacity. However, it lacks clarity on units, does not explicitly mention total allocated capacity or percentage thresholds, and does not provide sufficient information to determine if usage exceeds 10%, limiting its utility for answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description is easily understandable and concise, avoiding ambiguous or overly technical language relative to the question.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, relevant system context, and potential implications that relate to the question.\",\n    \"Determine Utility by confirming that the description provides sufficient information to effectively answer the monitoring question, highlighting any gaps between input needs and context content.\"\n] \n \nRubric:\nNone \n \nScore: 0.32689414155105034"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description is easily understandable and concise, avoiding ambiguous or overly technical language relative to the question.",
                            "Assess Coverage by verifying the description includes what is measured, units, relevant system context, and potential implications that relate to the question.",
                            "Determine Utility by confirming that the description provides sufficient information to effectively answer the monitoring question, highlighting any gaps between input needs and context content."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"BlendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are S3 storage costs exceeding $100, indicating a potential cost anomaly?\", \"retrieval_context\": [\" - This pattern captures S3 Glacier costs using unblended pricing. It identifies the actual costs charged for Glacier storage, retrieval, and data transfer.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\", \" - This pattern captures detailed S3 costs broken down by usage type. It identifies specific charges for storage classes, data transfer, and API requests for cost optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6058229822486851,
                        "reason": "The retrieval context addresses S3 storage costs and related charges, aligning with the question about S3 storage cost anomalies. However, it lacks explicit mention of cost thresholds (e.g., $100) or how to detect anomalies, limiting clarity and utility. The description is somewhat technical but mostly understandable, and it covers what is measured and units implicitly, yet it does not directly state implications or guidance on interpreting costs relative to the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus and scope.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise, avoiding ambiguity or overly technical jargon relative to the question.\",\n    \"Assess Coverage by verifying if the description details what is measured, units used, relevant context, and implications, ensuring alignment with the specific question asked.\",\n    \"Determine Utility by analyzing whether the description provides sufficient information to conclusively answer the monitoring question; if not, suggest improvements focusing on missing or unclear elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6058229822486851"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus and scope.",
                            "Evaluate Clarity by checking if the description is understandable and concise, avoiding ambiguity or overly technical jargon relative to the question.",
                            "Assess Coverage by verifying if the description details what is measured, units used, relevant context, and implications, ensuring alignment with the specific question asked.",
                            "Determine Utility by analyzing whether the description provides sufficient information to conclusively answer the monitoring question; if not, suggest improvements focusing on missing or unclear elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ELB\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric's UsageQuantity exceed 90% of its average value, indicating potential resource constraints?\", \"retrieval_context\": [\" - This pattern captures CloudWatch usage quantities. It identifies the raw usage numbers for CloudWatch metrics, log ingestion, and custom metrics for capacity planning.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures Redshift usage quantities. It identifies the raw usage numbers for Redshift clusters, including node hours and storage GB-months for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3977189077284154,
                        "reason": "The Retrieval Context describes usage quantities for various services but lacks clarity and conciseness directly related to the Input question about UsageQuantity exceeding 90% of its average value. It does not explain what is specifically measured by UsageQuantity, the units involved, or how to interpret values relative to averages or thresholds indicating resource constraints. The context misses critical details on how to assess or detect potential resource constraints from UsageQuantity metrics, limiting its utility for answering the question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description is clearly understandable and concise, reflecting the question's intent.\",\n    \"Evaluate the Coverage of the Retrieval Context by checking if it explains what is measured, including units, context, and possible implications relevant to the Input question.\",\n    \"Assess the Utility by determining whether the Retrieval Context provides sufficient information to answer the Input question effectively and directly.\",\n    \"If any criteria score below 8, identify specific missing or unclear aspects in the Retrieval Context that hinder understanding or relevance to the Input question, and suggest improvements accordingly.\"\n] \n \nRubric:\nNone \n \nScore: 0.3977189077284154"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description is clearly understandable and concise, reflecting the question's intent.",
                            "Evaluate the Coverage of the Retrieval Context by checking if it explains what is measured, including units, context, and possible implications relevant to the Input question.",
                            "Assess the Utility by determining whether the Retrieval Context provides sufficient information to answer the Input question effectively and directly.",
                            "If any criteria score below 8, identify specific missing or unclear aspects in the Retrieval Context that hinder understanding or relevance to the Input question, and suggest improvements accordingly."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2-OTHER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\", \\\"UsageType\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2970535201734351,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not explicitly address the overall UnblendedCost metric or whether it exceeds $1000, limiting direct relevance to the monitoring question. The description is somewhat clear about what costs are captured but lacks measurement units, thresholds, or implications related to cost spikes, reducing coverage and utility for answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description specifically addresses the question asked.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise relative to the complexity of the monitoring question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, the measurement units, relevant context, and potential implications to effectively support answering the question.\",\n    \"Judge Utility by determining if the description provides actionable information that directly helps a user answer the monitoring question, highlighting gaps between description and question if any.\"\n] \n \nRubric:\nNone \n \nScore: 0.2970535201734351"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description specifically addresses the question asked.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise relative to the complexity of the monitoring question.",
                            "Assess Coverage by verifying that the description explains what is measured, the measurement units, relevant context, and potential implications to effectively support answering the question.",
                            "Judge Utility by determining if the description provides actionable information that directly helps a user answer the monitoring question, highlighting gaps between description and question if any."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EBS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3168794943398266,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not directly address whether the UnblendedCost metric exceeds $1000 or indicate how to interpret a cost spike. The description is somewhat clear about what costs are captured but lacks explicit measurement units, threshold context, or implications related to the monitoring question, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0.0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent and scope.\",\n    \"Evaluate Clarity by checking if the description is concise, uses clear language, and avoids ambiguity in relation to the question asked.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications that help understand the metric in relation to the question.\",\n    \"Determine Utility by confirming the description provides sufficient information to answer the monitoring question accurately and confidently.\"\n] \n \nRubric:\nNone \n \nScore: 0.3168794943398266"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent and scope.",
                            "Evaluate Clarity by checking if the description is concise, uses clear language, and avoids ambiguity in relation to the question asked.",
                            "Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications that help understand the metric in relation to the question.",
                            "Determine Utility by confirming the description provides sufficient information to answer the monitoring question accurately and confidently."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"API-Gateway-Execution*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a potential issue with the API Gateway?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - API Gateway error logs. Gateway errors, integration failures, client request issues, and service connectivity problems. Critical for API health monitoring and troubleshooting.\", \" - This pattern captures API Gateway 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.46284319121358497,
                        "reason": "The retrieval context partially addresses the question by describing API Gateway error logs and 4XX client errors, which relate to potential issues. However, it lacks explicit mention of spikes or how to detect them, measurement units, or implications for identifying potential issues. The description is somewhat clear but not comprehensive or detailed enough to confidently answer the monitoring question about spikes in ERROR messages.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to ensure the description addresses the question directly and comprehensively.\",\n    \"Evaluate clarity by checking if the metric description is easily understandable and concise without ambiguity in relation to the monitoring question.\",\n    \"Assess coverage by verifying if the description includes what is measured, measurement units, relevant context, and the implications necessary to answer the question.\",\n    \"Determine utility by confirming that the description provides sufficient information for a user to confidently and accurately respond to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.46284319121358497"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to ensure the description addresses the question directly and comprehensively.",
                            "Evaluate clarity by checking if the metric description is easily understandable and concise without ambiguity in relation to the monitoring question.",
                            "Assess coverage by verifying if the description includes what is measured, measurement units, relevant context, and the implications necessary to answer the question.",
                            "Determine utility by confirming that the description provides sufficient information for a user to confidently and accurately respond to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/rds/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a potential issue with the RDS database?\", \"retrieval_context\": [\" - RDS database error logs. Connection failures, query errors, authentication issues, and database engine errors. Critical for database health monitoring and performance troubleshooting.\", \" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\", \" - RDS deadlock detection logs. Database deadlocks, transaction failures, and performance issues. Critical for database stability and transaction management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7220937947939057,
                        "reason": "The description addresses spikes in ERROR messages related to the RDS database, aligning well with the monitoring question and retrieval context focused on error logs and database health. It is clear and concise without jargon, making it understandable. However, it could improve coverage by explicitly mentioning what specific error types are measured, units (e.g., count or rate), and potential implications of spikes to enhance utility for troubleshooting.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (metric description) against the retrieval context (monitoring question) to ensure the description clearly addresses the specific question being asked.\",\n    \"Evaluate the clarity of the input by checking if the description is concise and easily understandable without jargon, considering the context provided by the question.\",\n    \"Assess the coverage of the input to see if it includes what is measured, units, relevant context, and possible implications that align with the retrieval context.\",\n    \"Determine the utility by verifying if the description, in relation to the question, provides sufficient information to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7220937947939057"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (metric description) against the retrieval context (monitoring question) to ensure the description clearly addresses the specific question being asked.",
                            "Evaluate the clarity of the input by checking if the description is concise and easily understandable without jargon, considering the context provided by the question.",
                            "Assess the coverage of the input to see if it includes what is measured, units, relevant context, and possible implications that align with the retrieval context.",
                            "Determine the utility by verifying if the description, in relation to the question, provides sufficient information to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/lambda/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a potential issue with the Lambda function?\", \"retrieval_context\": [\" - Lambda function error logs. Runtime exceptions, execution failures, cold start issues, and application errors. Critical for application health monitoring, debugging, and performance optimization.\", \" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\", \" - Lambda function timeout events. Execution time limit exceeded, performance degradation detection, and resource constraint monitoring. Critical for performance optimization and SLA compliance.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6622459324198198,
                        "reason": "The retrieval context addresses Lambda function error logs, including runtime exceptions and application errors, which relate to ERROR messages and potential issues. The language is clear and concise, suitable for understanding the question. However, it lacks explicit mention of spikes or frequency trends in error messages, and does not specify units or detailed implications of such spikes, limiting full coverage and utility for directly answering the question about spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses and supports answering the question.\",\n    \"Evaluate the clarity of the Retrieval Context to determine if it uses understandable and concise language appropriate to the Input question.\",\n    \"Check the coverage in the Retrieval Context by confirming inclusion of what is measured, units, context, and implications relevant to the Input.\",\n    \"Assess the utility of the Retrieval Context by judging whether it provides sufficient and pertinent information to help a user effectively respond to the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6622459324198198"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses and supports answering the question.",
                            "Evaluate the clarity of the Retrieval Context to determine if it uses understandable and concise language appropriate to the Input question.",
                            "Check the coverage in the Retrieval Context by confirming inclusion of what is measured, units, context, and implications relevant to the Input.",
                            "Assess the utility of the Retrieval Context by judging whether it provides sufficient and pertinent information to help a user effectively respond to the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/lambda/*\\\", \\\"message\\\": \\\"REPORT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in average Lambda function execution durations exceeding 5 seconds?\", \"retrieval_context\": [\" - Lambda function execution reports. Performance metrics, duration, memory usage, billing information, and cold start detection. Performance monitoring and cost optimization analysis.\", \" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\", \" - Lambda function timeout events. Execution time limit exceeded, performance degradation detection, and resource constraint monitoring. Critical for performance optimization and SLA compliance.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.30371605866744816,
                        "reason": "The retrieval context mentions Lambda function execution reports and performance metrics including duration, which is relevant to the question about execution durations. However, it does not specifically address spikes or durations exceeding 5 seconds, nor does it provide units, detailed measurement explanations, or implications of such spikes. The description is somewhat clear but lacks sufficient detail and direct relevance to fully answer the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate the clarity of the description by checking if it is understandable and concise without unnecessary jargon.\",\n    \"Assess coverage by verifying if the description explains what is measured, the units, relevant context, and possible implications.\",\n    \"Determine utility by judging whether the description provides enough information to effectively answer the monitoring question; if not, suggest concrete improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.30371605866744816"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate the clarity of the description by checking if it is understandable and concise without unnecessary jargon.",
                            "Assess coverage by verifying if the description explains what is measured, the units, relevant context, and possible implications.",
                            "Determine utility by judging whether the description provides enough information to effectively answer the monitoring question; if not, suggest concrete improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/ecs/*\\\", \\\"message\\\": \\\"STOPPED*\\\"}\", \"hyperparameters\": null, \"input\": \"Are ECS container stop events being triggered by resource constraints or service scaling issues?\", \"retrieval_context\": [\" - ECS container error logs. Container startup failures, application errors, service health check failures, and container lifecycle issues. Critical for container orchestration monitoring.\", \" - ECS container stop events. Container termination, health check failures, resource constraints, and service scaling. Container lifecycle management and availability monitoring.\", \" - ECS container warning logs. Non-critical issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.48740926615972724,
                        "reason": "The input question is clear and specific, asking whether ECS container stop events are caused by resource constraints or service scaling issues. The retrieval context mentions relevant topics such as container termination, resource constraints, and service scaling, but it is fragmented and lacks detailed explanation of metrics, units, or how to distinguish causes. While the context covers related areas, it does not provide sufficient clarity or actionable information to definitively answer the question, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input for clarity in its phrasing of the monitoring question, ensuring it is specific and comprehensible.\",\n    \"Assess the Retrieval Context (metric description) for clarity and conciseness, checking if it explains what is measured, units, and relevant context.\",\n    \"Compare Input and Retrieval Context for coverage and alignment: verify that the description provides sufficient information to answer the question posed.\",\n    \"Judge the utility by determining if the description effectively enables users to resolve the monitoring question; if inadequate, recommend targeted improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.48740926615972724"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input for clarity in its phrasing of the monitoring question, ensuring it is specific and comprehensible.",
                            "Assess the Retrieval Context (metric description) for clarity and conciseness, checking if it explains what is measured, units, and relevant context.",
                            "Compare Input and Retrieval Context for coverage and alignment: verify that the description provides sufficient information to answer the question posed.",
                            "Judge the utility by determining if the description effectively enables users to resolve the monitoring question; if inadequate, recommend targeted improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/lambda/*\\\", \\\"message\\\": \\\"TIMEOUT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Lambda function timeout events exceeding their execution limits, indicating potential performance degradation?\", \"retrieval_context\": [\" - Lambda function end events. Execution completion, success/failure tracking, and lifecycle management. Application performance monitoring and execution analytics.\", \" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\", \" - Lambda function timeout events. Execution time limit exceeded, performance degradation detection, and resource constraint monitoring. Critical for performance optimization and SLA compliance.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.619465860463779,
                        "reason": "The retrieval context addresses Lambda function timeout events and their relation to execution time limits and performance degradation, aligning with the input question's monitoring goal. However, the description lacks clarity and detail on specific measurements, units, and actionable insights, limiting its utility in fully answering whether timeout events are exceeding limits and indicating performance issues.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure they address the same monitoring goal.\",\n    \"Evaluate clarity by checking if the description uses understandable language that directly relates to the input question without unnecessary complexity.\",\n    \"Assess coverage by confirming the description includes what is measured, units, relevant context, and the implications pertinent to the monitoring question.\",\n    \"Determine utility by verifying if the description provides enough information to answer the input question effectively, including actionable insights or explanations.\"\n] \n \nRubric:\nNone \n \nScore: 0.619465860463779"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure they address the same monitoring goal.",
                            "Evaluate clarity by checking if the description uses understandable language that directly relates to the input question without unnecessary complexity.",
                            "Assess coverage by confirming the description includes what is measured, units, relevant context, and the implications pertinent to the monitoring question.",
                            "Determine utility by verifying if the description provides enough information to answer the input question effectively, including actionable insights or explanations."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"API-Gateway-Execution*\\\", \\\"message\\\": \\\"4XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the API Gateway 4XX client error logs indicating a significant spike in bad requests?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - API Gateway error logs. Gateway errors, integration failures, client request issues, and service connectivity problems. Critical for API health monitoring and troubleshooting.\", \" - This pattern captures API Gateway 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3747824954732243,
                        "reason": "The retrieval context identifies what API Gateway 4XX client error logs represent, including types of errors like bad requests and unauthorized access, which aligns with the input question. However, it lacks any information about measuring spikes, timeframes, or how to determine significance, limiting its utility for assessing whether there is a significant spike in bad requests. The description is somewhat clear but does not provide sufficient coverage or actionable guidance to answer the question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the specific question asked.\",\n    \"Evaluate the clarity of the Retrieval Context: check if the description is concise and easy to understand in relation to the Input.\",\n    \"Assess coverage by verifying that the description explains what is measured, units, context, and implications relevant to the Input's question.\",\n    \"Determine utility by judging if the description enables a user to effectively answer the Input question using the provided Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3747824954732243"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the specific question asked.",
                            "Evaluate the clarity of the Retrieval Context: check if the description is concise and easy to understand in relation to the Input.",
                            "Assess coverage by verifying that the description explains what is measured, units, context, and implications relevant to the Input's question.",
                            "Determine utility by judging if the description enables a user to effectively answer the Input question using the provided Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"API-Gateway-Execution*\\\", \\\"message\\\": \\\"5XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of 5XX server error responses from API Gateway?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - This pattern captures API Gateway 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues.\", \" - This pattern captures API Gateway 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5266956406486976,
                        "reason": "The retrieval context partially addresses the input by describing the types of 5XX errors captured but lacks explicit mention of measuring spikes or the units/timeframe involved. It is somewhat clear but repetitive and includes irrelevant 4XX error information, reducing clarity and coverage. The context provides some utility but insufficient detail to fully answer the question about spikes in 5XX error counts.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description addresses the specific measurement and details relevant to the question.\",\n    \"Evaluate Clarity by verifying if the Retrieval Context is understandable and concise, enabling straightforward comprehension in relation to the Input.\",\n    \"Assess Coverage by ensuring the Retrieval Context includes what is measured, units, context, and implications necessary to answer the Input fully.\",\n    \"Judge Utility by determining if the Retrieval Context provides sufficient information to directly support answering the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.5266956406486976"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description addresses the specific measurement and details relevant to the question.",
                            "Evaluate Clarity by verifying if the Retrieval Context is understandable and concise, enabling straightforward comprehension in relation to the Input.",
                            "Assess Coverage by ensuring the Retrieval Context includes what is measured, units, context, and implications necessary to answer the Input fully.",
                            "Judge Utility by determining if the Retrieval Context provides sufficient information to directly support answering the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"API-Gateway-Execution*\\\", \\\"message\\\": \\\"5XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the 5XX server error responses in API Gateway increasing, indicating a potential issue with backend services?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - This pattern captures API Gateway 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues.\", \" - This pattern captures API Gateway 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.598036670506381,
                        "reason": "The retrieval context partially addresses the monitoring question by describing 5XX server error responses in API Gateway, including internal server errors, service unavailable, and gateway timeout issues, which aligns with the question's focus on backend service problems. However, the description is somewhat repetitive and lacks clarity on measurement units, how the errors are tracked over time, or implications of an increase. It also includes unrelated 4XX client error information, which may reduce clarity and utility. Overall, it provides a basic understanding but lacks sufficient detail to fully support answering the question about error trends.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description is concise and understandable within the context of the monitoring question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, the units used, relevant context, and potential implications, specifically in relation to the monitoring question.\",\n    \"Determine Utility by confirming the description provides sufficient information to directly answer or support answering the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.598036670506381"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description is concise and understandable within the context of the monitoring question.",
                            "Assess Coverage by verifying that the description explains what is measured, the units used, relevant context, and potential implications, specifically in relation to the monitoring question.",
                            "Determine Utility by confirming the description provides sufficient information to directly answer or support answering the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/ecs/*\\\", \\\"message\\\": \\\"HEALTH_CHECK*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the ECS health check logs indicate a significant increase in failed health checks?\", \"retrieval_context\": [\" - ECS container error logs. Container startup failures, application errors, service health check failures, and container lifecycle issues. Critical for container orchestration monitoring.\", \" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5739631257837363,
                        "reason": "The retrieval context addresses ECS health check logs and mentions health check failures, successful checks, and configuration issues, which aligns with the input question about failed health checks. However, the description lacks specific metrics, units, or indications of what constitutes a 'significant increase,' limiting clarity and utility. It provides relevant context but does not offer sufficient detail for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.\",\n    \"Evaluate the description's clarity by checking if it is concise, easily understandable, and free of ambiguity in relation to the question posed.\",\n    \"Assess the coverage by verifying that the description includes what is measured, its units, the relevant context, and the implications necessary to answer the input question.\",\n    \"Determine the utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; if gaps exist, note what specific details should be added.\"\n] \n \nRubric:\nNone \n \nScore: 0.5739631257837363"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.",
                            "Evaluate the description's clarity by checking if it is concise, easily understandable, and free of ambiguity in relation to the question posed.",
                            "Assess the coverage by verifying that the description includes what is measured, its units, the relevant context, and the implications necessary to answer the input question.",
                            "Determine the utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; if gaps exist, note what specific details should be added."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/lambda/*\\\", \\\"message\\\": \\\"START*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Lambda function start events indicating cold starts or high latency?\", \"retrieval_context\": [\" - Lambda function error logs. Runtime exceptions, execution failures, cold start issues, and application errors. Critical for application health monitoring, debugging, and performance optimization.\", \" - Lambda function execution reports. Performance metrics, duration, memory usage, billing information, and cold start detection. Performance monitoring and cost optimization analysis.\", \" - Lambda function start events. Cold start detection, function initialization, and execution lifecycle monitoring. Performance analysis and resource allocation tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7268941427229488,
                        "reason": "The input question is clear and specific, focusing on spikes in Lambda function start events related to cold starts or high latency. The retrieval context covers relevant aspects such as cold start detection, function initialization, execution lifecycle, performance metrics, and error logs, which are pertinent to answering the question. However, the context is somewhat broad and fragmented across three points without explicit mention of spike detection or latency measurement units, limiting direct applicability. More focused details on how spikes are identified or quantified would improve clarity and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to ensure it clearly defines what needs to be monitored or understood.\",\n    \"Assess the Retrieval Context (metric description) for clarity, coverage of measurement details (what is measured, units, context, implications), and conciseness.\",\n    \"Compare the Retrieval Context against the Input to determine if the description directly supports answering the monitoring question with sufficient detail.\",\n    \"Assign a quality score based on how well the description enables a user to answer the question, and if below threshold, suggest targeted improvements for clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.7268941427229488"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to ensure it clearly defines what needs to be monitored or understood.",
                            "Assess the Retrieval Context (metric description) for clarity, coverage of measurement details (what is measured, units, context, implications), and conciseness.",
                            "Compare the Retrieval Context against the Input to determine if the description directly supports answering the monitoring question with sufficient detail.",
                            "Assign a quality score based on how well the description enables a user to answer the question, and if below threshold, suggest targeted improvements for clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"API-Gateway-Execution*\\\", \\\"message\\\": \\\"4XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the 4XX client error responses in API Gateway exceeding ten percent of total requests?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - API Gateway error logs. Gateway errors, integration failures, client request issues, and service connectivity problems. Critical for API health monitoring and troubleshooting.\", \" - This pattern captures API Gateway 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31824255193614454,
                        "reason": "The retrieval context identifies the types of 4XX client errors and their relevance to API Gateway monitoring, aligning somewhat with the question about 4XX error responses. However, it lacks clarity and coverage regarding measurement specifics such as the percentage of total requests, units, or thresholds like the ten percent mentioned in the question. Consequently, it provides limited utility for directly answering whether 4XX errors exceed ten percent of total requests.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to assess if the description clearly aligns with the question's intent.\",\n    \"Evaluate Clarity by determining if the description is concise and easy to understand in the context of the question provided.\",\n    \"Assess Coverage by verifying that the description includes what is measured, units, relevant context, and implications necessary to address the question.\",\n    \"Judge Utility by confirming whether the description sufficiently enables a user to answer the monitoring question based on the information given.\"\n] \n \nRubric:\nNone \n \nScore: 0.31824255193614454"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to assess if the description clearly aligns with the question's intent.",
                            "Evaluate Clarity by determining if the description is concise and easy to understand in the context of the question provided.",
                            "Assess Coverage by verifying that the description includes what is measured, units, relevant context, and implications necessary to address the question.",
                            "Judge Utility by confirming whether the description sufficiently enables a user to answer the monitoring question based on the information given."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/ecs/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ECS container error logs indicating a potential issue with container startup or operation?\", \"retrieval_context\": [\" - ECS container error logs. Container startup failures, application errors, service health check failures, and container lifecycle issues. Critical for container orchestration monitoring.\", \" - ECS container warning logs. Non-critical issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.767917870566917,
                        "reason": "The input is clear and concise, directly addressing spikes in ECS container error logs related to startup or operational issues, which aligns well with the retrieval context focused on error logs and container health. However, it lacks explicit mention of units, frequency, or specific implications, slightly limiting full coverage and utility for detailed monitoring or action.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input by determining if the metric description is understandable and concise, ensuring it matches the terminology and intent of the Retrieval Context (the monitoring question).\",\n    \"Assess the coverage of the Input by verifying whether the description fully explains what is measured, including units, context, and implications relevant to the Retrieval Context.\",\n    \"Evaluate utility by judging if the Input (metric description) sufficiently supports answering the Retrieval Context (monitoring question) directly and effectively.\",\n    \"Compare Input and Retrieval Context together to ensure alignment and coherence; a high-quality description should clearly relate to the monitoring question, enabling accurate interpretation and action.\"\n] \n \nRubric:\nNone \n \nScore: 0.767917870566917"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input by determining if the metric description is understandable and concise, ensuring it matches the terminology and intent of the Retrieval Context (the monitoring question).",
                            "Assess the coverage of the Input by verifying whether the description fully explains what is measured, including units, context, and implications relevant to the Retrieval Context.",
                            "Evaluate utility by judging if the Input (metric description) sufficiently supports answering the Retrieval Context (monitoring question) directly and effectively.",
                            "Compare Input and Retrieval Context together to ensure alignment and coherence; a high-quality description should clearly relate to the monitoring question, enabling accurate interpretation and action."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/ecs/*\\\", \\\"message\\\": \\\"WARN*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the ECS container warning logs indicating resource constraints or performance degradation?\", \"retrieval_context\": [\" - ECS container error logs. Container startup failures, application errors, service health check failures, and container lifecycle issues. Critical for container orchestration monitoring.\", \" - ECS container warning logs. Non-critical issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8010986944634346,
                        "reason": "The retrieval context clearly aligns with the input question by describing ECS container warning logs as indicators of resource constraints and performance degradation, addressing the question's focus. The description is concise and understandable, mentioning non-critical issues and proactive monitoring. However, it lacks specific details on units, exact log content, or how to interpret the warnings, which limits full coverage and utility for directly answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly aligns with the question's focus.\",\n    \"Evaluate clarity by checking if the metric description is understandable and concise in explaining the key elements related to the Input question.\",\n    \"Assess coverage by determining whether the description explains what is measured, including units, context, and implications relevant to the Input question.\",\n    \"Judge utility by verifying if the description provides enough detail to directly help answer the monitoring question, highlighting any missing information or ambiguities.\"\n] \n \nRubric:\nNone \n \nScore: 0.8010986944634346"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly aligns with the question's focus.",
                            "Evaluate clarity by checking if the metric description is understandable and concise in explaining the key elements related to the Input question.",
                            "Assess coverage by determining whether the description explains what is measured, including units, context, and implications relevant to the Input question.",
                            "Judge utility by verifying if the description provides enough detail to directly help answer the monitoring question, highlighting any missing information or ambiguities."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/rds/*\\\", \\\"message\\\": \\\"WARN*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in RDS database performance warnings that could indicate a resource bottleneck?\", \"retrieval_context\": [\" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\", \" - RDS deadlock detection logs. Database deadlocks, transaction failures, and performance issues. Critical for database stability and transaction management.\", \" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.591775803703833,
                        "reason": "The retrieval context addresses RDS performance warnings and related logs, which aligns with the question about spikes in performance warnings indicating resource bottlenecks. However, the description lacks explicit mention of how spikes are detected or measured, units of measurement, and direct implications for resource bottlenecks. The language is generally clear but somewhat high-level, limiting the user's ability to confidently answer the question based solely on the provided context.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description directly addresses the question's focus.\",\n    \"Evaluate the description for clarity by assessing if the language is straightforward, concise, and free of jargon, ensuring it is easy to understand in relation to the question asked.\",\n    \"Assess coverage by verifying if the description includes what is measured, the units used, relevant context, and potential implications, ensuring these details support answering the provided question.\",\n    \"Determine utility by judging if the description enables a user to confidently answer the monitoring question based on the information given in the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.591775803703833"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description directly addresses the question's focus.",
                            "Evaluate the description for clarity by assessing if the language is straightforward, concise, and free of jargon, ensuring it is easy to understand in relation to the question asked.",
                            "Assess coverage by verifying if the description includes what is measured, the units used, relevant context, and potential implications, ensuring these details support answering the provided question.",
                            "Determine utility by judging if the description enables a user to confidently answer the monitoring question based on the information given in the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"API-Gateway-Execution*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Gateway errors that could indicate a problem with the API?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - API Gateway error logs. Gateway errors, integration failures, client request issues, and service connectivity problems. Critical for API health monitoring and troubleshooting.\", \" - This pattern captures API Gateway 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7130512302962996,
                        "reason": "The description is fairly clear and covers key elements such as types of errors (4XX client errors) and their relevance to API health monitoring, which aligns with the monitoring question about spikes in Gateway errors. However, it lacks explicit mention of units or metrics (e.g., error counts or rates) and does not directly address how spikes are detected or quantified, limiting its utility for directly answering the question about error spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) against the Retrieval Context (monitoring question) to assess if the description is clear and concise, enabling easy understanding.\",\n    \"Evaluate whether the description thoroughly covers key elements such as what is measured, units, context, and potential implications relevant to the monitoring question.\",\n    \"Determine the utility of the description by checking if it provides sufficient information to directly or indirectly help answer the monitoring question.\",\n    \"If the description scores below 8, identify specific gaps where clarity, coverage, or utility could be enhanced to better link the description with the question context.\"\n] \n \nRubric:\nNone \n \nScore: 0.7130512302962996"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) against the Retrieval Context (monitoring question) to assess if the description is clear and concise, enabling easy understanding.",
                            "Evaluate whether the description thoroughly covers key elements such as what is measured, units, context, and potential implications relevant to the monitoring question.",
                            "Determine the utility of the description by checking if it provides sufficient information to directly or indirectly help answer the monitoring question.",
                            "If the description scores below 8, identify specific gaps where clarity, coverage, or utility could be enhanced to better link the description with the question context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/rds/*\\\", \\\"message\\\": \\\"SLOW_QUERY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there SLOW_QUERY messages indicating a performance issue in the RDS database?\", \"retrieval_context\": [\" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\", \" - RDS deadlock detection logs. Database deadlocks, transaction failures, and performance issues. Critical for database stability and transaction management.\", \" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6499999999999999,
                        "reason": "The retrieval context is somewhat clear and related to RDS performance issues, mentioning slow query logs and performance thresholds, which aligns with the question about SLOW_QUERY messages. However, it lacks explicit mention of 'SLOW_QUERY' messages specifically and does not clarify the exact metric components such as units, frequency, or severity of slow queries. This limits coverage and utility, as a user may not confidently determine if the slow query messages indicate a performance issue without more detailed explanation or direct references to the SLOW_QUERY metric.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input (question) and Retrieval Context (description) together: assess whether the description is understandable and concise in relation to what the question is asking.\",\n    \"Assess coverage by ensuring the description sufficiently explains the metric components (what is measured, units, context, implications) to address the monitoring question accurately.\",\n    \"Determine the utility by verifying if the description provides enough relevant information to help a user confidently answer the given question.\",\n    \"If any of clarity, coverage, or utility is insufficient, identify specific gaps between the description and question, and suggest focused improvements to enhance alignment and completeness.\"\n] \n \nRubric:\nNone \n \nScore: 0.6499999999999999"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input (question) and Retrieval Context (description) together: assess whether the description is understandable and concise in relation to what the question is asking.",
                            "Assess coverage by ensuring the description sufficiently explains the metric components (what is measured, units, context, implications) to address the monitoring question accurately.",
                            "Determine the utility by verifying if the description provides enough relevant information to help a user confidently answer the given question.",
                            "If any of clarity, coverage, or utility is insufficient, identify specific gaps between the description and question, and suggest focused improvements to enhance alignment and completeness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/lambda/*\\\", \\\"message\\\": \\\"END*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in END* log messages indicating a large number of failed Lambda executions?\", \"retrieval_context\": [\" - Lambda function end events. Execution completion, success/failure tracking, and lifecycle management. Application performance monitoring and execution analytics.\", \" - Lambda function error logs. Runtime exceptions, execution failures, cold start issues, and application errors. Critical for application health monitoring, debugging, and performance optimization.\", \" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6770549130854603,
                        "reason": "The retrieval context addresses Lambda function end events, error logs, and out-of-memory events, which are relevant to monitoring failed Lambda executions. However, it does not explicitly mention 'END* log messages' or spikes in such logs, limiting direct alignment with the question. The description is clear and concise, covering execution completion, success/failure tracking, and error types, providing useful context. It explains what is measured and the implications for application health and performance, but lacks specific details on detecting spikes or the exact metric units, reducing full coverage and utility for answering the question precisely.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise, ensuring it can be easily linked to the monitoring question.\",\n    \"Assess Coverage by verifying the description explains what is measured, units, context, and implications, as relevant to the specific question provided.\",\n    \"Judge Utility by determining if the description, in relation to the question, provides enough insight or data to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6770549130854603"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise, ensuring it can be easily linked to the monitoring question.",
                            "Assess Coverage by verifying the description explains what is measured, units, context, and implications, as relevant to the specific question provided.",
                            "Judge Utility by determining if the description, in relation to the question, provides enough insight or data to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/ecs/*\\\", \\\"message\\\": \\\"STARTED*\\\"}\", \"hyperparameters\": null, \"input\": \"Are ECS container start events being logged at an increased rate, indicating potential issues with application deployment or initialization?\", \"retrieval_context\": [\" - ECS container error logs. Container startup failures, application errors, service health check failures, and container lifecycle issues. Critical for container orchestration monitoring.\", \" - ECS container start events. Container initialization, service deployment, and request processing readiness. Application availability and deployment monitoring.\", \" - ECS container warning logs. Non-critical issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5936239000806853,
                        "reason": "The retrieval context addresses ECS container start events and related monitoring aspects, which aligns with the question about increased logging rates indicating deployment or initialization issues. However, the description lacks explicit mention of metrics, units, or how to measure the rate increase, limiting clarity and coverage. While it provides relevant context on container initialization and deployment monitoring, it does not fully enable confident assessment of the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to assess if the description clearly addresses the question being asked.\",\n    \"Evaluate the description for clarity by checking if the language is understandable and concise without ambiguity.\",\n    \"Assess coverage by verifying if the description provides information on what metric is measured, the units used, the relevant context, and any implications related to the metric.\",\n    \"Judge utility by determining whether the description enables a user to confidently answer the monitoring question based on the information provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.5936239000806853"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to assess if the description clearly addresses the question being asked.",
                            "Evaluate the description for clarity by checking if the language is understandable and concise without ambiguity.",
                            "Assess coverage by verifying if the description provides information on what metric is measured, the units used, the relevant context, and any implications related to the metric.",
                            "Judge utility by determining whether the description enables a user to confidently answer the monitoring question based on the information provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/lambda/*\\\", \\\"message\\\": \\\"OUT_OF_MEMORY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Lambda function out-of-memory events causing a spike in memory usage?\", \"retrieval_context\": [\" - Lambda function execution reports. Performance metrics, duration, memory usage, billing information, and cold start detection. Performance monitoring and cost optimization analysis.\", \" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\", \" - Lambda function timeout events. Execution time limit exceeded, performance degradation detection, and resource constraint monitoring. Critical for performance optimization and SLA compliance.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6695407287645254,
                        "reason": "The description addresses Lambda function out-of-memory events and their relation to memory limits and resource constraints, which aligns with the input question about memory usage spikes. It is concise and mentions performance degradation and resource optimization, providing relevant context. However, it lacks explicit details on how out-of-memory events directly cause spikes in memory usage or specific measurement units, limiting full coverage and utility for directly answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the specific query.\",\n    \"Evaluate Clarity by verifying that the description is concise and easily understandable in the context of the input question.\",\n    \"Assess Coverage by checking whether the description details what is measured, including units, relevant context, and implications that relate directly to the input question.\",\n    \"Determine Utility by confirming the description provides sufficient information to effectively answer the input question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6695407287645254"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the specific query.",
                            "Evaluate Clarity by verifying that the description is concise and easily understandable in the context of the input question.",
                            "Assess Coverage by checking whether the description details what is measured, including units, relevant context, and implications that relate directly to the input question.",
                            "Determine Utility by confirming the description provides sufficient information to effectively answer the input question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/elasticache/*\\\", \\\"message\\\": \\\"CONNECTION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in CONNECTION_FAILED messages indicating a potential issue with ElastiCache connections?\", \"retrieval_context\": [\" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures ElastiCache connection logs. It identifies client connections, connection failures, and connection pool events.\", \" - This pattern captures ElastiCache error logs. It identifies Redis/Memcached errors, connection failures, memory issues, and cache engine problems that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6859740776788577,
                        "reason": "The description directly addresses the monitoring question by focusing on ElastiCache connection failures, which aligns with the input about CONNECTION_FAILED messages. It is clear and concise in explaining that the pattern captures connection logs and failures. However, it lacks specific measurement details, units, or explicit implications about spikes, limiting full coverage and utility for answering the question about spikes in connection failures.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input and Retrieval Context to ensure the metric description directly addresses the monitoring question.\",\n    \"Evaluate clarity by checking if the description is concise and comprehensible in explaining what the metric measures.\",\n    \"Assess coverage by verifying the description includes measurement details, units, relevant context, and implications related to the question.\",\n    \"Determine utility by confirming that the description enables a user to effectively answer the monitoring question based on the provided information.\"\n] \n \nRubric:\nNone \n \nScore: 0.6859740776788577"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input and Retrieval Context to ensure the metric description directly addresses the monitoring question.",
                            "Evaluate clarity by checking if the description is concise and comprehensible in explaining what the metric measures.",
                            "Assess coverage by verifying the description includes measurement details, units, relevant context, and implications related to the question.",
                            "Determine utility by confirming that the description enables a user to effectively answer the monitoring question based on the provided information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/rds/*\\\", \\\"message\\\": \\\"DEADLOCK*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the RDS deadlock detection logs indicating a significant increase in database deadlocks?\", \"retrieval_context\": [\" - RDS database error logs. Connection failures, query errors, authentication issues, and database engine errors. Critical for database health monitoring and performance troubleshooting.\", \" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\", \" - RDS deadlock detection logs. Database deadlocks, transaction failures, and performance issues. Critical for database stability and transaction management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4157843815615442,
                        "reason": "The retrieval context mentions RDS deadlock detection logs and their relation to database deadlocks and performance issues, which aligns with the monitoring question. However, the description lacks clarity and detail about what is specifically measured, such as the frequency or count of deadlocks, units, or how to interpret a 'significant increase.' It provides some context on importance but does not offer sufficient information for confidently answering whether there is a significant increase in deadlocks.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the specific question asked.\",\n    \"Evaluate the clarity of the description to determine if it is understandable and concise in relation to the question's focus.\",\n    \"Check coverage by verifying if the description explains what is measured, including units, context, and implications relevant to answering the question.\",\n    \"Assess utility by determining if the description provides sufficient information for a user to confidently answer the monitoring question; if lacking, suggest improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.4157843815615442"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the specific question asked.",
                            "Evaluate the clarity of the description to determine if it is understandable and concise in relation to the question's focus.",
                            "Check coverage by verifying if the description explains what is measured, including units, context, and implications relevant to answering the question.",
                            "Assess utility by determining if the description provides sufficient information for a user to confidently answer the monitoring question; if lacking, suggest improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/applicationloadbalancer/*\\\", \\\"message\\\": \\\"TARGET*\\\"}\", \"hyperparameters\": null, \"input\": \"Do spikes in TARGET deregistered messages correlate with drops in HealthyHostCount?\", \"retrieval_context\": [\" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - This pattern captures ALB target-related logs. It identifies target health check events, target registration/deregistration, and target group changes.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2037326883595964,
                        "reason": "The retrieval context mentions logs related to target registration/deregistration and health checks but does not explicitly address the correlation between spikes in TARGET deregistered messages and drops in HealthyHostCount. The descriptions are somewhat clear but lack specific details on metrics, units, or implications needed to answer the input question fully, resulting in limited utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate if the Input (question) and Retrieval Context (metric description) align clearly, ensuring the description addresses the question directly.\",\n    \"Assess Clarity by checking whether the Retrieval Context is concise and understandable, without ambiguity, in relation to the Input.\",\n    \"Judge Coverage by verifying the description fully explains what is measured, including units, context, and implications pertinent to the Input question.\",\n    \"Determine Utility by confirming the description provides sufficient information to accurately answer the Input question, ensuring practical relevance between both.\"\n] \n \nRubric:\nNone \n \nScore: 0.2037326883595964"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate if the Input (question) and Retrieval Context (metric description) align clearly, ensuring the description addresses the question directly.",
                            "Assess Clarity by checking whether the Retrieval Context is concise and understandable, without ambiguity, in relation to the Input.",
                            "Judge Coverage by verifying the description fully explains what is measured, including units, context, and implications pertinent to the Input question.",
                            "Determine Utility by confirming the description provides sufficient information to accurately answer the Input question, ensuring practical relevance between both."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/rds/*\\\", \\\"message\\\": \\\"CONNECTION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in CONNECTION_FAILED events indicating a potential database connectivity issue?\", \"retrieval_context\": [\" - RDS connection logs. Connection attempts, successful connections, connection failures, and connection pool events. Database connectivity monitoring and capacity planning.\", \" - RDS database error logs. Connection failures, query errors, authentication issues, and database engine errors. Critical for database health monitoring and performance troubleshooting.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7037326889485939,
                        "reason": "The input question is clear and specific, asking about spikes in CONNECTION_FAILED events indicating database connectivity issues. The retrieval context is concise and generally clear, covering relevant logs such as RDS database error logs, connection logs, and Redshift cluster error logs, all mentioning connection failures. However, the context lacks explicit mention of metrics or time-series data that would indicate spikes, limiting its direct utility for detecting event spikes. Including information on event frequency or temporal patterns would improve coverage and better support answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (question) for specificity and clarity; ensure it defines the user's information need plainly.\",\n    \"Assess the Retrieval Context (metric description) for clarity, verifying it is concise and comprehensible.\",\n    \"Check if the Retrieval Context sufficiently covers what is measured, the units, relevant context, and the implications to address the Input question.\",\n    \"Determine the utility by examining how well the Retrieval Context enables answering the Input question; note gaps and make improvement suggestions if coverage or clarity is insufficient.\"\n] \n \nRubric:\nNone \n \nScore: 0.7037326889485939"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (question) for specificity and clarity; ensure it defines the user's information need plainly.",
                            "Assess the Retrieval Context (metric description) for clarity, verifying it is concise and comprehensible.",
                            "Check if the Retrieval Context sufficiently covers what is measured, the units, relevant context, and the implications to address the Input question.",
                            "Determine the utility by examining how well the Retrieval Context enables answering the Input question; note gaps and make improvement suggestions if coverage or clarity is insufficient."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/applicationloadbalancer/*\\\", \\\"message\\\": \\\"4XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in 4XX* errors indicating a potential issue with application logic or user authentication?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - This pattern captures ALB 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors from the load balancer.\", \" - This pattern captures API Gateway 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6377540668798145,
                        "reason": "The description addresses the presence of 4XX client errors relevant to the question about spikes in such errors, mentioning types like bad requests and unauthorized access. However, it lacks explicit details on measurement units, how spikes are detected, or implications for application logic or user authentication, limiting clarity and utility for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly addresses the question.\",\n    \"Evaluate the description for clarity by checking if it is understandable and concise relative to the information sought in the input.\",\n    \"Assess coverage by verifying that the description explains what is measured, the units, the relevant context, and possible implications as they relate to answering the question.\",\n    \"Determine the utility of the description by confirming it provides enough detail to effectively help the user answer the provided monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6377540668798145"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly addresses the question.",
                            "Evaluate the description for clarity by checking if it is understandable and concise relative to the information sought in the input.",
                            "Assess coverage by verifying that the description explains what is measured, the units, the relevant context, and possible implications as they relate to answering the question.",
                            "Determine the utility of the description by confirming it provides enough detail to effectively help the user answer the provided monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/elasticache/*\\\", \\\"message\\\": \\\"MEMORY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in MEMORY* log messages indicating high memory pressure on the ElastiCache cluster?\", \"retrieval_context\": [\" - This pattern captures ElastiCache eviction logs. It identifies when keys are evicted from the cache due to memory pressure or expiration policies.\", \" - This pattern captures ElastiCache memory-related logs. It identifies memory usage, eviction events, and memory pressure situations in the cache cluster.\", \" - This pattern captures ElastiCache warning logs. It identifies performance warnings, memory pressure, and non-critical cache issues that start with 'WARN'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8,
                        "reason": "The description is relevant and directly addresses spikes in MEMORY* log messages related to memory pressure on ElastiCache, referencing memory usage, eviction events, and warnings. It uses clear language and covers what is measured (memory-related logs), the context (ElastiCache cluster), and implications (memory pressure and eviction). However, it could improve by explicitly mentioning units or frequency of spikes and providing more detail on how to interpret the logs to confidently answer the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Input) against the monitoring question (Retrieval Context) to determine if the description is relevant and directly addresses the user's query.\",\n    \"Evaluate clarity by checking if the description uses understandable language and avoids ambiguity, ensuring the user can easily interpret the metric in the context of the question.\",\n    \"Assess coverage by verifying if the description includes essential components\u2014what is measured, units, context, and implications\u2014especially in relation to the monitoring question to fully inform the user.\",\n    \"Determine utility by confirming whether the description enables the user to confidently answer the monitoring question based on the provided details and context.\"\n] \n \nRubric:\nNone \n \nScore: 0.8"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Input) against the monitoring question (Retrieval Context) to determine if the description is relevant and directly addresses the user's query.",
                            "Evaluate clarity by checking if the description uses understandable language and avoids ambiguity, ensuring the user can easily interpret the metric in the context of the question.",
                            "Assess coverage by verifying if the description includes essential components\u2014what is measured, units, context, and implications\u2014especially in relation to the monitoring question to fully inform the user.",
                            "Determine utility by confirming whether the description enables the user to confidently answer the monitoring question based on the provided details and context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/elasticache/*\\\", \\\"message\\\": \\\"WARN*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the WARN messages in ElastiCache logs indicating memory pressure or performance degradation?\", \"retrieval_context\": [\" - This pattern captures ElastiCache eviction logs. It identifies when keys are evicted from the cache due to memory pressure or expiration policies.\", \" - This pattern captures ElastiCache memory-related logs. It identifies memory usage, eviction events, and memory pressure situations in the cache cluster.\", \" - This pattern captures ElastiCache warning logs. It identifies performance warnings, memory pressure, and non-critical cache issues that start with 'WARN'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8304592703997324,
                        "reason": "The description directly addresses the monitoring question by explaining that the pattern captures ElastiCache warning logs related to performance warnings, memory pressure, and non-critical cache issues, which aligns well with the question about WARN messages indicating memory pressure or performance degradation. It is clear and concise, specifying the types of logs captured and their relevance. However, it lacks explicit mention of units or detailed implications, and could better explain how to interpret these warnings in the context of monitoring, slightly limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) against the Retrieval Context (monitoring question) to assess if the description provides relevant information directly answering the question.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise within the context of the question it should inform.\",\n    \"Assess Coverage by verifying the description explains what is measured, includes units, contextual factors, and implications, relating all to the monitoring question's focus.\",\n    \"Judge Utility by determining if the description enables a user to effectively use the metric to answer the monitoring question, ensuring alignment between description and contextual needs.\"\n] \n \nRubric:\nNone \n \nScore: 0.8304592703997324"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) against the Retrieval Context (monitoring question) to assess if the description provides relevant information directly answering the question.",
                            "Evaluate Clarity by checking if the description is understandable and concise within the context of the question it should inform.",
                            "Assess Coverage by verifying the description explains what is measured, includes units, contextual factors, and implications, relating all to the monitoring question's focus.",
                            "Judge Utility by determining if the description enables a user to effectively use the metric to answer the monitoring question, ensuring alignment between description and contextual needs."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudfront/*\\\", \\\"message\\\": \\\"ORIGIN*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ORIGIN_ERROR messages indicating a problem with the origin servers?\", \"retrieval_context\": [\" - CodeDeploy error logs. Deployment failures, rollback events, and application deployment issues. Release management monitoring and deployment troubleshooting.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures CloudFront error logs. It identifies distribution errors, origin failures, and edge location issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.313617782894929,
                        "reason": "The retrieval context mentions CloudFront error logs and origin failures, which is somewhat related to ORIGIN_ERROR messages, but it does not explicitly address spikes or provide details on measuring or interpreting these errors. The description lacks clarity on what is measured, units, or implications, limiting its utility in answering the question about spikes in ORIGIN_ERROR messages.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to determine if the description directly addresses the question asked.\",\n    \"Evaluate the Clarity of the Retrieval Context by checking if the description is understandable and concise in relation to the terminology used in the Input.\",\n    \"Assess Coverage by verifying if the Retrieval Context explains what is measured, includes units, context, and implications relevant to the Input question.\",\n    \"Determine Utility by judging whether the information in the Retrieval Context enables a user to effectively answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.313617782894929"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to determine if the description directly addresses the question asked.",
                            "Evaluate the Clarity of the Retrieval Context by checking if the description is understandable and concise in relation to the terminology used in the Input.",
                            "Assess Coverage by verifying if the Retrieval Context explains what is measured, includes units, context, and implications relevant to the Input question.",
                            "Determine Utility by judging whether the information in the Retrieval Context enables a user to effectively answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudfront/*\\\", \\\"message\\\": \\\"CACHE*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in CACHE_HIT_RATIO that could indicate a caching issue?\", \"retrieval_context\": [\" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\", \" - This pattern captures CloudFront cache-related logs. It identifies cache hits, cache misses, cache invalidation events, and cache performance metrics.\", \" - This pattern captures ElastiCache warning logs. It identifies performance warnings, memory pressure, and non-critical cache issues that start with 'WARN'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.18519527944504227,
                        "reason": "The retrieval context mentions cache-related logs and performance metrics but does not specifically describe CACHE_HIT_RATIO or how to identify spikes in it. The description is somewhat relevant but lacks clarity, detail on what CACHE_HIT_RATIO measures, units, or implications for caching issues, limiting its utility in confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description is concise, understandable, and has no ambiguous terminology relative to the question asked.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to answering the monitoring question.\",\n    \"Determine Utility by confirming the description enables a user to confidently and effectively answer the monitoring question using the provided information.\"\n] \n \nRubric:\nNone \n \nScore: 0.18519527944504227"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description is concise, understandable, and has no ambiguous terminology relative to the question asked.",
                            "Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to answering the monitoring question.",
                            "Determine Utility by confirming the description enables a user to confidently and effectively answer the monitoring question using the provided information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/elasticache/*\\\", \\\"message\\\": \\\"EVICTION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in EVICTION* logs indicating frequent cache evictions, potentially impacting performance?\", \"retrieval_context\": [\" - This pattern captures ElastiCache eviction logs. It identifies when keys are evicted from the cache due to memory pressure or expiration policies.\", \" - This pattern captures ElastiCache memory-related logs. It identifies memory usage, eviction events, and memory pressure situations in the cache cluster.\", \" - This pattern captures ElastiCache warning logs. It identifies performance warnings, memory pressure, and non-critical cache issues that start with 'WARN'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6817574482861011,
                        "reason": "The Retrieval Context provides relevant information about eviction logs, memory pressure, and warnings related to ElastiCache, which aligns with the Input question about eviction spikes and performance impact. However, it lacks explicit details on how to identify spikes, the units or frequency of evictions, and the direct implications on performance, limiting its utility for fully answering the question. Adding specifics on metrics, thresholds, or examples of spike detection would improve clarity and coverage.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input (monitoring question) and Retrieval Context (metric description) together: ensure the description is understandable and concise enough for the question's intent.\",\n    \"Check the coverage in the Retrieval Context to confirm it explains what is measured, units, relevant environment, and implications, verifying the description aligns with the scope of the Input question.\",\n    \"Assess the utility by determining if the Retrieval Context provides sufficient information for a user to answer the Input question effectively.\",\n    \"If any criteria score below 8, identify specific gaps where Input and Retrieval Context fail to align and suggest targeted improvements to the description for better clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.6817574482861011"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input (monitoring question) and Retrieval Context (metric description) together: ensure the description is understandable and concise enough for the question's intent.",
                            "Check the coverage in the Retrieval Context to confirm it explains what is measured, units, relevant environment, and implications, verifying the description aligns with the scope of the Input question.",
                            "Assess the utility by determining if the Retrieval Context provides sufficient information for a user to answer the Input question effectively.",
                            "If any criteria score below 8, identify specific gaps where Input and Retrieval Context fail to align and suggest targeted improvements to the description for better clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudfront/*\\\", \\\"message\\\": \\\"5XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in 5XX* error responses indicating a potential issue with CloudFront's internal infrastructure?\", \"retrieval_context\": [\" - This pattern captures CloudFront 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors from the CDN.\", \" - This pattern captures CloudFront 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues from the CDN.\", \" - This pattern captures CloudFront error logs. It identifies distribution errors, origin failures, and edge location issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6407150506689177,
                        "reason": "The Retrieval Context partially addresses the Input by describing the 5XX error pattern related to CloudFront internal server errors, which aligns with the question about spikes in 5XX errors. However, it lacks explicit mention of spikes or trends, units of measurement, or implications for monitoring, limiting clarity and utility for directly answering the question about potential issues indicated by error spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it is understandable and concise relative to the Input's terminology and intent.\",\n    \"Assess coverage by verifying that the Retrieval Context explains what is measured, units, context, and implications, specifically in relation to the Input's query.\",\n    \"Determine utility by confirming that the Retrieval Context provides sufficient information to directly help answer the Input\u2019s monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6407150506689177"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.",
                            "Evaluate the clarity of the Retrieval Context by checking if it is understandable and concise relative to the Input's terminology and intent.",
                            "Assess coverage by verifying that the Retrieval Context explains what is measured, units, context, and implications, specifically in relation to the Input's query.",
                            "Determine utility by confirming that the Retrieval Context provides sufficient information to directly help answer the Input\u2019s monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/applicationloadbalancer/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the ERROR messages in the Application Load Balancer logs exceed ten per minute?\", \"retrieval_context\": [\" - App Runner error logs. Containerized application errors, deployment failures, and service health issues. Container orchestration monitoring and application troubleshooting.\", \" - This pattern captures ALB 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues from the load balancer.\", \" - This pattern captures Application Load Balancer error logs. It identifies load balancer errors, target health check failures, and routing issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.659263739685912,
                        "reason": "The Input directly addresses the Retrieval Context by focusing on ERROR messages in Application Load Balancer logs, which aligns with the first pattern describing ALB error logs. The question is clear and uses consistent terminology such as 'ERROR messages' and 'Application Load Balancer logs.' However, the Input lacks explicit units or measurement details beyond 'per minute,' and it does not explain implications or provide context on why exceeding ten errors per minute matters. Including these details would improve coverage and utility for answering the monitoring question fully.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Input) against the monitoring question (Retrieval Context) to check if the description directly addresses the information needed.\",\n    \"Evaluate the clarity of the Input to ensure it is understandable without ambiguity, and verify if it uses terminology consistent with the question context.\",\n    \"Assess coverage by confirming the Input explains what is measured, includes units, provides relevant context, and highlights implications related to the Retrieval Context.\",\n    \"Determine utility by judging whether the Input enables a user to effectively answer the Retrieval Context question; if gaps exist, identify specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.659263739685912"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Input) against the monitoring question (Retrieval Context) to check if the description directly addresses the information needed.",
                            "Evaluate the clarity of the Input to ensure it is understandable without ambiguity, and verify if it uses terminology consistent with the question context.",
                            "Assess coverage by confirming the Input explains what is measured, includes units, provides relevant context, and highlights implications related to the Retrieval Context.",
                            "Determine utility by judging whether the Input enables a user to effectively answer the Retrieval Context question; if gaps exist, identify specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudfront/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the ERROR messages in CloudFront logs indicate a spike in failed requests?\", \"retrieval_context\": [\" - This pattern captures CloudFront 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors from the CDN.\", \" - This pattern captures CloudFront 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues from the CDN.\", \" - This pattern captures CloudFront error logs. It identifies distribution errors, origin failures, and edge location issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7129670376997946,
                        "reason": "The description addresses the question by explaining that the pattern captures CloudFront error logs, including distribution errors, origin failures, and edge location issues starting with 'ERROR', which relates to failed requests. It is fairly clear and concise, but it lacks explicit mention of how these errors indicate a spike or the units/timeframe for measuring the spike. The description covers the types of errors but does not fully explain the metric's implications or provide actionable guidance to determine if there is a spike in failed requests.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the question's focus.\",\n    \"Assess clarity by verifying that the description is concise, free of ambiguity, and easily understandable in relation to the question.\",\n    \"Evaluate coverage by checking if the description explains what the metric measures, including units, context, and any implications relevant to the question.\",\n    \"Determine utility by confirming that the description provides actionable information that helps answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.7129670376997946"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the question's focus.",
                            "Assess clarity by verifying that the description is concise, free of ambiguity, and easily understandable in relation to the question.",
                            "Evaluate coverage by checking if the description explains what the metric measures, including units, context, and any implications relevant to the question.",
                            "Determine utility by confirming that the description provides actionable information that helps answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/applicationloadbalancer/*\\\", \\\"message\\\": \\\"HEALTH_CHECK*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in HEALTH_CHECK_FAILED messages indicating a potential issue with the application load balancer?\", \"retrieval_context\": [\" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - This pattern captures ALB health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\", \" - This pattern captures Application Load Balancer error logs. It identifies load balancer errors, target health check failures, and routing issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6186927116916936,
                        "reason": "The retrieval context addresses health check failures related to the Application Load Balancer (ALB) and ECS, which aligns with the input question about spikes in HEALTH_CHECK_FAILED messages. However, the description lacks explicit mention of spikes or how to detect them, does not specify units or metrics, and provides limited operational context or implications. While it identifies relevant log types and failure categories, it is not sufficiently detailed or clear to fully enable a user to assess spikes or potential issues based solely on the provided information.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the specific question asked.\",\n    \"Evaluate the clarity of the description in the Retrieval Context to ensure it is concise and understandable relative to the terminology and scope implied by the Input.\",\n    \"Assess the coverage in the Retrieval Context by verifying if it explains what is measured, includes units, provides operational context, and possible implications as relevant to the Input question.\",\n    \"Determine utility by judging whether the description enables a user to effectively answer the Input question based on the information provided in the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6186927116916936"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the specific question asked.",
                            "Evaluate the clarity of the description in the Retrieval Context to ensure it is concise and understandable relative to the terminology and scope implied by the Input.",
                            "Assess the coverage in the Retrieval Context by verifying if it explains what is measured, includes units, provides operational context, and possible implications as relevant to the Input question.",
                            "Determine utility by judging whether the description enables a user to effectively answer the Input question based on the information provided in the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudfront/*\\\", \\\"message\\\": \\\"4XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are spikes in this metric indicative of a problem with client-side requests being rejected by our CDN?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - This pattern captures CloudFront 4XX client error responses. It identifies bad requests, unauthorized access, forbidden resources, and not found errors from the CDN.\", \" - This pattern captures VPC Flow Logs rejection events. It identifies network traffic that was rejected by security groups, NACLs, or other network policies.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7377540668798146,
                        "reason": "The retrieval context clearly addresses the question by describing patterns that capture client-side 4XX errors from CloudFront and API Gateway, which relate to client requests being rejected by the CDN. The description is understandable and concise, explaining the types of errors and their sources. However, it lacks explicit mention of the units or measurement context (e.g., counts or rates of errors) and does not directly state the implications of spikes in these metrics, limiting its utility for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.\",\n    \"Evaluate the description's clarity by determining if it is understandable and concise in relation to the technical terms or concepts implied by the question.\",\n    \"Assess coverage by verifying that the description explains what is measured, the units involved, the measurement context, and the implications needed to answer the question.\",\n    \"Judge utility by confirming that the description provides sufficient and relevant information that directly aids in answering the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7377540668798146"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.",
                            "Evaluate the description's clarity by determining if it is understandable and concise in relation to the technical terms or concepts implied by the question.",
                            "Assess coverage by verifying that the description explains what is measured, the units involved, the measurement context, and the implications needed to answer the question.",
                            "Judge utility by confirming that the description provides sufficient and relevant information that directly aids in answering the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/applicationloadbalancer/*\\\", \\\"message\\\": \\\"5XX*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in 5XX* server error responses indicating a potential issue with application availability?\", \"retrieval_context\": [\" - This pattern captures ALB 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues from the load balancer.\", \" - This pattern captures API Gateway 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues.\", \" - This pattern captures API Gateway 5XX server error responses. It identifies internal server errors, service unavailable, and gateway timeout issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4043740260322318,
                        "reason": "The description identifies the types of 5XX errors captured (internal server errors, service unavailable, gateway timeout) and the sources (ALB and API Gateway), which partially addresses the question about spikes in 5XX responses. However, it lacks clarity on what is specifically measured (e.g., error counts or rates), units, or how to detect spikes, limiting actionable insights. The explanation is somewhat repetitive and does not explicitly connect the errors to application availability issues, reducing coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question asked.\",\n    \"Evaluate the clarity of the description by checking if the terminology and explanations are easy to understand and concise relative to the input question.\",\n    \"Assess coverage by verifying if the description includes what is measured, units, relevant context, and the implications that inform the input question.\",\n    \"Judge utility by determining if the description provides actionable insights or information that enables the user to confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.4043740260322318"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question asked.",
                            "Evaluate the clarity of the description by checking if the terminology and explanations are easy to understand and concise relative to the input question.",
                            "Assess coverage by verifying if the description includes what is measured, units, relevant context, and the implications that inform the input question.",
                            "Judge utility by determining if the description provides actionable insights or information that enables the user to confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/s3/*\\\", \\\"message\\\": \\\"ACCESS_DENIED*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ACCESS_DENIED* logs indicating unauthorized access to our S3 buckets?\", \"retrieval_context\": [\" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - S3 bucket not found logs. Non-existent bucket access attempts, incorrectly named buckets, and configuration errors. Data access troubleshooting and configuration validation.\", \" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6970535196497865,
                        "reason": "The retrieval context addresses unauthorized access attempts and access denied errors related to S3 buckets, aligning well with the monitoring question about spikes in ACCESS_DENIED logs. It clearly mentions relevant issues like bucket policy violations and IAM permission problems, providing useful context for security monitoring. However, the description lacks explicit mention of how spikes or frequency changes are measured or represented, and does not specify units or detailed implications, limiting full confidence in answering the question about spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to verify that the description clearly addresses the question's focus.\",\n    \"Evaluate the clarity of the description to ensure it is easily understandable and concise when linked to the question at hand.\",\n    \"Assess the coverage within the Retrieval Context by checking if it explains what is measured, units, relevant context, and implications necessary to answer the Input.\",\n    \"Judge the utility of the description by determining if it provides sufficient information to confidently answer the monitoring question; if not, identify specific gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.6970535196497865"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to verify that the description clearly addresses the question's focus.",
                            "Evaluate the clarity of the description to ensure it is easily understandable and concise when linked to the question at hand.",
                            "Assess the coverage within the Retrieval Context by checking if it explains what is measured, units, relevant context, and implications necessary to answer the Input.",
                            "Judge the utility of the description by determining if it provides sufficient information to confidently answer the monitoring question; if not, identify specific gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/elasticache/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the ERROR messages in ElastiCache logs exceed ten, indicating a potential issue?\", \"retrieval_context\": [\" - EMR cluster error logs. Big data processing failures, Hadoop ecosystem errors, and cluster health issues. Data processing pipeline monitoring and troubleshooting.\", \" - This pattern captures ElastiCache error logs. It identifies Redis/Memcached errors, connection failures, memory issues, and cache engine problems that start with 'ERROR'.\", \" - This pattern captures ElastiCache warning logs. It identifies performance warnings, memory pressure, and non-critical cache issues that start with 'WARN'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31480471999111614,
                        "reason": "The description clarifies that the pattern captures ElastiCache error logs including various error types starting with 'ERROR', which aligns with the question about ERROR messages. However, it lacks explicit measurement details such as counting the number of ERROR messages or indicating units and thresholds (e.g., exceeding ten). The context does not provide sufficient detail to confidently determine if ERROR messages exceed ten, limiting utility and coverage relative to the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) for clarity: Ensure the description is easy to understand and directly addresses the question without ambiguity.\",\n    \"Evaluate coverage by checking if the description explicitly explains what is measured, including units, context, and potential implications, linking these to the question asked in the Input.\",\n    \"Assess utility by determining whether the description provides sufficient detail and relevance to enable a user to confidently answer the monitoring question.\",\n    \"Ensure that the description and the question are aligned, verifying that the description\u2019s scope matches the question\u2019s intent and that any gaps between the two are identified and noted.\"\n] \n \nRubric:\nNone \n \nScore: 0.31480471999111614"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) for clarity: Ensure the description is easy to understand and directly addresses the question without ambiguity.",
                            "Evaluate coverage by checking if the description explicitly explains what is measured, including units, context, and potential implications, linking these to the question asked in the Input.",
                            "Assess utility by determining whether the description provides sufficient detail and relevance to enable a user to confidently answer the monitoring question.",
                            "Ensure that the description and the question are aligned, verifying that the description\u2019s scope matches the question\u2019s intent and that any gaps between the two are identified and noted."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/s3/*\\\", \\\"message\\\": \\\"NO_SUCH_BUCKET*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the NO_SUCH_BUCKET logs indicating a spike in incorrect or non-existent bucket access attempts?\", \"retrieval_context\": [\" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - S3 bucket not found logs. Non-existent bucket access attempts, incorrectly named buckets, and configuration errors. Data access troubleshooting and configuration validation.\", \" - S3 object not found logs. Non-existent object access attempts, incorrectly named objects, and data retrieval errors. Data access troubleshooting and object lifecycle management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8101434198331148,
                        "reason": "The description clearly addresses the input question by explaining that NO_SUCH_BUCKET logs relate to non-existent bucket access attempts and incorrectly named buckets, which aligns well with the query about spikes in such attempts. It is concise and understandable without jargon. However, it lacks explicit mention of units or how to interpret spikes quantitatively, and could improve by detailing implications or how to use the logs for monitoring trends, which would enhance user confidence in answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (monitoring question) with the retrieval context (metric description) to ensure the description addresses the specific query being asked.\",\n    \"Evaluate clarity by checking if the description is concise and understandable, avoiding jargon or ambiguity that could confuse a user seeking to answer the question.\",\n    \"Assess coverage by verifying that the description includes what is measured, units, relevant context, and possible implications related to the monitoring question.\",\n    \"Determine utility by judging if the description enables a user to confidently answer the input question; if gaps are found, note specific areas where improvements can aid user comprehension and relevance.\"\n] \n \nRubric:\nNone \n \nScore: 0.8101434198331148"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (monitoring question) with the retrieval context (metric description) to ensure the description addresses the specific query being asked.",
                            "Evaluate clarity by checking if the description is concise and understandable, avoiding jargon or ambiguity that could confuse a user seeking to answer the question.",
                            "Assess coverage by verifying that the description includes what is measured, units, relevant context, and possible implications related to the monitoring question.",
                            "Determine utility by judging if the description enables a user to confidently answer the input question; if gaps are found, note specific areas where improvements can aid user comprehension and relevance."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/s3/*\\\", \\\"message\\\": \\\"NO_SUCH_KEY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are NO_SUCH_KEY errors causing a spike in S3 object access attempts?\", \"retrieval_context\": [\" - S3 bucket not found logs. Non-existent bucket access attempts, incorrectly named buckets, and configuration errors. Data access troubleshooting and configuration validation.\", \" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\", \" - S3 object not found logs. Non-existent object access attempts, incorrectly named objects, and data retrieval errors. Data access troubleshooting and object lifecycle management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3362012712752896,
                        "reason": "The retrieval context addresses S3 object not found logs and non-existent object access attempts, which are related to NO_SUCH_KEY errors, but it does not explicitly mention NO_SUCH_KEY or directly link these errors to spikes in access attempts. The description is somewhat clear but lacks specific measurement details, units, or implications needed to fully answer whether NO_SUCH_KEY errors cause spikes in access attempts, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate the Clarity of the description by checking if it is concise and understandable within the context of the input question.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to the input question.\",\n    \"Determine Utility by confirming the description provides sufficient information to answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.3362012712752896"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description directly addresses the question's focus.",
                            "Evaluate the Clarity of the description by checking if it is concise and understandable within the context of the input question.",
                            "Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to the input question.",
                            "Determine Utility by confirming the description provides sufficient information to answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/s3/*\\\", \\\"message\\\": \\\"GET*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in S3 GET operation logs indicating a potential data access bottleneck?\", \"retrieval_context\": [\" - S3 GET operation logs. Object downloads, object retrieval operations, and object access events. Data access monitoring, usage analytics, and performance tracking.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7218678837380095,
                        "reason": "The description directly addresses spikes in S3 GET operation logs, aligning well with the monitoring question about potential data access bottlenecks. It is clear and focused on the relevant metric. However, it lacks explicit mention of units, detailed implications, or how spikes are quantified, which limits full coverage and utility for comprehensive analysis.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) with the Retrieval Context (monitoring question) to assess if the description addresses the specific question directly and accurately.\",\n    \"Evaluate the clarity of the Input by checking if the description is easy to understand and avoids ambiguity, ensuring it complements the context provided by the question.\",\n    \"Assess the coverage of the Input in relation to the Retrieval Context by confirming that the description includes what is measured, units, relevant context, and implications needed to answer the question.\",\n    \"Determine the utility by verifying if the description enables a user to effectively answer the monitoring question, highlighting any gaps between the Input and the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.7218678837380095"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) with the Retrieval Context (monitoring question) to assess if the description addresses the specific question directly and accurately.",
                            "Evaluate the clarity of the Input by checking if the description is easy to understand and avoids ambiguity, ensuring it complements the context provided by the question.",
                            "Assess the coverage of the Input in relation to the Retrieval Context by confirming that the description includes what is measured, units, relevant context, and implications needed to answer the question.",
                            "Determine the utility by verifying if the description enables a user to effectively answer the monitoring question, highlighting any gaps between the Input and the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sqs/*\\\", \\\"message\\\": \\\"VISIBILITY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in VISIBILITY* logs indicating a potential issue with message visibility timeouts?\", \"retrieval_context\": [\" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\", \" - This pattern captures SQS visibility timeout logs. It identifies when messages become visible again after processing timeouts.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5575761459770591,
                        "reason": "The Input is clear and concise, directly asking about spikes in VISIBILITY* logs related to message visibility timeouts, which aligns with the Retrieval Context's focus on SQS visibility timeout logs. However, the Input lacks detail on what constitutes a spike, the units or metrics measured, and the implications of such spikes, limiting its coverage and utility for fully addressing the question posed in the Retrieval Context.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Assess the clarity of the Input description ensuring it is understandable and concise relative to the Retrieval Context.\",\n    \"Evaluate the coverage in the Input by verifying it explains what is measured, including units, context, and implications, as informed by the Retrieval Context.\",\n    \"Determine the utility by checking if the Input description adequately supports answering the question posed in the Retrieval Context.\",\n    \"Compare Input and Retrieval Context to ensure alignment and identify any gaps where the Input lacks information necessary for addressing the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.5575761459770591"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Assess the clarity of the Input description ensuring it is understandable and concise relative to the Retrieval Context.",
                            "Evaluate the coverage in the Input by verifying it explains what is measured, including units, context, and implications, as informed by the Retrieval Context.",
                            "Determine the utility by checking if the Input description adequately supports answering the question posed in the Retrieval Context.",
                            "Compare Input and Retrieval Context to ensure alignment and identify any gaps where the Input lacks information necessary for addressing the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/dynamodb/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a potential issue with DynamoDB operations?\", \"retrieval_context\": [\" - RDS database error logs. Connection failures, query errors, authentication issues, and database engine errors. Critical for database health monitoring and performance troubleshooting.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures DynamoDB error logs. It identifies throttling errors, capacity issues, and database operation failures that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6935074646186247,
                        "reason": "The retrieval context directly addresses the input question by describing patterns capturing DynamoDB error logs, including throttling errors and capacity issues starting with 'ERROR', which aligns with detecting spikes in error messages. The description is clear and concise, using relevant terminology. However, it lacks explicit mention of how spikes are measured or quantified, units of measurement, and implications of such spikes, limiting full coverage and utility for answering the question comprehensively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate the Clarity of the metric description: check if it is understandable and concise in relation to the terminology and intent expressed in the Input.\",\n    \"Assess the Coverage in the metric description by verifying if it explains what is measured, the units, any contextual background, and implications that relate to the Input question.\",\n    \"Determine the Utility by judging if the metric description provides sufficient information to enable answering the Input question effectively and accurately.\"\n] \n \nRubric:\nNone \n \nScore: 0.6935074646186247"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate the Clarity of the metric description: check if it is understandable and concise in relation to the terminology and intent expressed in the Input.",
                            "Assess the Coverage in the metric description by verifying if it explains what is measured, the units, any contextual background, and implications that relate to the Input question.",
                            "Determine the Utility by judging if the metric description provides sufficient information to enable answering the Input question effectively and accurately."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sns/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there ERROR messages in the SNS logs indicating message delivery failures or service issues?\", \"retrieval_context\": [\" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\", \" - This pattern captures SNS delivery logs. It identifies successful message deliveries, delivery failures, and retry attempts.\", \" - This pattern captures SNS error logs. It identifies message delivery failures, subscription errors, and service issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8075858186811317,
                        "reason": "The retrieval context directly addresses the input question by describing a pattern that captures SNS error logs related to message delivery failures and service issues starting with 'ERROR'. It is clear and concise, using terminology consistent with the input. However, it lacks explicit mention of units or detailed implications of these errors, which slightly limits coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question posed.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable and concise relative to the terminology and concepts introduced in the Input.\",\n    \"Assess Coverage by verifying that the Retrieval Context explains the metric meaning, units, context, and implications specifically relevant to the Input question.\",\n    \"Determine Utility by confirming that the Retrieval Context provides sufficient information to effectively answer the Input question; if not, identify missing elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.8075858186811317"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question posed.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable and concise relative to the terminology and concepts introduced in the Input.",
                            "Assess Coverage by verifying that the Retrieval Context explains the metric meaning, units, context, and implications specifically relevant to the Input question.",
                            "Determine Utility by confirming that the Retrieval Context provides sufficient information to effectively answer the Input question; if not, identify missing elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/dynamodb/*\\\", \\\"message\\\": \\\"THROTTLING*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in THROTTLING messages indicating that DynamoDB is being throttled due to exceeded capacity?\", \"retrieval_context\": [\" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\", \" - This pattern captures DynamoDB error logs. It identifies throttling errors, capacity issues, and database operation failures that start with 'ERROR'.\", \" - This pattern captures DynamoDB throttling logs. It identifies when requests are throttled due to exceeded provisioned capacity or on-demand limits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7362012711837791,
                        "reason": "The Input is clear and directly related to the Retrieval Context by focusing on spikes in THROTTLING messages indicating DynamoDB throttling due to exceeded capacity. However, it lacks explicit mention of units or detailed measurement context, and does not address other related error types or Lambda memory issues mentioned in the Retrieval Context, limiting full coverage and utility for comprehensive monitoring.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate Clarity by checking if the Input (metric description) is concise and easily understandable, and if it aligns clearly with the Retrieval Context (monitoring question).\",\n    \"Assess Coverage by verifying whether the Input sufficiently explains what is measured, including units and context relevant to the Retrieval Context.\",\n    \"Determine Utility by confirming if the Input provides enough information for a user to answer the Retrieval Context question effectively.\",\n    \"Compare Input and Retrieval Context to ensure the description directly addresses the monitoring question without ambiguity or missing information.\"\n] \n \nRubric:\nNone \n \nScore: 0.7362012711837791"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate Clarity by checking if the Input (metric description) is concise and easily understandable, and if it aligns clearly with the Retrieval Context (monitoring question).",
                            "Assess Coverage by verifying whether the Input sufficiently explains what is measured, including units and context relevant to the Retrieval Context.",
                            "Determine Utility by confirming if the Input provides enough information for a user to answer the Retrieval Context question effectively.",
                            "Compare Input and Retrieval Context to ensure the description directly addresses the monitoring question without ambiguity or missing information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sqs/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there ERROR messages in the SQS logs indicating message processing failures or queue access errors?\", \"retrieval_context\": [\" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures SQS dead letter queue logs. It identifies when messages are moved to dead letter queues after failed processing attempts.\", \" - This pattern captures SQS error logs. It identifies message processing failures, queue access errors, and service issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8033384515734966,
                        "reason": "The retrieval context directly addresses the input question by describing patterns that capture SQS error logs related to message processing failures and queue access errors, matching the question's intent. The language is clear and concise, specifying the types of errors identified. However, it lacks explicit measurement units or detailed implications of these errors, which slightly limits coverage and utility for fully confident monitoring.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent and scope.\",\n    \"Evaluate Clarity by assessing if the description's language is simple, concise, and free of ambiguity relative to the question posed.\",\n    \"Assess Coverage by verifying that the description includes what is measured, measurement units, relevant context, and implications in alignment with the question's focus.\",\n    \"Determine Utility by judging whether the description provides sufficient information to confidently answer the monitoring question; if gaps exist, suggest improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.8033384515734966"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent and scope.",
                            "Evaluate Clarity by assessing if the description's language is simple, concise, and free of ambiguity relative to the question posed.",
                            "Assess Coverage by verifying that the description includes what is measured, measurement units, relevant context, and implications in alignment with the question's focus.",
                            "Determine Utility by judging whether the description provides sufficient information to confidently answer the monitoring question; if gaps exist, suggest improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sqs/*\\\", \\\"message\\\": \\\"DEAD_LETTER*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in DEAD_LETTER* logs indicating a high rate of message failures?\", \"retrieval_context\": [\" - Lambda function error logs. Runtime exceptions, execution failures, cold start issues, and application errors. Critical for application health monitoring, debugging, and performance optimization.\", \" - RDS deadlock detection logs. Database deadlocks, transaction failures, and performance issues. Critical for database stability and transaction management.\", \" - This pattern captures SQS dead letter queue logs. It identifies when messages are moved to dead letter queues after failed processing attempts.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6562176500885799,
                        "reason": "The description clearly identifies that the pattern captures SQS dead letter queue logs and explains that these logs indicate messages moved to dead letter queues after failed processing attempts, which relates directly to spikes in DEAD_LETTER logs and message failures. However, it lacks specific details on units, frequency, or how to detect spikes, limiting clarity and utility for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question.\",\n    \"Evaluate the clarity of the description by determining if it is understandable and concise relative to the question posed.\",\n    \"Assess the coverage of the description by verifying if it explains what is measured, including units, context, and implications relevant to the question.\",\n    \"Determine the utility by judging if the description provides enough information to allow a user to answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.6562176500885799"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question.",
                            "Evaluate the clarity of the description by determining if it is understandable and concise relative to the question posed.",
                            "Assess the coverage of the description by verifying if it explains what is measured, including units, context, and implications relevant to the question.",
                            "Determine the utility by judging if the description provides enough information to allow a user to answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sqs/*\\\", \\\"message\\\": \\\"MESSAGE*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in MESSAGE* log events indicating a potential issue with SQS message processing?\", \"retrieval_context\": [\" - This pattern captures SQS dead letter queue logs. It identifies when messages are moved to dead letter queues after failed processing attempts.\", \" - This pattern captures SQS error logs. It identifies message processing failures, queue access errors, and service issues that start with 'ERROR'.\", \" - This pattern captures SQS message-related logs. It identifies message sends, receives, deletions, and processing events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5761588294104214,
                        "reason": "The retrieval context partially addresses the input by describing patterns that capture SQS error logs, message-related logs, and dead letter queue logs, which relate to message processing issues. However, it lacks explicit mention of detecting spikes or the metric aspect of monitoring event frequency. The description is clear and uses relevant terminology but does not fully explain what is measured, units, or how spikes are identified, limiting its utility for directly answering the monitoring question about spikes in MESSAGE* log events.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.\",\n    \"Evaluate Clarity: Determine if the description is written in an understandable and concise manner that aligns with the question's terminology and scope.\",\n    \"Evaluate Coverage: Check whether the description explains what is measured, including units, operational context, and potential implications relevant to the question.\",\n    \"Evaluate Utility: Assess if the description provides sufficient and relevant information to directly help answer the monitoring question, ensuring the link between the input and retrieval context is meaningful.\"\n] \n \nRubric:\nNone \n \nScore: 0.5761588294104214"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.",
                            "Evaluate Clarity: Determine if the description is written in an understandable and concise manner that aligns with the question's terminology and scope.",
                            "Evaluate Coverage: Check whether the description explains what is measured, including units, operational context, and potential implications relevant to the question.",
                            "Evaluate Utility: Assess if the description provides sufficient and relevant information to directly help answer the monitoring question, ensuring the link between the input and retrieval context is meaningful."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/s3/*\\\", \\\"message\\\": \\\"PUT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of S3 PUT operation logs indicating a potential issue with data ingestion or storage?\", \"retrieval_context\": [\" - S3 PUT operation logs. Object uploads, multipart upload events, object creation operations, and data ingestion. Data pipeline monitoring and storage operations tracking.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6108062771886773,
                        "reason": "The retrieval context partially addresses the question by mentioning S3 PUT operation logs and their relation to data ingestion and storage operations, which aligns with the focus on spikes in PUT operation logs. However, the description lacks clarity and detail about what constitutes a spike, the units of measurement, and how to interpret changes in log counts. It also mixes other log types (error and access denied logs) that are less relevant to the specific question, reducing clarity and focus. Overall, the context provides some useful information but does not fully enable confident assessment of spikes or their implications.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description explicitly addresses the question's focus.\",\n    \"Evaluate the Clarity of the description by checking if the language used is simple, concise, and free of ambiguity relative to the question.\",\n    \"Assess Coverage by verifying that the description includes what is measured, the units, relevant context, and the meaning behind metric values in relation to the question.\",\n    \"Determine Utility by judging if the description provides sufficient information to confidently answer the monitoring question; if not, identify specific gaps to improve.\"\n] \n \nRubric:\nNone \n \nScore: 0.6108062771886773"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description explicitly addresses the question's focus.",
                            "Evaluate the Clarity of the description by checking if the language used is simple, concise, and free of ambiguity relative to the question.",
                            "Assess Coverage by verifying that the description includes what is measured, the units, relevant context, and the meaning behind metric values in relation to the question.",
                            "Determine Utility by judging if the description provides sufficient information to confidently answer the monitoring question; if not, identify specific gaps to improve."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/dynamodb/*\\\", \\\"message\\\": \\\"CONDITIONAL_CHECK*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in CONDITIONAL_CHECK errors indicating a potential consistency issue with DynamoDB writes?\", \"retrieval_context\": [\" - RDS deadlock detection logs. Database deadlocks, transaction failures, and performance issues. Critical for database stability and transaction management.\", \" - This pattern captures DynamoDB conditional check logs. It identifies conditional write failures, optimistic locking conflicts, and conditional expression evaluation.\", \" - This pattern captures DynamoDB error logs. It identifies throttling errors, capacity issues, and database operation failures that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7640572476383388,
                        "reason": "The input description accurately reflects the relevant metric from the retrieval context by focusing on conditional check errors related to DynamoDB writes, which aligns with the first context entry about conditional write failures and optimistic locking conflicts. The terminology is clear and concise, making the monitoring question understandable. However, the description could improve coverage by explicitly mentioning the units or frequency of errors and the implications of spikes beyond potential consistency issues. Despite this, the description sufficiently supports a user in identifying and investigating conditional check error spikes in DynamoDB.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input metric description to the Retrieval Context to ensure the description accurately reflects and covers the relevant metric in relation to the monitoring question.\",\n    \"Evaluate the clarity of the Input description by checking if the terminology and explanations are understandable and concise within the scope given by the Retrieval Context.\",\n    \"Assess coverage by verifying that the description includes all necessary details (what is measured, units, context, implications) relevant to the Retrieval Context and the monitoring question.\",\n    \"Determine utility by analyzing whether the Input description, in conjunction with the Retrieval Context, sufficiently supports a user in answering the provided monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7640572476383388"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input metric description to the Retrieval Context to ensure the description accurately reflects and covers the relevant metric in relation to the monitoring question.",
                            "Evaluate the clarity of the Input description by checking if the terminology and explanations are understandable and concise within the scope given by the Retrieval Context.",
                            "Assess coverage by verifying that the description includes all necessary details (what is measured, units, context, implications) relevant to the Retrieval Context and the monitoring question.",
                            "Determine utility by analyzing whether the Input description, in conjunction with the Retrieval Context, sufficiently supports a user in answering the provided monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/dynamodb/*\\\", \\\"message\\\": \\\"PROVISIONED*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in PROVISIONED messages indicating a potential capacity issue?\", \"retrieval_context\": [\" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - This pattern captures DynamoDB provisioned capacity logs. It identifies provisioned throughput changes, capacity planning, and billing events.\", \" - This pattern captures DynamoDB throttling logs. It identifies when requests are throttled due to exceeded provisioned capacity or on-demand limits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6679178692681618,
                        "reason": "The description is somewhat clear and relates to monitoring provisioned messages for capacity issues, aligning with the question about spikes in PROVISIONED messages. However, it lacks explicit details on what exactly is measured (e.g., message counts or rates), units, and the direct implications of spikes. While it references relevant log patterns, it does not fully explain how these logs indicate spikes or capacity problems, limiting coverage and utility for answering the monitoring question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) with the Retrieval Context (monitoring question) to assess Clarity; ensure the description is understandable and concise in relation to the question asked.\",\n    \"Evaluate Coverage by verifying if the description fully explains what is measured, units, context, and implications, referencing the monitoring question to check relevance and completeness.\",\n    \"Assess Utility by determining if the description provides sufficient information to answer the monitoring question effectively, ensuring practical applicability.\",\n    \"If any aspect scores below 8, identify specific gaps between the description and the monitoring question and suggest improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.6679178692681618"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) with the Retrieval Context (monitoring question) to assess Clarity; ensure the description is understandable and concise in relation to the question asked.",
                            "Evaluate Coverage by verifying if the description fully explains what is measured, units, context, and implications, referencing the monitoring question to check relevance and completeness.",
                            "Assess Utility by determining if the description provides sufficient information to answer the monitoring question effectively, ensuring practical applicability.",
                            "If any aspect scores below 8, identify specific gaps between the description and the monitoring question and suggest improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sns/*\\\", \\\"message\\\": \\\"PUBLISH*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any SNS publish operation logs indicating failed delivery attempts?\", \"retrieval_context\": [\" - This pattern captures SNS delivery logs. It identifies successful message deliveries, delivery failures, and retry attempts.\", \" - This pattern captures SNS error logs. It identifies message delivery failures, subscription errors, and service issues that start with 'ERROR'.\", \" - This pattern captures SNS publish operation logs. It identifies message publishing events, topic notifications, and delivery attempts.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6991976564635248,
                        "reason": "The retrieval context partially addresses the question by describing patterns that capture SNS publish operation logs and delivery failures, including error logs indicating delivery failures. However, it does not explicitly confirm whether failed delivery attempts are logged within SNS publish operation logs specifically, nor does it clarify the units or implications of these logs. The description provides useful information but lacks explicit clarity and direct linkage to the presence of failed delivery attempts in publish operation logs, limiting full confidence in answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely addresses the question.\",\n    \"Evaluate the Clarity and Coverage of the metric description to determine if it explains what is measured, units, context, and implications relevant to the question.\",\n    \"Assess the Utility by verifying whether the description provides sufficient information to help a user confidently answer the monitoring question.\",\n    \"If the description lacks clarity, coverage, or utility, identify specific gaps between the Input and Retrieval Context to suggest targeted improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6991976564635248"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely addresses the question.",
                            "Evaluate the Clarity and Coverage of the metric description to determine if it explains what is measured, units, context, and implications relevant to the question.",
                            "Assess the Utility by verifying whether the description provides sufficient information to help a user confidently answer the monitoring question.",
                            "If the description lacks clarity, coverage, or utility, identify specific gaps between the Input and Retrieval Context to suggest targeted improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/s3/*\\\", \\\"message\\\": \\\"DELETE*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of S3 DELETE operation logs indicating potential issues with data integrity or availability?\", \"retrieval_context\": [\" - S3 DELETE operation logs. Object deletions, bucket deletions, cleanup operations, and data lifecycle management. Data governance, compliance monitoring, and storage optimization.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4758466747025336,
                        "reason": "The Retrieval Context mentions S3 DELETE operation logs and related activities but lacks clarity on whether it tracks spikes or trends in these logs, which is central to the Input question. It provides some coverage on what is logged and the context (data governance, compliance), but does not specify units, measurement methods, or implications related to spikes or potential data integrity issues. Consequently, the utility is limited as it does not enable a user to effectively identify or interpret spikes in DELETE operations or their impact on data integrity or availability. To improve, the context should explicitly describe how spikes are detected, what metrics or units are used, and how these relate to potential issues flagged by the logs.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely addresses the question (Clarity).\",\n    \"Evaluate whether the Retrieval Context sufficiently explains what is measured, the units involved, relevant context, and potential implications to ensure comprehensive Coverage relative to the Input question.\",\n    \"Determine if the Retrieval Context provides actionable information that enables a user to effectively answer the Input question, assessing Utility in relation to the question asked.\",\n    \"If any of the above evaluations score below 8, identify specific gaps between the Input and Retrieval Context and suggest targeted improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.4758466747025336"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely addresses the question (Clarity).",
                            "Evaluate whether the Retrieval Context sufficiently explains what is measured, the units involved, relevant context, and potential implications to ensure comprehensive Coverage relative to the Input question.",
                            "Determine if the Retrieval Context provides actionable information that enables a user to effectively answer the Input question, assessing Utility in relation to the question asked.",
                            "If any of the above evaluations score below 8, identify specific gaps between the Input and Retrieval Context and suggest targeted improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sqs/*\\\", \\\"message\\\": \\\"QUEUE*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any QUEUE* log messages indicating a queue has been deleted or is experiencing health issues?\", \"retrieval_context\": [\" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - EKS pod lifecycle logs. Pod creation, deletion, scheduling, and health check events. Container orchestration and application deployment monitoring.\", \" - This pattern captures SQS queue-related logs. It identifies queue creation, deletion, configuration changes, and queue health events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7119202917328539,
                        "reason": "The retrieval context clearly identifies that the QUEUE* pattern captures logs related to queue deletion and health events, directly addressing the input's focus on queue deletion and health issues. It explains what is measured (queue creation, deletion, configuration changes, and health events) and provides relevant context about SQS queues. However, it lacks specific details on the units or formats of the log messages and does not explicitly mention how to identify problematic health issues, which limits full utility for answering the input question comprehensively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Assess the Input (monitoring question) to determine its specificity and what information it seeks.\",\n    \"Evaluate the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise relative to the Input.\",\n    \"Check Coverage by verifying if the metric description explains what is measured, the units, relevant context, and potential implications, and if these details align with the information requested by the Input.\",\n    \"Judge Utility by determining whether the description sufficiently equips a user to answer the Input's question, highlighting gaps between the description and the question's needs.\"\n] \n \nRubric:\nNone \n \nScore: 0.7119202917328539"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Assess the Input (monitoring question) to determine its specificity and what information it seeks.",
                            "Evaluate the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise relative to the Input.",
                            "Check Coverage by verifying if the metric description explains what is measured, the units, relevant context, and potential implications, and if these details align with the information requested by the Input.",
                            "Judge Utility by determining whether the description sufficiently equips a user to answer the Input's question, highlighting gaps between the description and the question's needs."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/dynamodb/*\\\", \\\"message\\\": \\\"CAPACITY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CAPACITY* log messages indicating throttling or errors due to insufficient capacity?\", \"retrieval_context\": [\" - ECS container warning logs. Non-critical issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - This pattern captures DynamoDB throttling logs. It identifies when requests are throttled due to exceeded provisioned capacity or on-demand limits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7060086655223377,
                        "reason": "The description directly addresses the question by explaining that the pattern captures DynamoDB throttling logs related to capacity limits, which aligns with the monitoring question about CAPACITY* log messages indicating throttling or errors. The language is mostly clear and concise, though some terms like 'provisioned capacity' and 'on-demand limits' may require prior knowledge. The description covers what is measured (throttling due to capacity limits) and provides relevant context about resource constraints and performance indicators. However, it lacks explicit mention of units or detailed implications for interpretation, and the inclusion of EKS and ECS logs adds some ambiguity since the question focuses on CAPACITY* logs, presumably DynamoDB-related. Overall, it provides useful information but could be improved by clarifying scope and implications for interpretation.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate the clarity of the description by checking if it uses simple, unambiguous language and is concise without losing meaning.\",\n    \"Assess coverage by verifying that the description explains what is measured, units used, relevant context, and potential implications for interpretation.\",\n    \"Determine utility by confirming if the description provides enough information for a user to confidently answer the monitoring question; if not, identify gaps for improvement.\"\n] \n \nRubric:\nNone \n \nScore: 0.7060086655223377"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate the clarity of the description by checking if it uses simple, unambiguous language and is concise without losing meaning.",
                            "Assess coverage by verifying that the description explains what is measured, units used, relevant context, and potential implications for interpretation.",
                            "Determine utility by confirming if the description provides enough information for a user to confidently answer the monitoring question; if not, identify gaps for improvement."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sns/*\\\", \\\"message\\\": \\\"DELIVERY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any failed SNS message deliveries that exceed ten retries?\", \"retrieval_context\": [\" - This pattern captures SNS delivery logs. It identifies successful message deliveries, delivery failures, and retry attempts.\", \" - This pattern captures SNS error logs. It identifies message delivery failures, subscription errors, and service issues that start with 'ERROR'.\", \" - This pattern captures SNS publish operation logs. It identifies message publishing events, topic notifications, and delivery attempts.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.36791786861878395,
                        "reason": "The Retrieval Context uses clear language to describe SNS logs related to deliveries, failures, and retries, addressing the Input's focus on failed SNS message deliveries and retries. However, it lacks specific information about the metric of 'exceeding ten retries,' such as how retries are counted or flagged, units, or thresholds. This limits its utility in fully answering whether any failed deliveries exceed ten retries. Adding explicit details on retry counts and how to identify failures beyond ten retries would improve alignment.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate Clarity by checking if the Input (monitoring question) and Retrieval Context (metric description) use clear, concise language understandable to the target audience.\",\n    \"Assess Coverage by verifying that the Retrieval Context adequately explains what metric is measured, its units, relevant context, and implications in relation to the Input question.\",\n    \"Determine Utility by confirming that the Retrieval Context provides sufficient information to effectively answer the Input question, ensuring relevance and completeness.\",\n    \"Identify gaps or ambiguities between the Input and Retrieval Context that hinder understanding or usefulness, and suggest precise improvements to align the description better with the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.36791786861878395"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate Clarity by checking if the Input (monitoring question) and Retrieval Context (metric description) use clear, concise language understandable to the target audience.",
                            "Assess Coverage by verifying that the Retrieval Context adequately explains what metric is measured, its units, relevant context, and implications in relation to the Input question.",
                            "Determine Utility by confirming that the Retrieval Context provides sufficient information to effectively answer the Input question, ensuring relevance and completeness.",
                            "Identify gaps or ambiguities between the Input and Retrieval Context that hinder understanding or usefulness, and suggest precise improvements to align the description better with the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sns/*\\\", \\\"message\\\": \\\"TOPIC*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any SNS topic-related log messages indicating a configuration change or health event?\", \"retrieval_context\": [\" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - This pattern captures SNS subscription-related logs. It identifies subscription confirmations, unsubscriptions, and subscription health events.\", \" - This pattern captures SNS topic-related logs. It identifies topic creation, deletion, configuration changes, and topic health events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7981668808679474,
                        "reason": "The description clearly addresses the question by focusing on SNS topic-related logs that capture configuration changes and health events, matching the monitoring question's focus. It is concise and understandable, specifying what is measured (topic creation, deletion, configuration changes, health events). However, it lacks explicit mention of units or detailed implications, slightly limiting full coverage and utility for comprehensive analysis.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.\",\n    \"Evaluate the description's clarity by checking if it is understandable and concise when linked to the question asked.\",\n    \"Assess coverage by verifying if the description includes what is measured, units, context, and implications relevant to the question.\",\n    \"Determine utility by confirming whether the description provides sufficient information to enable a user to answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.7981668808679474"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.",
                            "Evaluate the description's clarity by checking if it is understandable and concise when linked to the question asked.",
                            "Assess coverage by verifying if the description includes what is measured, units, context, and implications relevant to the question.",
                            "Determine utility by confirming whether the description provides sufficient information to enable a user to answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/sns/*\\\", \\\"message\\\": \\\"SUBSCRIPTION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any SNS subscription confirmation or health events in the logs indicating a potential issue?\", \"retrieval_context\": [\" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - This pattern captures SNS subscription-related logs. It identifies subscription confirmations, unsubscriptions, and subscription health events.\", \" - This pattern captures SNS topic-related logs. It identifies topic creation, deletion, configuration changes, and topic health events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6980366705063812,
                        "reason": "The retrieval context directly addresses the question by identifying SNS subscription confirmations and health events, which aligns well with the monitoring question. The description is fairly clear and concise, mentioning specific log types such as subscription confirmations, unsubscriptions, and health events. However, it lacks detailed explanation of what metrics or units are measured, the implications of these events, and how to interpret them for potential issues, limiting its coverage and utility for actionable insights.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to ensure the description directly addresses the question's focus and intent.\",\n    \"Evaluate the Clarity of the description relative to the question by checking if the description uses clear, concise language that is easy to comprehend in the context of the question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, the units used, relevant context, and the implications that would help answer the question.\",\n    \"Judge Utility by determining if the description provides actionable insights or sufficient information that enables the user to effectively respond to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6980366705063812"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to ensure the description directly addresses the question's focus and intent.",
                            "Evaluate the Clarity of the description relative to the question by checking if the description uses clear, concise language that is easy to comprehend in the context of the question.",
                            "Assess Coverage by verifying that the description explains what is measured, the units used, relevant context, and the implications that would help answer the question.",
                            "Judge Utility by determining if the description provides actionable insights or sufficient information that enables the user to effectively respond to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudtrail/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages related to API call failures?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - API Gateway error logs. Gateway errors, integration failures, client request issues, and service connectivity problems. Critical for API health monitoring and troubleshooting.\", \" - Lambda function error logs. Runtime exceptions, execution failures, cold start issues, and application errors. Critical for application health monitoring, debugging, and performance optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.677054911688881,
                        "reason": "The retrieval context addresses error logs related to API Gateway and Lambda functions, which are relevant to monitoring ERROR messages for API call failures. It provides contextual background on types of errors and their importance for health monitoring and troubleshooting, aiding utility. However, it lacks explicit mention of spikes or how to detect them, and does not specify units or metrics for measuring error frequency, limiting clarity and coverage in directly answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the monitoring question.\",\n    \"Evaluate Clarity by checking if the description is concise and easy to understand in the context of the input question.\",\n    \"Assess Coverage by verifying the description explains what is measured, including units, contextual background, and implications relevant to the input question.\",\n    \"Judge Utility by determining whether the description provides sufficient information to effectively answer or guide the answer to the input monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.677054911688881"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the monitoring question.",
                            "Evaluate Clarity by checking if the description is concise and easy to understand in the context of the input question.",
                            "Assess Coverage by verifying the description explains what is measured, including units, contextual background, and implications relevant to the input question.",
                            "Judge Utility by determining whether the description provides sufficient information to effectively answer or guide the answer to the input monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/vpcflowlogs/*\\\", \\\"message\\\": \\\"ACCEPT*\\\"}\", \"hyperparameters\": null, \"input\": \"Does the number of ACCEPT messages in VPC Flow Logs indicate a potential security group or NACL issue?\", \"retrieval_context\": [\" - This pattern captures VPC Flow Logs acceptance events. It identifies network traffic that was allowed through security groups, NACLs, or other network policies.\", \" - This pattern captures VPC Flow Logs rejection events. It identifies network traffic that was rejected by security groups, NACLs, or other network policies.\", \" - This pattern captures VPC Flow Logs reset events. It identifies TCP connection resets, which can indicate network issues, service restarts, or connection problems.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6717870576116306,
                        "reason": "The description directly addresses the monitoring question by explaining that ACCEPT messages in VPC Flow Logs represent allowed network traffic through security groups and NACLs, which is relevant to identifying potential issues. The language is clear and straightforward, avoiding ambiguity. However, the description lacks explicit mention of measurement units, detailed context on how the number of ACCEPT messages might indicate a problem, and implications for security monitoring, limiting its utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input and Retrieval Context to ensure the metric description directly addresses the monitoring question.\",\n    \"Evaluate the clarity of the description by checking if the language is straightforward and free of ambiguity in relation to the question.\",\n    \"Assess coverage by verifying if the description includes what is measured, units, context, and implications relevant to the input question.\",\n    \"Determine utility by judging whether the description provides sufficient information for the user to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6717870576116306"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input and Retrieval Context to ensure the metric description directly addresses the monitoring question.",
                            "Evaluate the clarity of the description by checking if the language is straightforward and free of ambiguity in relation to the question.",
                            "Assess coverage by verifying if the description includes what is measured, units, context, and implications relevant to the input question.",
                            "Determine utility by judging whether the description provides sufficient information for the user to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudtrail/*\\\", \\\"message\\\": \\\"AUTHENTICATION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in AUTHENTICATION* log messages indicating a surge in login attempts or authentication failures?\", \"retrieval_context\": [\" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - RDS database error logs. Connection failures, query errors, authentication issues, and database engine errors. Critical for database health monitoring and performance troubleshooting.\", \" - This pattern captures CloudTrail authentication logs. It identifies login attempts, authentication failures, and credential usage events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7017986212528241,
                        "reason": "The Retrieval Context provides relevant information about authentication logs, including login attempts and authentication failures, which aligns well with the Input question about spikes in AUTHENTICATION* log messages. However, it lacks explicit mention of how spikes or surges are detected or measured, and does not specify units or thresholds, limiting its completeness. The context covers what is measured and the general context but could be clearer and more detailed on implications and detection methods to fully support answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the clarity of the Input (monitoring question) with the detail provided in the Retrieval Context (metric description), ensuring the description is understandable and concise relative to the question.\",\n    \"Assess whether the Retrieval Context covers all necessary aspects (what is measured, units, context, and implications) to fully address the Input question.\",\n    \"Evaluate how effectively the Retrieval Context supports answering the Input question, focusing on the relevance and applicability of the description to solving the query.\",\n    \"If gaps in coverage or clarity reduce utility, identify specific areas in the Retrieval Context that need elaboration or simplification to better align with the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7017986212528241"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the clarity of the Input (monitoring question) with the detail provided in the Retrieval Context (metric description), ensuring the description is understandable and concise relative to the question.",
                            "Assess whether the Retrieval Context covers all necessary aspects (what is measured, units, context, and implications) to fully address the Input question.",
                            "Evaluate how effectively the Retrieval Context supports answering the Input question, focusing on the relevance and applicability of the description to solving the query.",
                            "If gaps in coverage or clarity reduce utility, identify specific areas in the Retrieval Context that need elaboration or simplification to better align with the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudtrail/*\\\", \\\"message\\\": \\\"API_CALL*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the number of API_CALL errors in CloudTrail logs exceed ten, indicating a potential system failure?\", \"retrieval_context\": [\" - API Gateway error logs. Gateway errors, integration failures, client request issues, and service connectivity problems. Critical for API health monitoring and troubleshooting.\", \" - This pattern captures CloudTrail API call logs. It identifies AWS API calls, their parameters, responses, and execution results.\", \" - This pattern captures CloudTrail error logs. It identifies API call failures, authentication errors, and service access issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6562176500885798,
                        "reason": "The retrieval context addresses the monitoring inquiry by describing patterns that capture CloudTrail error logs and API call failures, which aligns with the question about API_CALL errors. The description is generally clear and concise, mentioning error types and log sources. However, it lacks specific details on the number of errors, units, or thresholds (e.g., exceeding ten errors) and does not explicitly connect the implications to potential system failure, limiting full coverage and utility for directly answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input question with the Retrieval Context description to assess if the description clearly addresses the specific monitoring inquiry.\",\n    \"Evaluate Clarity by checking if the description is understandable, concise, and free of ambiguity in relation to the question posed.\",\n    \"Assess Coverage by confirming that the description details what is measured, including relevant units, context, and implications connected to the input question.\",\n    \"Determine Utility by verifying if the description provides sufficient information to directly answer or guide the resolution of the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6562176500885798"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input question with the Retrieval Context description to assess if the description clearly addresses the specific monitoring inquiry.",
                            "Evaluate Clarity by checking if the description is understandable, concise, and free of ambiguity in relation to the question posed.",
                            "Assess Coverage by confirming that the description details what is measured, including relevant units, context, and implications connected to the input question.",
                            "Determine Utility by verifying if the description provides sufficient information to directly answer or guide the resolution of the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudtrail/*\\\", \\\"message\\\": \\\"AUTHORIZATION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in AUTHORIZATION messages indicating potential security issues or performance bottlenecks?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - Authorization logs across all services. Permission checks, access control decisions, and policy evaluations. Security monitoring and compliance auditing.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6731058578630005,
                        "reason": "The input question is clear and specific, asking about spikes in AUTHORIZATION messages related to security or performance issues. The retrieval context provides relevant descriptions of authorization logs, including permission checks and security monitoring, which partially address the question. However, it lacks explicit mention of detecting spikes or performance bottlenecks, limiting its utility for directly answering the question. Including details on how spikes or anomalies in authorization messages are identified would improve clarity and coverage.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity, ensuring it clearly defines what information the user seeks.\",\n    \"Assess the Retrieval Context (metric description) for clarity and completeness, verifying it explains what is measured, including units, scope, and any relevant implications.\",\n    \"Compare the Input and Retrieval Context to determine if the description adequately addresses the question, supporting users in finding precise answers.\",\n    \"Score the description based on Clarity, Coverage, and Utility in relation to the question; if below 8, suggest targeted improvements to enhance alignment or detail.\"\n] \n \nRubric:\nNone \n \nScore: 0.6731058578630005"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity, ensuring it clearly defines what information the user seeks.",
                            "Assess the Retrieval Context (metric description) for clarity and completeness, verifying it explains what is measured, including units, scope, and any relevant implications.",
                            "Compare the Input and Retrieval Context to determine if the description adequately addresses the question, supporting users in finding precise answers.",
                            "Score the description based on Clarity, Coverage, and Utility in relation to the question; if below 8, suggest targeted improvements to enhance alignment or detail."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/s3/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the ERROR messages in S3 error logs related to access denied or bucket policy violations?\", \"retrieval_context\": [\" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - S3 bucket not found logs. Non-existent bucket access attempts, incorrectly named buckets, and configuration errors. Data access troubleshooting and configuration validation.\", \" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6622459331201855,
                        "reason": "The Input clearly identifies the focus on ERROR messages in S3 error logs related to access denied or bucket policy violations, aligning well with the Retrieval Context's emphasis on access denied errors and bucket policy violations. However, the Input does not explicitly mention other related issues like storage operation failures or service connectivity issues present in the Retrieval Context, limiting completeness. While the Input is useful for addressing part of the Retrieval Context, expanding it to cover additional error types would improve its utility and actionability.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate if the Input (metric description) clearly communicates what is being measured, including relevant units and context.\",\n    \"Assess the Retrieval Context (monitoring question) to determine if it aligns with or is addressed by the Input description.\",\n    \"Judge the completeness of the Input by verifying it provides sufficient coverage to understand implications and actionability related to the Retrieval Context.\",\n    \"Determine the overall utility by seeing if the Input can effectively help answer the Retrieval Context's question, and suggest improvements if the score is below 8.\"\n] \n \nRubric:\nNone \n \nScore: 0.6622459331201855"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate if the Input (metric description) clearly communicates what is being measured, including relevant units and context.",
                            "Assess the Retrieval Context (monitoring question) to determine if it aligns with or is addressed by the Input description.",
                            "Judge the completeness of the Input by verifying it provides sufficient coverage to understand implications and actionability related to the Retrieval Context.",
                            "Determine the overall utility by seeing if the Input can effectively help answer the Retrieval Context's question, and suggest improvements if the score is below 8."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudtrail/*\\\", \\\"message\\\": \\\"SECURITY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any security-related log messages indicating potential unauthorized access or policy breaches?\", \"retrieval_context\": [\" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - Authorization logs across all services. Permission checks, access control decisions, and policy evaluations. Security monitoring and compliance auditing.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8622459338205513,
                        "reason": "The retrieval context clearly and concisely addresses the input question by detailing relevant security-related log messages, including authorization logs, authentication logs, and S3 access denied logs. It covers what is measured (permission checks, login attempts, unauthorized access), the context (across all services, bucket policies, IAM permissions), and implications (security monitoring, compliance auditing, incident detection). This provides actionable information to identify potential unauthorized access or policy breaches. A minor improvement could be explicitly linking these logs to specific indicators of policy breaches or unauthorized access for even clearer utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) against the Retrieval Context (metric description) to ensure the description is clearly understandable and concise, directly addressing the question's intent.\",\n    \"Evaluate whether the Retrieval Context sufficiently covers all key aspects: what is measured, measurement units, relevant context, and potential implications, as required to answer the Input.\",\n    \"Assess the utility of the Retrieval Context in enabling a user to answer the provided Input question; determine if the description provides actionable and relevant information.\",\n    \"If any of clarity, coverage, or utility is lacking in relation to the Input question, identify and specify concrete improvements to enhance alignment and completeness.\"\n] \n \nRubric:\nNone \n \nScore: 0.8622459338205513"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) against the Retrieval Context (metric description) to ensure the description is clearly understandable and concise, directly addressing the question's intent.",
                            "Evaluate whether the Retrieval Context sufficiently covers all key aspects: what is measured, measurement units, relevant context, and potential implications, as required to answer the Input.",
                            "Assess the utility of the Retrieval Context in enabling a user to answer the provided Input question; determine if the description provides actionable and relevant information.",
                            "If any of clarity, coverage, or utility is lacking in relation to the Input question, identify and specify concrete improvements to enhance alignment and completeness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/route53/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a significant increase in DNS resolution or health check failures?\", \"retrieval_context\": [\" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - This pattern captures Route 53 error logs. It identifies DNS resolution failures, health check errors, and routing issues that start with 'ERROR'.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5814174467280909,
                        "reason": "The retrieval context addresses the question by identifying error logs related to DNS resolution failures and health check errors, which aligns with the monitoring question about spikes in ERROR messages. However, it lacks explicit mention of measuring spikes or significant increases, units, or how to interpret the data over time. The language is generally clear but does not fully explain the implications or provide guidance on detecting spikes, limiting its utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description addresses the question directly and completely.\",\n    \"Evaluate Clarity by checking if the metric description uses clear, concise language that is easy to understand in relation to the question provided.\",\n    \"Assess Coverage by verifying that the metric description explains what is being measured, includes units, context, and implications necessary to answer the question fully.\",\n    \"Determine Utility by confirming that the information in the metric description can effectively help a user respond to the monitoring question without ambiguity or missing details.\"\n] \n \nRubric:\nNone \n \nScore: 0.5814174467280909"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description addresses the question directly and completely.",
                            "Evaluate Clarity by checking if the metric description uses clear, concise language that is easy to understand in relation to the question provided.",
                            "Assess Coverage by verifying that the metric description explains what is being measured, includes units, context, and implications necessary to answer the question fully.",
                            "Determine Utility by confirming that the information in the metric description can effectively help a user respond to the monitoring question without ambiguity or missing details."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/vpcflowlogs/*\\\", \\\"message\\\": \\\"REJECT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in REJECT* messages indicating a potential security group or NACL issue?\", \"retrieval_context\": [\" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - This pattern captures VPC Flow Logs acceptance events. It identifies network traffic that was allowed through security groups, NACLs, or other network policies.\", \" - This pattern captures VPC Flow Logs rejection events. It identifies network traffic that was rejected by security groups, NACLs, or other network policies.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6562176500885799,
                        "reason": "The Retrieval Context directly addresses the Input by explaining that the pattern captures VPC Flow Logs rejection events related to security groups and NACLs, which is relevant to detecting spikes in REJECT* messages. However, the description lacks details on measurement units, how spikes are identified, or operational implications, limiting clarity and completeness. While it is mostly clear and relevant, the context could better support confident and precise answers by including more specifics on detection and interpretation of spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and relevantly.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if the metric description is concise, easily understandable, and free of ambiguity in relation to the Input.\",\n    \"Assess the coverage within the Retrieval Context by verifying it explains what is measured, units, operational context, and possible implications clearly enough to support answering the Input question.\",\n    \"Determine the utility by analyzing if the provided Retrieval Context sufficiently equips a user to answer the Input question accurately and confidently; if gaps exist, note them for improvement.\"\n] \n \nRubric:\nNone \n \nScore: 0.6562176500885799"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and relevantly.",
                            "Evaluate the clarity of the Retrieval Context by checking if the metric description is concise, easily understandable, and free of ambiguity in relation to the Input.",
                            "Assess the coverage within the Retrieval Context by verifying it explains what is measured, units, operational context, and possible implications clearly enough to support answering the Input question.",
                            "Determine the utility by analyzing if the provided Retrieval Context sufficiently equips a user to answer the Input question accurately and confidently; if gaps exist, note them for improvement."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/route53/*\\\", \\\"message\\\": \\\"DNS*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in DNS query failures captured by this log pattern?\", \"retrieval_context\": [\" - This pattern captures Route 53 DNS-related logs. It identifies DNS queries, responses, resolution events, and DNS configuration changes.\", \" - This pattern captures Route 53 error logs. It identifies DNS resolution failures, health check errors, and routing issues that start with 'ERROR'.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7562176500885799,
                        "reason": "The retrieval context clearly relates to the input question by describing patterns that capture DNS query failures and errors, which directly address the question about spikes in DNS query failures. The context is concise and well-structured, listing different log types relevant to DNS failures. However, it lacks explicit mention of how spikes or trends are identified or measured, and does not specify units or time frames, which limits full coverage and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to verify that the description clearly relates to and can address the monitoring question.\",\n    \"Evaluate the Clarity of the Retrieval Context: check if the description is concise, well-structured, and easy to understand in relation to the Input.\",\n    \"Assess the Coverage by verifying if the description includes what is measured, units, context, and implications that directly support answering the Input question.\",\n    \"Determine the Utility by judging whether the description provides sufficient and relevant information to enable a user to confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7562176500885799"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to verify that the description clearly relates to and can address the monitoring question.",
                            "Evaluate the Clarity of the Retrieval Context: check if the description is concise, well-structured, and easy to understand in relation to the Input.",
                            "Assess the Coverage by verifying if the description includes what is measured, units, context, and implications that directly support answering the Input question.",
                            "Determine the Utility by judging whether the description provides sufficient and relevant information to enable a user to confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudwatch/*\\\", \\\"message\\\": \\\"ALARM*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any ALARM* messages indicating a threshold breach for the service?\", \"retrieval_context\": [\" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - Service Discovery logs. Service registration, health checks, and service mesh operations. Microservices architecture monitoring and service connectivity.\", \" - This pattern captures CloudWatch alarm logs. It identifies alarm state changes, threshold breaches, and alarm evaluation events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8018331190271244,
                        "reason": "The response directly addresses the monitoring question by focusing on CloudWatch alarm logs that capture alarm state changes and threshold breaches, aligning well with the input query about ALARM* messages. The description is clear and concise, explaining what is measured (alarm state changes and threshold breaches) and providing relevant context (CloudWatch alarms). However, it could improve coverage by explicitly mentioning units or specific implications of threshold breaches, and utility could be enhanced by detailing how to identify or interpret these messages to confirm threshold breaches.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input and Retrieval Context to ensure the description directly addresses the specific monitoring question provided.\",\n    \"Evaluate Clarity by assessing if the description is concise and easily understandable within the context of the question.\",\n    \"Check Coverage by verifying the description explains what is measured, including units, context, and implications relevant to the question.\",\n    \"Assess Utility by determining whether the description effectively enables the user to answer the monitoring question based on the information given.\"\n] \n \nRubric:\nNone \n \nScore: 0.8018331190271244"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input and Retrieval Context to ensure the description directly addresses the specific monitoring question provided.",
                            "Evaluate Clarity by assessing if the description is concise and easily understandable within the context of the question.",
                            "Check Coverage by verifying the description explains what is measured, including units, context, and implications relevant to the question.",
                            "Assess Utility by determining whether the description effectively enables the user to answer the monitoring question based on the information given."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/route53/*\\\", \\\"message\\\": \\\"FAILOVER*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there multiple consecutive failover events in the Route 53 logs, indicating a potential issue with endpoint availability?\", \"retrieval_context\": [\" - This pattern captures Route 53 error logs. It identifies DNS resolution failures, health check errors, and routing issues that start with 'ERROR'.\", \" - This pattern captures Route 53 failover logs. It identifies failover events, primary/secondary endpoint switches, and disaster recovery activations.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5614727203335145,
                        "reason": "The description addresses the presence of failover events and endpoint switches relevant to the question about consecutive failover events, but it lacks explicit mention of multiple consecutive events or how to identify them. The clarity is good, with concise explanations of the log patterns, but it does not specify metrics, units, or implications for endpoint availability issues. Therefore, while somewhat useful, it does not fully enable confident determination of multiple consecutive failovers.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the description (retrieval context) directly against the monitoring question (input) to ensure the description addresses the specific question being asked.\",\n    \"Evaluate the clarity of the description to confirm it is concise and easily understandable without requiring additional interpretation.\",\n    \"Check the coverage within the description to verify it explains what metric is measured, the units used, the relevant context, and the implications related to the question.\",\n    \"Assess the utility by determining if the description enables the user to confidently answer the monitoring question based on the provided information.\"\n] \n \nRubric:\nNone \n \nScore: 0.5614727203335145"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the description (retrieval context) directly against the monitoring question (input) to ensure the description addresses the specific question being asked.",
                            "Evaluate the clarity of the description to confirm it is concise and easily understandable without requiring additional interpretation.",
                            "Check the coverage within the description to verify it explains what metric is measured, the units used, the relevant context, and the implications related to the question.",
                            "Assess the utility by determining if the description enables the user to confidently answer the monitoring question based on the provided information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/vpcflowlogs/*\\\", \\\"message\\\": \\\"TIMEOUT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the VPC Flow Log timeouts indicative of a network congestion issue affecting user requests?\", \"retrieval_context\": [\" - This pattern captures VPC Flow Logs error events. It identifies network errors, connection failures, and routing issues that start with 'ERROR'.\", \" - This pattern captures VPC Flow Logs reset events. It identifies TCP connection resets, which can indicate network issues, service restarts, or connection problems.\", \" - This pattern captures VPC Flow Logs timeout events. It identifies network connections that timed out, indicating potential network issues or service unavailability.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.49580198948657567,
                        "reason": "The Retrieval Context clearly describes VPC Flow Log timeout events and their indication of potential network issues, aligning with the Input's question about timeouts and network congestion. However, it lacks specific details on whether these timeouts directly indicate network congestion affecting user requests, missing units, metrics, or implications needed for a definitive answer. The context also includes information on reset and error events, which are somewhat extraneous to the timeout-focused question, reducing specificity and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate Clarity by determining if the Input (monitoring question) and Retrieval Context (metric description) are both clearly and concisely presented, ensuring the description is understandable in reference to the question.\",\n    \"Assess Coverage by verifying if the Retrieval Context adequately explains what is measured, including units, relevant context, and implications, in a way that aligns with the Input's question needs.\",\n    \"Judge Utility by checking whether the Retrieval Context provides sufficient and relevant information to directly help a user answer the Input's question.\",\n    \"Compare Input and Retrieval Context to ensure the description is specifically tailored to the question, with no missing or extraneous information that would hinder effective understanding or use.\"\n] \n \nRubric:\nNone \n \nScore: 0.49580198948657567"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate Clarity by determining if the Input (monitoring question) and Retrieval Context (metric description) are both clearly and concisely presented, ensuring the description is understandable in reference to the question.",
                            "Assess Coverage by verifying if the Retrieval Context adequately explains what is measured, including units, relevant context, and implications, in a way that aligns with the Input's question needs.",
                            "Judge Utility by checking whether the Retrieval Context provides sufficient and relevant information to directly help a user answer the Input's question.",
                            "Compare Input and Retrieval Context to ensure the description is specifically tailored to the question, with no missing or extraneous information that would hinder effective understanding or use."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/route53/*\\\", \\\"message\\\": \\\"ROUTING*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ROUTING* log messages indicating a surge in routing policy evaluations?\", \"retrieval_context\": [\" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\", \" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\", \" - This pattern captures Route 53 routing logs. It identifies routing policy evaluations, traffic routing decisions, and routing configuration changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5485175581913143,
                        "reason": "The description partially addresses the question by identifying that the pattern captures Route 53 routing logs and routing policy evaluations, which relates to spikes in routing messages. However, it lacks clarity and detail on how spikes are detected, the units or metrics used, and the implications of such spikes, limiting its utility for effectively answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the question's focus.\",\n    \"Evaluate clarity by assessing if the metric description is easy to understand and succinct while maintaining relevance to the input question.\",\n    \"Assess coverage by verifying the description includes what is measured, units, context, and implications that relate directly to the input question.\",\n    \"Determine utility by checking whether the description provides sufficient detail to enable the user to answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.5485175581913143"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the question's focus.",
                            "Evaluate clarity by assessing if the metric description is easy to understand and succinct while maintaining relevance to the input question.",
                            "Assess coverage by verifying the description includes what is measured, units, context, and implications that relate directly to the input question.",
                            "Determine utility by checking whether the description provides sufficient detail to enable the user to answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/vpcflowlogs/*\\\", \\\"message\\\": \\\"RESET*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in RESET* events indicating potential network connectivity issues?\", \"retrieval_context\": [\" - API Gateway error logs. Gateway errors, integration failures, client request issues, and service connectivity problems. Critical for API health monitoring and troubleshooting.\", \" - This pattern captures VPC Flow Logs reset events. It identifies TCP connection resets, which can indicate network issues, service restarts, or connection problems.\", \" - This pattern captures VPC Flow Logs timeout events. It identifies network connections that timed out, indicating potential network issues or service unavailability.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6962673110112472,
                        "reason": "The input question is clear and specific about spikes in RESET* events indicating network connectivity issues. The retrieval context explains that the pattern captures VPC Flow Logs reset events and identifies TCP connection resets, which can indicate network issues, aligning well with the input. However, the context lacks details on units, how spikes are detected or measured, and does not explicitly link the implications to network connectivity issues, limiting full interpretability and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) to ensure it is clearly stated and specific enough to guide the relevance of the metric description.\",\n    \"Assess the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise relative to the Input question.\",\n    \"Check Coverage by verifying that the Retrieval Context explains what is measured, including units, relevant context, and implications that relate directly to the Input question.\",\n    \"Determine Utility by evaluating how effectively the Retrieval Context helps answer the Input question, considering if any missing details hinder the user's ability to interpret the metric for the given query.\"\n] \n \nRubric:\nNone \n \nScore: 0.6962673110112472"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) to ensure it is clearly stated and specific enough to guide the relevance of the metric description.",
                            "Assess the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise relative to the Input question.",
                            "Check Coverage by verifying that the Retrieval Context explains what is measured, including units, relevant context, and implications that relate directly to the Input question.",
                            "Determine Utility by evaluating how effectively the Retrieval Context helps answer the Input question, considering if any missing details hinder the user's ability to interpret the metric for the given query."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/route53/*\\\", \\\"message\\\": \\\"HEALTH_CHECK*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in health check failures that correlate with drops in HealthyHostCount?\", \"retrieval_context\": [\" - ECS health check logs. Health check failures, successful checks, configuration issues, and service availability monitoring. Container health and service reliability tracking.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.306186054499353,
                        "reason": "The retrieval context mentions health check failures and related logs but lacks a clear, concise explanation of what the metric measures, its units, or how to interpret spikes in failures relative to HealthyHostCount drops. It does not explicitly connect health check failures to HealthyHostCount or provide guidance on correlating these metrics, limiting its utility for answering the monitoring question. To improve, the description should explicitly define the metrics, their relationship, and how to detect and interpret correlations between failures and host count changes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) against the Retrieval Context (metric description) to assess if the description clearly and concisely addresses the question.\",\n    \"Evaluate the description for coverage by checking if it explains what the metric measures, its units, relevant context, and the implications that help interpret the data in relation to the question.\",\n    \"Assess the utility by determining whether the description enables a user to answer the monitoring question effectively based on the information provided.\",\n    \"If any deficiencies in clarity, coverage, or utility are found when relating the Input to the Retrieval Context, provide a specific and actionable suggestion for improving the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.306186054499353"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) against the Retrieval Context (metric description) to assess if the description clearly and concisely addresses the question.",
                            "Evaluate the description for coverage by checking if it explains what the metric measures, its units, relevant context, and the implications that help interpret the data in relation to the question.",
                            "Assess the utility by determining whether the description enables a user to answer the monitoring question effectively based on the information provided.",
                            "If any deficiencies in clarity, coverage, or utility are found when relating the Input to the Retrieval Context, provide a specific and actionable suggestion for improving the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudwatch/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a potential issue with metric collection or alarm evaluation?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - Step Functions error logs. Workflow execution failures, state machine errors, and serverless workflow issues. Business process automation monitoring and troubleshooting.\", \" - This pattern captures CloudWatch error logs. It identifies metric collection failures, alarm evaluation errors, and monitoring service issues that start with 'ERROR'.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6971465035174808,
                        "reason": "The retrieval context clearly identifies the first pattern as capturing CloudWatch error logs related to metric collection failures and alarm evaluation errors, directly addressing the user's question about spikes in ERROR messages. It explains what is measured (error logs starting with 'ERROR') and provides relevant context about the types of errors captured. However, it lacks explicit mention of units, how spikes are detected, or implications of these spikes, which limits its utility for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) to determine the specific information or insight the user is seeking.\",\n    \"Assess the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise relative to the question.\",\n    \"Judge the coverage in the metric description by confirming it explains what is measured, the units, relevant context, and potential implications related to the Input.\",\n    \"Analyze the utility by verifying if the metric description enables the user to effectively answer the monitoring question based on the provided context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6971465035174808"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) to determine the specific information or insight the user is seeking.",
                            "Assess the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise relative to the question.",
                            "Judge the coverage in the metric description by confirming it explains what is measured, the units, relevant context, and potential implications related to the Input.",
                            "Analyze the utility by verifying if the metric description enables the user to effectively answer the monitoring question based on the provided context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudwatch/*\\\", \\\"message\\\": \\\"METRIC*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any errors or warnings in the metric collection process that could be causing inconsistent data?\", \"retrieval_context\": [\" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - Kinesis streaming error logs. Stream processing failures, data ingestion errors, and service connectivity issues. Real-time data processing monitoring and troubleshooting.\", \" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7817574473971188,
                        "reason": "The description addresses errors and warnings in the metric collection process by detailing warning logs and error logs across EKS, Kinesis, and RDS, covering non-critical issues, resource constraints, and data ingestion errors. The language is clear and aligns well with the input question's focus on errors and warnings causing inconsistent data. However, it lacks explicit mention of how these errors directly impact data consistency or specific implications for troubleshooting inconsistent data, which slightly limits full coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the specific information requested in the question.\",\n    \"Evaluate clarity by assessing if the description uses concise and understandable language that aligns with the terminology and intent of the input question.\",\n    \"Assess coverage by verifying that the description includes what the metric measures, units, relevant context, and implications directly related to the input question.\",\n    \"Determine utility by confirming that the description provides enough information to enable a user to effectively answer or analyze the monitoring question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.7817574473971188"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the specific information requested in the question.",
                            "Evaluate clarity by assessing if the description uses concise and understandable language that aligns with the terminology and intent of the input question.",
                            "Assess coverage by verifying that the description includes what the metric measures, units, relevant context, and implications directly related to the input question.",
                            "Determine utility by confirming that the description provides enough information to enable a user to effectively answer or analyze the monitoring question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/vpcflowlogs/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the ERROR messages in VPC Flow Logs indicate a significant spike in network errors?\", \"retrieval_context\": [\" - This pattern captures VPC Flow Logs error events. It identifies network errors, connection failures, and routing issues that start with 'ERROR'.\", \" - This pattern captures VPC Flow Logs reset events. It identifies TCP connection resets, which can indicate network issues, service restarts, or connection problems.\", \" - This pattern captures VPC Flow Logs timeout events. It identifies network connections that timed out, indicating potential network issues or service unavailability.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5324822670967791,
                        "reason": "The retrieval context explains that the ERROR pattern captures network errors, connection failures, and routing issues starting with 'ERROR', which aligns with identifying error messages in VPC Flow Logs. However, it lacks details on measuring a 'significant spike' such as units, time frames, or thresholds, limiting its utility to fully answer the question about a spike in network errors.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains what is being measured relevant to the question.\",\n    \"Evaluate whether the description provides comprehensive coverage including units, measurement context, and implications that align with the Input question's intent.\",\n    \"Determine the utility of the description by judging if it directly supports or enables answering the Input question effectively.\",\n    \"If any gap in clarity, coverage, or utility exists between the Input and Retrieval Context, identify specific improvements to bridge the discrepancy.\"\n] \n \nRubric:\nNone \n \nScore: 0.5324822670967791"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains what is being measured relevant to the question.",
                            "Evaluate whether the description provides comprehensive coverage including units, measurement context, and implications that align with the Input question's intent.",
                            "Determine the utility of the description by judging if it directly supports or enables answering the Input question effectively.",
                            "If any gap in clarity, coverage, or utility exists between the Input and Retrieval Context, identify specific improvements to bridge the discrepancy."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudwatch/*\\\", \\\"message\\\": \\\"DASHBOARD*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any dashboard update or creation events that could be causing the dashboard to become unresponsive?\", \"retrieval_context\": [\" - RDS database warning logs. Performance warnings, configuration issues, and non-critical database events. Proactive monitoring and capacity planning.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures CloudWatch dashboard logs. It identifies dashboard creation, updates, and visualization events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4237008017963433,
                        "reason": "The retrieval context partially addresses the input by mentioning dashboard creation and update events in CloudWatch logs, which relates to the question about dashboard update or creation events causing unresponsiveness. However, the description lacks clarity and detail, as it does not explain what specific metrics are measured, the units, or how these events might impact dashboard responsiveness. The inclusion of unrelated RDS and Redshift logs adds noise without relevance, reducing overall clarity and utility. Consequently, the description provides limited coverage and insufficient utility for effectively answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question.\",\n    \"Evaluate Clarity by checking if the description is concise and understandable, avoiding jargon unless clearly defined.\",\n    \"Assess Coverage by verifying that the description explains what is measured, includes measurement units, provides relevant context, and outlines implications.\",\n    \"Determine Utility by confirming the description enables the user to effectively answer the monitoring question, bridging any gaps between metric details and user needs.\"\n] \n \nRubric:\nNone \n \nScore: 0.4237008017963433"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question.",
                            "Evaluate Clarity by checking if the description is concise and understandable, avoiding jargon unless clearly defined.",
                            "Assess Coverage by verifying that the description explains what is measured, includes measurement units, provides relevant context, and outlines implications.",
                            "Determine Utility by confirming the description enables the user to effectively answer the monitoring question, bridging any gaps between metric details and user needs."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/cloudwatch/*\\\", \\\"message\\\": \\\"LOG*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in LogIngestionLatency that correlate with the number of LOG* messages?\", \"retrieval_context\": [\" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - Latency monitoring logs across all services. Response time tracking, performance degradation detection, and user experience monitoring. Application performance optimization and SLA compliance.\", \" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.1989013055841292,
                        "reason": "The Retrieval Context mentions latency monitoring logs and authentication logs but does not specifically address LogIngestionLatency or its correlation with LOG* message counts. The descriptions are general and lack details on what is measured, units, or how spikes are detected, limiting clarity and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.\",\n    \"Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the terminology and scope indicated by the Input.\",\n    \"Assess the coverage in the Retrieval Context to verify if it explains what is measured, includes units, context, and potential implications relevant to the Input query.\",\n    \"Determine the utility by judging whether the Retrieval Context provides sufficient information for a user to confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.1989013055841292"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.",
                            "Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the terminology and scope indicated by the Input.",
                            "Assess the coverage in the Retrieval Context to verify if it explains what is measured, includes units, context, and potential implications relevant to the Input query.",
                            "Determine the utility by judging whether the Retrieval Context provides sufficient information for a user to confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/guardduty/*\\\", \\\"message\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the GuardDuty threat detection logs indicate a spike in unusual activity or security incidents?\", \"retrieval_context\": [\" - Detective security investigation logs. Threat analysis, security incident investigation, and behavioral analytics. Security forensics and threat intelligence correlation.\", \" - GuardDuty threat detection logs. Security alerts, threat intelligence, compromised credentials, unusual activity, and security incident response. Critical for security monitoring and threat hunting.\", \" - This pattern captures CloudTrail security-related logs. It identifies security events, compliance violations, and security policy changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.549084100233302,
                        "reason": "The description references GuardDuty threat detection logs and their relevance to security alerts and threat intelligence, aligning with the monitoring question about spikes in unusual activity or incidents. However, it lacks explicit details on what specific metrics are measured, the units or thresholds indicating a spike, and how to interpret the logs in context. This limits clarity and utility for directly answering the question. To improve, the description should explicitly define the metric parameters, explain how spikes are detected or quantified, and clarify implications for security monitoring.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly and concisely explains the metric relevant to the question.\",\n    \"Evaluate if the description covers all essential elements: what is measured, units, context, and implications, in relation to the monitoring question.\",\n    \"Assess whether the description's clarity and coverage enable the user to utilize it effectively to answer the monitoring question.\",\n    \"If the description scores below 8, identify specific gaps in clarity, coverage, or utility with respect to the question and suggest concrete ways to improve it.\"\n] \n \nRubric:\nNone \n \nScore: 0.549084100233302"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly and concisely explains the metric relevant to the question.",
                            "Evaluate if the description covers all essential elements: what is measured, units, context, and implications, in relation to the monitoring question.",
                            "Assess whether the description's clarity and coverage enable the user to utilize it effectively to answer the monitoring question.",
                            "If the description scores below 8, identify specific gaps in clarity, coverage, or utility with respect to the question and suggest concrete ways to improve it."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/securityhub/*\\\", \\\"message\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the Security Hub findings indicate any high-severity vulnerabilities or compliance issues?\", \"retrieval_context\": [\" - Detective security investigation logs. Threat analysis, security incident investigation, and behavioral analytics. Security forensics and threat intelligence correlation.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\", \" - Security Hub findings and compliance logs. Security posture assessment, compliance monitoring, security control evaluation, and risk management. Centralized security governance and audit reporting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.332082129433083,
                        "reason": "The retrieval context references Security Hub findings and compliance logs related to security posture and compliance monitoring but lacks specific mention of high-severity vulnerabilities or explicit severity levels. The description is somewhat clear but does not directly define or quantify what constitutes high-severity issues, limiting its ability to fully answer the question. More detailed information on severity classification and explicit vulnerability or compliance issue indicators would improve utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly defines the metric in terms relevant to the question.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise when read alongside the question, ensuring no ambiguity relative to what the question asks.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, contextual details, and implications that directly support answering the input question.\",\n    \"Determine Utility by judging if the combined information from the description enables a user to confidently and effectively answer the monitoring question; if not, suggest improvements focused on missing alignment.\"\n] \n \nRubric:\nNone \n \nScore: 0.332082129433083"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly defines the metric in terms relevant to the question.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise when read alongside the question, ensuring no ambiguity relative to what the question asks.",
                            "Assess Coverage by verifying the description includes what is measured, units, contextual details, and implications that directly support answering the input question.",
                            "Determine Utility by judging if the combined information from the description enables a user to confidently and effectively answer the monitoring question; if not, suggest improvements focused on missing alignment."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/glue/*\\\", \\\"message\\\": \\\"JOB*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any errors or warnings in the Glue job execution logs that could indicate a problem with the ETL process?\", \"retrieval_context\": [\" - Glue ETL job error logs. Data processing failures, transformation errors, and pipeline execution issues. Data engineering monitoring and ETL job troubleshooting.\", \" - Glue job execution logs. ETL job lifecycle, data processing metrics, and transformation workflow monitoring. Data pipeline orchestration and performance tracking.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5738260092153833,
                        "reason": "The retrieval context addresses Glue job error logs and ETL job lifecycle issues, which aligns with the question about errors or warnings in Glue job execution logs. However, the description is somewhat general and lacks specific details on what errors or warnings to look for, units of measurement, or explicit implications for the ETL process. The language is clear but not sufficiently detailed to directly answer the question about identifying problems in the ETL process from the logs.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description uses understandable language and is concise relative to the question\u2019s terminology.\",\n    \"Assess Coverage by verifying if the description explains what is measured, specifies units, provides relevant context, and discusses implications tied to the question.\",\n    \"Determine Utility by confirming that the description enables a user to directly answer the input question using the information provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.5738260092153833"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description uses understandable language and is concise relative to the question\u2019s terminology.",
                            "Assess Coverage by verifying if the description explains what is measured, specifies units, provides relevant context, and discusses implications tied to the question.",
                            "Determine Utility by confirming that the description enables a user to directly answer the input question using the information provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/config/*\\\", \\\"message\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any configuration changes detected in the AWS Config compliance logs that could be causing a resource to become non-compliant?\", \"retrieval_context\": [\" - AWS Config compliance logs. Configuration drift detection, compliance auditing, change tracking, and governance monitoring. Resource inventory management and regulatory compliance.\", \" - ECS container warning logs. Non-critical issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - This pattern captures CloudTrail security-related logs. It identifies security events, compliance violations, and security policy changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4689539852743979,
                        "reason": "The retrieval context mentions AWS Config compliance logs and related functions like configuration drift detection and compliance auditing, which aligns with the input question about configuration changes causing non-compliance. However, the description is vague and lacks specific details on how configuration changes are detected or how they relate directly to resource non-compliance. It does not clarify units, measurement methods, or implications, limiting clarity, coverage, and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) for alignment to ensure the description addresses the question directly.\",\n    \"Evaluate Clarity of the description by checking if it is understandable and concise in relation to the terminology and detail expected in the Input.\",\n    \"Assess Coverage by verifying the description explains what is measured, units, context, and implications relative to the question asked.\",\n    \"Determine Utility by confirming whether the description provides sufficient information to confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.4689539852743979"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) for alignment to ensure the description addresses the question directly.",
                            "Evaluate Clarity of the description by checking if it is understandable and concise in relation to the terminology and detail expected in the Input.",
                            "Assess Coverage by verifying the description explains what is measured, units, context, and implications relative to the question asked.",
                            "Determine Utility by confirming whether the description provides sufficient information to confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/detective/*\\\", \\\"message\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the number of security incidents in the Detective logs exceed ten, indicating a potential security breach?\", \"retrieval_context\": [\" - Detective security investigation logs. Threat analysis, security incident investigation, and behavioral analytics. Security forensics and threat intelligence correlation.\", \" - GuardDuty threat detection logs. Security alerts, threat intelligence, compromised credentials, unusual activity, and security incident response. Critical for security monitoring and threat hunting.\", \" - This pattern captures CloudTrail security-related logs. It identifies security events, compliance violations, and security policy changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3095349460400408,
                        "reason": "The Retrieval Context mentions types of logs related to security incidents and threat detection but does not provide specific information about the number of security incidents, measurement units, or thresholds such as 'exceed ten.' While it is somewhat relevant to the question about security incidents in Detective logs, it lacks clarity and coverage needed to confidently answer whether incidents exceed ten or indicate a breach.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) against the Retrieval Context (metric description) to check if the description directly addresses the question's intent.\",\n    \"Evaluate Clarity by assessing whether the Retrieval Context is understandable and concise when connected to the specific Input question.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, relevant context, and possible implications that relate to answering the Input question.\",\n    \"Determine Utility by judging if the description enables a user to confidently answer the Input question using the information in the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3095349460400408"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) against the Retrieval Context (metric description) to check if the description directly addresses the question's intent.",
                            "Evaluate Clarity by assessing whether the Retrieval Context is understandable and concise when connected to the specific Input question.",
                            "Assess Coverage by verifying the description includes what is measured, units, relevant context, and possible implications that relate to answering the Input question.",
                            "Determine Utility by judging if the description enables a user to confidently answer the Input question using the information in the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/eks/*\\\", \\\"message\\\": \\\"WARN*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the EKS cluster warning logs indicating resource constraints or performance degradation?\", \"retrieval_context\": [\" - ECS container warning logs. Non-critical issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - EKS cluster error logs. Kubernetes cluster errors, node failures, pod scheduling issues, and cluster health problems. Container orchestration monitoring and troubleshooting.\", \" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8047425868465276,
                        "reason": "The retrieval context clearly defines EKS cluster warning logs as including resource constraints and performance degradation indicators, directly addressing the input question. The description is concise and understandable, mentioning the nature of issues and the purpose of monitoring. However, it lacks specific units, detailed context, and explicit implications, which slightly limits its completeness and utility for fully informed answers.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly defines the metric relevant to the question.\",\n    \"Evaluate the clarity and conciseness of the description in the Retrieval Context to ensure it is easily understandable when addressing the Input.\",\n    \"Check the coverage in the Retrieval Context to verify it explains what is measured, units, context, and implications in relation to the Input question.\",\n    \"Determine the utility by assessing if the Retrieval Context information enables a user to effectively answer the monitoring question posed in the Input.\"\n] \n \nRubric:\nNone \n \nScore: 0.8047425868465276"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly defines the metric relevant to the question.",
                            "Evaluate the clarity and conciseness of the description in the Retrieval Context to ensure it is easily understandable when addressing the Input.",
                            "Check the coverage in the Retrieval Context to verify it explains what is measured, units, context, and implications in relation to the Input question.",
                            "Determine the utility by assessing if the Retrieval Context information enables a user to effectively answer the monitoring question posed in the Input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/macie/*\\\", \\\"message\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in sensitive data identification events that could indicate a security breach?\", \"retrieval_context\": [\" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - GuardDuty threat detection logs. Security alerts, threat intelligence, compromised credentials, unusual activity, and security incident response. Critical for security monitoring and threat hunting.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.29818611399403905,
                        "reason": "The retrieval context relates broadly to security monitoring and threat detection but does not specifically address sensitive data identification events or spikes in such events as asked in the input. While the context mentions relevant logs and security alerts, it lacks clarity and coverage regarding what is measured, units, or implications related to sensitive data identification spikes. Therefore, it provides limited utility for answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly relates to and addresses the question posed.\",\n    \"Evaluate Clarity by checking if the description is concise and easily understandable, directly supporting the user's interpretation of the Input.\",\n    \"Assess Coverage by verifying the description includes what is measured, units of measurement, relevant context, and potential implications that align with the question.\",\n    \"Judge Utility by determining if the description sufficiently enables a user to answer the monitoring question; if not, identify specific gaps in relation to the Input.\"\n] \n \nRubric:\nNone \n \nScore: 0.29818611399403905"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly relates to and addresses the question posed.",
                            "Evaluate Clarity by checking if the description is concise and easily understandable, directly supporting the user's interpretation of the Input.",
                            "Assess Coverage by verifying the description includes what is measured, units of measurement, relevant context, and potential implications that align with the question.",
                            "Judge Utility by determining if the description sufficiently enables a user to answer the monitoring question; if not, identify specific gaps in relation to the Input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/glue/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a failure in the Glue ETL job?\", \"retrieval_context\": [\" - Glue ETL job error logs. Data processing failures, transformation errors, and pipeline execution issues. Data engineering monitoring and ETL job troubleshooting.\", \" - Glue job execution logs. ETL job lifecycle, data processing metrics, and transformation workflow monitoring. Data pipeline orchestration and performance tracking.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.41310975204308614,
                        "reason": "The retrieval context mentions Glue ETL job error logs and related issues, which aligns with the input question about ERROR message spikes in the Glue ETL job. However, the description is fragmented and lacks clarity, detailed explanation of what the metric measures, units, or how to interpret spikes. It also includes unrelated Redshift cluster logs, which may cause ambiguity. Overall, it provides partial but insufficient information for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question asked.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if the description is concise, understandable, and free of ambiguity when read alongside the Input.\",\n    \"Assess coverage by verifying that the Retrieval Context explains what the metric measures, its units, context, and implications relevant to the Input question.\",\n    \"Determine utility by judging if the Retrieval Context provides enough information for a user to confidently answer the Input question based on the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.41310975204308614"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question asked.",
                            "Evaluate the clarity of the Retrieval Context by checking if the description is concise, understandable, and free of ambiguity when read alongside the Input.",
                            "Assess coverage by verifying that the Retrieval Context explains what the metric measures, its units, context, and implications relevant to the Input question.",
                            "Determine utility by judging if the Retrieval Context provides enough information for a user to confidently answer the Input question based on the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/elasticmapreduce/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there ERROR messages in the EMR cluster error logs indicating a failure or issue with big data processing?\", \"retrieval_context\": [\" - EMR cluster error logs. Big data processing failures, Hadoop ecosystem errors, and cluster health issues. Data processing pipeline monitoring and troubleshooting.\", \" - EMR job execution logs. Big data job lifecycle, processing metrics, and cluster operations. Data processing pipeline orchestration and performance tracking.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5810971433389323,
                        "reason": "The retrieval context partially addresses the monitoring question by mentioning EMR cluster error logs and big data processing failures, which aligns with the focus on ERROR messages indicating failures. However, the description is somewhat broad and lacks specific details about the nature of ERROR messages, their units, or explicit implications for failure detection. While it provides relevant context on Hadoop ecosystem errors and cluster health, it does not clearly explain how to identify or interpret ERROR messages, limiting its utility for directly answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate clarity by verifying if the metric description is concise and easily understandable without ambiguity.\",\n    \"Assess coverage by checking if the metric description explains what is measured, including units, relevant context, and implications tied to the question.\",\n    \"Determine utility by judging whether the description provides sufficient information to effectively answer the monitoring question; if lacking, identify specific gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.5810971433389323"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate clarity by verifying if the metric description is concise and easily understandable without ambiguity.",
                            "Assess coverage by checking if the metric description explains what is measured, including units, relevant context, and implications tied to the question.",
                            "Determine utility by judging whether the description provides sufficient information to effectively answer the monitoring question; if lacking, identify specific gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/athena/*\\\", \\\"message\\\": \\\"QUERY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any errors or warnings in the Athena query execution logs that could be causing slow query times?\", \"retrieval_context\": [\" - Athena query error logs. SQL query failures, syntax errors, and data access issues. Data analytics troubleshooting and query optimization.\", \" - Athena query execution logs. SQL query processing, performance metrics, and data warehouse operations. Business intelligence monitoring and data exploration tracking.\", \" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5620887136138197,
                        "reason": "The retrieval context includes relevant information about Athena query error logs and execution logs, which relate to errors, warnings, and performance metrics. However, it lacks explicit mention of warnings or specific error types that could cause slow query times, and does not clearly connect errors or warnings to query slowness. The description is somewhat clear but does not fully cover the implications or detailed context needed to confidently answer the question about errors or warnings causing slow queries.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question being asked.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise enough to directly relate to the Input question.\",\n    \"Assess Coverage by verifying the description explains what is measured, units involved, context, and potential implications relevant to the Input.\",\n    \"Determine Utility by judging if the description provides sufficient information to confidently answer the monitoring question, indicating effective alignment between Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.5620887136138197"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question being asked.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise enough to directly relate to the Input question.",
                            "Assess Coverage by verifying the description explains what is measured, units involved, context, and potential implications relevant to the Input.",
                            "Determine Utility by judging if the description provides sufficient information to confidently answer the monitoring question, indicating effective alignment between Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/redshift/*\\\", \\\"message\\\": \\\"QUERY*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any SQL queries in the Redshift logs that are taking longer than 10 seconds to execute?\", \"retrieval_context\": [\" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - Redshift query execution logs. SQL query processing, performance metrics, and data warehouse operations. Business intelligence monitoring and query optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5912292501670289,
                        "reason": "The retrieval context references Redshift query execution logs and performance metrics relevant to SQL query durations, which aligns with the monitoring question about queries exceeding 10 seconds. However, the description lacks explicit mention of the 10-second threshold, units of measurement, or detailed explanation of how to identify such slow queries. While the context supports understanding, it does not fully clarify what is measured or how to use the logs to answer the question, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context to ensure the description directly addresses the given monitoring question.\",\n    \"Evaluate the clarity of the description by considering both the language used and how the context supports understanding of the metric.\",\n    \"Assess coverage by verifying that the description and context together explain what is measured, including units, relevant environment, and potential impact on the system.\",\n    \"Determine utility by checking if the combined information from Input and Retrieval Context enables a user to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.5912292501670289"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context to ensure the description directly addresses the given monitoring question.",
                            "Evaluate the clarity of the description by considering both the language used and how the context supports understanding of the metric.",
                            "Assess coverage by verifying that the description and context together explain what is measured, including units, relevant environment, and potential impact on the system.",
                            "Determine utility by checking if the combined information from Input and Retrieval Context enables a user to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/eks/*\\\", \\\"message\\\": \\\"NODE*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any EKS node registration or scaling events in the logs that could be causing a sudden increase in UnHealthyHostCount?\", \"retrieval_context\": [\" - EKS cluster error logs. Kubernetes cluster errors, node failures, pod scheduling issues, and cluster health problems. Container orchestration monitoring and troubleshooting.\", \" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\", \" - EKS node management logs. Node registration, health checks, scaling events, and node lifecycle management. Infrastructure monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6647874728223326,
                        "reason": "The retrieval context addresses EKS node registration, scaling events, and related logs, which directly relate to the monitoring question about node events causing UnHealthyHostCount increases. The description uses clear and relevant language, mentioning node management logs, health checks, and scaling events. However, it lacks explicit details on how these logs correlate with UnHealthyHostCount metrics, units, or specific implications, limiting its utility for fully answering the question. Including examples of log entries or explicit links to UnHealthyHostCount changes would improve coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question posed.\",\n    \"Evaluate the clarity of the description by checking if it uses understandable and concise language relevant to the monitoring question.\",\n    \"Assess coverage by verifying that the description explains what is measured, including units, context, and possible implications that relate to the question.\",\n    \"Determine utility by judging whether the description provides sufficient information to effectively answer the monitoring question; if gaps exist, suggest specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6647874728223326"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question posed.",
                            "Evaluate the clarity of the description by checking if it uses understandable and concise language relevant to the monitoring question.",
                            "Assess coverage by verifying that the description explains what is measured, including units, context, and possible implications that relate to the question.",
                            "Determine utility by judging whether the description provides sufficient information to effectively answer the monitoring question; if gaps exist, suggest specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/eks/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages related to Kubernetes node failures or pod scheduling issues?\", \"retrieval_context\": [\" - CodeDeploy error logs. Deployment failures, rollback events, and application deployment issues. Release management monitoring and deployment troubleshooting.\", \" - EKS cluster error logs. Kubernetes cluster errors, node failures, pod scheduling issues, and cluster health problems. Container orchestration monitoring and troubleshooting.\", \" - EKS cluster warning logs. Non-critical cluster issues, resource constraints, configuration warnings, and performance degradation indicators. Proactive monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6693521079891056,
                        "reason": "The retrieval context addresses the question by mentioning EKS cluster error logs related to Kubernetes cluster errors, node failures, and pod scheduling issues, which aligns with the monitoring question about spikes in ERROR messages. The description is fairly clear and concise, covering relevant error types and contexts. However, it lacks explicit mention of measurement units, how spikes are detected, or implications of these errors, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the description (Retrieval Context) with the monitoring question (Input) to ensure the description addresses the question directly and adequately.\",\n    \"Evaluate clarity by checking if the description is concise and easily understandable within the context of the provided question.\",\n    \"Assess coverage by verifying if the description explains what is measured, the measurement units, relevant context, and potential implications, ensuring it aligns well with the input question.\",\n    \"Judge utility by determining if the description provides sufficient information for the user to confidently answer the monitoring question, noting gaps or missing details between the Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6693521079891056"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the description (Retrieval Context) with the monitoring question (Input) to ensure the description addresses the question directly and adequately.",
                            "Evaluate clarity by checking if the description is concise and easily understandable within the context of the provided question.",
                            "Assess coverage by verifying if the description explains what is measured, the measurement units, relevant context, and potential implications, ensuring it aligns well with the input question.",
                            "Judge utility by determining if the description provides sufficient information for the user to confidently answer the monitoring question, noting gaps or missing details between the Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/eks/*\\\", \\\"message\\\": \\\"POD*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in POD termination events that correlate with increased CPU utilization?\", \"retrieval_context\": [\" - ECS container stop events. Container termination, health check failures, resource constraints, and service scaling. Container lifecycle management and availability monitoring.\", \" - EKS pod lifecycle logs. Pod creation, deletion, scheduling, and health check events. Container orchestration and application deployment monitoring.\", \" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3625599916972207,
                        "reason": "The retrieval context partially addresses the question by mentioning pod lifecycle logs and container termination events, which relate to POD termination events. However, it lacks explicit information about spikes or correlation with CPU utilization, and does not provide units, measurement details, or implications needed to confidently answer the question. The description is somewhat clear but insufficiently detailed for full coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly aligns with and addresses the question.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise in relation to the terminology and intent expressed in the question.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications needed to answer the question effectively.\",\n    \"Judge Utility by determining if the combined Input and Retrieval Context enable a user to confidently answer the monitoring question based on the description provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.3625599916972207"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly aligns with and addresses the question.",
                            "Evaluate Clarity by checking if the description is understandable and concise in relation to the terminology and intent expressed in the question.",
                            "Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications needed to answer the question effectively.",
                            "Judge Utility by determining if the combined Input and Retrieval Context enable a user to confidently answer the monitoring question based on the description provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/athena/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there ERROR messages in the Athena query error logs indicating a failure to connect to S3?\", \"retrieval_context\": [\" - Athena query error logs. SQL query failures, syntax errors, and data access issues. Data analytics troubleshooting and query optimization.\", \" - S3 error logs. Access denied errors, bucket policy violations, storage operation failures, and service connectivity issues. Data access monitoring and security incident detection.\", \" - S3 object not found logs. Non-existent object access attempts, incorrectly named objects, and data retrieval errors. Data access troubleshooting and object lifecycle management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.48580175615676585,
                        "reason": "The retrieval context partially addresses the monitoring question by mentioning Athena query error logs and S3 error logs, which are relevant to detecting errors and connectivity issues. However, it lacks a direct explanation of ERROR messages specifically indicating failure to connect to S3 within Athena logs. The description is somewhat clear but does not explicitly link the error types to the specific failure scenario asked, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate Clarity by checking if the description is concise and easy to understand in relation to the specific monitoring question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, including units, context, and possible implications relevant to the question.\",\n    \"Judge Utility by determining if the description provides sufficient information for a user to confidently answer the monitoring question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.48580175615676585"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate Clarity by checking if the description is concise and easy to understand in relation to the specific monitoring question.",
                            "Assess Coverage by verifying that the description explains what is measured, including units, context, and possible implications relevant to the question.",
                            "Judge Utility by determining if the description provides sufficient information for a user to confidently answer the monitoring question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/kinesis/*\\\", \\\"message\\\": \\\"STREAM*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any errors in the Kinesis stream management logs indicating a failure to scale or process data?\", \"retrieval_context\": [\" - Kinesis stream management logs. Stream creation, deletion, scaling, and data flow monitoring. Real-time data processing and streaming analytics.\", \" - Kinesis streaming error logs. Stream processing failures, data ingestion errors, and service connectivity issues. Real-time data processing monitoring and troubleshooting.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7090205877650202,
                        "reason": "The description addresses Kinesis stream management logs and mentions scaling and data flow monitoring, which aligns with the question about errors indicating failure to scale or process data. However, it lacks explicit mention of error types or units, and does not clearly explain how to identify such errors or their implications, limiting clarity and utility for directly answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the specific question asked.\",\n    \"Evaluate the clarity of the metric description to ensure it is understandable and concise, reflecting directly on the input's terminology and focus.\",\n    \"Assess coverage by verifying if the description explains what is measured, including units, context, and implications relevant to the monitoring question.\",\n    \"Determine utility by judging whether the description provides sufficient information to enable a user to accurately answer the monitoring question based on the given context.\"\n] \n \nRubric:\nNone \n \nScore: 0.7090205877650202"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the specific question asked.",
                            "Evaluate the clarity of the metric description to ensure it is understandable and concise, reflecting directly on the input's terminology and focus.",
                            "Assess coverage by verifying if the description explains what is measured, including units, context, and implications relevant to the monitoring question.",
                            "Determine utility by judging whether the description provides sufficient information to enable a user to accurately answer the monitoring question based on the given context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/kinesis/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a failure to process stream data?\", \"retrieval_context\": [\" - EMR cluster error logs. Big data processing failures, Hadoop ecosystem errors, and cluster health issues. Data processing pipeline monitoring and troubleshooting.\", \" - Kinesis streaming error logs. Stream processing failures, data ingestion errors, and service connectivity issues. Real-time data processing monitoring and troubleshooting.\", \" - Step Functions error logs. Workflow execution failures, state machine errors, and serverless workflow issues. Business process automation monitoring and troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.743721373049064,
                        "reason": "The description addresses spikes in ERROR messages related to stream data processing failures, aligning with the monitoring question. It references relevant contexts such as Kinesis streaming, Step Functions, and EMR cluster error logs, which are pertinent to the question. However, the description lacks explicit mention of measurement units, how spikes are detected, and the implications of these spikes, limiting its clarity and utility for effective troubleshooting.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the specific question being asked.\",\n    \"Evaluate the description's clarity by checking if it uses concise and understandable language relevant to the question context.\",\n    \"Assess coverage by verifying if the description includes what is measured, the measurement units, relevant context, and implications that align with the retrieval question.\",\n    \"Determine utility by confirming if the description provides enough information to effectively answer the monitoring question; if not, identify specific missing details.\"\n] \n \nRubric:\nNone \n \nScore: 0.743721373049064"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the specific question being asked.",
                            "Evaluate the description's clarity by checking if it uses concise and understandable language relevant to the question context.",
                            "Assess coverage by verifying if the description includes what is measured, the measurement units, relevant context, and implications that align with the retrieval question.",
                            "Determine utility by confirming if the description provides enough information to effectively answer the monitoring question; if not, identify specific missing details."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/redshift/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in database errors that could indicate a problem with the Redshift cluster?\", \"retrieval_context\": [\" - RDS database error logs. Connection failures, query errors, authentication issues, and database engine errors. Critical for database health monitoring and performance troubleshooting.\", \" - RDS deadlock detection logs. Database deadlocks, transaction failures, and performance issues. Critical for database stability and transaction management.\", \" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.588079707484883,
                        "reason": "The Retrieval Context is relevant to the Input question as it mentions Redshift cluster error logs and database errors, which align with monitoring spikes in database errors. However, the context lacks clarity and detail, as it does not explicitly define what constitutes a 'spike,' the units or metrics used to measure errors, or how to interpret these logs operationally. It provides general categories of errors but does not offer sufficient information to fully answer the question or guide troubleshooting steps.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure relevance and alignment.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it explains the metric in simple and concise language that directly relates to the Input question.\",\n    \"Assess the coverage by verifying if the Retrieval Context includes details about what is measured, the units used, the operational context, and possible implications relevant to the monitoring question.\",\n    \"Determine the utility of the Retrieval Context by judging if it sufficiently equips a user to answer the Input question; if not, identify specific gaps to improve.\"\n] \n \nRubric:\nNone \n \nScore: 0.588079707484883"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure relevance and alignment.",
                            "Evaluate the clarity of the Retrieval Context by checking if it explains the metric in simple and concise language that directly relates to the Input question.",
                            "Assess the coverage by verifying if the Retrieval Context includes details about what is measured, the units used, the operational context, and possible implications relevant to the monitoring question.",
                            "Determine the utility of the Retrieval Context by judging if it sufficiently equips a user to answer the Input question; if not, identify specific gaps to improve."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/codebuild/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in CodeBuild error logs indicating a potential issue with the build process?\", \"retrieval_context\": [\" - CodeBuild error logs. Build failures, compilation errors, and CI/CD pipeline issues. Software development monitoring and build process troubleshooting.\", \" - CodeBuild execution logs. Build lifecycle, compilation metrics, and CI/CD pipeline monitoring. Software development automation and build process tracking.\", \" - CodePipeline error logs. Pipeline execution failures, stage failures, and CI/CD workflow issues. Software delivery automation monitoring and pipeline troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5522568600567652,
                        "reason": "The retrieval context mentions CodeBuild error logs and related issues such as build failures and compilation errors, which aligns with the input question about spikes in CodeBuild error logs. However, the description lacks clarity and detail on what exactly is measured (e.g., error counts or frequency), units, or how to identify spikes. The context is somewhat broad and does not explicitly address how to detect or interpret spikes, limiting its utility for confidently answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (metric description) to verify if the description clearly addresses the specific monitoring question.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable and concise enough to relate directly to the Input without ambiguity.\",\n    \"Assess Coverage by confirming the description includes what is measured, units, context, and possible implications relevant to the Input.\",\n    \"Determine Utility by judging if the Retrieval Context provides sufficient information for a user to confidently answer the Input question based on the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.5522568600567652"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (metric description) to verify if the description clearly addresses the specific monitoring question.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable and concise enough to relate directly to the Input without ambiguity.",
                            "Assess Coverage by confirming the description includes what is measured, units, context, and possible implications relevant to the Input.",
                            "Determine Utility by judging if the Retrieval Context provides sufficient information for a user to confidently answer the Input question based on the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/apprunner/*\\\", \\\"message\\\": \\\"DEPLOYMENT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any DEPLOYMENT errors in the App Runner deployment logs that could be causing a service disruption?\", \"retrieval_context\": [\" - App Runner deployment logs. Containerized application deployment, service scaling, and application lifecycle management. Container orchestration and application hosting.\", \" - App Runner error logs. Containerized application errors, deployment failures, and service health issues. Container orchestration monitoring and application troubleshooting.\", \" - CodeDeploy error logs. Deployment failures, rollback events, and application deployment issues. Release management monitoring and deployment troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5710651775867774,
                        "reason": "The input question is clear and specific, asking about deployment errors in App Runner logs causing service disruption. The retrieval context mentions App Runner error and deployment logs, including deployment failures and service health issues, which are relevant. However, the context is somewhat broad and lacks explicit details on how deployment errors are identified or linked to service disruption, limiting its direct utility in fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to understand what information the user seeks.\",\n    \"Assess the Retrieval Context (metric description) for clarity, ensuring it is concise and understandable.\",\n    \"Check if the Retrieval Context sufficiently covers key elements: what is measured, units, context, and implications, ensuring alignment with the Input's needs.\",\n    \"Determine the utility of the Retrieval Context by verifying if it directly helps answer the Input question, and if not, identify gaps or missing information.\"\n] \n \nRubric:\nNone \n \nScore: 0.5710651775867774"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to understand what information the user seeks.",
                            "Assess the Retrieval Context (metric description) for clarity, ensuring it is concise and understandable.",
                            "Check if the Retrieval Context sufficiently covers key elements: what is measured, units, context, and implications, ensuring alignment with the Input's needs.",
                            "Determine the utility of the Retrieval Context by verifying if it directly helps answer the Input question, and if not, identify gaps or missing information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/costexplorer/*\\\", \\\"message\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in UnHealthyHostCount that correlate with drops in HealthyHostCount?\", \"retrieval_context\": [\" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\", \" - This pattern captures VPC Flow Logs reset events. It identifies TCP connection resets, which can indicate network issues, service restarts, or connection problems.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.20758581862890044,
                        "reason": "The input question about spikes in UnHealthyHostCount correlating with drops in HealthyHostCount is clear but does not align well with the retrieval context, which focuses on Redshift error logs, VPC Flow Logs reset events, and Route 53 health check logs. The input lacks details on what is measured, units, or implications related to these specific logs, limiting its utility for answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) against the Retrieval Context (monitoring question) to assess if the description clearly addresses the specific question asked.\",\n    \"Evaluate Clarity by checking if the Input is understandable and concise, ensuring it uses terminology that aligns with the Retrieval Context.\",\n    \"Assess Coverage by verifying that the Input includes details on what is measured, units, context, and implications relevant to the Retrieval Context.\",\n    \"Judge Utility by determining if the Input provides sufficient information to directly help the user answer the Retrieval Context\u2019s question.\"\n] \n \nRubric:\nNone \n \nScore: 0.20758581862890044"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) against the Retrieval Context (monitoring question) to assess if the description clearly addresses the specific question asked.",
                            "Evaluate Clarity by checking if the Input is understandable and concise, ensuring it uses terminology that aligns with the Retrieval Context.",
                            "Assess Coverage by verifying that the Input includes details on what is measured, units, context, and implications relevant to the Retrieval Context.",
                            "Judge Utility by determining if the Input provides sufficient information to directly help the user answer the Retrieval Context\u2019s question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/codebuild/*\\\", \\\"message\\\": \\\"BUILD*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any BUILD_FAILED messages in the logs indicating a failed build?\", \"retrieval_context\": [\" - CodeBuild error logs. Build failures, compilation errors, and CI/CD pipeline issues. Software development monitoring and build process troubleshooting.\", \" - CodeBuild execution logs. Build lifecycle, compilation metrics, and CI/CD pipeline monitoring. Software development automation and build process tracking.\", \" - Step Functions error logs. Workflow execution failures, state machine errors, and serverless workflow issues. Business process automation monitoring and troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6413822088664723,
                        "reason": "The retrieval context broadly covers build failures, compilation errors, and CI/CD pipeline issues relevant to the input question about BUILD_FAILED messages, providing useful background. However, it lacks a clear, concise definition of what BUILD_FAILED messages specifically represent, their format, or units, and does not explicitly link these logs to the detection of failed builds. This limits clarity and utility for directly answering the question. Including explicit details about the nature of BUILD_FAILED messages and how to identify them in the logs would improve coverage and support the monitoring question more effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input to determine if the metric description is clear and concise, ensuring it is easily understandable on its own.\",\n    \"Assess the Retrieval Context to verify that the description provides comprehensive coverage, including what is measured, units, context, and implications relevant to the monitoring question.\",\n    \"Compare the Input and Retrieval Context collectively to judge Utility, confirming that the description effectively supports answering the provided question.\",\n    \"If the combined evaluation yields a score below 8, identify specific gaps in clarity, coverage, or utility and suggest targeted improvements to enhance the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.6413822088664723"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input to determine if the metric description is clear and concise, ensuring it is easily understandable on its own.",
                            "Assess the Retrieval Context to verify that the description provides comprehensive coverage, including what is measured, units, context, and implications relevant to the monitoring question.",
                            "Compare the Input and Retrieval Context collectively to judge Utility, confirming that the description effectively supports answering the provided question.",
                            "If the combined evaluation yields a score below 8, identify specific gaps in clarity, coverage, or utility and suggest targeted improvements to enhance the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/elasticmapreduce/*\\\", \\\"message\\\": \\\"JOB*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any errors or warnings in the EMR job execution logs indicating a problem with data processing?\", \"retrieval_context\": [\" - EMR cluster error logs. Big data processing failures, Hadoop ecosystem errors, and cluster health issues. Data processing pipeline monitoring and troubleshooting.\", \" - EMR job execution logs. Big data job lifecycle, processing metrics, and cluster operations. Data processing pipeline orchestration and performance tracking.\", \" - Glue ETL job error logs. Data processing failures, transformation errors, and pipeline execution issues. Data engineering monitoring and ETL job troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5714973542164316,
                        "reason": "The Input question is clear and specific about errors or warnings in EMR job execution logs related to data processing problems. The Retrieval Context mentions EMR job execution logs and related topics such as job lifecycle, processing metrics, and cluster operations, which are relevant. However, the context lacks explicit details about the presence or types of errors or warnings, their units, or how to identify them in the logs. It also mixes information about other logs (EMR cluster error logs and Glue ETL job error logs) without clearly distinguishing their relevance to the question. Enhancing the context with explicit descriptions of error types, warning indicators, and how to interpret EMR job execution logs would improve clarity, coverage, and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate if the Input (monitoring question) and Retrieval Context (metric description) are clearly and concisely written, ensuring the description is understandable and directly relevant to the question.\",\n    \"Check if the Retrieval Context adequately covers key information such as what is being measured, units, operational context, and implications, and confirm that this coverage aligns with the focus of the Input question.\",\n    \"Assess the Utility by determining whether the description provides sufficient detail to allow a user to accurately answer the monitoring question, verifying that the Input and Retrieval Context together form a coherent and informative pair.\",\n    \"If any aspect (clarity, coverage, utility) is insufficient, identify how improvements in the Retrieval Context can better address the Input question's requirements, suggesting specific enhancements to the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.5714973542164316"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate if the Input (monitoring question) and Retrieval Context (metric description) are clearly and concisely written, ensuring the description is understandable and directly relevant to the question.",
                            "Check if the Retrieval Context adequately covers key information such as what is being measured, units, operational context, and implications, and confirm that this coverage aligns with the focus of the Input question.",
                            "Assess the Utility by determining whether the description provides sufficient detail to allow a user to accurately answer the monitoring question, verifying that the Input and Retrieval Context together form a coherent and informative pair.",
                            "If any aspect (clarity, coverage, utility) is insufficient, identify how improvements in the Retrieval Context can better address the Input question's requirements, suggesting specific enhancements to the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/states/*\\\", \\\"message\\\": \\\"EXECUTION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there EXECUTION errors in the logs indicating a failure to complete workflows?\", \"retrieval_context\": [\" - CodePipeline error logs. Pipeline execution failures, stage failures, and CI/CD workflow issues. Software delivery automation monitoring and pipeline troubleshooting.\", \" - Glue ETL job error logs. Data processing failures, transformation errors, and pipeline execution issues. Data engineering monitoring and ETL job troubleshooting.\", \" - Step Functions error logs. Workflow execution failures, state machine errors, and serverless workflow issues. Business process automation monitoring and troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.14767440409021754,
                        "reason": "The response does not provide a metric description addressing execution errors in logs or workflow failures as requested. While the retrieval context lists relevant log types and failure scenarios, the response fails to clarify what is measured, units, or implications, resulting in poor alignment, clarity, coverage, and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (the metric description) and the Retrieval Context (the monitoring question) to check for alignment; the description should clearly address the question's focus.\",\n    \"Evaluate Clarity by assessing whether the metric description is understandable and concise relative to the terminology and intent shown in the retrieval context.\",\n    \"Assess Coverage by verifying that the description includes what is measured, units, relevant context, and implications that would aid in answering the retrieval context query.\",\n    \"Judge Utility by determining if the description provides enough detailed and relevant information to directly help a user answer the monitoring question in the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.14767440409021754"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (the metric description) and the Retrieval Context (the monitoring question) to check for alignment; the description should clearly address the question's focus.",
                            "Evaluate Clarity by assessing whether the metric description is understandable and concise relative to the terminology and intent shown in the retrieval context.",
                            "Assess Coverage by verifying that the description includes what is measured, units, relevant context, and implications that would aid in answering the retrieval context query.",
                            "Judge Utility by determining if the description provides enough detailed and relevant information to directly help a user answer the monitoring question in the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"message\\\": \\\"THROUGHPUT*\\\"}\", \"hyperparameters\": null, \"input\": \"Is the throughput exceeding 5000 requests per second, indicating potential application overload?\", \"retrieval_context\": [\" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\", \" - RDS slow query logs. Performance threshold exceeded queries, database optimization opportunities, and performance tuning insights. Query performance analysis and optimization.\", \" - Throughput monitoring logs across all services. Request rate tracking, capacity utilization, and performance metrics. Application scalability monitoring and capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4973077223521936,
                        "reason": "The retrieval context partially addresses the monitoring question by mentioning throughput monitoring and request rate tracking, which relate to measuring throughput. However, it lacks clarity and specificity about the exact metric of requests per second or the threshold of 5000 requests, and does not explicitly link exceeding this threshold to application overload. The description is somewhat vague and does not provide units or clear implications, limiting its utility to definitively answer the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to determine if the description clearly addresses the question's intent.\",\n    \"Evaluate the description's clarity and conciseness, ensuring it is understandable without excessive jargon or ambiguity in relation to the question posed.\",\n    \"Assess the coverage by verifying the description explains what is measured, the units involved, relevant context, and implications aligned with the monitoring question.\",\n    \"Judge the utility by determining if the description provides sufficient knowledge to definitively answer the monitoring question, suggesting improvements if the score is below 8.\"\n] \n \nRubric:\nNone \n \nScore: 0.4973077223521936"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to determine if the description clearly addresses the question's intent.",
                            "Evaluate the description's clarity and conciseness, ensuring it is understandable without excessive jargon or ambiguity in relation to the question posed.",
                            "Assess the coverage by verifying the description explains what is measured, the units involved, relevant context, and implications aligned with the monitoring question.",
                            "Judge the utility by determining if the description provides sufficient knowledge to definitively answer the monitoring question, suggesting improvements if the score is below 8."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/states/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there ERROR messages in the Step Functions logs indicating workflow execution failures?\", \"retrieval_context\": [\" - CodePipeline error logs. Pipeline execution failures, stage failures, and CI/CD workflow issues. Software delivery automation monitoring and pipeline troubleshooting.\", \" - Step Functions error logs. Workflow execution failures, state machine errors, and serverless workflow issues. Business process automation monitoring and troubleshooting.\", \" - Step Functions execution logs. Workflow lifecycle, state transitions, and business process automation. Serverless workflow orchestration and process monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6320821300824606,
                        "reason": "The input question is clear and specific, asking about ERROR messages in Step Functions logs related to workflow execution failures. The retrieval context mentions Step Functions error logs and workflow execution failures, which aligns with the input, but the descriptions are somewhat broad and lack detail on what constitutes an ERROR message or how to identify them specifically. Additionally, the inclusion of CodePipeline logs introduces unrelated context, which may cause some ambiguity. Overall, the retrieval context partially covers the input but could be more focused and detailed to fully enable actionable insights.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to determine if it clearly defines what needs to be monitored or answered.\",\n    \"Assess the Retrieval Context (metric description) for clarity and conciseness, ensuring it is easily understandable and free of ambiguity.\",\n    \"Compare the Input and Retrieval Context to check if the metric description adequately covers all aspects needed to answer the question, including what is measured, units, context, and implications.\",\n    \"Determine the utility of the Retrieval Context by verifying if the description enables a user to effectively answer the Input question and provide actionable insights.\"\n] \n \nRubric:\nNone \n \nScore: 0.6320821300824606"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to determine if it clearly defines what needs to be monitored or answered.",
                            "Assess the Retrieval Context (metric description) for clarity and conciseness, ensuring it is easily understandable and free of ambiguity.",
                            "Compare the Input and Retrieval Context to check if the metric description adequately covers all aspects needed to answer the question, including what is measured, units, context, and implications.",
                            "Determine the utility of the Retrieval Context by verifying if the description enables a user to effectively answer the Input question and provide actionable insights."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"message\\\": \\\"AUTHENTICATION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any authentication failures indicated by the AUTHENTICATION* messages?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - This pattern captures CloudTrail authentication logs. It identifies login attempts, authentication failures, and credential usage events.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7244692273408053,
                        "reason": "The description addresses authentication logs and mentions authentication failures, which aligns with the question about AUTHENTICATION* messages indicating failures. It is fairly clear and concise, covering login attempts, credential validation, and failure detection. However, it lacks explicit mention of units or detailed implications, and does not directly confirm how to identify or quantify failures from the messages, limiting its utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses what the question asks.\",\n    \"Evaluate Clarity by determining if the description is concise and understandable in the context of the question.\",\n    \"Assess Coverage by checking if the description explains what is measured, including units, context, and implications relevant to the question.\",\n    \"Determine Utility by verifying whether the description provides sufficient information to help a user effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7244692273408053"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses what the question asks.",
                            "Evaluate Clarity by determining if the description is concise and understandable in the context of the question.",
                            "Assess Coverage by checking if the description explains what is measured, including units, context, and implications relevant to the question.",
                            "Determine Utility by verifying whether the description provides sufficient information to help a user effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"message\\\": \\\"LATENCY*\\\"}\", \"hyperparameters\": null, \"input\": \"Is the average latency exceeding 500ms, indicating potential performance degradation?\", \"retrieval_context\": [\" - Lambda function out-of-memory events. Memory limit exceeded, resource constraint detection, and performance degradation. Critical for resource optimization and application stability.\", \" - Lambda function timeout events. Execution time limit exceeded, performance degradation detection, and resource constraint monitoring. Critical for performance optimization and SLA compliance.\", \" - Latency monitoring logs across all services. Response time tracking, performance degradation detection, and user experience monitoring. Application performance optimization and SLA compliance.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5191421781720902,
                        "reason": "The retrieval context is relevant to the input question as it addresses latency monitoring and performance degradation, but it lacks specific details about average latency values, units (ms), or thresholds like 500ms. The context is somewhat clear and concise but does not provide enough coverage to confidently determine if average latency exceeds 500ms, limiting its utility for directly answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description is relevant and directly addresses the question.\",\n    \"Evaluate the Clarity of the Retrieval Context to confirm it is understandable and concise, facilitating comprehension of the Input's monitoring question.\",\n    \"Assess Coverage in the Retrieval Context to verify it explains what is measured, the units, the context, and the implications, thereby supporting the Input's question effectively.\",\n    \"Determine Utility by judging if the Retrieval Context enables a user to confidently answer the Input's question, identifying any gaps or ambiguities between them.\"\n] \n \nRubric:\nNone \n \nScore: 0.5191421781720902"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description is relevant and directly addresses the question.",
                            "Evaluate the Clarity of the Retrieval Context to confirm it is understandable and concise, facilitating comprehension of the Input's monitoring question.",
                            "Assess Coverage in the Retrieval Context to verify it explains what is measured, the units, the context, and the implications, thereby supporting the Input's question effectively.",
                            "Determine Utility by judging if the Retrieval Context enables a user to confidently answer the Input's question, identifying any gaps or ambiguities between them."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/codedeploy/*\\\", \\\"message\\\": \\\"DEPLOYMENT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any error messages in the CodeDeploy execution logs indicating failed deployments or application updates?\", \"retrieval_context\": [\" - CodeDeploy error logs. Deployment failures, rollback events, and application deployment issues. Release management monitoring and deployment troubleshooting.\", \" - CodeDeploy execution logs. Deployment lifecycle, application updates, and release management. CI/CD pipeline monitoring and deployment automation tracking.\", \" - CodePipeline error logs. Pipeline execution failures, stage failures, and CI/CD workflow issues. Software delivery automation monitoring and pipeline troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6758466726430611,
                        "reason": "The retrieval context addresses error logs related to CodeDeploy and deployment failures, which aligns with the input question about error messages indicating failed deployments or application updates. However, the description is somewhat general and lacks explicit details on how to identify specific error messages or interpret them in the execution logs. Including examples of error message types or guidance on locating these messages would improve clarity and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's focus.\",\n    \"Evaluate the description's clarity and coverage to determine if it sufficiently explains what is measured, including units, context, and implications relevant to the Input question.\",\n    \"Assess the utility by checking if the description provides actionable information that directly helps answer the Input question.\",\n    \"If any aspect (clarity, coverage, utility) is insufficient, provide a specific suggestion targeting the gap between the Retrieval Context and the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6758466726430611"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's focus.",
                            "Evaluate the description's clarity and coverage to determine if it sufficiently explains what is measured, including units, context, and implications relevant to the Input question.",
                            "Assess the utility by checking if the description provides actionable information that directly helps answer the Input question.",
                            "If any aspect (clarity, coverage, utility) is insufficient, provide a specific suggestion targeting the gap between the Retrieval Context and the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"message\\\": \\\"AUDIT*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any unusual patterns or spikes in the number of AUDIT messages that could indicate a security issue?\", \"retrieval_context\": [\" - Audit logs across all services. Security events, access tracking, and change monitoring. Security auditing and compliance reporting.\", \" - Authentication logs across all services. Login attempts, credential validation, and identity verification. Security monitoring and access control auditing.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.47513206217339016,
                        "reason": "The retrieval context broadly covers audit logs and security monitoring but lacks specific information about the number or patterns of AUDIT messages, spikes, or unusual activity. It mentions types of logs and their purposes but does not provide measurable data, units, or explicit indicators of unusual patterns, limiting its utility to answer the question fully.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to verify if the description clearly addresses the question's focus.\",\n    \"Evaluate if the description provides sufficient coverage by detailing what is measured, units, contextual background, and implications relevant to the question.\",\n    \"Assess the clarity of the description in terms of understandability and conciseness, ensuring it can be easily interpreted in relation to the question.\",\n    \"Determine the utility by confirming if the description enables the user to effectively answer the question; if not, identify specific gaps between the description and question.\"\n] \n \nRubric:\nNone \n \nScore: 0.47513206217339016"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to verify if the description clearly addresses the question's focus.",
                            "Evaluate if the description provides sufficient coverage by detailing what is measured, units, contextual background, and implications relevant to the question.",
                            "Assess the clarity of the description in terms of understandability and conciseness, ensuring it can be easily interpreted in relation to the question.",
                            "Determine the utility by confirming if the description enables the user to effectively answer the question; if not, identify specific gaps between the description and question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"message\\\": \\\"AUTHORIZATION*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there excessive AUTHORIZATION* messages indicating a potential security issue or misconfigured permissions?\", \"retrieval_context\": [\" - API Gateway 4XX client error logs. Bad requests, unauthorized access, forbidden resources, and not found errors. Client-side issue detection and API usage monitoring.\", \" - Authorization logs across all services. Permission checks, access control decisions, and policy evaluations. Security monitoring and compliance auditing.\", \" - S3 access denied logs. Unauthorized access attempts, bucket policy violations, IAM permission issues, and security incident detection. Critical for data security monitoring and compliance auditing.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7268941421369994,
                        "reason": "The retrieval context addresses authorization-related logs relevant to the question about excessive AUTHORIZATION* messages, covering permission checks, access control, and security monitoring. However, it lacks explicit mention of message volume or thresholds indicating 'excessive' messages, and does not specify units or how to interpret the logs quantitatively. The language is generally clear but could be more concise and focused on directly answering the monitoring question to fully enable confident assessment.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question posed.\",\n    \"Evaluate the clarity of the metric description by checking if the language is concise and understandable within the context of the question.\",\n    \"Assess the coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the monitoring question.\",\n    \"Judge the utility by determining whether the description enables a user to confidently answer the monitoring question, considering both clarity and coverage together.\"\n] \n \nRubric:\nNone \n \nScore: 0.7268941421369994"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question posed.",
                            "Evaluate the clarity of the metric description by checking if the language is concise and understandable within the context of the question.",
                            "Assess the coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the monitoring question.",
                            "Judge the utility by determining whether the description enables a user to confidently answer the monitoring question, considering both clarity and coverage together."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/codepipeline/*\\\", \\\"message\\\": \\\"PIPELINE*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any error messages in the CodePipeline execution logs indicating a failed pipeline run?\", \"retrieval_context\": [\" - CodeBuild error logs. Build failures, compilation errors, and CI/CD pipeline issues. Software development monitoring and build process troubleshooting.\", \" - CodePipeline error logs. Pipeline execution failures, stage failures, and CI/CD workflow issues. Software delivery automation monitoring and pipeline troubleshooting.\", \" - CodePipeline execution logs. Pipeline lifecycle, stage transitions, and CI/CD workflow monitoring. Software delivery automation and release management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.38877344212278875,
                        "reason": "The retrieval context partially addresses the monitoring question by mentioning CodePipeline error logs and pipeline execution failures, which relate to error messages indicating failed runs. However, it lacks clarity and detail about what specific error messages are captured, their format, or how to identify failures in the logs. The description is somewhat broad and does not explicitly explain what is measured or how to interpret the logs for the question, limiting its utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate the Clarity of the description by checking if it is concise and understandable in the context of the monitoring question.\",\n    \"Assess the Coverage by verifying if the description explains what is measured, including units, relevant context, and implications that relate to the question.\",\n    \"Determine the Utility by judging whether the description provides sufficient information for a user to confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.38877344212278875"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate the Clarity of the description by checking if it is concise and understandable in the context of the monitoring question.",
                            "Assess the Coverage by verifying if the description explains what is measured, including units, relevant context, and implications that relate to the question.",
                            "Determine the Utility by judging whether the description provides sufficient information for a user to confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/servicediscovery/*\\\", \\\"message\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do drops in HealthyHostCount correlate with spikes in UnHealthyHostCount?\", \"retrieval_context\": [\" - Redshift cluster error logs. Database errors, connection failures, query issues, and cluster health problems. Data warehouse monitoring and performance troubleshooting.\", \" - This pattern captures Route 53 health check logs. It identifies health check failures, successful health checks, and health check configuration issues.\", \" - This pattern captures VPC Flow Logs reset events. It identifies TCP connection resets, which can indicate network issues, service restarts, or connection problems.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.12227001285074597,
                        "reason": "The retrieval context does not address the monitoring question about the correlation between HealthyHostCount and UnHealthyHostCount; instead, it provides unrelated log patterns for Redshift, VPC Flow Logs, and Route 53 health checks. The description lacks clarity, coverage of the specific metric definitions, units, or implications, and thus offers minimal utility for answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent and scope.\",\n    \"Evaluate Clarity by checking if the description is concise and understandable, ensuring it uses terminology appropriate for a user with cloud telemetry expertise.\",\n    \"Assess Coverage by verifying the description explains what is measured, the units used, the context in which the metric applies, and the implications of the metric values relative to the question.\",\n    \"Determine Utility by judging if the description equips the user to effectively answer the monitoring question; if gaps are found in clarity or coverage that hinder this, suggest improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.12227001285074597"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent and scope.",
                            "Evaluate Clarity by checking if the description is concise and understandable, ensuring it uses terminology appropriate for a user with cloud telemetry expertise.",
                            "Assess Coverage by verifying the description explains what is measured, the units used, the context in which the metric applies, and the implications of the metric values relative to the question.",
                            "Determine Utility by judging if the description equips the user to effectively answer the monitoring question; if gaps are found in clarity or coverage that hinder this, suggest improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/codepipeline/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there ERROR messages in the codepipeline logs indicating a failure or issue with software delivery?\", \"retrieval_context\": [\" - CodeBuild error logs. Build failures, compilation errors, and CI/CD pipeline issues. Software development monitoring and build process troubleshooting.\", \" - CodePipeline error logs. Pipeline execution failures, stage failures, and CI/CD workflow issues. Software delivery automation monitoring and pipeline troubleshooting.\", \" - CodePipeline execution logs. Pipeline lifecycle, stage transitions, and CI/CD workflow monitoring. Software delivery automation and release management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6320821307318385,
                        "reason": "The Retrieval Context references CodePipeline error logs and execution logs related to pipeline failures and CI/CD workflow issues, which aligns with the Input's focus on ERROR messages indicating failures in software delivery. However, the context lacks explicit mention of ERROR messages specifically and does not clarify units, detailed implications, or how to identify such errors, limiting clarity and completeness for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate if the Input (the monitoring question) is clearly reflected and addressed by the Retrieval Context (the metric description).\",\n    \"Check the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the Input's terminology and intent.\",\n    \"Assess the coverage in the Retrieval Context to verify that it explains what is measured, units, context, and implications relevant to answering the Input question.\",\n    \"Determine the utility of the Retrieval Context by judging if a user can effectively use it to answer the Input question, ensuring alignment and completeness.\"\n] \n \nRubric:\nNone \n \nScore: 0.6320821307318385"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate if the Input (the monitoring question) is clearly reflected and addressed by the Retrieval Context (the metric description).",
                            "Check the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the Input's terminology and intent.",
                            "Assess the coverage in the Retrieval Context to verify that it explains what is measured, units, context, and implications relevant to answering the Input question.",
                            "Determine the utility of the Retrieval Context by judging if a user can effectively use it to answer the Input question, ensuring alignment and completeness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"message\\\": \\\"COMPLIANCE*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there excessive COMPLIANCE* messages indicating a potential issue with regulatory compliance or audit events?\", \"retrieval_context\": [\" - AWS Config compliance logs. Configuration drift detection, compliance auditing, change tracking, and governance monitoring. Resource inventory management and regulatory compliance.\", \" - Audit logs across all services. Security events, access tracking, and change monitoring. Security auditing and compliance reporting.\", \" - Compliance logs across all services. Regulatory compliance, audit events, and governance monitoring. Compliance tracking and regulatory reporting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6510473577729747,
                        "reason": "The input question directly addresses the presence of excessive COMPLIANCE messages related to regulatory compliance and audit events, aligning well with the retrieval context focused on compliance logs and audit monitoring. The input is clear and understandable, specifying the concern about potential issues. However, it lacks detailed explanation of what exactly is measured, units, or implications, limiting full coverage and utility for comprehensive monitoring or reporting.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Input is understandable, concise, and free of ambiguity in relation to what the Retrieval Context asks.\",\n    \"Assess Coverage by verifying if the Input explains what is measured, units, context, and possible implications relevant to the Retrieval Context.\",\n    \"Determine Utility by considering whether the Input provides enough information to effectively answer the Retrieval Context\u2019s question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6510473577729747"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Input is understandable, concise, and free of ambiguity in relation to what the Retrieval Context asks.",
                            "Assess Coverage by verifying if the Input explains what is measured, units, context, and possible implications relevant to the Retrieval Context.",
                            "Determine Utility by considering whether the Input provides enough information to effectively answer the Retrieval Context\u2019s question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/codedeploy/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any ERROR messages in the CodeDeploy logs indicating a failed deployment or rollback event?\", \"retrieval_context\": [\" - CodeDeploy error logs. Deployment failures, rollback events, and application deployment issues. Release management monitoring and deployment troubleshooting.\", \" - CodeDeploy execution logs. Deployment lifecycle, application updates, and release management. CI/CD pipeline monitoring and deployment automation tracking.\", \" - CodePipeline error logs. Pipeline execution failures, stage failures, and CI/CD workflow issues. Software delivery automation monitoring and pipeline troubleshooting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6035466004474279,
                        "reason": "The retrieval context broadly references CodeDeploy error logs and deployment failures, which aligns with the question about ERROR messages indicating failed deployments or rollbacks. However, the description lacks specific details on what exactly is measured (e.g., types of error messages), units or formats used, and explicit implications for identifying failures or rollbacks. This limits the utility for confidently answering the monitoring question. Including clearer definitions of error message types, examples, and how to interpret them would improve clarity, coverage, and usefulness.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description clearly and concisely addresses the question's focus.\",\n    \"Evaluate the description for coverage by checking if it details what is measured, units used, contextual information, and potential implications relevant to the question.\",\n    \"Assess the utility by determining whether the description provides sufficient information to enable the user to confidently answer the monitoring question.\",\n    \"If any aspect scores below 8, identify specific missing or unclear elements in the description and suggest precise improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.6035466004474279"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description clearly and concisely addresses the question's focus.",
                            "Evaluate the description for coverage by checking if it details what is measured, units used, contextual information, and potential implications relevant to the question.",
                            "Assess the utility by determining whether the description provides sufficient information to enable the user to confidently answer the monitoring question.",
                            "If any aspect scores below 8, identify specific missing or unclear elements in the description and suggest precise improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"LogGroup\\\": \\\"/aws/apprunner/*\\\", \\\"message\\\": \\\"ERROR*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in ERROR messages indicating a potential issue with containerized applications or deployments?\", \"retrieval_context\": [\" - App Runner error logs. Containerized application errors, deployment failures, and service health issues. Container orchestration monitoring and application troubleshooting.\", \" - CodeDeploy error logs. Deployment failures, rollback events, and application deployment issues. Release management monitoring and deployment troubleshooting.\", \" - ECS container error logs. Container startup failures, application errors, service health check failures, and container lifecycle issues. Critical for container orchestration monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6817574467303823,
                        "reason": "The input clearly asks about spikes in ERROR messages related to containerized applications or deployments, and the retrieval context provides relevant sources of error logs from App Runner, ECS, and CodeDeploy that cover container errors and deployment failures. However, the input lacks explicit mention of what metric is measured (e.g., error count or rate), units, or implications of spikes, and the retrieval context does not explicitly define these metrics or how to interpret them. This limits the user's ability to confidently answer the monitoring question solely based on the provided information. Including specific metric definitions and guidance on interpreting spikes would improve clarity, coverage, and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input and Retrieval Context together, determining if the description clearly and concisely explains the metric relevant to the question.\",\n    \"Assess the coverage by verifying that the Input includes explanation of what is measured, units, relevant context, and implications, and that the Retrieval Context supports or complements this information.\",\n    \"Examine the utility by checking if the description allows a user to confidently answer the monitoring question using the provided metric information within the retrieval context.\",\n    \"If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context and suggest improvements to enhance understanding, coverage, or practical relevance.\"\n] \n \nRubric:\nNone \n \nScore: 0.6817574467303823"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input and Retrieval Context together, determining if the description clearly and concisely explains the metric relevant to the question.",
                            "Assess the coverage by verifying that the Input includes explanation of what is measured, units, relevant context, and implications, and that the Retrieval Context supports or complements this information.",
                            "Examine the utility by checking if the description allows a user to confidently answer the monitoring question using the provided metric information within the retrieval context.",
                            "If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context and suggest improvements to enhance understanding, coverage, or practical relevance."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        }
    }
}
