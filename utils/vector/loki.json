[
  {
    "query": {
      "message": "error*",
      "k8s_app": "*"
    },
    "description": "This metric captures log entries from all Kubernetes applications where the message begins with \"error\". It measures the count of error-related log events generated by any k8s app within a given time frame. Units are the number of error log occurrences per interval. This data is essential for monitoring application health, diagnosing runtime failures, and supporting incident response."
  },
  {
    "query": {
      "message": "error*",
      "container": "*"
    },
    "description": "Container error logs capturing runtime failures, application errors, and exceptions from all containers. In Loki, **{\"message\": \"error*\", \"container\": \"*\"}** filters logs indicating error-level events critical for monitoring container health and diagnosing issues. Alerts should trigger when error log counts exceed a defined threshold (e.g., >5 errors per minute), signaling potential service degradation or failures. High error rates often indicate unstable containers or failing applications, requiring immediate investigation, while low or zero errors suggest normal operation.  \nExample alert rule snippet:  \n```\nsum(rate({container=~\".*\"} |= \"error\" [1m])) > 5\n```  \nThis can be visualized in dashboards as an error rate graph per container to quickly identify problematic instances."
  },
  {
    "query": {
      "message": "error*",
      "node_name": "*"
    },
    "description": "Node error logs filtered by `{\"message\": \"error*\", \"node_name\": \"*\"}` capture all error-level messages originating from Kubernetes nodes, including infrastructure failures, hardware issues, and node-level exceptions. This log stream is essential for monitoring node health and detecting critical infrastructure problems early.  \n\n**Purpose:** Enables SREs to identify and troubleshoot node-specific errors that could impact cluster stability and performance.  \n\n**Alert Threshold:** Trigger an alert if error log entries exceed a baseline threshold (e.g., more than 5 errors per node within 5 minutes), indicating potential node instability or hardware failure.  \n\n**Impact:**  \n- **High error rates:** Suggest critical node issues such as hardware faults, kernel panics, or resource exhaustion, potentially leading to node downtime or degraded cluster performance. Immediate investigation and remediation are required.  \n- **Low or zero error rates:** Indicate healthy node operation with no detected critical errors.  \n\n**Example Usage:**  \n- **Dashboard:** Visualize error counts per node over time using a query like `count_over_time({message=~\"error.*\", node_name=~\".*\"}[5m])` to spot spikes or trends.  \n- **Alert Rule:** Alert if `count_over_time({message=~\"error.*\", node_name=~\".*\"}[5m]) > 5` for any node, triggering a high-severity incident for SRE response."
  },
  {
    "query": {
      "message": "error*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"error\" (indicating an operational failure or service issue) and includes any Kubernetes `region` label. In Loki, it is used to identify and aggregate error occurrences across different regions to monitor system health and detect regional issues.\n\n**Purpose:**  \nTo track and alert on error events by region, enabling rapid identification of failing components or degraded service areas within the Kubernetes cluster.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"error*\", \"region\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 10 error logs per 5 minutes per region), signaling a potential incident requiring investigation.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or widespread errors in a specific region, suggesting service degradation or outages that could impact users or dependent systems. Immediate attention is needed to mitigate impact.  \n- **Low or zero values:** Suggest normal operation with no recent error logs, indicating system stability in that region.\n\n**Example Usage:**  \n- **Dashboard:** A panel displaying the count of error logs grouped by `region` over time, helping SREs quickly spot error spikes in specific regions.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"error.*\", region=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 error logs occur in any region within 5 minutes."
  },
  {
    "query": {
      "message": "error*",
      "service": "*"
    },
    "description": "Service error logs capturing all error-level messages from any Kubernetes microservice, including API failures and service exceptions. In Loki, this filter helps SREs monitor the health and stability of the service mesh by isolating critical error events across all services. Alerts should trigger when error counts exceed a defined threshold (e.g., more than 5 errors per minute per service), indicating potential service degradation or outages. High error rates often signal systemic issues requiring immediate investigation, while low or zero errors typically reflect normal operation. Example alert rule: `sum by (service) (count_over_time({message=~\"error.*\", service=~\".*\"}[1m])) > 5` triggers when any service logs more than 5 errors in one minute. This query can also be visualized in dashboards to track error trends per service over time."
  },
  {
    "query": {
      "message": "error*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"error\" and the \"job\" field contains any value. It identifies occurrences of error-related events within specific Kubernetes jobs. The measurement is a count of such log entries, representing the number of error messages recorded per job. This helps monitor failure events in Kubernetes workloads."
  },
  {
    "query": {
      "message": "error*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"error\" and the environment field is present with any value. It identifies occurrences of error-related events within different Kubernetes environments. The measurement is a count of such log entries, representing the number of error events per environment."
  },
  {
    "query": {
      "message": "error*",
      "host": "*"
    },
    "description": "This pattern captures logs where the message contains 'error', indicating a failure in operation or service, and the log entry is labeled with Kubernetes 'host'. Example log entry: message='error connection failed', host='example-host'."
  },
  {
    "query": {
      "message": "error*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"error\" (case-sensitive) and the level field contains any value. It identifies occurrences of error-related events recorded in logs, indicating potential failures or issues in the system. The pattern measures the count of such error-prefixed messages without specifying a unit, as it represents discrete log entries."
  },
  {
    "query": {
      "message": "error*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"error\" (indicating an operational failure or issue) and includes any Kubernetes environment label (`env`). It is used in Loki to filter and monitor error occurrences across different deployment environments.\n\n**Purpose:**  \nTo identify and track error messages in logs across all environments, enabling timely detection of failures or service disruptions.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of error messages exceeds a defined threshold within a specific time window (e.g., more than 5 errors in 5 minutes), signaling a potential incident requiring investigation.\n\n**Impact of Values:**  \n- **High error counts:** Indicate increased failure rates or systemic issues impacting service reliability, potentially affecting user experience or system stability.  \n- **Low or zero error counts:** Suggest normal operation with no detected failures.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the number of error messages per environment over time, using a Loki query like:  \n  `count_over_time({message=~\"error.*\", env=~\".*\"}[5m]) by (env)`  \n- **Alert Rule:** Trigger an alert if errors exceed 5 within 5 minutes in any environment:  \n  ```\n  alert: HighErrorRate\n  expr: count_over_time({message=~\"error.*\", env=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High error rate detected in {{ $labels.env }}\"\n    description: \"More than 5 error messages detected in the last 5 minutes in environment {{ $labels.env }}.\"\n  ```"
  },
  {
    "query": {
      "message": "error*",
      "app": "*"
    },
    "description": "This pattern captures logs where the message contains 'error', indicating a failure in operation or service, and the log entry is labeled with Kubernetes 'app'. Example log entry: message='error connection failed', app='example-app'."
  },
  {
    "query": {
      "message": "error*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"error\" (indicating an operational failure or issue) and includes the `node` label identifying the Kubernetes node where the error occurred. In Loki, this filter helps isolate node-specific error logs for monitoring system health and troubleshooting.\n\n**Purpose:**  \nTo detect and aggregate error events per Kubernetes node, enabling targeted investigation of node-level issues affecting service reliability.\n\n**Alert Threshold:**  \nAn alert could be triggered when the count of error logs matching this pattern exceeds a defined threshold within a time window (e.g., >10 errors per node in 5 minutes), signaling a potential node failure or degraded performance.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or severe errors on a node, potentially leading to service disruption or degraded user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Suggest normal operation with no recent error events on the node.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of `{\"message\": \"error*\", \"node\": \"*\"}` logs grouped by `node` over time to identify nodes with rising error rates.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (node) (count_over_time({message=~\"error.*\"}[5m])) > 10\n  ```  \n  This triggers an alert if any node logs more than 10 error messages in 5 minutes."
  },
  {
    "query": {
      "message": "error*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern captures logs where the message contains 'error', indicating a failure in operation or service, and the log entry is labeled with Kubernetes 'app_kubernetes_io/name'. Example log entry: message='error connection failed', app_kubernetes_io/name='example-app_kubernetes_io/name'."
  },
  {
    "query": {
      "message": "error*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"error,\" indicating an operational failure or issue. The \"instance\" field identifies the specific Kubernetes instance generating the log. It measures the count of such error-related log entries per Kubernetes instance. The unit is the number of error logs recorded for each instance."
  },
  {
    "query": {
      "message": "error*",
      "filename": "*"
    },
    "description": "This log pattern **{\"message\": \"error*\", \"filename\": \"*\"}** is used in Loki to identify log entries where the message starts with or contains the word \"error\", indicating potential failures or issues within components identified by the Kubernetes 'filename' label. It helps SREs monitor error occurrences across different application modules or services.\n\n**Purpose:**  \nTo detect and aggregate error-level logs from various sources, enabling early identification of operational problems or service degradations.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of error logs exceeds a defined threshold within a specific time window (e.g., more than 10 error messages in 5 minutes), signaling an abnormal spike in errors that may require immediate investigation.\n\n**Impact of Values:**  \n- **High values:** A surge in error logs suggests systemic issues, failures, or outages affecting service reliability and user experience. Prompt action is needed to mitigate impact.  \n- **Low or zero values:** Indicates normal operation with no detected errors, implying system stability.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({filename=~\".*\"} |= \"error\" [5m])\n```\nto count error messages per filename over the last 5 minutes. An alert can be configured to fire if this count exceeds 10, helping SREs quickly detect and respond to error spikes."
  },
  {
    "query": {
      "message": "error*",
      "application": "*"
    },
    "description": "This pattern captures logs where the message contains 'error', indicating a failure in operation or service, and the log entry is labeled with Kubernetes 'application'. Example log entry: message='error connection failed', application='example-application'."
  },
  {
    "query": {
      "message": "error*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"error\" (indicating an error event) and includes any Kubernetes `component` label, helping to isolate errors by service or subsystem. In Loki, it is used to monitor and aggregate error occurrences per component, enabling targeted troubleshooting.\n\n**Alert threshold:** An alert should trigger when the count of error logs for any component exceeds a defined threshold within a given time window (e.g., >5 errors in 5 minutes), signaling potential service degradation or failure.\n\n**Impact:**  \n- **High error counts:** Indicate increased failure rates or instability in the affected component, requiring immediate investigation to prevent outages.  \n- **Low or zero error counts:** Suggest normal operation and system stability.\n\n**Example usage:**  \n- **Dashboard:** A panel showing the rate of error logs per component over time using the query:  \n  `count_over_time({message=~\"error.*\", component=~\".+\"}[5m]) by (component)`  \n- **Alert rule:** Trigger an alert if any component logs more than 5 errors in 5 minutes:  \n  ```\n  alert: HighErrorRate\n  expr: count_over_time({message=~\"error.*\", component=~\".+\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High error rate detected in {{ $labels.component }}\"\n    description: \"More than 5 error messages logged by component {{ $labels.component }} in the last 5 minutes.\"\n  ```"
  },
  {
    "query": {
      "message": "error*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"error\" (indicating an error event) and the `source` field is any value, typically representing the Kubernetes component or service emitting the log. In Loki, this filter helps identify and aggregate error occurrences across all sources for monitoring system health.\n\n**Purpose:** To detect and track error messages from all Kubernetes sources, enabling early identification of failures or degraded service conditions.\n\n**Alert Threshold:** An alert could be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 error logs in 5 minutes), signaling a potential incident requiring investigation.\n\n**Impact of Values:**  \n- **High values:** A spike in error logs suggests increased failure rates or systemic issues, potentially impacting service availability or performance. Immediate attention is needed to prevent outages.  \n- **Low or zero values:** Indicates normal operation with no recent error events detected, implying system stability.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching `{message=~\"error.*\", source=~\".*\"}` over time, broken down by `source`, helps visualize which components are generating errors.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (source) (count_over_time({message=~\"error.*\", source=~\".*\"}[5m])) > 10\n  ```  \n  This triggers an alert if any source produces more than 10 error logs in 5 minutes."
  },
  {
    "query": {
      "message": "error*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod (`pod_name=\"*\"`) where the `message` field contains the substring \"error\", indicating potential failures or issues within that pod’s operation or service. In Loki, this filter helps identify error occurrences across all pods, enabling SREs to monitor system health and detect problems early.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of error messages exceeds a defined threshold within a given time window (e.g., more than 5 error logs in 5 minutes for a single pod), signaling a potential incident requiring investigation.\n\n**Impact of Values:**  \n- **High error counts:** Suggest persistent or severe issues affecting pod stability or service reliability, possibly leading to outages or degraded performance. Immediate attention is needed.  \n- **Low or zero error counts:** Indicate normal operation with no detected errors, reflecting healthy pod behavior.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of error messages per pod over time using a Loki query like:  \n  `count_over_time({pod_name=~\".*\"} |= \"error\" [5m])`  \n  This visualizes error trends and helps identify pods with increasing error rates.  \n- **Alert Rule:**  \n  ```yaml\n  alert: HighPodErrorRate\n  expr: count_over_time({pod_name=~\".*\"} |= \"error\" [5m]) > 5\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High error rate detected in pod {{ $labels.pod_name }}\"\n    description: \"More than 5 error messages logged in pod {{ $labels.pod_name }} over the last 5 minutes.\"\n  ```  \nThis enables proactive detection and response to pod-level errors in Kubernetes environments."
  },
  {
    "query": {
      "message": "error*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"error\" (indicating an error event) and includes any Kubernetes `container_name`, helping identify container-level failures or issues in your cluster. In Loki, it is used to filter and aggregate error occurrences per container to monitor service health.\n\n**Alert threshold:** Trigger an alert if the number of error messages per container exceeds a defined threshold within a set time window (e.g., >5 errors in 5 minutes), signaling a potential service degradation or failure.\n\n**Impact:**  \n- **High error counts:** Indicate frequent or persistent failures in the containerized service, requiring immediate investigation to prevent outages or data loss.  \n- **Low or zero error counts:** Suggest stable operation with no detected errors, implying healthy service behavior.\n\n**Example usage:**  \nIn a Grafana dashboard, use the query:  \n```\n{container_name=~\".*\"} |= \"error\"\n```\nto visualize error rates per container over time.  \n\nFor an alert rule, define:  \n```\nsum by (container_name) (count_over_time({container_name=~\".*\"} |= \"error\" [5m])) > 5\n```\nto notify when any container logs more than 5 errors in 5 minutes, enabling proactive incident response."
  },
  {
    "query": {
      "message": "error*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"error\" (indicating an operational failure or service issue) and includes the Kubernetes `cluster` label to identify the source cluster. In Loki, this filter helps SREs monitor error occurrences across clusters by isolating logs that signify failures.\n\n**Purpose:** To track and quantify error events per cluster, enabling rapid detection of service degradation or outages.\n\n**Alert Threshold:** An alert should trigger when the count of error logs exceeds a defined threshold within a set time window (e.g., more than 10 error messages in 5 minutes), signaling a potential incident requiring investigation.\n\n**Impact of Values:**  \n- **High error counts:** Indicate increased failure rates, possibly reflecting service instability, degraded performance, or outages that need immediate attention.  \n- **Low or zero error counts:** Suggest normal operation with no detected failures.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of error logs per cluster over time using the query:  \n  `count_over_time({message=~\"error.*\", cluster=~\".*\"}[5m]) by (cluster)`  \n- **Alert Rule:** Trigger an alert if the error count per cluster exceeds 10 within 5 minutes:  \n  ```\n  alert: HighErrorRate\n  expr: count_over_time({message=~\"error.*\", cluster=~\".*\"}[5m]) > 10\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High error rate detected in cluster {{ $labels.cluster }}\"\n    description: \"More than 10 error messages in the last 5 minutes in cluster {{ $labels.cluster }}.\"\n  ```"
  },
  {
    "query": {
      "message": "error*",
      "pod": "*"
    },
    "description": "This log query pattern **{\"message\": \"error*\", \"pod\": \"*\"}** in Loki is used to identify all log entries from any Kubernetes pod where the message field contains the substring \"error\". Its primary purpose is to detect error events indicating failures or issues within application components running in pods.\n\n**Thresholds and Alerting:**  \nAn alert can be configured to trigger when the count of such error logs exceeds a defined threshold within a specific time window (e.g., more than 5 error logs in 5 minutes). This threshold should be set based on the normal error rate for your environment to minimize false positives.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high count of error logs suggests a degradation or failure in the application or infrastructure, potentially impacting service availability or performance. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates normal operation with no detected error-level issues in pod logs.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching **{\"message\": \"error*\", \"pod\": \"*\"}** grouped by pod name to quickly identify which pods are generating errors.  \n- **Alert Rule (Prometheus-style):**  \n  ```\n  sum by (pod) (count_over_time({message=~\"error.*\", pod=~\".*\"}[5m])) > 5\n  ```  \n  This rule triggers an alert if any pod logs more than 5 error messages in 5 minutes, signaling a potential incident requiring attention."
  },
  {
    "query": {
      "message": "error*",
      "namespace": "*"
    },
    "description": "Namespace error logs matching **{\"message\": \"error*\", \"namespace\": \"*\"}** capture all error-level events across every Kubernetes namespace in Loki. This query helps SREs monitor application failures, resource constraints, and namespace-specific exceptions critical for maintaining multi-tenant stability and resource isolation. Alerts should trigger when error counts exceed a defined threshold (e.g., >50 errors in 5 minutes) indicating potential service degradation or cascading failures. High error rates suggest urgent investigation to prevent outages, while low or zero errors indicate healthy namespace operations.  \nExample alert rule snippet:  \n```\nsum by (namespace) (count_over_time({message=~\"error.*\", namespace=~\".*\"}[5m])) > 50\n```  \nThis can be visualized in dashboards as a time series of error counts per namespace, enabling rapid identification of problematic namespaces requiring immediate attention."
  },
  {
    "query": {
      "message": "exception*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application exception logs captured by **{\"message\": \"exception*\", \"k8s_app\": \"*\"}** in Loki identify unhandled code exceptions, stack traces, and application crashes across all Kubernetes applications. This log stream is essential for detecting runtime errors that degrade application stability and user experience.  \n\n**Purpose:** Enables SREs to monitor and troubleshoot critical application failures by aggregating exception events from all k8s apps in a centralized, queryable format.\n\n**Alert Threshold:** Trigger alerts when the rate of exception logs exceeds a defined threshold, e.g., more than 5 exceptions per minute per application, indicating potential widespread failures or regressions.\n\n**Impact:**  \n- **High values:** Signal increased application instability, potential outages, or bugs requiring immediate investigation.  \n- **Low or zero values:** Indicate stable application behavior with no recent unhandled exceptions.\n\n**Example Usage:**  \n- **Dashboard:** Visualize exception counts over time per k8s_app to identify spikes or trends.  \n- **Alert Rule:**  \n```yaml\nalert: HighExceptionRate  \nexpr: sum by (k8s_app) (rate({message=~\"exception.*\"}[1m])) > 5  \nfor: 5m  \nlabels:  \n  severity: critical  \nannotations:  \n  summary: \"High exception rate detected in {{ $labels.k8s_app }}\"  \n  description: \"More than 5 exceptions per minute observed in {{ $labels.k8s_app }} over the last 5 minutes, indicating potential application instability.\"\n```"
  },
  {
    "query": {
      "message": "exception*",
      "container": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes container where the message field contains the word \"exception,\" typically indicating an unhandled error or stack trace in the application. In Loki, it helps identify runtime failures or critical errors that may affect service stability. An alert based on this pattern should trigger when the count of such exception logs exceeds a defined threshold within a short time window (e.g., more than 5 exceptions in 5 minutes), signaling a potential incident requiring investigation. A high rate of these logs often correlates with increased error rates, degraded user experience, or service outages, while low or zero counts indicate normal operation. For example, an alert rule could use a query like `{container=~\".*\"} |= \"exception\"` and fire if `count_over_time({container=~\".*\"} |= \"exception\"[5m]) > 5`. This query can also be visualized in a dashboard panel showing exception frequency per container over time to quickly spot error spikes."
  },
  {
    "query": {
      "message": "exception*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"exception,\" indicating the occurrence of an unhandled code exception or related error. It also requires the presence of a Kubernetes node identifier in the node_name field. This metric counts the number of such exception-related log entries per node. The unit of measurement is the count of matching log events."
  },
  {
    "query": {
      "message": "exception*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"exception\" and the region field is present with any value. It measures the count of such log entries, indicating occurrences of unhandled exceptions or error stack traces within a specific Kubernetes region. The unit of measurement is the number of log entries matching these criteria per region."
  },
  {
    "query": {
      "message": "exception*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\" and the `service` label identifies the Kubernetes service emitting the log. It is used in Loki to detect occurrences of unhandled exceptions or stack traces within specific services, which often indicate runtime errors or failures in the application.\n\n**Purpose:**  \nTo monitor and alert on unexpected exceptions in application services, enabling rapid identification and investigation of potential issues affecting service stability or functionality.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"exception*\", \"service\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 exception logs per 5 minutes per service). Thresholds may be adjusted based on normal service behavior and error tolerance.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high rate of exception logs suggests increased error rates, potential service degradation, or failures requiring immediate attention.  \n- **Low or zero values:** Indicates normal operation with no detected unhandled exceptions, implying stable service health.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of exception logs per service over time, helping SREs quickly identify which services are experiencing error spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (service) (count_over_time({message=~\"exception.*\"}[5m])) > 5\n  ```  \n  This rule triggers an alert if any service logs more than 5 exceptions in 5 minutes, prompting investigation.\n\nBy monitoring **{\"message\": \"exception*\", \"service\": \"*\"}**, SREs gain actionable insights into application errors, enabling proactive incident response and improved system reliability."
  },
  {
    "query": {
      "message": "exception*",
      "job": "*"
    },
    "description": "This pattern **{\"message\": \"exception*\", \"job\": \"*\"}** in Loki filters log entries where the message contains the term \"exception\"—typically indicating unhandled errors or stack traces—and associates them with a specific Kubernetes job label. It helps SREs quickly identify and monitor runtime failures within individual jobs.\n\n**Purpose:**  \nTo detect and track occurrences of exceptions in application logs per Kubernetes job, enabling early identification of critical errors affecting service reliability.\n\n**Alert Threshold:**  \nTrigger an alert if the count of exception logs exceeds a defined threshold within a set time window (e.g., >5 exceptions in 5 minutes), signaling a potential systemic failure or degradation.\n\n**Impact of Values:**  \n- **High exception counts:** May indicate recurring failures, crashes, or unstable code paths requiring immediate investigation.  \n- **Low or zero exceptions:** Suggest stable operation, but sudden spikes should be treated as anomalies.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of exception logs over time grouped by `job` to spot trends or spikes.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (job) (count_over_time({message=~\"exception.*\", job=~\".*\"}[5m])) > 5\n  ```  \nThis rule alerts when any job logs more than 5 exceptions within 5 minutes, prompting SREs to investigate the affected job."
  },
  {
    "query": {
      "message": "exception*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message contains the word \"exception\" (case-insensitive), indicating that an unhandled code exception or stack trace has occurred within the application. The presence of the Kubernetes label \"environment\" allows filtering and grouping by deployment environment (e.g., staging, production).\n\n**Purpose:**  \nTo identify and monitor runtime exceptions that may indicate application errors or failures requiring investigation.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"exception*\", \"environment\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 exceptions in 5 minutes), signaling a potential issue impacting application stability.\n\n**Impact:**  \n- **High values:** A spike or sustained high rate of exceptions suggests increased error rates, degraded user experience, or potential system instability that needs immediate attention.  \n- **Low or zero values:** Indicates normal operation with no recent unhandled exceptions detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of exception logs over time, grouped by the \"environment\" label to compare error rates across environments.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"(?i)exception.*\", environment=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 exception logs occur within 5 minutes in any environment."
  },
  {
    "query": {
      "message": "exception*",
      "host": "*"
    },
    "description": "This log query pattern **{\"message\": \"exception*\", \"host\": \"*\"}** in Loki is designed to capture all log entries from any Kubernetes host where the message field contains the word \"exception\". Its primary purpose is to identify occurrences of unhandled exceptions or error stack traces in application logs, which often indicate runtime failures or critical issues requiring immediate attention.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 exception logs per 5 minutes per host). This threshold can be adjusted based on the normal error rate of the application.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high number of exception logs suggests increased application instability, potential outages, or critical bugs affecting user experience or system reliability. Immediate investigation and remediation are warranted.  \n- **Low or zero values:** Indicates normal operation with no recent unhandled exceptions detected, implying stable application behavior.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: HighExceptionRate\nexpr: sum by (host) (count_over_time({message=~\"exception.*\", host=~\".*\"}[5m])) > 5\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"High rate of exceptions detected on {{ $labels.host }}\"\n  description: \"More than 5 exception logs have been recorded in the last 5 minutes on host {{ $labels.host }}. Immediate investigation is recommended.\"\n```\n\n**Example Dashboard Panel:**  \nA time series graph showing the count of exception logs per host over time, enabling SREs to quickly identify hosts with rising exception rates and correlate with other system metrics for root cause analysis."
  },
  {
    "query": {
      "message": "exception*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"exception\" and includes any Kubernetes log level, capturing unhandled exceptions or stack traces that indicate errors in the application. In Loki, it helps identify critical failures that may require immediate attention. Alerts should be triggered when the count of such exception logs exceeds a defined threshold within a short time window (e.g., more than 5 exceptions in 5 minutes), signaling a potential systemic issue. A high frequency of these logs typically indicates severe application instability or failure, while low or zero counts suggest normal operation. For example, an alert rule could use a Loki query like `{message=~\"exception.*\", level=~\".*\"}` to count exceptions and fire when the rate surpasses the threshold. In dashboards, this pattern can be visualized as a time series graph showing exception counts over time, enabling SREs to quickly detect and respond to error spikes."
  },
  {
    "query": {
      "message": "exception*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\" and the `env` label identifies the Kubernetes environment. It is used in Loki to detect occurrences of unhandled exceptions or stack traces that may indicate application errors or failures.  \n\n**Purpose:**  \nTo monitor and alert on the frequency of exceptions in a given environment, enabling early detection of runtime errors that could impact service stability.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"exception*\", \"env\": \"<env>\"}** exceeds a defined threshold within a specified time window (e.g., more than 5 exceptions in 5 minutes), signaling a potential issue requiring investigation.\n\n**Impact of Values:**  \n- **High values:** A spike in exception logs suggests increased error rates, potentially leading to degraded service performance or outages. Immediate attention is needed to identify and resolve root causes.  \n- **Low or zero values:** Indicates normal operation with no detected unhandled exceptions, implying stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of exception logs per environment over time, helping SREs visualize error trends and correlate with deployments or incidents.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (env) (count_over_time({message=~\"exception.*\", env=~\".*\"}[5m])) > 5\n  ```  \n  This rule triggers an alert if more than 5 exception logs occur in any environment within 5 minutes."
  },
  {
    "query": {
      "message": "exception*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"exception\" and the app field is any value. It identifies occurrences of unhandled exceptions or error stack traces within applications labeled by the Kubernetes app identifier. The measurement is a count of such log entries, representing the frequency of exception events per application."
  },
  {
    "query": {
      "message": "exception*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\" and the entry is tagged with a Kubernetes `node` label. It is used in Loki to identify occurrences of unhandled exceptions or error stack traces originating from specific nodes in the cluster. Monitoring the frequency of these logs helps SREs detect runtime errors that may indicate application instability or failures on particular nodes.\n\n**Alert threshold:** Trigger an alert if the count of logs matching this pattern exceeds a defined threshold (e.g., more than 5 exceptions per node within 5 minutes), signaling a potential issue requiring investigation.\n\n**Impact:**  \n- **High values:** A spike in exception logs suggests increased application errors, which can lead to degraded service reliability or outages on affected nodes. Immediate attention is needed to prevent user impact.  \n- **Low or zero values:** Indicates normal operation with no recent unhandled exceptions detected.\n\n**Example usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"exception.*\", node=~\".*\"}[5m]) by (node)\n```  \nThis query counts exception logs per node over the last 5 minutes. An alert can be configured to fire when this count exceeds the threshold, enabling targeted troubleshooting on nodes exhibiting abnormal error rates."
  },
  {
    "query": {
      "message": "exception*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\"—typically indicating an unhandled error, stack trace, or failure in the application—and the log is labeled with the Kubernetes `app_kubernetes_io/name` to identify the specific application emitting the exception. In Loki, this filter helps isolate error conditions per application for monitoring and alerting.\n\n**Purpose:**  \nTo detect and track occurrences of exceptions within individual Kubernetes applications, enabling rapid identification of failing services or code issues.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., >5 exceptions in 5 minutes). Thresholds may vary by application criticality and baseline error rates.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or recurring exceptions, suggesting potential application instability, bugs, or degraded service health requiring immediate investigation.  \n- **Low or zero values:** Suggest normal operation with no recent unhandled exceptions detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of exception logs per application (`app_kubernetes_io/name`), enabling SREs to spot spikes or trends.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"exception.*\", app_kubernetes_io/name=~\".+\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 exception logs occur for any application within 5 minutes.\n\nBy monitoring this pattern, SREs can proactively detect and respond to application errors before they escalate into outages."
  },
  {
    "query": {
      "message": "exception*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\" and the `instance` label identifies the Kubernetes instance emitting the log. It is used in Loki to detect occurrences of unhandled exceptions or error stack traces within application logs, which often indicate runtime failures or critical issues.\n\n**Purpose:**  \nTo monitor and alert on the frequency of exceptions per instance, enabling rapid identification of problematic services or pods.\n\n**Alert Threshold:**  \nA typical alert might trigger if the count of logs matching `{\"message\": \"exception*\", \"instance\": \"*\"}` exceeds a threshold such as 5 exceptions within 5 minutes for a single instance. This threshold should be adjusted based on normal application behavior and error tolerance.\n\n**Impact of Values:**  \n- **High values:** A spike in exception logs suggests increased application errors, potentially leading to degraded service, failed requests, or outages. Immediate investigation is warranted.  \n- **Low or zero values:** Indicates stable application behavior with no recent unhandled exceptions detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of exception logs per instance over time, helping SREs spot trends or sudden error bursts.  \n- **Alert Rule (PromQL for Loki):**  \n  ```\n  count_over_time({message=~\"exception.*\", instance=~\".*\"}[5m]) > 5\n  ```  \n  This rule fires if more than 5 exception logs occur in 5 minutes on any instance, triggering an alert for investigation."
  },
  {
    "query": {
      "message": "exception*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"exception\" and the filename field contains any value. It identifies occurrences of unhandled code exceptions or stack traces recorded in Kubernetes logs. Each matched entry represents one instance of such an exception event. The measurement unit is the count of these exception log entries."
  },
  {
    "query": {
      "message": "exception*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\" and the `application` label is set, typically indicating that the application has encountered an unhandled exception or error stack trace. In Loki, this filter helps identify runtime errors that may affect application stability or user experience.\n\n**Purpose:**  \nTo surface critical error events by capturing logs that include exceptions, enabling SREs to monitor application health and detect failures early.\n\n**Alert Threshold Guidance:**  \n- A common alert threshold might be when the count of exception logs exceeds a baseline rate (e.g., more than 5 exceptions per minute) within a given application, signaling a potential incident.  \n- Thresholds should be tuned per application based on normal error rates to reduce noise.\n\n**Impact of Values:**  \n- **High values:** A spike in exception logs often indicates a degradation in service quality, increased error rates, or potential outages requiring immediate investigation.  \n- **Low or zero values:** Normal operation with no recent unhandled exceptions detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of logs matching `{message=~\"exception.*\", application=~\".*\"}` grouped by `application` to quickly identify which services are experiencing errors.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (application) (rate({message=~\"exception.*\", application=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any application logs more than 5 exceptions per 5 minutes, prompting SREs to investigate.\n\nBy monitoring this pattern, SREs gain actionable insights into application errors and can respond proactively to maintain system reliability."
  },
  {
    "query": {
      "message": "exception*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\" and includes any Kubernetes `component` label. It is used in Loki to identify occurrences of unhandled exceptions or stack traces within specific components of your cluster, signaling potential runtime errors or failures.  \n\n**Purpose:** Enables detection and monitoring of error conditions by capturing logs indicative of exceptions across all components, helping SREs quickly pinpoint problematic services.\n\n**Alert Threshold:** An alert should trigger when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 exception logs per component in 5 minutes), indicating an abnormal spike in errors.\n\n**Impact:**  \n- **High values:** Suggest frequent or recurring exceptions, potentially leading to degraded service reliability, increased error rates, or outages requiring immediate investigation.  \n- **Low or zero values:** Indicate normal operation with no recent unhandled exceptions detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing a time series graph of exception log counts per component, enabling trend analysis and rapid identification of error spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"exception.*\", component=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 exception logs occur in any component within 5 minutes."
  },
  {
    "query": {
      "message": "exception*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"exception\" and the source field contains any value. It identifies occurrences of unhandled exceptions or error stack traces in application logs. Each match represents one log event containing an exception message, labeled by its Kubernetes source. The measurement unit is the count of such exception log entries."
  },
  {
    "query": {
      "message": "exception*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"exception,\" indicating the occurrence of an unhandled error or stack trace in the application. Each matched log entry is associated with a specific Kubernetes pod, identified by the pod_name label. This metric counts the number of such exception-related log entries per pod. The unit of measurement is the count of exception log occurrences."
  },
  {
    "query": {
      "message": "exception*",
      "container_name": "*"
    },
    "description": "This log query pattern **{\"message\": \"exception*\", \"container_name\": \"*\"}** is used in Loki to identify log entries where the message contains the term \"exception,\" typically indicating unhandled exceptions or error stack traces within any Kubernetes container. Its purpose is to surface runtime errors that may impact application stability or availability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of such exception logs exceeds a defined threshold within a given time window (e.g., more than 5 exception logs in 5 minutes), signaling a potential issue requiring investigation.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high rate of exception logs often indicates critical application failures, bugs, or infrastructure problems that could degrade service performance or cause outages. Immediate attention is recommended.  \n- **Low or zero values:** Normal operation with no or minimal exceptions logged, indicating stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of exception logs per container over time to quickly identify which services are experiencing errors.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({container_name=~\".*\", message=~\"exception.*\"}[5m]) > 5\n  ```  \nThis rule triggers an alert if more than 5 exception logs occur in any container within 5 minutes, enabling proactive incident response."
  },
  {
    "query": {
      "message": "exception*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\"—typically indicating an unhandled error, stack trace, or failure in application code—and includes the `cluster` label identifying the Kubernetes cluster source. In Loki, this filter helps SREs monitor and quantify runtime exceptions across clusters.\n\n**Purpose:**  \nTo detect and track occurrences of exceptions that may signal application instability or failures requiring investigation.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of exception logs exceeds a defined threshold within a given time window (e.g., more than 10 exceptions per 5 minutes per cluster), indicating a potential systemic issue.\n\n**Impact of Values:**  \n- **High values:** A spike in exception logs suggests increased error rates, possibly impacting service reliability and user experience. Immediate investigation is warranted.  \n- **Low or zero values:** Indicates stable application behavior with no recent unhandled exceptions detected.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of exception logs over time per cluster using a query like:  \n  `count_over_time({message=~\"exception.*\", cluster=~\".*\"}[5m]) by (cluster)`  \n- **Alert Rule:** Trigger an alert if exceptions exceed 10 in 5 minutes for any cluster:  \n  ```\n  alert: HighExceptionRate\n  expr: count_over_time({message=~\"exception.*\", cluster=~\".*\"}[5m]) > 10\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High exception rate detected in cluster {{ $labels.cluster }}\"\n    description: \"More than 10 exceptions logged in the last 5 minutes in cluster {{ $labels.cluster }}. Immediate investigation recommended.\"\n  ```"
  },
  {
    "query": {
      "message": "exception*",
      "pod": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod where the message contains the word \"exception,\" typically indicating an unhandled code exception or stack trace. In Loki, it helps identify runtime errors that may affect application stability or availability. \n\n**Purpose:** To surface critical errors that require immediate investigation by capturing exceptions across all pods.\n\n**Alert Threshold:** Trigger an alert if the count of exception logs exceeds a defined threshold (e.g., more than 5 exceptions within 5 minutes per pod), signaling a potential incident.\n\n**Impact:**  \n- **High values:** A spike in exceptions often indicates systemic issues such as application crashes, degraded service, or failing dependencies, potentially leading to outages or degraded user experience.  \n- **Low values:** Occasional exceptions may be normal, but persistent low-level exceptions should still be monitored for emerging problems.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of exception logs per pod over time using the query:  \n  `count_over_time({message=~\"exception.*\", pod=~\".*\"}[5m]) by (pod)`  \n- **Alert Rule:**  \n  ```\n  alert: HighExceptionRate\n  expr: count_over_time({message=~\"exception.*\", pod=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High exception rate detected in pod {{ $labels.pod }}\"\n    description: \"More than 5 exceptions logged in pod {{ $labels.pod }} within the last 5 minutes.\"\n  ```"
  },
  {
    "query": {
      "message": "exception*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"exception\" (case-insensitive), indicating that an unhandled code exception or error stack trace has occurred within a Kubernetes namespace identified by the `namespace` label. In Loki, this helps surface runtime errors that may impact application stability or availability.\n\n**Purpose:**  \nTo detect and monitor occurrences of exceptions across all namespaces, enabling rapid identification of failing services or components.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"exception*\", \"namespace\": \"*\"}** exceeds a defined threshold within a short time window (e.g., >5 exceptions in 5 minutes per namespace), signaling a potential incident requiring investigation.\n\n**Impact of Values:**  \n- **High values:** A spike in exception logs often indicates critical application issues such as crashes, failed transactions, or degraded service health, potentially affecting user experience or system reliability.  \n- **Low or zero values:** Normal operation with no detected unhandled exceptions, implying stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of exception logs per namespace over the last hour, helping SREs quickly identify namespaces with rising error rates.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (namespace) (count_over_time({message=~\"(?i).*exception.*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any namespace logs more than 5 exceptions in 5 minutes, prompting immediate investigation."
  },
  {
    "query": {
      "message": "timeout*",
      "k8s_app": "*"
    },
    "description": "Kubernetes timeout log entries across all applications, identified by logs where the message starts with \"timeout\" and tagged by k8s_app. This query helps SREs monitor request timeouts and service delays that can indicate performance bottlenecks or failures impacting user experience and SLA adherence. Alerts should trigger when timeout log frequency exceeds a defined threshold (e.g., more than 5 timeouts per minute per app), signaling potential service degradation or outages. High timeout rates often correlate with increased latency, failed requests, and customer impact, while low or zero values indicate healthy service responsiveness. Example alert rule: `count_over_time({message=~\"timeout.*\", k8s_app=~\".*\"}[1m]) > 5` triggers a critical alert if more than 5 timeout logs occur within one minute. This query can also be visualized in dashboards as a time series graph showing timeout counts per app to quickly identify trends and troubleshoot issues."
  },
  {
    "query": {
      "message": "timeout*",
      "container": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes container where the message contains the word \"timeout,\" indicating that a request or service operation exceeded its allowed time limit. In Loki, **{\"message\": \"timeout*\", \"container\": \"*\"}** helps identify potential performance bottlenecks or connectivity issues causing delays or failures.\n\n**Alert threshold:** An alert is typically triggered when the count of timeout log entries exceeds a defined threshold within a specific time window (e.g., more than 5 timeout messages in 5 minutes), signaling a systemic issue affecting service reliability.\n\n**Impact of values:**  \n- **High values:** A spike in timeout logs suggests degraded service responsiveness, possible network issues, or overloaded components, potentially leading to user-facing errors or cascading failures.  \n- **Low or zero values:** Indicates normal operation with timely responses and no detected delays.\n\n**Example alert rule snippet:**  \n```\nsum by (container) (count_over_time({container=~\".*\"} |= \"timeout\" [5m])) > 5\n```\nThis triggers an alert if any container logs more than 5 timeout messages in 5 minutes.\n\n**Example dashboard usage:**  \nVisualize the rate of timeout messages per container over time to quickly identify which services are experiencing latency issues and correlate with other metrics like CPU or network usage for root cause analysis."
  },
  {
    "query": {
      "message": "timeout*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"timeout\" and includes the Kubernetes `node_name` label, indicating that a request or service operation on that specific node exceeded its allowed time limit. In Loki, this helps identify nodes experiencing latency or connectivity issues causing timeouts.\n\n**Purpose:**  \nTo monitor and surface timeout-related errors per node, enabling SREs to quickly pinpoint problematic nodes affecting application reliability or performance.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of timeout messages for a given `node_name` exceeds a defined threshold within a set time window—for example, more than 5 timeout logs per node within 5 minutes—indicating a potential service degradation or network issue.\n\n**Impact of Values:**  \n- **High values:** Suggest persistent or widespread timeout issues on the node, potentially leading to degraded service availability or user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate normal operation with no recent timeout errors detected on the node.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"timeout.*\", node_name=~\".*\"}[5m]) by (node_name)\n```\nThis query counts timeout occurrences per node over the last 5 minutes. An alert can be configured to fire if this count exceeds the threshold, helping SREs proactively address node-specific timeout issues."
  },
  {
    "query": {
      "message": "timeout*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"timeout\" and the `region` label is set, typically indicating that a request or service operation exceeded its allowed time limit within a specific Kubernetes region. In Loki, this helps identify timeout-related issues scoped by region, enabling targeted troubleshooting.\n\n**Purpose:** To detect and monitor timeout errors across different Kubernetes regions, facilitating rapid identification of performance bottlenecks or service disruptions.\n\n**Alert Threshold:** An alert should trigger when the count of timeout log entries exceeds a defined threshold within a given time window (e.g., more than 5 timeout messages per 5 minutes per region), signaling potential systemic issues affecting service availability or responsiveness.\n\n**Impact:**  \n- **High values:** A surge in timeout logs suggests degraded service performance, possible network issues, or overloaded components in the affected region, requiring immediate investigation.  \n- **Low or zero values:** Indicates normal operation with no recent timeout errors, implying stable service behavior.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of timeout messages per region over time using a Loki query like:  \n  `count_over_time({message=~\"timeout.*\", region=~\".+\"}[5m]) by (region)`  \n- **Alert Rule:** Trigger an alert if timeout count exceeds 5 in 5 minutes for any region:  \n  ```\n  alert: HighTimeoutRate\n  expr: count_over_time({message=~\"timeout.*\", region=~\".+\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High timeout rate detected in region {{ $labels.region }}\"\n    description: \"More than 5 timeout errors occurred in the last 5 minutes in region {{ $labels.region }}.\"\n  ```"
  },
  {
    "query": {
      "message": "timeout*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the message includes the word \"timeout\" and the entry is associated with any Kubernetes service. It identifies events indicating that a request or operation exceeded the allowed time limit. The measured value is the occurrence count of such timeout events per service. Units are counts of timeout incidents within the log data."
  },
  {
    "query": {
      "message": "timeout*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"timeout\" (e.g., \"timeout connection failed\") and includes any Kubernetes `job` label. It is used in Loki to identify timeout-related errors originating from specific Kubernetes jobs, which often indicate that requests or services are exceeding their allowed time limits.  \n\n**Purpose:**  \nTo detect and monitor timeout errors across different Kubernetes jobs, enabling SREs to quickly identify performance bottlenecks or service disruptions caused by slow or unresponsive components.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of timeout messages for a given job exceeds a defined threshold within a specific time window—for example, more than 5 timeout errors in 5 minutes. Thresholds can be adjusted based on service criticality and normal error rates.\n\n**Impact of Values:**  \n- **High counts:** Indicate persistent or widespread timeout issues, potentially causing degraded service availability or user experience. Immediate investigation and remediation are required.  \n- **Low counts:** May represent transient network glitches or isolated incidents, often not critical but worth monitoring for trends.\n\n**Example Usage:**  \nIn a Loki alert rule, you might use a query like:  \n```\ncount_over_time({job=~\".*\", message=~\"timeout.*\"}[5m]) > 5\n```\nThis triggers an alert if more than 5 timeout messages occur within 5 minutes for any job.  \n\nIn a dashboard, you can graph the rate of timeout messages per job over time to visualize trends and quickly spot anomalies."
  },
  {
    "query": {
      "message": "timeout*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"timeout\" and includes any Kubernetes environment label. It identifies events indicating that a request or service operation exceeded its allowed time limit. The measurement reflects occurrences of timeout events, counted as discrete log entries. The environment field specifies the Kubernetes deployment context where the timeout occurred."
  },
  {
    "query": {
      "message": "timeout*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"timeout\" and the `host` label is present, typically indicating that a request or service operation exceeded its allowed time limit on a specific Kubernetes host. In Loki, this helps identify timeout-related failures that may impact service availability or performance.\n\n**Purpose:** To detect and monitor timeout errors across hosts, enabling timely investigation of latency or connectivity issues.\n\n**Alert Threshold:** An alert should trigger when the count of timeout log entries exceeds a defined threshold within a given time window—for example, more than 5 timeout messages per host within 5 minutes—indicating a potential systemic problem.\n\n**Impact:**  \n- **High values:** Suggest persistent or widespread timeout issues, possibly causing degraded user experience, failed requests, or cascading failures. Immediate investigation and remediation are required.  \n- **Low values:** May represent occasional, transient timeouts that are less critical but should still be monitored for trends.\n\n**Example Alert Rule (PromQL for Loki):**  \n```\ncount_over_time({message=~\"timeout.*\", host=~\".+\"}[5m]) > 5\n```\nThis triggers if more than 5 timeout messages occur on any host within 5 minutes.\n\n**Example Dashboard Usage:**  \nA panel showing the rate of timeout messages per host over time, helping SREs quickly identify which hosts are experiencing elevated timeout errors and correlate with other metrics like latency or error rates."
  },
  {
    "query": {
      "message": "timeout*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"timeout\" and includes any Kubernetes log level, helping identify requests or services that exceeded their allowed time limits. In Loki, it serves to detect potential performance bottlenecks or connectivity issues caused by timeouts. Alerts should be triggered when the count of such timeout logs exceeds a defined threshold within a specific time window (e.g., more than 5 timeout occurrences in 5 minutes), indicating a systemic problem affecting service reliability. A high volume of timeout logs typically signals degraded user experience or failing dependencies, while low or zero counts suggest normal operation. For example, an alert rule in Prometheus Alertmanager using this pattern might be:\n\n```\nsum by (level) (count_over_time({message=~\"timeout.*\", level=~\".*\"}[5m])) > 5\n```\n\nThis can also be visualized in a dashboard panel showing the rate of timeout messages over time, enabling SREs to quickly identify and respond to emerging timeout issues."
  },
  {
    "query": {
      "message": "timeout*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"timeout\" and includes any environment label in the Kubernetes 'env' field. It identifies events indicating that a request or service operation exceeded its allowed time limit. The measurement is the occurrence count of such timeout events, with no specific time unit involved. Example: message='timeout connection failed', env='production'."
  },
  {
    "query": {
      "message": "timeout*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"timeout\" and the app field is present with any value. It identifies events indicating that a request or service operation exceeded its allowed time limit. The pattern does not measure duration but flags occurrences of timeout-related messages. Example: message=\"timeout connection failed\", app=\"example-app\"."
  },
  {
    "query": {
      "message": "timeout*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"timeout\" and the entry is associated with a specific Kubernetes `node`. It is used in Loki to identify occurrences of timeout errors indicating that requests or services on that node have exceeded their allowed time limits, which can signal network issues, resource contention, or service degradation.\n\n**Purpose:**  \nTo monitor and detect timeout-related failures per node, enabling SREs to quickly identify nodes experiencing performance bottlenecks or connectivity problems.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of timeout messages for a given node exceeds a defined threshold within a specific time window (e.g., more than 5 timeout errors in 5 minutes). Thresholds may vary based on service criticality and baseline error rates.\n\n**Impact of Values:**  \n- **High counts:** Indicate persistent or widespread timeout issues on the node, potentially leading to degraded service availability or increased latency. Immediate investigation and remediation are required.  \n- **Low counts:** May represent transient or isolated incidents, often not critical but worth monitoring for trends.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the number of timeout messages per node over the last hour, enabling quick identification of nodes with rising timeout errors.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\"timeout.*\", node=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 timeout messages occur on any node within 5 minutes.\n\nBy leveraging this pattern, SREs can proactively detect and respond to timeout-related issues at the node level, improving system reliability and user experience."
  },
  {
    "query": {
      "message": "timeout*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"timeout\" and the \"app_kubernetes_io/name\" label is present with any value. It identifies events indicating that a request or operation exceeded the allowed time limit. The measurement reflects occurrences of timeout events but does not specify a unit of time. This helps monitor services labeled by Kubernetes app names for timeout-related issues."
  },
  {
    "query": {
      "message": "timeout*",
      "instance": "*"
    },
    "description": "This log pattern **{\"message\": \"timeout*\", \"instance\": \"*\"}** is used in Loki to identify occurrences where a service or request has exceeded its allowed time limit, indicated by log messages containing the word \"timeout\". The `instance` label helps pinpoint which Kubernetes pod or node generated the timeout, enabling targeted troubleshooting.\n\n**Purpose:**  \nTo detect and monitor timeout errors that may indicate performance bottlenecks, network issues, or service unavailability within specific instances.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of timeout log entries exceeds a defined threshold within a given time window—for example, more than 5 timeout messages per instance within 5 minutes—signaling a potential service degradation.\n\n**Impact of Values:**  \n- **High timeout counts:** Suggest persistent or widespread latency issues, potentially causing request failures, degraded user experience, or cascading service outages. Immediate investigation and remediation are required.  \n- **Low or zero timeout counts:** Indicate normal operation with timely responses and healthy service performance.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of timeout messages per instance over time, helping SREs quickly identify which pods or nodes are experiencing timeouts.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (instance) (rate({message=~\"timeout.*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any instance logs more than 5 timeout messages in 5 minutes.\n\nBy monitoring this pattern, SREs can proactively detect and respond to latency and availability issues at the instance level."
  },
  {
    "query": {
      "message": "timeout*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"timeout\" and the log is associated with any Kubernetes `filename`. It is used in Loki to identify occurrences of timeout errors, which typically indicate that a request or service operation exceeded its allowed time limit, potentially causing degraded performance or failures.\n\n**Purpose:**  \nTo detect and monitor timeout-related issues in Kubernetes workloads by filtering logs that report timeout events.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of timeout messages exceeds a defined threshold within a given time window (e.g., more than 5 timeout messages in 5 minutes), signaling a potential systemic issue affecting service reliability.\n\n**Impact of Values:**  \n- **High counts:** Suggest frequent timeouts, indicating possible network issues, overloaded services, or misconfigured timeouts that could lead to service degradation or outages. Immediate investigation is recommended.  \n- **Low counts or zero:** Indicate normal operation with no recent timeout errors, implying stable service performance.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching `{\"message\": \"timeout*\", \"filename\": \"*\"}` over time, helping visualize spikes in timeout errors.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\"timeout.*\", filename=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 timeout messages occur within 5 minutes.\n\nBy monitoring this pattern, SREs can proactively detect and respond to timeout-related issues impacting Kubernetes services."
  },
  {
    "query": {
      "message": "timeout*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"timeout\" and the `application` label is set, typically indicating that a request or service operation exceeded its allowed time limit within a specific Kubernetes application. In Loki, this helps identify timeout-related issues per application, which are critical for diagnosing performance bottlenecks or service unavailability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the rate of timeout messages exceeds a defined threshold, for example, more than 5 timeout occurrences per minute per application. This threshold can be adjusted based on normal traffic patterns and service-level objectives (SLOs).\n\n**Impact of Threshold Values:**  \n- **High threshold:** May delay detection of emerging timeout issues, potentially increasing user impact before alerting.  \n- **Low threshold:** May cause frequent alerts due to transient or insignificant timeouts, leading to alert fatigue.\n\n**Example Alert Rule (PromQL for Loki):**  \n```promql\nsum by (application) (rate({message=~\"timeout.*\", application=~\".+\"}[1m])) > 5\n```\nThis rule triggers an alert if any application logs more than 5 timeout messages per minute.\n\n**Dashboard Usage:**  \nVisualize the per-application timeout rate over time to quickly identify which applications are experiencing increased timeout errors, enabling targeted investigation and remediation."
  },
  {
    "query": {
      "message": "timeout*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"timeout\" and includes any Kubernetes component label, helping identify requests or services that exceeded their allowed time limits. In Loki, it is used to monitor and alert on timeout occurrences across different components. An alert threshold might be set when the count of timeout messages exceeds a defined rate (e.g., more than 5 timeouts per minute), indicating potential service degradation or connectivity issues. High timeout rates often signal performance bottlenecks or network problems, while low or zero timeouts suggest healthy service responsiveness. For example, an alert rule could use a query like `count_over_time({component=~\".*\"} |= \"timeout\" [1m]) > 5` to trigger notifications, and a dashboard panel might display the rate of timeout messages per component over time to help SREs quickly identify and troubleshoot affected services."
  },
  {
    "query": {
      "message": "timeout*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"timeout\" and the source field contains any value. It identifies events indicating that a request or service operation exceeded its allowed time limit. The measurement is the occurrence count of such timeout events, recorded as discrete log entries. No specific time unit is measured; the pattern only detects the presence of timeout-related messages."
  },
  {
    "query": {
      "message": "timeout*",
      "pod_name": "*"
    },
    "description": "This query filters log entries in Loki where the log message begins with the term \"timeout\" and includes a Kubernetes pod label \"pod_name\". It returns logs indicating timeout-related events originating from specific pods, typically reflecting network delays, service unavailability, or request processing exceeding predefined time limits. The data source is Kubernetes pod logs ingested into Loki, with timestamps and metadata labels such as \"pod_name\". Timeout occurrences are measured as discrete log events without explicit units but represent incidents where operations failed to complete within expected durations. An unusually high frequency or sudden spike in these timeout logs for a given pod may indicate systemic issues like network instability, resource exhaustion, or service degradation, warranting alerting to enable timely investigation and remediation."
  },
  {
    "query": {
      "message": "timeout*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"timeout,\" indicating that a request or service operation exceeded its allowed time limit. It specifically filters logs associated with any Kubernetes container, identified by the presence of a container_name label. The pattern measures occurrences of timeout events but does not quantify duration or units of time. Its purpose is to detect and isolate timeout-related errors within containerized environments."
  },
  {
    "query": {
      "message": "timeout*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"timeout\" and the `cluster` label identifies the Kubernetes cluster emitting the log. It is used in Loki to detect occurrences of timeout errors indicating that requests or services exceeded their allowed time limits, which can signal performance degradation or service unavailability.\n\n**Purpose:**  \nTo monitor and alert on timeout events across Kubernetes clusters, enabling SREs to quickly identify and investigate latency or connectivity issues affecting system reliability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of timeout log entries exceeds a defined threshold within a specific time window (e.g., more than 5 timeout messages in 5 minutes per cluster). Thresholds may be adjusted based on baseline traffic and service criticality.\n\n**Impact:**  \n- **High values:** Indicate frequent timeouts, suggesting potential service outages, network issues, or resource exhaustion that require immediate investigation.  \n- **Low or zero values:** Indicate normal operation with no detected timeout errors.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({message=~\"timeout.*\", cluster=~\".*\"}[5m])\n```\nto count timeout occurrences per cluster over the last 5 minutes. An alert rule could trigger if this count exceeds 5, signaling a spike in timeout errors that warrants SRE attention."
  },
  {
    "query": {
      "message": "timeout*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"timeout\" and the pod label is present. It identifies occurrences of timeout events related to Kubernetes pods, indicating that a request or operation exceeded the allowed time limit. The measurement is a count of such timeout events per pod, with no specific time unit attached. Example: message=\"timeout connection failed\", pod=\"example-pod\"."
  },
  {
    "query": {
      "message": "timeout*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"timeout\" and includes any following text. It specifically captures events indicating that a request or service operation exceeded the allowed time limit. The \"namespace\" field identifies the Kubernetes namespace associated with the log entry. This pattern measures occurrences of timeout-related events but does not quantify duration or units of time."
  },
  {
    "query": {
      "message": "500*",
      "k8s_app": "*"
    },
    "description": "Kubernetes logs filtered by **{\"message\": \"500*\", \"k8s_app\": \"*\"}** capture HTTP 500 Internal Server Error responses from all Kubernetes applications. This query helps SREs identify server-side failures indicating issues such as application crashes, misconfigurations, or resource exhaustion. An alert should trigger when the rate of 500 errors exceeds a defined threshold (e.g., more than 5 errors per minute), signaling a potential outage or degraded service. High values indicate critical application instability requiring immediate investigation, while low or zero values suggest healthy application behavior. Example usage: create a dashboard panel showing the count of 500 errors per app over time, or define an alert rule that fires if the 500 error rate surpasses the threshold for 5 consecutive minutes, enabling proactive incident response."
  },
  {
    "query": {
      "message": "500*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500-series server errors, and the `container` field is present, typically representing Kubernetes container logs. It helps SREs identify internal server errors originating from specific containers. An alert threshold might be set to trigger when the count of such log entries exceeds a defined rate (e.g., more than 5 errors per minute), signaling potential service instability or failure. High values suggest frequent server errors requiring immediate investigation, while low or zero values indicate normal operation. For example, in a Loki alert rule, you could use a query like `{container=~\".*\"} |= \"500\"` and alert if `count_over_time({container=~\".*\"} |= \"500\"[1m]) > 5`. This can also be visualized in a dashboard panel showing the rate of 500 errors per container over time to quickly spot problematic services."
  },
  {
    "query": {
      "message": "500*",
      "node_name": "*"
    },
    "description": "This query pattern **{\"message\": \"500*\", \"node_name\": \"*\"}** in Loki filters log entries where the `message` field starts with the string \"500\"—typically indicating HTTP 500-series server error responses—and includes any value for the `node_name` label, which identifies the Kubernetes node emitting the log. The data source is Kubernetes pod logs ingested into Loki, with `node_name` representing the node hostname or identifier. This pattern is used to isolate server error logs per node for troubleshooting and monitoring purposes. Metrics derived from these logs (e.g., error count per node over time) are typically unitless counts of error occurrences. An unusually high or sudden increase in the frequency of these 500-prefixed messages on a specific node should trigger an alert, as it may indicate server instability, application failures, or infrastructure issues requiring immediate investigation."
  },
  {
    "query": {
      "message": "500*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500 Internal Server Errors, and includes any Kubernetes `region` label. It is used in Loki to identify and aggregate server error occurrences across different regions. An alert should be triggered when the rate of these 500 errors exceeds a defined threshold (e.g., more than 5 errors per minute per region), signaling potential service instability or outages. High values indicate increased failure rates impacting user experience and system reliability, while low values suggest normal operation. For example, in a Grafana dashboard, you might use this pattern to create a panel showing the count of 500 errors grouped by `region` over time, or define an alert rule that fires when the error count surpasses the threshold in any region, enabling rapid incident response."
  },
  {
    "query": {
      "message": "500*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500 Internal Server Errors, and includes any Kubernetes `service` label. It is used in Loki to identify backend service failures that may impact application availability or user experience.  \n\n**Purpose:** Detect spikes or sustained occurrences of HTTP 500 errors per service to quickly identify and troubleshoot internal server issues.\n\n**Alert Threshold:** An alert could be triggered when the rate of logs matching this pattern exceeds a defined threshold, for example, more than 5 errors per minute per service, indicating a potential service degradation or outage.\n\n**Impact:**  \n- **High values:** A high or increasing count of 500 errors suggests critical backend failures affecting users, requiring immediate investigation.  \n- **Low or zero values:** Indicates normal operation with no internal server errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching `{\"message\": \"500*\", \"service\": \"*\"}` grouped by `service` over the last 5 minutes to monitor error trends.  \n- **Alert Rule:**  \n  ```\n  sum by (service) (rate({message=~\"500.*\", service=~\".*\"}[1m])) > 5\n  ```  \n  This triggers an alert if any service logs more than 5 HTTP 500 errors per minute."
  },
  {
    "query": {
      "message": "500*",
      "job": "*"
    },
    "description": "This log pattern **{\"message\": \"500*\", \"job\": \"*\"}** in Loki matches log entries where the message starts with \"500\", typically indicating HTTP 500 Internal Server Errors, and includes any Kubernetes job label. It helps SREs monitor backend service failures by filtering error responses tied to specific jobs.  \n\n**Purpose:** To identify and track occurrences of internal server errors across different Kubernetes jobs, enabling targeted troubleshooting and reliability improvements.\n\n**Alert Threshold:** An alert should be triggered when the rate of 500 errors exceeds a defined threshold, for example, more than 5 errors per minute per job, indicating a potential service degradation or outage.\n\n**Impact:**  \n- **High values:** A spike in 500 errors suggests backend instability, increased failure rates, or systemic issues affecting user experience and service availability. Immediate investigation is required.  \n- **Low or zero values:** Indicates stable service operation with minimal internal server errors, reflecting healthy backend performance.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({job=~\".*\", message=~\"500.*\"}[1m]) > 5\n```\nThis query counts the number of 500 errors per job over the last minute and triggers an alert if the count exceeds 5, helping SREs quickly detect and respond to backend failures."
  },
  {
    "query": {
      "message": "500*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500-series server errors, and includes any Kubernetes `environment` label. It is used in Loki to identify internal server errors occurring in specific deployment environments.  \n\n**Purpose:** To monitor and alert on the frequency of HTTP 500 errors, which typically signify server-side issues affecting application availability or stability.\n\n**Alert Threshold:** An alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 occurrences in 5 minutes), signaling a potential incident requiring investigation.\n\n**Impact:**  \n- **High values:** Indicate a spike in server errors, potentially causing degraded service or outages, requiring immediate SRE attention.  \n- **Low or zero values:** Suggest normal operation with no recent internal server errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching `{\"message\": \"500*\", \"environment\": \"*\"}` grouped by `environment` to identify which environment is experiencing errors.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"500.*\", environment=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 HTTP 500 errors occur in any environment within 5 minutes."
  },
  {
    "query": {
      "message": "500*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500-level server errors, and includes the `host` label from Kubernetes to identify the source node. In Loki, this helps isolate internal server error occurrences per host for monitoring and troubleshooting.\n\n**Purpose:** Detect and track internal server errors (HTTP 500) across hosts to quickly identify failing services or infrastructure issues.\n\n**Alert Threshold:** An alert could be triggered when the count of logs matching this pattern exceeds a defined threshold within a time window, for example, more than 10 occurrences in 5 minutes per host, signaling a potential service degradation.\n\n**Impact:**  \n- **High values:** Indicate frequent server errors, potentially causing service outages or degraded user experience, requiring immediate investigation.  \n- **Low or zero values:** Suggest normal operation with no recent internal server errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of `500*` messages grouped by `host` over time, highlighting hosts with rising error rates.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"500.*\", host=~\".*\"}[5m]) > 10\n  ```\n  This triggers an alert if more than 10 HTTP 500 errors occur on any host within 5 minutes."
  },
  {
    "query": {
      "message": "500*",
      "level": "*"
    },
    "description": "This log pattern **{\"message\": \"500*\", \"level\": \"*\"}** is used in Loki to identify log entries where the message starts with \"500\", typically indicating HTTP 500 Internal Server Errors, regardless of the Kubernetes log level. It helps SREs monitor backend service failures that may impact application availability or user experience.\n\n**Purpose:**  \nDetect occurrences of server-side errors (HTTP 500) in application logs to quickly identify and respond to internal failures.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a specific time window (e.g., more than 5 occurrences in 5 minutes), signaling a potential service degradation or outage.\n\n**Impact of Values:**  \n- **High values:** A spike in 500 errors suggests critical backend issues, requiring immediate investigation to prevent downtime or data loss.  \n- **Low or zero values:** Indicates normal operation with no detected internal server errors.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\n{message=~\"500.*\", level=~\".*\"} | count_over_time({message=~\"500.*\", level=~\".*\"}[5m])\n```\nSet an alert to fire if the count exceeds 5 within 5 minutes, enabling proactive incident response."
  },
  {
    "query": {
      "message": "500*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500-level server errors, and includes any Kubernetes environment label (`env`). It helps SREs monitor internal server errors across all environments. An alert should be triggered when the rate of these 500 errors exceeds a defined threshold (e.g., more than 5 errors per minute), signaling potential service instability or outages. High values indicate frequent server failures requiring immediate investigation, while low values suggest normal operation. For example, an alert rule in Loki could use a query like `{message=~\"500.*\", env=~\".*\"}` to count occurrences over a 1-minute window and trigger if the count surpasses 5. This pattern can also be visualized on dashboards to track error trends by environment, enabling proactive incident response."
  },
  {
    "query": {
      "message": "500*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500-series server errors, and includes any Kubernetes application label (`app`). In Loki, it helps identify internal server errors generated by specific applications.  \n\n**Purpose:** To monitor and detect occurrences of server-side errors (HTTP 500) within application logs, enabling timely identification of service disruptions or failures.\n\n**Alert Threshold:** An alert could be triggered when the rate of logs matching **{\"message\": \"500*\", \"app\": \"*\"}** exceeds a defined threshold, for example, more than 5 errors per minute per application, signaling a potential outage or degradation.\n\n**Impact:**  \n- **High values:** A spike in 500 errors typically indicates serious backend issues affecting application availability or functionality, requiring immediate investigation.  \n- **Low or zero values:** Normal operation, indicating stable backend responses without server errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of 500 errors over time, grouped by `app`, to quickly identify which applications are experiencing server errors.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (app) (rate({message=~\"500.*\"}[1m])) > 5\n  ```  \n  This triggers an alert if any application logs more than 5 HTTP 500 errors per minute."
  },
  {
    "query": {
      "message": "500*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"500\", indicating an HTTP 500 Internal Server Error. It captures logs originating from any Kubernetes node, identified by the node label. The pattern measures the count of such error occurrences per node. Each matched entry represents one instance of a server error on the specified node."
  },
  {
    "query": {
      "message": "500*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This log query pattern **{\"message\": \"500*\", \"app_kubernetes_io/name\": \"*\"}** is used in Loki to filter log entries where the HTTP response status code starts with '500', indicating server-side errors, and the logs are associated with any Kubernetes application identified by the `app_kubernetes_io/name` label. This helps SREs monitor internal server errors specific to each application.\n\n**Purpose:**  \nTo detect and analyze occurrences of HTTP 5xx errors across Kubernetes applications, enabling rapid identification of service disruptions or failures.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a given time window—for example, more than 5 occurrences of 500 errors per minute per application. Thresholds may vary based on service criticality and baseline error rates.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent server errors, potentially signaling outages, degraded service health, or bugs requiring immediate investigation.  \n- **Low or zero values:** Suggest stable service operation with no recent server-side errors.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({app_kubernetes_io/name=~\".+\", message=~\"500.*\"}[1m])\n```\nto count 500 errors per application over the last minute. An alert rule could trigger if this count exceeds 5, helping SREs proactively respond to service issues."
  },
  {
    "query": {
      "message": "500*",
      "instance": "*"
    },
    "description": "This log pattern **{\"message\": \"500*\", \"instance\": \"*\"}** in Loki filters entries where the HTTP response message contains a status code starting with \"500\", indicating server-side errors, and associates these errors with specific Kubernetes instances. It helps SREs monitor internal server errors per instance to quickly identify and troubleshoot failing services.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 occurrences in 5 minutes), signaling a potential service degradation or outage.\n\n**Impact:**  \n- **High values:** A spike in 500 errors suggests critical backend issues affecting service availability or reliability, requiring immediate investigation.  \n- **Low or zero values:** Indicates stable service health with no recent internal server errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of 500 errors per instance over time using a query like:  \n  `count_over_time({message=~\"500.*\", instance=~\".*\"}[5m]) by (instance)`  \n- **Alert Rule:** Trigger an alert if the error count exceeds 5 in 5 minutes:  \n  ```\n  alert: HighInternalServerErrors\n  expr: count_over_time({message=~\"500.*\", instance=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High rate of 500 errors on instance {{ $labels.instance }}\"\n    description: \"More than 5 internal server errors detected in the last 5 minutes on instance {{ $labels.instance }}.\"\n  ```"
  },
  {
    "query": {
      "message": "500*",
      "filename": "*"
    },
    "description": "This log query pattern **{\"message\": \"500*\", \"filename\": \"*\"}** is used in Loki to filter log entries where the `message` field starts with \"500\", typically indicating HTTP 500-series server errors, and the log is associated with any Kubernetes `filename`. This helps SREs identify internal server errors originating from specific application components or pods.\n\n**Purpose:**  \nTo detect and monitor occurrences of HTTP 500 errors in application logs, which often signal server-side issues requiring immediate attention.\n\n**Alert Threshold:**  \nAn alert can be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window—for example, more than 10 occurrences in 5 minutes—indicating a potential service degradation or outage.\n\n**Impact of Values:**  \n- **High values:** A spike in 500 errors suggests increased server failures, potentially impacting user experience and service reliability. Immediate investigation and remediation are needed.  \n- **Low or zero values:** Indicates normal operation with no detected internal server errors.\n\n**Example Usage in Dashboard or Alert Rule:**  \n- **Dashboard:** Display a graph showing the rate of logs matching `{\"message\": \"500*\", \"filename\": \"*\"}` over time to visualize error trends.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\"500.*\", filename=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 HTTP 500 errors occur within 5 minutes.\n\nBy monitoring this pattern, SREs can proactively detect server errors, assess their severity, and respond before they escalate into larger incidents."
  },
  {
    "query": {
      "message": "500*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500-series server errors, and the `application` label is present (any value). In Loki, this helps identify internal server errors generated by any application running in the Kubernetes cluster.\n\n**Purpose:**  \nTo monitor and alert on occurrences of server-side errors (HTTP 500 errors) across all applications, enabling rapid detection of backend failures or service disruptions.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"500*\", \"application\": \"*\"}** exceeds a defined threshold within a set time window—for example, more than 5 occurrences in 5 minutes—indicating a potential service degradation or outage.\n\n**Impact:**  \n- **High values:** A spike in 500 errors suggests critical issues affecting application stability or availability, requiring immediate investigation and remediation.  \n- **Low or zero values:** Indicates normal operation with no detected server errors, implying healthy application behavior.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of 500 errors per application over time, using a Loki query like:  \n  `count_over_time({message=~\"500.*\", application=~\".*\"}[5m]) by (application)`  \n- **Alert Rule:** Trigger an alert if the error count exceeds 5 in 5 minutes:  \n  ```yaml\n  alert: HighServerErrorRate\n  expr: sum by (application) (count_over_time({message=~\"500.*\", application=~\".*\"}[5m])) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High rate of HTTP 500 errors in {{ $labels.application }}\"\n    description: \"More than 5 HTTP 500 errors detected in the last 5 minutes for application {{ $labels.application }}.\"\n  ```"
  },
  {
    "query": {
      "message": "500*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"500\", indicating HTTP 500 Internal Server Error responses, and includes any Kubernetes `component` label. It is used in Loki to identify backend service errors that may signal server-side issues affecting application availability or stability.\n\n**Purpose:**  \nTo detect and monitor occurrences of HTTP 500 errors across Kubernetes components, enabling early identification of internal server problems.\n\n**Alert Threshold:**  \nAn alert should trigger when the rate of logs matching **{\"message\": \"500*\", \"component\": \"*\"}** exceeds a defined threshold, for example, more than 5 errors per minute per component, indicating a potential service degradation.\n\n**Impact:**  \n- **High values:** A spike in 500 errors suggests critical backend failures, likely causing user-facing disruptions and requiring immediate investigation.  \n- **Low or zero values:** Indicates normal operation with no internal server errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of 500 errors per component over the last 15 minutes, helping SREs spot trends or sudden increases.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (component) (rate({message=~\"500.*\", component=~\".*\"}[1m])) > 5\n  ```  \nThis rule triggers an alert if any component logs more than 5 HTTP 500 errors per minute, prompting investigation into the affected service."
  },
  {
    "query": {
      "message": "500*",
      "source": "*"
    },
    "description": "This log pattern **{\"message\": \"500*\", \"source\": \"*\"}** is used in Loki to filter log entries where the message starts with \"500\", typically indicating HTTP 500 Internal Server Error responses, from any Kubernetes source. It helps SREs identify backend service failures or server errors that may impact application availability or user experience.\n\n**Alert Threshold:**  \nAn alert can be configured to trigger when the count of these 500 error logs exceeds a defined threshold within a specific time window (e.g., more than 10 occurrences in 5 minutes), signaling a potential service degradation or outage.\n\n**Impact of Values:**  \n- **High values:** A spike in 500 errors suggests critical backend issues requiring immediate investigation to prevent downtime or data loss.  \n- **Low or zero values:** Indicates stable service health with no recent internal server errors.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\n{message=~\"500.*\", source=~\".*\"} | count_over_time({message=~\"500.*\", source=~\".*\"}[5m])\n```  \nThis counts the number of 500 error logs over the last 5 minutes. An alert rule can then fire if this count exceeds the threshold, enabling proactive incident response."
  },
  {
    "query": {
      "message": "500*",
      "pod_name": "*"
    },
    "description": "This log pattern **{\"message\": \"500*\", \"pod_name\": \"*\"}** in Loki is used to identify log entries from any Kubernetes pod where the message starts with \"500\", typically indicating HTTP 500 Internal Server Error responses. It helps SREs monitor backend service failures at the pod level.\n\n**Purpose:**  \nTo detect and track occurrences of internal server errors across pods, enabling rapid identification of problematic services or deployments.\n\n**Alert Threshold:**  \nAn alert might be triggered if the count of logs matching this pattern exceeds a defined threshold within a given time window, for example:  \n- More than 5 occurrences in 5 minutes per pod, or  \n- A sustained increase compared to baseline error rates.\n\n**Impact:**  \n- **High values:** Indicate frequent internal server errors, potentially causing service degradation or outages, requiring immediate investigation.  \n- **Low or zero values:** Suggest stable service health with no recent internal server errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"500*\" messages grouped by `pod_name` over time, highlighting pods with rising error rates.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\"500.*\", pod_name=~\".*\"}[5m]) > 5\n  ```  \nThis triggers an alert if any pod logs more than 5 HTTP 500 errors in 5 minutes, prompting SREs to investigate the affected pod."
  },
  {
    "query": {
      "message": "500*",
      "container_name": "*"
    },
    "description": "This log query pattern **{\"message\": \"500*\", \"container_name\": \"*\"}** in Loki is used to identify log entries from any Kubernetes container where the message starts with \"500\", typically indicating HTTP 500 Internal Server Error responses. It helps SREs monitor backend service errors that may impact application availability or user experience.\n\n**Purpose:**  \n- Detect occurrences of server-side errors (HTTP 500) across all containers in the cluster.  \n- Enable rapid identification of problematic services generating internal errors.\n\n**Alert Threshold:**  \n- An alert could be triggered when the rate of logs matching this pattern exceeds a defined threshold, e.g., more than 5 errors per minute per container, indicating a potential service degradation or outage.\n\n**Impact of Values:**  \n- **High values:** A spike in 500 errors suggests critical backend failures, requiring immediate investigation to prevent downtime or data loss.  \n- **Low or zero values:** Indicates healthy service operation with no internal server errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching **{\"message\": \"500*\", \"container_name\": \"*\"}** per container over time to spot error spikes.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by(container_name) (rate({message=~\"500.*\", container_name=~\".*\"}[1m])) > 5\n  ```  \n  This triggers an alert if any container logs more than 5 HTTP 500 errors per minute.\n\nBy monitoring this pattern, SREs can proactively detect and respond to backend failures impacting system reliability."
  },
  {
    "query": {
      "message": "500*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"500\", indicating an HTTP 500 Internal Server Error. It also requires the presence of any value in the cluster field, representing the Kubernetes cluster name. The pattern measures the count of such error log entries per cluster. The unit of measurement is the number of log occurrences."
  },
  {
    "query": {
      "message": "500*",
      "pod": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod where the message begins with \"500\", indicating HTTP 500 Internal Server Errors. In Loki, it helps identify backend service failures by filtering logs that report server errors across pods. An alert using this pattern should trigger when the count of such 500 errors exceeds a defined threshold (e.g., more than 5 errors within 5 minutes), signaling potential service instability or outages. High values indicate frequent server errors requiring immediate investigation, while low or zero values suggest normal operation. Example alert rule snippet:\n\n```\nsum by (pod) (count_over_time({message=~\"500.*\", pod=~\".*\"}[5m])) > 5\n```\n\nThis triggers an alert if any pod logs more than 5 HTTP 500 errors in 5 minutes, enabling SREs to quickly detect and respond to backend failures."
  },
  {
    "query": {
      "message": "500*",
      "namespace": "*"
    },
    "description": "This log query pattern **{\"message\": \"500*\", \"namespace\": \"*\"}** in Loki is designed to capture all log entries where the message starts with \"500\", typically indicating HTTP 500 Internal Server Errors, across any Kubernetes namespace. Its primary purpose is to help SREs monitor and identify backend service failures that may impact application availability or user experience.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of these 500 error logs exceeds a defined threshold within a specific time window (e.g., more than 10 occurrences in 5 minutes), signaling a potential service degradation or outage.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high count of 500 errors suggests critical backend issues requiring immediate investigation to prevent downtime or data loss.  \n- **Low or zero values:** Indicates normal operation with no detected internal server errors, implying stable service health.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of 500 errors per namespace over time using a Loki query like:  \n  `count_over_time({message=~\"500.*\", namespace=~\".*\"}[5m]) by (namespace)`  \n- **Alert Rule:** Trigger an alert if the count exceeds 10 in 5 minutes:  \n  ```\n  alert: HighInternalServerErrors\n  expr: count_over_time({message=~\"500.*\", namespace=~\".*\"}[5m]) > 10\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High rate of HTTP 500 errors in namespace {{ $labels.namespace }}\"\n    description: \"More than 10 HTTP 500 errors detected in the last 5 minutes in namespace {{ $labels.namespace }}. Immediate investigation recommended.\"\n  ```"
  },
  {
    "query": {
      "message": "404*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application 404 error logs. Not found errors, missing resources, and HTTP 404 responses across all k8s apps. Useful for API endpoint monitoring and resource availability tracking."
  },
  {
    "query": {
      "message": "404*",
      "container": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"container\": \"*\"}** in Loki filters for log entries where the message contains \"404\"—typically indicating HTTP 404 Not Found errors—and the entry is associated with any Kubernetes container. This helps SREs monitor occurrences of missing resources or broken links within containerized applications.\n\n**Purpose:**  \nTo identify and track 404 errors generated by applications running in Kubernetes containers, enabling early detection of potential issues like broken endpoints, misconfigured routes, or missing assets.\n\n**Alert Threshold:**  \nAn alert could be triggered when the rate of 404 errors exceeds a defined threshold, for example:  \n- More than 10 occurrences per minute per container, or  \n- A sustained increase (e.g., 3x baseline) in 404 errors over a 5-minute window.\n\n**Impact:**  \n- **High values:** A spike in 404 errors may indicate broken URLs, missing resources, or client misconfigurations, potentially leading to degraded user experience or failed service integrations. Immediate investigation is recommended.  \n- **Low or zero values:** Normal operation, indicating resources are generally accessible and endpoints are correctly configured.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing the count of 404 errors over time per container, helping visualize trends and identify problematic services.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (container) (rate({message=~\"404.*\", container=~\".*\"}[1m])) > 10\n  ```  \n  This triggers an alert if any container logs more than 10 404 errors per minute.\n\nBy monitoring this pattern, SREs can proactively detect and resolve issues related to missing resources in Kubernetes environments."
  },
  {
    "query": {
      "message": "404*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with \"404\" and the \"node_name\" field is present with any value. It identifies occurrences of HTTP 404 errors, indicating that a requested resource was not found. The pattern measures the count of such log entries per Kubernetes node. Each matched entry corresponds to one instance of a 404 error on the specified node."
  },
  {
    "query": {
      "message": "404*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message contains \"404\"—indicating HTTP 404 Not Found errors—and includes any Kubernetes region label. In Loki, **{\"message\": \"404*\", \"region\": \"*\"}** helps identify and filter requests resulting in missing resources across all regions.\n\n**Purpose:**  \nTo monitor the frequency of 404 errors per region, enabling detection of potential issues such as broken links, misconfigured services, or missing endpoints.\n\n**Alert Threshold:**  \nAn alert should trigger when the rate of 404 errors exceeds a defined threshold, for example, more than 50 errors per minute in any single region, sustained over 5 minutes. This threshold can be adjusted based on normal traffic patterns.\n\n**Impact:**  \n- **High values:** Indicate a significant number of client requests are failing due to missing resources, which may degrade user experience and signal underlying application or routing problems.  \n- **Low values:** Typically expected in healthy systems; occasional 404s are normal due to user errors or outdated links.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of 404 errors grouped by `region` over time, helping SREs quickly identify regions with abnormal error spikes.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (region) (rate({message=~\"404.*\", region=~\".*\"}[1m])) > 50\n  ```\n  This triggers an alert if any region experiences more than 50 404 errors per minute.\n\nBy leveraging this pattern, SREs can proactively detect and respond to resource-not-found issues impacting specific Kubernetes regions."
  },
  {
    "query": {
      "message": "404*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains \"404\" (indicating HTTP 404 Not Found errors) and the `service` label identifies the Kubernetes service emitting the log. In Loki, it helps SREs monitor the frequency of \"404\" errors per service, which often signal missing resources, broken links, or misconfigured endpoints.\n\n**Purpose:**  \nTo detect and quantify \"404 Not Found\" errors across services, enabling early identification of potential issues affecting user experience or service functionality.\n\n**Alert Threshold:**  \nAn alert could be triggered when the rate of \"404\" errors exceeds a defined threshold, for example:  \n- More than 5 errors per minute per service, sustained over 5 minutes.  \nThis threshold should be adjusted based on normal traffic patterns and service behavior.\n\n**Impact:**  \n- **High values:** A spike in \"404\" errors may indicate broken URLs, missing assets, or misrouted requests, potentially leading to degraded user experience or failed integrations. Immediate investigation is recommended.  \n- **Low or zero values:** Normal operation, indicating resources are generally found as expected.\n\n**Example Usage:**  \n- **Dashboard Query:**  \n  ```  \n  sum by (service) (rate({message=~\"404.*\"}[5m]))  \n  ```  \n  This query shows the per-service rate of \"404\" errors over the last 5 minutes, helping visualize trends and identify problematic services.\n\n- **Alert Rule Example:**  \n  ```yaml\n  alert: High404ErrorRate\n  expr: sum by (service) (rate({message=~\"404.*\"}[5m])) > 5\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High 404 error rate detected for {{ $labels.service }}\"\n    description: \"Service {{ $labels.service }} is experiencing more than 5 '404 Not Found' errors per minute over the last 5 minutes.\"\n  ```"
  },
  {
    "query": {
      "message": "404*",
      "job": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"job\": \"*\"}** in Loki matches log entries where the message contains a \"404\" status code—commonly indicating that a requested resource was not found—and the entry is associated with any Kubernetes job label. This pattern helps SREs monitor and analyze occurrences of missing resources across different jobs or services.\n\n**Purpose:**  \nTo track and quantify \"404 Not Found\" errors generated by various Kubernetes jobs, enabling early detection of broken links, misconfigured routes, or missing assets that could impact user experience or service functionality.\n\n**Alert Threshold:**  \nAn alert should be triggered when the rate of 404 errors exceeds a defined threshold, for example:  \n- More than 10 occurrences per minute per job, sustained over 5 minutes.  \nThis threshold can be adjusted based on normal traffic patterns and service criticality.\n\n**Impact of Values:**  \n- **High values:** A spike in 404 errors may indicate broken endpoints, missing resources, or misrouted requests, potentially leading to degraded user experience or failed integrations. Immediate investigation is recommended.  \n- **Low or zero values:** Indicates normal operation with no missing resource errors detected.\n\n**Example Usage in Dashboard or Alert Rule:**  \n- **Dashboard:** A panel showing a time series graph of the count of logs matching **{\"message\": \"404*\", \"job\": \"*\"}** grouped by job label, highlighting trends and spikes in 404 errors per service.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (job) (rate({job=~\".*\"} |= \"404\" [1m])) > 10\n  ```  \n  This rule fires if any job produces more than 10 \"404\" errors per minute.\n\n**Example Log Entry:**  \n`message=\"404 connection failed\", job=\"example-job\"`"
  },
  {
    "query": {
      "message": "404*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"404\" and the environment field is present with any value. It identifies occurrences of HTTP 404 errors, indicating that a requested resource was not found. The measurement is a count of such log entries, representing the number of 404 error events recorded. The environment field specifies the Kubernetes environment in which the error occurred."
  },
  {
    "query": {
      "message": "404*",
      "host": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"host\": \"*\"}** is used in Loki to identify log entries where the message contains a HTTP 404 status code, indicating that a requested resource was not found. The presence of the `host` label allows filtering or grouping by the Kubernetes node or pod emitting the log.\n\n**Purpose:**  \nTo monitor and alert on occurrences of \"404 Not Found\" errors across your Kubernetes environment, helping SREs detect missing resources, broken links, or misconfigured services.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 50 occurrences in 5 minutes per host), signaling a potential issue affecting users or services.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high rate of 404 errors may indicate broken endpoints, missing assets, or routing issues, potentially degrading user experience or service functionality. Immediate investigation is recommended.  \n- **Low or zero values:** Normal operation, indicating resources are generally accessible and requests are successful.\n\n**Example Usage:**  \n- **Dashboard:** A graph showing the rate of 404 errors over time, grouped by `host`, to identify which nodes or pods are experiencing the most \"not found\" errors.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (host) (count_over_time({message=~\"404.*\", host=~\".*\"}[5m])) > 50\n  ```  \n  This triggers an alert if any host logs more than 50 \"404\" messages in 5 minutes."
  },
  {
    "query": {
      "message": "404*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message contains \"404\"—indicating HTTP 404 Not Found errors—and includes any Kubernetes log level. In Loki, it is used to identify occurrences of missing resources or broken links in your application or services.\n\n**Purpose:**  \nTo monitor and alert on the frequency of 404 errors, which often signal broken endpoints, misconfigured routes, or missing assets that can degrade user experience.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"404*\", \"level\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 50 occurrences in 5 minutes). Thresholds should be adjusted based on normal traffic patterns and error tolerance.\n\n**Impact of Values:**  \n- **High values:** Indicate a spike in missing resources, potentially causing user-facing issues or broken functionality that requires immediate investigation.  \n- **Low or zero values:** Suggest normal operation with no significant missing resource errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing the rate of 404 errors over time using a Loki query like:  \n  `count_over_time({message=~\"404.*\", level=~\".*\"}[5m])`  \n- **Alert Rule:** Trigger an alert if the count exceeds 50 in 5 minutes:  \n  ```\n  alert: High404ErrorRate\n  expr: count_over_time({message=~\"404.*\", level=~\".*\"}[5m]) > 50\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of 404 errors detected\"\n    description: \"More than 50 HTTP 404 errors occurred in the last 5 minutes, indicating potential broken links or missing resources.\"\n  ```"
  },
  {
    "query": {
      "message": "404*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"404\", indicating HTTP 404 errors (resource not found), across any Kubernetes environment (`env`). In Loki, it helps identify and filter logs related to missing resources or broken links in your services.\n\n**Purpose:**  \nTo monitor occurrences of 404 errors in different environments, enabling SREs to detect potential issues such as broken endpoints, misconfigured routes, or missing assets.\n\n**Alert Threshold:**  \nAn alert should be triggered when the rate of 404 errors exceeds a defined threshold, for example, more than 50 occurrences within 5 minutes in a given environment. This threshold can be adjusted based on normal traffic patterns.\n\n**Impact:**  \n- **High values:** A spike in 404 errors may indicate broken links, deployment issues, or client misconfigurations, potentially leading to degraded user experience or failed service integrations.  \n- **Low values:** A low or zero count of 404 errors generally indicates healthy routing and resource availability.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of 404 errors over time per environment using a Loki query like:  \n  `count_over_time({env=~\".*\"} |= \"404\" [5m])`  \n- **Alert Rule (Prometheus-style):**  \n  ```\n  alert: High404ErrorRate\n  expr: sum by (env) (rate({env=~\".*\"} |= \"404\" [5m])) > 0.17\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of 404 errors in {{ $labels.env }}\"\n    description: \"More than 50 HTTP 404 errors detected in the last 5 minutes in environment {{ $labels.env }}.\"\n  ```"
  },
  {
    "query": {
      "message": "404*",
      "app": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"app\": \"*\"}** in Loki captures all log entries from any Kubernetes application (`app` label) where the message contains a \"404\" status code, indicating that a requested resource was not found. This pattern helps SREs monitor client-side errors that may signal broken links, missing resources, or misconfigured services.\n\n**Purpose:**  \nTo track the frequency of 404 errors across all applications, enabling early detection of issues affecting user experience or service availability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the rate of 404 errors exceeds a defined threshold, for example, more than 5 errors per minute sustained over 5 minutes. This threshold can be adjusted based on normal traffic patterns and business impact.\n\n**Impact of Values:**  \n- **High values:** A spike in 404 errors may indicate broken endpoints, missing assets, or routing issues, potentially leading to degraded user experience or lost revenue. Immediate investigation is warranted.  \n- **Low or zero values:** Indicates normal operation with no missing resources detected.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: High404ErrorRate\nexpr: sum(rate({app=~\".+\", message=~\"404.*\"}[1m])) by (app) > 5\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High 404 error rate detected for {{ $labels.app }}\"\n  description: \"The application {{ $labels.app }} is experiencing more than 5 '404 Not Found' errors per minute over the last 5 minutes.\"\n```\n\n**Example Dashboard Query:**  \n```logql\nsum(rate({message=~\"404.*\", app=~\".+\"}[5m])) by (app)\n```\nThis query shows the per-application rate of 404 errors over the last 5 minutes, helping SREs identify which apps are experiencing resource-not-found issues."
  },
  {
    "query": {
      "message": "404*",
      "node": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"node\": \"*\"}** in Loki captures entries where the log message contains the HTTP status code \"404\", indicating that a requested resource was not found, and associates these logs with a specific Kubernetes node. This helps SREs monitor and troubleshoot missing resources or broken links on individual nodes.\n\n**Purpose:**  \nTo track and analyze occurrences of 404 errors per node, enabling identification of nodes experiencing frequent resource-not-found issues which may indicate misconfigurations, missing assets, or routing problems.\n\n**Alert Threshold:**  \nAn alert could be triggered when the rate of 404 errors exceeds a defined threshold, for example, more than 50 occurrences within 5 minutes on a single node. This threshold should be adjusted based on normal traffic patterns and application behavior.\n\n**Impact of Values:**  \n- **High 404 rates:** Suggest potential issues such as broken links, missing files, or misrouted requests on the affected node, which can degrade user experience and indicate underlying system problems.  \n- **Low or zero 404 rates:** Indicate normal operation with resources being correctly served.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the count of 404 errors grouped by node over time to quickly identify problematic nodes.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (node) (rate({message=~\"404.*\"}[5m])) > 50\n  ```  \n  This rule triggers an alert if any node logs more than 50 404 errors in 5 minutes, prompting investigation.\n\nBy monitoring this pattern, SREs can proactively detect and resolve resource availability issues on specific Kubernetes nodes."
  },
  {
    "query": {
      "message": "404*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with \"404\", indicating HTTP 404 errors (resource not found). It also requires the presence of the Kubernetes label \"app_kubernetes_io/name\" with any value, identifying the application source. The pattern measures the count of such log entries, representing the number of 404 error occurrences per application."
  },
  {
    "query": {
      "message": "404*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"404\", indicating HTTP 404 errors (resource not found), and includes the `instance` label identifying the Kubernetes instance emitting the log. In Loki, this helps isolate and monitor \"404 Not Found\" errors per instance.\n\n**Purpose:**  \nTo track and alert on the frequency of 404 errors across Kubernetes instances, enabling SREs to detect missing resources, broken links, or misconfigured services.\n\n**Alert Threshold:**  \nAn alert could be triggered when the rate of 404 errors exceeds a defined threshold, for example, more than 10 errors per minute per instance, indicating a potential issue requiring investigation.\n\n**Impact:**  \n- **High values:** A spike in 404 errors may signal broken endpoints, missing assets, or routing problems, potentially degrading user experience or indicating a misconfiguration.  \n- **Low values:** Occasional 404s are normal (e.g., probing or user errors) and typically do not require action.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing the rate of 404 errors over time per instance using a Loki query like:  \n  `sum(rate({message=~\"404.*\", instance=~\".*\"}[5m])) by (instance)`  \n- **Alert Rule:** Trigger an alert if the 5-minute rate of 404 errors per instance exceeds 10:  \n  ```yaml\n  alert: High404ErrorRate\n  expr: sum(rate({message=~\"404.*\", instance=~\".*\"}[5m])) by (instance) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High 404 error rate on instance {{ $labels.instance }}\"\n    description: \"Instance {{ $labels.instance }} is experiencing more than 10 HTTP 404 errors per minute.\"\n  ```"
  },
  {
    "query": {
      "message": "404*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"404\", indicating an HTTP 404 Not Found error. It specifically captures logs labeled with any Kubernetes filename. The pattern measures the occurrence count of such 404 error messages within the logs. Each matched entry corresponds to one instance of a resource not found event."
  },
  {
    "query": {
      "message": "404*",
      "application": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"application\": \"*\"}** in Loki captures all log entries where the message contains a \"404\" status code, indicating that a requested resource was not found, across all applications labeled by the Kubernetes `application` field. This is useful for SREs to monitor client errors that may signal broken links, misconfigured services, or missing resources.\n\n**Purpose:**  \nTo track and analyze occurrences of 404 errors per application, helping identify potential issues affecting user experience or service reliability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the rate of 404 errors exceeds a defined threshold, for example, more than 5 errors per minute per application sustained over 5 minutes. This threshold can be adjusted based on normal traffic patterns and business impact.\n\n**Impact of Values:**  \n- **High 404 rate:** May indicate broken endpoints, missing resources, or misrouted requests, potentially leading to degraded user experience or lost revenue. Immediate investigation is recommended.  \n- **Low or zero 404 rate:** Indicates healthy routing and resource availability, suggesting no immediate issues related to missing resources.\n\n**Example Usage in Dashboard or Alert Rule:**  \nA Prometheus-style alert rule using Loki query could be:  \n```\nsum(rate({application=~\".*\", message=~\"404.*\"}[1m])) by (application) > 5\n```\nThis query calculates the per-application rate of 404 errors over the last minute and triggers an alert if it exceeds 5 errors per minute. Dashboards can visualize this metric as a time series graph to monitor trends and correlate spikes with deployments or incidents."
  },
  {
    "query": {
      "message": "404*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"404\", indicating HTTP 404 errors (resource not found), and includes any Kubernetes component label. In Loki, it helps identify and filter logs related to missing resources across components. An alert using this pattern should trigger when the rate of 404 errors exceeds a defined threshold (e.g., more than 5 errors per minute), signaling potential issues like broken links or misconfigured services. High values may indicate widespread resource access problems impacting user experience or service functionality, while low values typically reflect normal occasional missing resource requests.  \nExample alert rule snippet:  \n```\nsum(rate({component=~\".*\"} |= \"404\" [1m])) by (component) > 5\n```  \nThis triggers an alert if any component logs more than 5 HTTP 404 errors per minute. In dashboards, this pattern can be used to visualize 404 error trends per component, helping SREs quickly identify and troubleshoot problematic services."
  },
  {
    "query": {
      "message": "404*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message contains \"404\"—indicating HTTP 404 Not Found errors—and includes any Kubernetes source label. In Loki, it helps identify when services or endpoints are returning resource-not-found errors. An alert using this pattern might trigger when the count of 404 errors exceeds a defined threshold (e.g., more than 50 occurrences within 5 minutes), signaling potential broken links, misconfigured routes, or missing resources. A high rate of 404s can indicate user experience issues or backend problems, while a low rate is generally expected and less concerning. For example, an alert rule could count log entries matching **{message=~\"404.*\", source=~\".*\"}** over 5 minutes and fire if the count > 50. In a dashboard, this pattern can be used to graph 404 error trends per source to quickly identify problematic services or endpoints."
  },
  {
    "query": {
      "message": "404*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the message contains \"404\", indicating HTTP 404 errors (resource not found) generated by a specific Kubernetes pod identified by `pod_name`. In Loki, it helps SREs monitor the frequency of \"404\" errors per pod to detect potential issues such as broken links, misconfigured services, or missing resources.\n\n**Alert Threshold:**  \nAn alert could be triggered if the rate of 404 errors exceeds a defined threshold, for example, more than 10 errors per minute per pod, sustained over 5 minutes. This threshold should be adjusted based on normal traffic patterns and service behavior.\n\n**Impact:**  \n- **High values:** A spike in 404 errors may indicate client requests to unavailable endpoints, broken APIs, or deployment issues causing service degradation or user experience problems. Immediate investigation is recommended.  \n- **Low or zero values:** Indicates normal operation with no missing resource errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Visualize a time series graph showing the count of logs matching `{message=~\"404.*\", pod_name=~\".*\"}` grouped by `pod_name` to identify pods with frequent 404 errors.  \n- **Alert Rule (PromQL for Loki):**  \n  ```\n  sum by (pod_name) (rate({message=~\"404.*\", pod_name=~\".*\"}[5m])) > 10\n  ```  \nThis rule triggers an alert if any pod logs more than 10 \"404\" errors per minute averaged over 5 minutes."
  },
  {
    "query": {
      "message": "404*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"404\", indicating an HTTP 404 Not Found error. It specifically filters logs that include any Kubernetes container name under the \"container_name\" label. The pattern measures the count of such log occurrences as discrete events. Each event corresponds to one log entry reporting a \"404\" status from a particular container."
  },
  {
    "query": {
      "message": "404*",
      "cluster": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"cluster\": \"*\"}** is used in Loki to identify all log entries where the message contains a \"404\" status, indicating that a requested resource was not found, across any Kubernetes cluster. This helps SREs monitor client-side errors that may point to broken links, misconfigured services, or missing resources.\n\n**Purpose:**  \nTo track the frequency of \"404 Not Found\" errors per cluster, enabling early detection of issues affecting user experience or service availability.\n\n**Alert Threshold:**  \nAn alert could be triggered when the rate of 404 errors exceeds a defined threshold, for example, more than 50 occurrences within 5 minutes per cluster, signaling a potential widespread issue.\n\n**Impact:**  \n- **High values:** A spike in 404 errors may indicate broken endpoints, misrouted requests, or configuration problems, potentially leading to degraded service or user frustration.  \n- **Low values:** Occasional 404s are normal (e.g., probing or invalid requests) and typically do not require action.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching **{\"message\": \"404*\", \"cluster\": \"*\"}** grouped by cluster, to visualize trends and identify clusters with rising 404 errors.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (cluster) (rate({message=~\"404.*\"}[5m])) > 50\n  ```  \n  This triggers an alert if any cluster experiences more than 50 \"404\" errors in 5 minutes."
  },
  {
    "query": {
      "message": "404*",
      "pod": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"pod\": \"*\"}** in Loki captures all log entries from any Kubernetes pod where the message contains a \"404\" status code, typically indicating that a requested resource was not found. It helps SREs monitor client errors related to missing endpoints or resources across pods.\n\n**Purpose:**  \nTo detect and quantify occurrences of \"404 Not Found\" errors within pod logs, enabling early identification of broken links, misconfigured services, or missing resources that could impact user experience or service functionality.\n\n**Alert Threshold:**  \nAn alert should be triggered when the rate of 404 errors exceeds a defined threshold, for example, more than 10 errors per minute per pod or a sustained increase over a 5-minute window. Thresholds should be adjusted based on normal traffic patterns and service expectations.\n\n**Impact:**  \n- **High values:** A spike in 404 errors may indicate broken URLs, missing API endpoints, or misrouted requests, potentially leading to degraded user experience or failed integrations. It may also signal security probing or misconfigurations.  \n- **Low values:** Occasional 404s are normal due to user errors or transient issues and typically do not require action.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the count of 404 errors over time per pod using a Loki query like:  \n  `count_over_time({message=~\"404.*\", pod=~\".*\"}[5m])`  \n- **Alert Rule:** Trigger an alert if the 404 error rate exceeds 10 per minute per pod:  \n  ```\n  alert: High404ErrorRate\n  expr: sum by (pod) (rate({message=~\"404.*\", pod=~\".*\"}[1m])) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High 404 error rate detected on pod {{ $labels.pod }}\"\n    description: \"Pod {{ $labels.pod }} is experiencing more than 10 '404 Not Found' errors per minute.\"\n  ```"
  },
  {
    "query": {
      "message": "404*",
      "namespace": "*"
    },
    "description": "This log pattern **{\"message\": \"404*\", \"namespace\": \"*\"}** is used in Loki to identify all log entries where the message starts with or contains \"404\", indicating HTTP 404 errors (resource not found) occurring within any Kubernetes namespace. This helps SREs monitor missing resource requests across the cluster.\n\n**Purpose:**  \nTo track and quantify occurrences of 404 errors per namespace, enabling detection of potential issues such as broken links, misconfigured services, or missing endpoints.\n\n**Alert Threshold:**  \nAn alert could be triggered when the rate of 404 errors exceeds a defined threshold, for example:  \n- More than 50 occurrences within 5 minutes in any single namespace, or  \n- A sudden spike (e.g., 3x increase) compared to the baseline error rate.\n\n**Impact:**  \n- **High values:** Indicate frequent client requests to unavailable resources, which may degrade user experience, signal configuration problems, or reveal broken dependencies. Immediate investigation is recommended.  \n- **Low values:** Normal or expected occasional 404s, typically not a concern.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\nsum by (namespace) (rate({message=~\"404.*\", namespace=~\".*\"}[5m]))\n```\nThis query calculates the per-namespace rate of 404 errors over the last 5 minutes. An alert rule can then trigger if this rate exceeds the defined threshold, helping SREs proactively address missing resource issues."
  },
  {
    "query": {
      "message": "memory*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries where the message field contains the word \"memory\" and the entry includes a Kubernetes application label (k8s_app). It identifies logs related to memory usage, errors, or performance events within Kubernetes applications. The pattern does not measure a specific unit but captures textual indicators of memory-related activity. Example: message=\"memory connection failed\", k8s_app=\"example-k8s_app\"."
  },
  {
    "query": {
      "message": "memory*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"memory\" and the container field is present with any value. It identifies logs related to memory usage or memory-related events within Kubernetes containers. The pattern does not measure a specific metric or unit but serves to filter relevant memory-related log messages for further analysis. Example: message=\"memory connection failed\", container=\"example-container\"."
  },
  {
    "query": {
      "message": "memory*",
      "node_name": "*"
    },
    "description": "This log pattern **{\"message\": \"memory*\", \"node_name\": \"*\"}** in Loki captures entries where the log message contains the keyword \"memory\" and is associated with a specific Kubernetes node (via the `node_name` label). It is primarily used to monitor memory-related events such as memory pressure, allocation failures, or performance degradation on individual nodes.\n\n**Purpose:**  \nTo detect and analyze memory utilization issues or errors on Kubernetes nodes, enabling SREs to proactively identify nodes experiencing memory stress or failures.\n\n**Alert Thresholds:**  \nAn alert should be triggered if the frequency of memory-related log entries exceeds a defined threshold within a given time window, for example:  \n- More than 5 memory error or warning messages per node within 5 minutes, or  \n- Specific keywords indicating critical memory failures (e.g., \"out of memory\", \"memory allocation failed\") appear at any time.\n\n**Impact:**  \n- **High frequency or critical memory errors:** May indicate memory leaks, insufficient node memory, or application memory exhaustion, potentially leading to pod evictions, degraded performance, or node instability.  \n- **Low or no memory-related logs:** Typically indicates normal memory operation without notable issues.\n\n**Example Usage in Dashboard or Alert Rule:**  \n- **Dashboard:** Visualize the count of memory-related log entries per node over time to identify trends or spikes in memory issues.  \n- **Alert Rule (PromQL for Loki):**  \n  ```  \n  count_over_time({message=~\"memory.*\", node_name=~\".*\"}[5m]) > 5  \n  ```  \n  This rule triggers an alert if more than 5 memory-related log entries occur on any node within 5 minutes, signaling potential memory problems requiring investigation."
  },
  {
    "query": {
      "message": "memory*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the keyword \"memory\" (e.g., \"memory usage high\", \"memory allocation failed\") and includes a Kubernetes `region` label. It is used in Loki to identify logs related to memory utilization or memory-related errors across different regions.\n\n**Purpose:**  \nTo monitor memory-related events and performance issues in Kubernetes clusters by region, enabling targeted troubleshooting and capacity planning.\n\n**Alert Thresholds:**  \nAn alert should be triggered if the number of memory-related error logs (e.g., containing \"memory error\", \"out of memory\") exceeds a defined threshold within a given time window (e.g., > 10 errors in 5 minutes), or if memory usage metrics inferred from logs indicate sustained high utilization (e.g., > 80% memory usage).\n\n**Impact:**  \n- **High frequency of memory-related logs:** May indicate memory leaks, insufficient memory allocation, or application crashes, potentially leading to degraded performance or outages.  \n- **Low frequency or absence:** Suggests normal memory operation, but absence of logs should be confirmed with metrics to avoid blind spots.\n\n**Example Usage:**  \n- **Dashboard:** Display a panel showing the count of logs matching `{\"message\": \"memory*\", \"region\": \"*\"}` grouped by `region` over time to identify regions experiencing memory issues.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"memory.*\", region=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 memory-related logs occur in any region within 5 minutes."
  },
  {
    "query": {
      "message": "memory*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"memory\" and the entry is associated with any Kubernetes service. It is used in Loki to identify logs related to memory usage, memory errors, or performance issues involving memory within services. An alert based on this pattern should be triggered when memory-related error messages or warnings exceed a defined threshold—such as more than 5 memory error logs within 5 minutes—indicating potential memory leaks, exhaustion, or degradation impacting service stability. High frequency or severity of these logs often correlates with increased memory pressure, which can lead to degraded performance or service outages, while low or no occurrences suggest normal memory operation. For example, an alert rule could count logs matching **{message=~\"memory.*\", service=~\".*\"}** over a 5-minute window and fire if the count exceeds 5, and a dashboard panel could display a time series graph of memory-related log counts per service to help SREs monitor memory health trends across the cluster."
  },
  {
    "query": {
      "message": "memory*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"memory\" and the job field is present with any value. It identifies logs related to memory usage or memory-related events within Kubernetes jobs. The pattern does not measure a specific metric or unit but serves to filter relevant memory-related log messages for further analysis. Example: message=\"memory connection failed\", job=\"example-job\"."
  },
  {
    "query": {
      "message": "memory*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"memory\" and includes any Kubernetes environment label. It identifies logs related to memory usage or memory-related events within a specific environment. The pattern does not measure a numeric value or unit but flags occurrences of memory-related messages for monitoring purposes. Example: message=\"memory connection failed\", environment=\"production\"."
  },
  {
    "query": {
      "message": "memory*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"memory\" and the host field is present with any value. It identifies logs related to memory usage or memory-related events on a Kubernetes host. The pattern does not measure a specific metric or unit but flags textual references to memory in log messages. Example: message=\"memory connection failed\", host=\"example-host\"."
  },
  {
    "query": {
      "message": "memory*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"memory\" and includes any additional text. It captures logs related to memory usage or memory-related events in the system. The \"level\" field indicates the severity or importance of the log, as defined by Kubernetes logging standards. This pattern does not measure a specific unit but identifies memory-related log messages for monitoring or troubleshooting purposes."
  },
  {
    "query": {
      "message": "memory*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"memory\" and the environment label (env) is present. It identifies logs related to memory usage or memory-related events within a Kubernetes environment. The pattern does not measure a specific metric or unit but flags occurrences of memory-related messages for monitoring or troubleshooting purposes. Example: message=\"memory connection failed\", env=\"example-env\"."
  },
  {
    "query": {
      "message": "memory*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"memory\" and the log is associated with any Kubernetes application (`app` label). It is used in Loki to identify logs related to memory usage, memory errors, or performance issues involving memory within applications.  \n\n**Purpose:**  \nTo monitor memory-related events that may indicate memory pressure, leaks, or failures impacting application stability and performance.\n\n**Alert Thresholds:**  \nAn alert should be triggered if the frequency of logs matching this pattern exceeds a defined threshold within a short time window (e.g., more than 10 memory-related error logs in 5 minutes), or if specific keywords indicating critical memory failures (e.g., \"out of memory\", \"memory leak detected\") appear.\n\n**Impact:**  \n- **High frequency or critical memory errors:** May lead to application crashes, degraded performance, or resource exhaustion, affecting service availability.  \n- **Low frequency or informational logs:** Typically indicate normal memory usage or minor warnings without immediate impact.\n\n**Example Usage:**  \n- **Dashboard:** Display a panel showing the count of logs matching `{message=~\"memory.*\", app=~\".*\"}` over time to track memory-related issues per application.  \n- **Alert Rule:**  \n  ```yaml\n  alert: HighMemoryErrorRate\n  expr: sum by (app) (count_over_time({message=~\"memory.*\", app=~\".*\"}[5m])) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of memory-related errors in {{ $labels.app }}\"\n    description: \"More than 10 memory-related logs detected in the last 5 minutes for app {{ $labels.app }}. Investigate potential memory issues.\"\n  ```"
  },
  {
    "query": {
      "message": "memory*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"memory\" and includes the Kubernetes node label, helping SREs monitor memory-related events or errors on specific nodes. It is used in Loki to filter logs indicative of memory pressure, leaks, or failures that could impact node stability or application performance. An alert should be triggered when the frequency of such memory-related log entries exceeds a defined threshold (e.g., more than 5 occurrences within 5 minutes per node), signaling potential memory issues requiring investigation. High volumes of these logs often indicate memory exhaustion or degradation, which can lead to pod evictions, degraded service performance, or node instability; low or zero occurrences typically suggest normal memory operation. For example, an alert rule might count logs matching **{\"message\": \"memory*\", \"node\": \"*\"}** over a 5-minute window per node and fire if the count surpasses 5, while a dashboard panel could display a time series graph of memory-related log counts per node to help visualize trends and identify problematic nodes quickly."
  },
  {
    "query": {
      "message": "memory*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the message field contains the word \"memory\" and the entry includes any value for the Kubernetes label \"app_kubernetes_io/name.\" It is used to identify logs related to memory usage or memory-related events within specific Kubernetes applications. The pattern does not measure a specific metric or unit but filters logs indicating memory activity or issues. This helps in monitoring memory-related performance or errors for targeted Kubernetes workloads."
  },
  {
    "query": {
      "message": "memory*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"memory\" and includes the Kubernetes 'instance' label, helping identify memory-related events such as high memory usage, memory allocation failures, or leaks on specific instances. It is used in Loki to filter logs that indicate potential memory performance issues affecting application stability or resource consumption.\n\n**Alert Threshold:** An alert should be triggered when the frequency of \"memory\"-related error or warning messages exceeds a defined threshold within a given time window (e.g., more than 5 memory error logs per minute on a single instance), or when logs indicate critical memory failures (e.g., \"out of memory\", \"memory allocation failed\").\n\n**Impact:**  \n- **High frequency or critical memory errors:** May lead to application crashes, degraded performance, or node instability, requiring immediate investigation and remediation.  \n- **Low or no memory-related logs:** Indicates normal memory operation and stable resource usage.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of logs matching **{\"message\": \"memory*\", \"instance\": \"*\"}** grouped by instance over time, highlighting spikes in memory-related errors.  \n- **Alert Rule:** Trigger an alert if the count of logs with **{\"message\": \"memory*\", \"instance\": \"*\"}** containing error keywords exceeds 5 per minute on any instance, signaling potential memory pressure or leaks needing SRE attention."
  },
  {
    "query": {
      "message": "memory*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"memory\" and the filename field is present. It identifies logs related to memory usage or memory-related events within Kubernetes components. The pattern does not measure a specific metric or unit but flags textual occurrences of memory-related messages for diagnostic purposes. Example: message=\"memory allocation failed\", filename=\"kubelet.log\"."
  },
  {
    "query": {
      "message": "memory*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"memory\" and the log is associated with any Kubernetes application label. It is used in Loki to identify logs related to memory usage, memory errors, or performance issues tied to memory consumption within applications.  \n\n**Purpose:**  \nTo monitor memory-related events across all applications, enabling detection of potential memory leaks, high memory utilization, or failures that could degrade application performance or stability.\n\n**Alert Thresholds:**  \nAn alert should be triggered if the frequency of memory-related log entries exceeds a defined threshold within a given time window—for example, more than 10 memory error or warning messages within 5 minutes—indicating abnormal memory behavior that requires investigation.\n\n**Impact:**  \n- **High frequency of memory-related logs:** May indicate memory leaks, insufficient memory allocation, or application crashes due to out-of-memory errors, potentially leading to degraded performance or downtime.  \n- **Low or no memory-related logs:** Suggests normal memory operation and stable application performance.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use the query `{message=~\"memory.*\", application=~\".*\"}` to count occurrences of memory-related logs per application over time. An alert can be configured to fire when the count exceeds the threshold, prompting SREs to investigate memory issues before they impact service availability."
  },
  {
    "query": {
      "message": "memory*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"memory\" and includes the Kubernetes \"component\" label, helping identify memory-related events or issues within specific components. It is used in Loki to filter logs that may indicate memory pressure, leaks, or failures affecting application or system performance. An alert should be triggered when the frequency of such memory-related log entries exceeds a defined threshold (e.g., more than 5 occurrences within 5 minutes), signaling potential memory resource exhaustion or degradation. High volumes of these logs often correlate with increased memory utilization or errors that can lead to application instability or outages, while low volumes typically indicate normal operation. For example, an alert rule might count logs matching **{\"message\": \"memory*\", \"component\": \"*\"}** over a time window and fire if the count surpasses the threshold. Similarly, a dashboard panel could display a time series graph of memory-related log counts per component to help SREs monitor memory health and quickly investigate anomalies."
  },
  {
    "query": {
      "message": "memory*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"memory\" and the source field is present with any value. It identifies logs related to memory usage or memory-related events within Kubernetes components. The pattern does not measure a specific metric or unit but serves to filter relevant memory-related log messages for analysis. Example: message=\"memory connection failed\", source=\"example-source\"."
  },
  {
    "query": {
      "message": "memory*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the message field contains the word \"memory\" and the pod_name label is present. It identifies logs related to memory usage or memory-related events within a specific Kubernetes pod. The pattern does not measure a specific unit but flags occurrences of memory-related messages for monitoring or troubleshooting purposes. Example: message=\"memory allocation failed\", pod_name=\"example-pod\"."
  },
  {
    "query": {
      "message": "memory*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"memory\" and the container_name field is present. It captures logs related to memory usage or memory-related events within a specific Kubernetes container. The pattern does not measure a numeric value or unit but identifies textual log messages indicating memory activity or issues. Example: message=\"memory allocation failed\", container_name=\"app-container\"."
  },
  {
    "query": {
      "message": "memory*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"memory\" and the cluster field is present. It identifies logs related to memory usage or memory-related events within a specific Kubernetes cluster. The pattern does not measure a numeric value or unit but flags occurrences of memory-related messages for monitoring or troubleshooting purposes. Example: message=\"memory connection failed\", cluster=\"example-cluster\"."
  },
  {
    "query": {
      "message": "memory*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the keyword \"memory\" and the entry is associated with a Kubernetes `pod`. It is used in Loki to identify logs related to memory usage, memory errors, or memory-related performance issues within specific pods.  \n\n**Purpose:**  \nTo monitor memory-related events in pods that may indicate memory pressure, leaks, or failures impacting application stability or performance.\n\n**Alert Thresholds:**  \nAn alert should be triggered if the frequency of logs matching this pattern exceeds a defined threshold within a given time window—for example, more than 10 memory-related error logs per 5 minutes in a single pod. Thresholds can be adjusted based on baseline memory behavior and application sensitivity.\n\n**Impact:**  \n- **High frequency:** Indicates potential memory exhaustion, leaks, or critical failures that could lead to pod crashes, degraded performance, or OOM (Out Of Memory) kills. Immediate investigation and remediation are required.  \n- **Low frequency or absence:** Normal operation or no detected memory issues.\n\n**Example Usage:**  \n- **Dashboard:** Display a panel showing the count of logs matching `{\"message\": \"memory*\", \"pod\": \"*\"}` grouped by pod over time to visualize spikes in memory-related events.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\".*memory.*\", pod=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 memory-related logs occur in any pod within 5 minutes.  \n\nThis pattern helps SREs proactively detect and respond to memory issues affecting pod stability and application reliability."
  },
  {
    "query": {
      "message": "memory*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"memory\" and includes any Kubernetes namespace label. It identifies logs related to memory usage or memory-related events within Kubernetes environments. The pattern does not measure a specific metric or unit but flags relevant log messages for monitoring or troubleshooting memory issues. Example: message=\"memory connection failed\", namespace=\"example-namespace\"."
  },
  {
    "query": {
      "message": "fail*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries from Kubernetes applications (`k8s_app`) where the `message` field contains the substring \"fail\", indicating failed operations, errors, or unsuccessful health checks within the application. In Loki, this filter helps identify and aggregate failure events across all Kubernetes apps for monitoring and alerting purposes.\n\n**Purpose:**  \nTo detect and track failure-related log messages emitted by any Kubernetes application, enabling early identification of issues affecting service reliability.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of log entries matching `{\"message\": \"fail*\", \"k8s_app\": \"*\"}` exceeds a defined threshold within a given time window (e.g., more than 5 failure messages in 5 minutes), signaling a potential systemic problem or degradation.\n\n**Impact of Values:**  \n- **High values:** A spike in failure logs suggests increased error rates, possible service outages, or degraded health, requiring immediate investigation.  \n- **Low or zero values:** Indicates normal operation with no recent failure messages detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of failure messages per Kubernetes app over time, helping SREs spot trends or sudden increases.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"fail.*\", k8s_app=~\".*\"}[5m]) > 5\n  ```  \n  This rule fires if more than 5 failure messages occur within 5 minutes across any Kubernetes app, prompting an alert for investigation."
  },
  {
    "query": {
      "message": "fail*",
      "container": "*"
    },
    "description": "This query filters log entries in Loki where the `message` field starts with the string \"fail\" (e.g., \"fail connection failed\") and the `container` label is present with any value. It is used to identify log events indicating failures or errors originating from any container within the monitored environment. The data source is container logs collected via Kubernetes logging pipelines, typically ingested in real-time. The query returns raw log lines matching these criteria, without aggregation or units, enabling rapid detection of failure-related messages. An unusual increase in the frequency or occurrence of such \"fail*\" messages should trigger an alert, as it may indicate systemic issues, degraded service health, or critical errors requiring immediate investigation."
  },
  {
    "query": {
      "message": "fail*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"fail,\" indicating a failure event such as an operation or health check failure. It specifically applies to logs that include a Kubernetes node identifier in the node_name field. The pattern measures the count of such failure-related log entries per node. The unit of measurement is the number of failure events recorded for each Kubernetes node."
  },
  {
    "query": {
      "message": "fail*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fail\" (e.g., \"fail connection failed\"), indicating failed operations or health check failures, and includes the Kubernetes `region` label to identify the source region. In Loki, this helps SREs monitor failure rates across different regions.\n\n**Purpose:** Detect and quantify failure events by region to quickly identify and localize issues affecting system reliability.\n\n**Alert Threshold:** An alert should trigger when the count of logs matching **{\"message\": \"fail*\", \"region\": \"*\"}** exceeds a predefined threshold within a set time window (e.g., more than 5 failures per minute per region), signaling a potential service degradation or outage.\n\n**Impact:**  \n- **High values:** Indicate frequent failures in a specific region, suggesting systemic problems that may impact user experience or service availability. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate normal operation with few or no failure events detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of failure logs per region over time, using a Loki query like:  \n  `count_over_time({message=~\"fail.*\"}[1m]) by (region)`  \n- **Alert Rule:** Trigger an alert if the failure count per region exceeds 5 within 1 minute:  \n  ```\n  sum by (region) (count_over_time({message=~\"fail.*\"}[1m])) > 5\n  ```  \nThis enables proactive detection of regional issues and supports targeted troubleshooting."
  },
  {
    "query": {
      "message": "fail*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fail\" (e.g., \"fail connection failed\"), indicating failed operations or health check failures within a Kubernetes service identified by the `service` label. In Loki, this pattern helps SREs quickly identify and aggregate failure events per service.\n\n**Purpose:**  \nTo detect and monitor failure occurrences across services, enabling timely investigation and remediation of issues impacting service reliability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"fail*\", \"service\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 failure logs per 5 minutes per service), signaling a potential service degradation or outage.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or ongoing failures in a service, potentially leading to degraded user experience or downtime. Immediate attention is required to diagnose and resolve root causes.  \n- **Low or zero values:** Suggest normal operation with no recent failure events detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of failure logs per service over time, using a Loki query like:  \n  `count_over_time({message=~\"fail.*\", service=~\".*\"}[5m]) by (service)`  \n- **Alert Rule:** Trigger an alert if the failure count exceeds 5 in 5 minutes for any service:  \n  ```\n  alert: ServiceFailureHigh\n  expr: count_over_time({message=~\"fail.*\", service=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High failure rate detected in {{ $labels.service }}\"\n    description: \"More than 5 failure logs detected in the last 5 minutes for service {{ $labels.service }}.\"\n  ```"
  },
  {
    "query": {
      "message": "fail*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fail\" (e.g., \"fail connection failed\"), indicating failed operations or health check failures within any Kubernetes job (`job=\"*\"`). In Loki, it helps identify and aggregate failure events across all jobs to monitor system reliability.\n\n**Purpose:** Detect and track failure occurrences across Kubernetes jobs to quickly surface operational issues.\n\n**Alert Threshold:** Trigger an alert if the count of logs matching `{\"message\": \"fail*\", \"job\": \"*\"}` exceeds a defined threshold within a given time window (e.g., more than 5 failures in 5 minutes), signaling a potential systemic problem.\n\n**Impact:**  \n- **High values:** Indicate frequent failures, possibly causing service degradation or outages, requiring immediate investigation.  \n- **Low or zero values:** Suggest stable operation with no recent failure events.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of failure logs per job over time using a Loki query like:  \n  `count_over_time({message=~\"fail.*\", job=~\".*\"}[5m])`  \n- **Alert Rule:**  \n  ```yaml\n  alert: HighFailureRate\n  expr: count_over_time({message=~\"fail.*\", job=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High failure rate detected in Kubernetes jobs\"\n    description: \"More than 5 failure logs detected in the last 5 minutes across jobs.\"\n  ```"
  },
  {
    "query": {
      "message": "fail*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fail\" (e.g., \"fail connection failed\") and includes any Kubernetes `environment` label. It is used in Loki to identify failed operations or health check failures within specific deployment environments.\n\n**Purpose:**  \nTo detect and monitor failure events that may indicate service disruptions, degraded performance, or infrastructure issues in different Kubernetes environments.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 failure logs in 5 minutes). Thresholds should be adjusted based on baseline failure rates for each environment.\n\n**Impact:**  \n- **High values:** A spike in failure logs suggests ongoing or escalating issues that could impact service availability or reliability, requiring immediate investigation.  \n- **Low or zero values:** Indicates stable operation with no recent failure events detected.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\n{environment=~\".*\"} |= \"fail\"\n```\nand create a Prometheus-style alert rule:  \n```\ncount_over_time({environment=~\".*\"} |= \"fail\"[5m]) > 5\n```\nThis rule triggers an alert if more than 5 failure logs occur within 5 minutes in any environment, enabling SREs to quickly identify and respond to issues."
  },
  {
    "query": {
      "message": "fail*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"fail,\" indicating a failure event such as an operation error or health check failure. The host field captures the Kubernetes node or pod name where the log originated. It measures the count of such failure-related log entries per host. The unit is the number of failure events recorded in the logs."
  },
  {
    "query": {
      "message": "fail*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"fail\" and includes any additional text. It captures events indicating failed operations or health checks. The \"level\" field records the severity or importance of the log entry as defined by Kubernetes. This pattern measures the occurrence count of such failure-related logs, with no specific unit beyond the number of matching entries."
  },
  {
    "query": {
      "message": "fail*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"fail\" and includes any subsequent characters. It specifically captures logs labeled with any Kubernetes environment value under the \"env\" field. The pattern measures the count of such log entries, representing occurrences of failed operations or health checks. The unit of measurement is the number of matching log entries."
  },
  {
    "query": {
      "message": "fail*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"fail\" and the entry includes any Kubernetes app label. It identifies occurrences of failed operations or health checks within applications. The measurement is a count of such log entries, representing the number of failure events per application. Units are discrete event counts."
  },
  {
    "query": {
      "message": "fail*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"fail\" (e.g., \"fail connection failed\") and includes the Kubernetes node label, indicating failed operations or health check issues on specific nodes. In Loki, this helps identify and isolate node-level failures quickly. An alert threshold might be set to trigger when the count of such failure logs exceeds a defined limit (e.g., more than 5 failure messages per node within 5 minutes), signaling potential node instability or service degradation. High values suggest persistent or widespread failures on a node, requiring immediate investigation, while low or zero values indicate normal operation. For example, an alert rule could use a query like `count_over_time({message=~\"fail.*\", node=~\".+\"}[5m]) > 5` to notify SREs of frequent failures per node. In dashboards, this pattern can be visualized as a time series graph showing failure counts per node, enabling rapid identification of problematic nodes."
  },
  {
    "query": {
      "message": "fail*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"fail,\" indicating a failure event such as an operation error or health check failure. It also requires the presence of the Kubernetes label \"app_kubernetes_io/name,\" identifying the application source of the log. The pattern measures the count of such failure-related log entries per application. The unit of measurement is the number of failure events recorded in the logs."
  },
  {
    "query": {
      "message": "fail*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fail\", indicating failed operations or health check failures on a specific Kubernetes instance identified by the `instance` label. In Loki, it helps surface critical failure events tied to particular pods or nodes.\n\n**Purpose:** To detect and monitor failure events that may impact application availability or stability.\n\n**Alert Threshold:** An alert should trigger when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 failure messages per 5 minutes), signaling a potential systemic issue.\n\n**Impact:**  \n- **High values:** A spike in failure logs suggests recurring errors or degraded service health, requiring immediate investigation to prevent outages.  \n- **Low or zero values:** Indicates normal operation with no recent failure events detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of `fail*` messages per instance to quickly identify which pods are experiencing failures.  \n- **Alert Rule (PromQL):**  \n  ```\n  count_over_time({message=~\"fail.*\", instance=~\".*\"}[5m]) > 5\n  ```  \nThis rule triggers an alert if more than 5 failure messages occur within 5 minutes for any instance, enabling proactive response to emerging issues."
  },
  {
    "query": {
      "message": "fail*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"fail\" (e.g., \"fail connection failed\"), and the log entry includes a `filename` label, typically representing the Kubernetes log source. It is used in Loki to identify failed operations, errors, or health check failures within containerized applications.\n\n**Purpose:**  \nTo detect and monitor failure events in application logs by filtering for messages indicating failure, enabling early identification of issues affecting system reliability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching **{\"message\": \"fail*\", \"filename\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 failure messages in 5 minutes). Thresholds should be tuned based on normal failure rates for the service.\n\n**Impact of Values:**  \n- **High values:** A spike in failure messages indicates increased error rates, potentially signaling service degradation, failed health checks, or operational issues requiring immediate investigation.  \n- **Low or zero values:** Normal operation with no detected failures, indicating system stability.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of failure messages over time, grouped by `filename` to identify which components are experiencing failures.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"fail.*\", filename=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 failure messages occur within 5 minutes, helping SREs respond proactively to emerging issues."
  },
  {
    "query": {
      "message": "fail*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"fail\" and includes any subsequent text. It specifically targets logs labeled with any Kubernetes application name. The pattern measures the count of such failure-related log messages within the system. Each matched entry represents one occurrence of a failed operation or health check associated with a Kubernetes application."
  },
  {
    "query": {
      "message": "fail*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fail\" (e.g., \"fail connection failed\"), indicating failed operations or health check failures within a specific Kubernetes `component`. In Loki, it helps identify and aggregate failure events by component to monitor system reliability.\n\n**Purpose:**  \nTo detect and track failure occurrences across Kubernetes components, enabling early identification of issues impacting service health.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"fail*\", \"component\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 failure logs per 5 minutes per component), signaling a potential systemic problem.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent failures in a component, suggesting degraded service health or ongoing incidents requiring immediate investigation.  \n- **Low or zero values:** Suggest normal operation with no recent failure logs detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of failure logs per component over time, using a Loki query like:  \n  `count_over_time({message=~\"fail.*\"}[5m]) by (component)`  \n- **Alert Rule:**  \n  ```\n  alert: ComponentFailureRateHigh\n  expr: count_over_time({message=~\"fail.*\"}[5m]) by (component) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High failure rate detected in {{ $labels.component }}\"\n    description: \"More than 5 failure logs in the last 5 minutes for component {{ $labels.component }}.\"\n  ```"
  },
  {
    "query": {
      "message": "fail*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"fail,\" indicating a failure event such as an operation error or health check failure. The \"source\" field identifies the Kubernetes component or service generating the log. It measures the count of such failure-related log entries, with each entry representing one occurrence. The unit of measurement is the number of log entries matching this pattern."
  },
  {
    "query": {
      "message": "fail*",
      "pod_name": "*"
    },
    "description": "This log pattern **{\"message\": \"fail*\", \"pod_name\": \"*\"}** is used in Loki to identify log entries from any Kubernetes pod where the message starts with or contains the substring \"fail\", signaling failed operations, errors, or health check failures within that pod. It helps SREs quickly detect and investigate issues impacting application stability or availability.\n\n**Purpose:**  \n- To monitor and surface failure-related events across all pods by filtering logs containing \"fail\" in their messages.  \n- Enables correlation of failures with specific pods via the `pod_name` label.\n\n**Alert Threshold:**  \n- An alert could be triggered when the count of such failure logs exceeds a defined threshold within a given time window (e.g., >5 failure messages per 5 minutes per pod), indicating a potential ongoing issue requiring immediate attention.\n\n**Impact of Values:**  \n- **High values:** A spike in failure logs suggests degraded service health, increased error rates, or failing components, potentially leading to outages or degraded user experience.  \n- **Low or zero values:** Indicates normal operation with no recent failure events detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of failure logs per pod over time using a query like:  \n  `count_over_time({message=~\"fail.*\", pod_name=~\".*\"}[5m]) by (pod_name)`  \n- **Alert Rule:**  \n  ```yaml\n  alert: PodFailureRateHigh\n  expr: count_over_time({message=~\"fail.*\", pod_name=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High failure rate detected in pod {{ $labels.pod_name }}\"\n    description: \"More than 5 failure messages detected in pod {{ $labels.pod_name }} over the last 5 minutes.\"\n  ```"
  },
  {
    "query": {
      "message": "fail*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fail\" (e.g., \"fail connection failed\"), indicating failed operations, errors, or health check failures within a specific Kubernetes container identified by `container_name`. In Loki, this filter helps isolate failure-related logs for targeted monitoring.\n\n**Purpose:**  \nTo detect and monitor failure events in containerized applications by capturing logs that indicate operational issues or health check failures.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"fail*\", \"container_name\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 failure messages in 5 minutes), signaling a potential systemic issue requiring investigation.\n\n**Impact of Values:**  \n- **High values:** A surge in failure logs suggests degraded application health, increased error rates, or failing dependencies, potentially impacting service availability or performance. Immediate attention is needed to prevent outages.  \n- **Low or zero values:** Indicates normal operation with no recent failure events detected, implying stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of failure messages per container over time using a Loki query like:  \n  `count_over_time({container_name=~\".*\"} |= \"fail\" [5m])`  \n  This helps identify containers experiencing frequent failures.  \n- **Alert Rule (Prometheus-style):**  \n  ```\n  alert: ContainerFailureRateHigh\n  expr: count_over_time({container_name=~\".*\"} |= \"fail\" [5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High failure rate detected in container {{ $labels.container_name }}\"\n    description: \"More than 5 failure messages detected in the last 5 minutes for container {{ $labels.container_name }}.\"\n  ```"
  },
  {
    "query": {
      "message": "fail*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the substring \"fail\" and the \"cluster\" field is present with any value. It identifies occurrences of failed operations or health checks within a specific Kubernetes cluster. The measurement is a count of such log entries per cluster. Units are discrete event counts labeled by cluster name."
  },
  {
    "query": {
      "message": "fail*",
      "pod": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod where the message begins with \"fail\", indicating failed operations, errors, or health check failures within that pod. In Loki, it helps identify and aggregate failure events across pods for monitoring system reliability.\n\n**Purpose:** To detect and track failure-related log messages per pod, enabling timely identification of issues affecting application stability or availability.\n\n**Alert Threshold:** An alert could be triggered when the count of logs matching **{\"message\": \"fail*\", \"pod\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 failure messages per pod in 5 minutes), signaling a potential incident.\n\n**Impact:**  \n- **High values:** Indicate frequent failures or degraded health in the pod, potentially leading to service disruption or degraded user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Suggest normal operation with no recent failure logs detected.\n\n**Example Usage:**  \nA Prometheus alert rule using Loki query:  \n```\nsum by (pod) (count_over_time({message=~\"fail.*\", pod=~\".*\"}[5m])) > 5\n```\nThis triggers an alert if any pod logs more than 5 failure messages in 5 minutes.\n\nIn a dashboard, this pattern can be visualized as a time series graph showing failure counts per pod, helping SREs quickly identify problematic pods and correlate failures with other metrics."
  },
  {
    "query": {
      "message": "fail*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"fail\", indicating failed operations or health check failures within any Kubernetes namespace. In Loki, it helps identify and aggregate failure events across namespaces to monitor system reliability. An alert using this pattern should trigger when the count of such failure logs exceeds a defined threshold (e.g., more than 5 failures within 5 minutes), signaling potential service degradation or outages. High values suggest recurring or widespread failures requiring immediate investigation, while low or zero values indicate stable operation. \n\nExample alert rule snippet:\n```\ncount_over_time({namespace=~\".*\"} |= \"fail\" [5m]) > 5\n```\n\nThis can be visualized in a dashboard as a time series graph showing failure counts per namespace, enabling SREs to quickly pinpoint problematic namespaces and respond accordingly."
  },
  {
    "query": {
      "message": "warn*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"warn\", indicating warning-level events that are non-critical. It specifically filters logs labeled with any Kubernetes application identifier in the \"k8s_app\" field. The metric counts the number of such warning log entries. The unit of measurement is the count of warning messages per log stream associated with Kubernetes applications."
  },
  {
    "query": {
      "message": "warn*",
      "container": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes container where the message begins with \"warn\", indicating warnings that may signal potential issues or misconfigurations but are not immediately critical. In Loki, **{\"message\": \"warn*\", \"container\": \"*\"}** helps SREs filter and monitor warning-level logs across all containers to proactively identify emerging problems.\n\n**Alert threshold:** An alert might be triggered if the count of such warning logs exceeds a defined threshold within a given time window (e.g., more than 50 warnings in 5 minutes), suggesting a systemic issue requiring investigation.\n\n**Impact:**  \n- **High values:** A surge in warning logs can indicate deteriorating system health, misconfigurations, or resource constraints that could escalate into critical failures if unaddressed.  \n- **Low values:** Few or no warnings typically imply stable operation, but sudden increases should prompt timely review.\n\n**Example usage:**  \nIn a Grafana dashboard, you could visualize the rate of warning messages per container using a Loki query like:  \n```\ncount_over_time({container=~\".*\"} |= \"warn\" [5m])\n```  \nAn alert rule might be configured to fire when this rate exceeds 50 warnings in 5 minutes, enabling SREs to detect and respond to abnormal warning patterns early."
  },
  {
    "query": {
      "message": "warn*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"warn\" (indicating warnings or potential issues) and includes the Kubernetes `node_name` label, helping to identify which node generated the warning. In Loki, this filter is used to surface non-critical warnings that may signal misconfigurations, resource constraints, or transient errors requiring attention before escalating to critical incidents.\n\n**Alert threshold:** An alert could be triggered if the count of such warning logs exceeds a defined threshold within a given time window (e.g., more than 10 warnings per node in 5 minutes), indicating a potential systemic issue or node instability.\n\n**Impact:**  \n- **High values:** A spike in warnings may precede failures or degraded performance, signaling the need for investigation or remediation on the affected node(s).  \n- **Low or zero values:** Normal operation with no immediate concerns related to warnings on nodes.\n\n**Example usage:**  \n- **Dashboard:** Display a time series panel showing the rate of warning messages per node using a Loki query like:  \n  `count_over_time({message=~\"warn.*\", node_name=~\".*\"}[5m]) by (node_name)`  \n- **Alert rule:** Trigger an alert if the warning count per node exceeds 10 in 5 minutes:  \n  ```yaml\n  alert: NodeWarningRateHigh\n  expr: count_over_time({message=~\"warn.*\", node_name=~\".*\"}[5m]) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: High warning rate on node {{ $labels.node_name }}\n    description: More than 10 warning messages detected on node {{ $labels.node_name }} in the last 5 minutes.\n  ```"
  },
  {
    "query": {
      "message": "warn*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"warn\", indicating non-critical warnings or potential misconfigurations, and includes the Kubernetes 'region' label to identify the source location. In Loki, it helps SREs monitor warning-level events across different regions to detect emerging issues before they escalate. An alert threshold might be set when the count of such warnings exceeds a defined rate (e.g., more than 10 warnings per minute per region), signaling potential instability or misconfiguration in that region. High values suggest increasing risk or degraded system health requiring investigation, while low values indicate normal operation. For example, a Grafana dashboard panel could use the query `{message=~\"warn.*\", region=~\".*\"}` to display warning counts by region over time, and an alert rule could trigger if warnings spike above the threshold, enabling proactive response."
  },
  {
    "query": {
      "message": "warn*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"warn\" and the service field is present with any value. It identifies warning-level log messages indicating potential issues that are non-critical. The measurement is a count of such warning log entries per service. Units are discrete log event counts."
  },
  {
    "query": {
      "message": "warn*",
      "job": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes job where the message begins with \"warn\", indicating warnings that may signal potential issues or misconfigurations but are not immediately critical. In Loki, **{\"message\": \"warn*\", \"job\": \"*\"}** helps SREs monitor warning-level events across all jobs to identify emerging problems early.\n\n**Alert threshold:** An alert could be triggered if the count of such warning messages exceeds a defined rate (e.g., more than 10 warnings per 5 minutes per job), suggesting a growing issue that requires investigation.\n\n**Impact:**  \n- **High values:** A spike in warnings may indicate misconfigurations, resource constraints, or intermittent failures that could escalate into critical incidents if unaddressed.  \n- **Low values:** Few or no warnings typically imply stable operation, but sudden increases should prompt review.\n\n**Example usage:**  \n- **Dashboard:** Display a time series graph showing the rate of warning messages per job using a Loki query like:  \n  `count_over_time({job=~\".*\"} |= \"warn\" [5m]) by (job)`  \n- **Alert rule:** Trigger an alert when the warning rate exceeds the threshold:  \n  `count_over_time({job=~\".*\"} |= \"warn\" [5m]) > 10`  \nThis enables proactive detection and resolution of issues before they impact system reliability."
  },
  {
    "query": {
      "message": "warn*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"warn\", indicating non-critical warnings or potential misconfigurations, across all Kubernetes environments. In Loki, it helps surface early signs of issues that may not yet impact system stability but could escalate if unaddressed. Alerts based on this pattern typically trigger when the count of such warning logs exceeds a defined threshold within a given time window (e.g., more than 50 warnings in 5 minutes), signaling a potential degradation or misconfiguration requiring investigation. A high volume of these warnings may indicate emerging problems that could lead to outages or performance issues, while low values generally reflect normal operation. For example, an alert rule could use a Loki query like `{message=~\"warn.*\", environment=~\".*\"}` to count warning logs per environment, triggering a PagerDuty notification if the count surpasses the threshold. Similarly, dashboards can visualize warning log trends over time to help SREs proactively identify and address issues before they escalate."
  },
  {
    "query": {
      "message": "warn*",
      "host": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes host where the message begins with \"warn\", indicating non-critical warnings or potential misconfigurations that may not immediately impact system functionality but could escalate if unresolved. In Loki, **{\"message\": \"warn*\", \"host\": \"*\"}** helps surface these warning-level events across all hosts for proactive monitoring.\n\n**Alert threshold:** Trigger an alert if the count of such warning messages exceeds a defined threshold (e.g., 100 warnings per 5 minutes) on any single host, signaling a potential issue that requires investigation before it affects system stability.\n\n**Impact:**  \n- **High values:** A spike in warnings may indicate emerging problems such as resource constraints, misconfigurations, or degraded service components that could lead to outages if ignored.  \n- **Low values:** Few or no warnings suggest normal operation with minimal risk.\n\n**Example usage:**  \nIn a Grafana dashboard, use a Loki query like `{message=~\"warn.*\", host=~\".*\"}` to visualize warning trends per host over time. For alerting, define a Prometheus alert rule that counts warning logs per host and fires when the count exceeds the threshold, enabling SREs to quickly identify and remediate issues before escalation."
  },
  {
    "query": {
      "message": "warn*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"warn\" (e.g., \"warn connection failed\") and includes any Kubernetes log level label. Its purpose in Loki is to identify non-critical warnings or potential misconfigurations that may not immediately disrupt service but could indicate underlying issues requiring attention.  \n\n**Alert Threshold:**  \nAn alert should be triggered when the count of these warning logs exceeds a defined threshold within a specific time window (e.g., more than 10 warnings in 5 minutes), signaling a potential degradation or emerging problem.  \n\n**Impact of Values:**  \n- **High values:** A spike in warning logs may indicate increasing instability, misconfigurations, or resource constraints that could escalate into critical failures if unaddressed.  \n- **Low values:** Occasional warnings are normal and may not require immediate action but should be monitored for trends.  \n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({message=~\"warn.*\", level=~\".*\"}[5m]) > 10\n```  \nThis query counts warning messages over the last 5 minutes and triggers an alert if the count exceeds 10, helping SREs proactively detect and investigate potential issues before they impact system reliability."
  },
  {
    "query": {
      "message": "warn*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"warn\", indicating non-critical warnings or potential misconfigurations, and includes any Kubernetes environment label (`env`). In Loki, **{\"message\": \"warn*\", \"env\": \"*\"}** helps SREs monitor warning-level events across all environments to identify emerging issues before they escalate.\n\n**Alert threshold:** An alert might trigger if the count of such warning logs exceeds a defined threshold within a given time window (e.g., more than 50 warning messages in 5 minutes), signaling a potential systemic issue or misconfiguration.\n\n**Impact:**  \n- **High values:** A spike in warnings may indicate deteriorating system health, misconfigurations, or intermittent failures that could lead to critical incidents if unaddressed.  \n- **Low values:** Few or no warnings suggest stable operation with minimal non-critical issues.\n\n**Example usage:**  \n- **Dashboard:** A panel showing the rate of warning messages per environment over time, enabling quick identification of environments with rising warning trends.  \n- **Alert rule:**  \n  ```\n  sum by (env) (count_over_time({message=~\"warn.*\", env=~\".*\"}[5m])) > 50\n  ```  \n  This triggers an alert when any environment logs more than 50 warnings in 5 minutes, prompting investigation before escalation."
  },
  {
    "query": {
      "message": "warn*",
      "app": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes application (`app=\"*\"`) where the message starts with \"warn\", indicating warnings that may signal potential issues or misconfigurations but are not immediately critical. In Loki, it helps identify and aggregate warning-level logs across all apps to monitor system health proactively.\n\n**Alert threshold:** An alert could be triggered if the count of these warning logs exceeds a defined threshold within a specific time window (e.g., more than 50 warnings in 5 minutes), suggesting a growing problem that requires investigation.\n\n**Impact:**  \n- **High values:** A spike in warning logs may indicate emerging issues such as resource constraints, misconfigurations, or degraded service performance that could escalate if unaddressed.  \n- **Low values:** Few or no warnings typically imply stable application behavior and healthy system operation.\n\n**Example usage:**  \nIn a Grafana dashboard, use this pattern to create a panel showing the rate of warning messages per app over time, helping SREs spot trends. For alerting, a PromQL-inspired Loki query might be:  \n```\ncount_over_time({app=\"*\"} |= \"warn\" [5m]) > 50\n```\nThis triggers an alert when more than 50 warning logs occur within 5 minutes, prompting timely investigation before issues worsen."
  },
  {
    "query": {
      "message": "warn*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"warn\" and includes a Kubernetes node label, capturing non-critical warnings or potential misconfigurations specific to individual nodes. In Loki, **{\"message\": \"warn*\", \"node\": \"*\"}** helps SREs monitor warning-level events that may indicate emerging issues before they escalate. \n\n**Alert threshold:** Trigger an alert if the count of such warning logs exceeds a defined threshold (e.g., 10 warnings per node within 5 minutes), signaling a potential problem requiring investigation.\n\n**Impact:**  \n- **High values:** A spike in warnings on a node may indicate misconfigurations, resource constraints, or transient failures that could degrade node stability or application performance if unaddressed.  \n- **Low values:** Occasional warnings are normal and typically do not require action.\n\n**Example usage:**  \n- **Dashboard:** Display a time series graph showing the rate of warning messages per node using a Loki query like:  \n  `count_over_time({message=~\"warn.*\", node=~\".*\"}[5m]) by (node)`  \n- **Alert rule:** Fire an alert when the warning count per node exceeds 10 within 5 minutes:  \n  ```\n  alert: NodeWarningSpike\n  expr: count_over_time({message=~\"warn.*\", node=~\".*\"}[5m]) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High warning rate on node {{ $labels.node }}\"\n    description: \"More than 10 warning messages detected on node {{ $labels.node }} in the last 5 minutes.\"\n  ```"
  },
  {
    "query": {
      "message": "warn*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"warn\", indicating non-critical warnings or potential misconfigurations, and includes the Kubernetes label `app_kubernetes_io/name` to identify the specific application emitting the warning. In Loki, this helps isolate warning-level logs per application for proactive monitoring.\n\n**Purpose:**  \nTo surface warning messages from applications, enabling SREs to detect early signs of issues that do not yet cause failures but may degrade performance or lead to errors if unaddressed.\n\n**Alert Threshold:**  \nAn alert could be triggered when the count of warning messages per application exceeds a defined threshold within a given time window (e.g., more than 10 warnings in 5 minutes), signaling a potential emerging problem.\n\n**Impact of Values:**  \n- **High warning counts:** May indicate recurring misconfigurations, resource constraints, or intermittent failures that require investigation before escalating to errors.  \n- **Low or zero warnings:** Suggests stable application behavior with no immediate concerns.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a query like:  \n```\ncount_over_time({app_kubernetes_io/name=~\".*\", message=~\"warn.*\"}[5m])\n```\nto monitor the number of warnings per application over the last 5 minutes. An alert rule could fire if this count exceeds 10 for any application, prompting an SRE to review the logs and address underlying issues before they escalate."
  },
  {
    "query": {
      "message": "warn*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"warn\", indicating non-critical warnings or potential misconfigurations, and includes the Kubernetes 'instance' label identifying the source. In Loki, **{\"message\": \"warn*\", \"instance\": \"*\"}** helps SREs monitor warning-level events across all instances to detect emerging issues before they escalate. \n\n**Alert threshold:** Trigger an alert if the count of such warning logs exceeds a defined threshold (e.g., 50 warnings per 5 minutes) for any single instance, signaling a potential problem requiring investigation.\n\n**Impact:**  \n- **High values:** A surge in warnings may indicate misconfigurations, resource constraints, or degraded service health that could lead to outages if unaddressed.  \n- **Low values:** Few or no warnings suggest stable operation with no immediate concerns.\n\n**Example usage:**  \n- **Dashboard:** Display a time series graph showing the rate of warning messages per instance using the query:  \n  `count_over_time({message=~\"warn.*\", instance=~\".*\"}[5m])`  \n- **Alert rule:** Fire an alert if the warning count per instance exceeds 50 within 5 minutes:  \n  `count_over_time({message=~\"warn.*\", instance=~\".*\"}[5m]) > 50`  \nThis enables proactive detection and remediation of issues before they impact service reliability."
  },
  {
    "query": {
      "message": "warn*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"warn\", indicating non-critical warnings or potential misconfigurations, and includes any Kubernetes log filename. In Loki, **{\"message\": \"warn*\", \"filename\": \"*\"}** helps identify warning-level events that may not immediately disrupt service but could signal emerging issues.\n\n**Purpose:**  \nTo surface warning messages that warrant attention before escalating to errors, enabling proactive investigation and remediation.\n\n**Alert Threshold:**  \nAn alert might trigger if the count of warning logs exceeds a defined threshold within a time window—for example, more than 50 warnings in 5 minutes—indicating a potential systemic issue or misconfiguration.\n\n**Impact of Values:**  \n- **High warning counts:** May suggest growing instability, misconfigurations, or resource constraints that could degrade service quality if unaddressed.  \n- **Low or zero warnings:** Typically indicates stable operation with no immediate concerns.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you could use a Loki query like:  \n```\ncount_over_time({message=~\"warn.*\", filename=~\".*\"}[5m])\n```\nto monitor warning frequency. An alert rule might fire when this count exceeds 50 within 5 minutes, prompting an SRE to investigate logs for underlying causes before they escalate into errors or outages."
  },
  {
    "query": {
      "message": "warn*",
      "application": "*"
    },
    "description": "This query filters log entries in Loki where the `message` field starts with the string \"warn\" (case-sensitive) and the `application` label is present with any value. It returns all warning-level log messages emitted by any application, typically indicating potential issues or non-critical warnings within the system. The data source is structured log streams ingested into Loki, with each log entry containing labels such as `application` to identify the originating service or component. Metrics derived from this query represent counts or occurrences of warning messages over time, useful for monitoring application health. An unusually high or sudden spike in these warning logs for a specific application may indicate emerging problems or misconfigurations that warrant investigation and should trigger alerts to prevent escalation into critical failures."
  },
  {
    "query": {
      "message": "warn*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"warn\" and includes any Kubernetes component label, capturing non-critical warnings or potential misconfigurations emitted by various system components. In Loki, it helps SREs monitor warning-level events that may indicate emerging issues before they escalate. An alert threshold might be set on the rate of these warnings (e.g., more than 10 warnings per minute) to detect abnormal behavior or degradation. High values suggest increasing instability or misconfiguration risks requiring investigation, while low values indicate normal operation with minimal warnings. For example, an alert rule could count log entries matching **{\"message\": \"warn*\", \"component\": \"*\"}** over a 5-minute window and trigger if the count exceeds 50, signaling a spike in warnings from any component. Similarly, a dashboard panel could visualize the warning rate per component to identify sources of recurring warnings."
  },
  {
    "query": {
      "message": "warn*",
      "source": "*"
    },
    "description": "This pattern matches log entries whose message begins with the text \"warn\" and that include any value in the source field. It identifies non-critical warning messages generated by Kubernetes components or applications. The pattern measures the occurrence count of such warning log entries. The unit of measurement is the number of matching log events."
  },
  {
    "query": {
      "message": "warn*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod (`pod_name=\"*\"`) where the message contains the substring \"warn\", typically indicating non-critical warnings or potential misconfigurations. In Loki, this filter helps surface warning-level events that may not cause immediate failures but could signal emerging issues.\n\n**Purpose:**  \nTo monitor warning messages across all pods, enabling early detection of conditions that might escalate if unaddressed.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of warning messages exceeds a defined threshold within a given time window (e.g., more than 10 warnings in 5 minutes), indicating a potential systemic issue or misconfiguration.\n\n**Impact of Values:**  \n- **High warning count:** May indicate recurring misconfigurations, degraded service health, or unstable components requiring investigation.  \n- **Low or zero warnings:** Suggests stable operation with no immediate concerns.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of warning messages per pod using the query:  \n  `count_over_time({pod_name=~\".*\"} |= \"warn\" [5m])`  \n- **Alert Rule:** Trigger an alert if warnings exceed 10 in 5 minutes:  \n  ```yaml\n  alert: HighWarningRate\n  expr: count_over_time({pod_name=~\".*\"} |= \"warn\" [5m]) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High warning rate detected in pod {{ $labels.pod_name }}\"\n    description: \"More than 10 warning messages have been logged in pod {{ $labels.pod_name }} over the last 5 minutes.\"\n  ```"
  },
  {
    "query": {
      "message": "warn*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"warn\" (indicating warnings or potential issues) and includes any Kubernetes container name (`container_name`). In Loki, it is used to surface non-critical warnings that may signal misconfigurations, degraded performance, or emerging problems that do not yet cause failures but warrant attention.\n\n**Purpose:**  \nTo proactively detect warning-level events across all containers, enabling early identification of issues before they escalate into errors or outages.\n\n**Alert Threshold:**  \nAn alert could be triggered when the count of logs matching **{\"message\": \"warn*\", \"container_name\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 10 warning messages per container in 5 minutes). Thresholds should be tuned based on baseline warning rates for your environment.\n\n**Impact of Values:**  \n- **High values:** May indicate systemic issues, misconfigurations, or resource constraints affecting one or more containers, potentially leading to degraded service or failures if unaddressed.  \n- **Low or zero values:** Typically indicate normal operation with no recent warnings.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of warning messages per container over time, helping SREs spot spikes or trends.  \n- **Alert Rule (PromQL):**  \n  ```\n  count_over_time({message=~\"warn.*\", container_name=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if any container emits more than 10 warning messages in 5 minutes, prompting investigation before issues escalate."
  },
  {
    "query": {
      "message": "warn*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"warn\", indicating non-critical warnings or potential misconfigurations within any Kubernetes cluster (identified by the \"cluster\" label). In Loki, it helps surface early signs of issues that do not cause immediate failures but may degrade system performance or reliability if left unaddressed.\n\n**Purpose:** To monitor warning-level events across all clusters, enabling proactive detection of emerging problems before they escalate.\n\n**Alert Threshold:** An alert could be triggered if the count of warning messages exceeds a defined threshold within a given time window (e.g., more than 50 warnings in 5 minutes), signaling a potential systemic issue requiring investigation.\n\n**Impact of Values:**  \n- **High warning counts:** May indicate widespread misconfigurations, resource constraints, or intermittent failures that could lead to service degradation or outages if unresolved.  \n- **Low warning counts:** Typically expected during normal operations; occasional warnings may not require immediate action but should be tracked over time.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of warning messages per cluster over time, using a Loki query like:  \n  `count_over_time({message=~\"warn.*\", cluster=~\".*\"}[5m]) by (cluster)`  \n- **Alert Rule:** Trigger an alert when warnings exceed 50 in 5 minutes for any cluster:  \n  `sum by (cluster) (count_over_time({message=~\"warn.*\", cluster=~\".*\"}[5m])) > 50`  \nThis enables SREs to quickly identify clusters experiencing abnormal warning levels and prioritize remediation efforts."
  },
  {
    "query": {
      "message": "warn*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"warn\" and the pod label is present with any value. It identifies warning-level messages generated by Kubernetes pods, indicating potential issues that are non-critical. The measurement is a count of such warning log entries, with no specific unit beyond the number of occurrences. For example, a log with message=\"warn connection failed\" and pod=\"example-pod\" would be included."
  },
  {
    "query": {
      "message": "warn*",
      "namespace": "*"
    },
    "description": "This query pattern **{\"message\": \"warn*\", \"namespace\": \"*\"}** in Loki filters log entries where the `message` field starts with the string \"warn\" (case-sensitive prefix match), capturing warning-level log messages across all Kubernetes namespaces (`namespace` label). It returns raw log lines emitted by applications or system components running within Kubernetes clusters, sourced from container logs ingested into Loki. The data unit is individual log entries matching the pattern, enabling monitoring of warning events without filtering by severity beyond the \"warn\" prefix. Unusual spikes or sustained increases in the volume of these warning messages within any namespace may indicate emerging issues such as misconfigurations, degraded service health, or transient failures, and should trigger alerts to prompt investigation before escalation to errors or critical failures."
  },
  {
    "query": {
      "message": "panic*",
      "k8s_app": "*"
    },
    "description": "This log pattern **{\"message\": \"panic*\", \"k8s_app\": \"*\"}** is used in Loki to identify occurrences of Go application panics within Kubernetes pods, where the log message contains the keyword \"panic\" and is labeled by the Kubernetes application name (`k8s_app`). Panics typically indicate critical runtime errors that cause the application to crash or become unstable.\n\n**Purpose:**  \nTo detect and monitor unexpected application crashes or severe errors in Kubernetes workloads, enabling rapid identification and remediation of stability issues.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of panic log entries exceeds a defined threshold within a short time window (e.g., more than 1 panic event per 5 minutes), as even a single panic can indicate a serious problem. Thresholds can be adjusted based on application criticality and baseline panic frequency.\n\n**Impact:**  \n- **High values:** Frequent panics suggest unstable application behavior, potential data loss, degraded service availability, and increased incident risk. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable application operation with no recent critical runtime failures.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of panic logs over time, grouped by `k8s_app`, to quickly identify which applications are experiencing crashes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (k8s_app) (count_over_time({message=~\"panic.*\", k8s_app=~\".*\"}[5m])) > 1\n  ```  \n  This triggers an alert if more than one panic log is detected for any Kubernetes app within 5 minutes."
  },
  {
    "query": {
      "message": "panic*",
      "container": "*"
    },
    "description": "This pattern captures logs where the message contains 'panic', indicating a Go application panic, often leading to crash, and the log entry is labeled with Kubernetes 'container'. Example log entry: message='panic connection failed', container='example-container'."
  },
  {
    "query": {
      "message": "panic*",
      "node_name": "*"
    },
    "description": "This pattern **{\"message\": \"panic*\", \"node_name\": \"*\"}** is used in Loki to identify log entries where the message contains the keyword \"panic,\" typically indicating a Go application panic event on a specific Kubernetes node (denoted by `node_name`). Such panics often cause application crashes or restarts, impacting service availability.\n\n**Purpose:**  \nTo detect and monitor occurrences of application panics across nodes, enabling rapid identification of stability issues in Go-based services running in the cluster.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of panic log entries exceeds a defined threshold within a short time window (e.g., more than 5 panics per node within 5 minutes), signaling a potential systemic failure or recurring crash.\n\n**Impact of Values:**  \n- **High values:** Frequent panics indicate unstable application behavior, likely causing degraded service performance, increased error rates, or downtime. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation with no detected panics, indicating stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the number of panic events per node over time, using a query like:  \n  `count_over_time({message=~\"panic.*\", node_name=~\".*\"}[5m]) by (node_name)`  \n- **Alert Rule:** Trigger an alert if panic count per node exceeds 5 in 5 minutes:  \n  ```yaml\n  alert: ApplicationPanicDetected\n  expr: count_over_time({message=~\"panic.*\", node_name=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High number of panics detected on node {{ $labels.node_name }}\"\n    description: \"More than 5 panic events have occurred on node {{ $labels.node_name }} in the last 5 minutes, indicating potential application instability.\"\n  ```"
  },
  {
    "query": {
      "message": "panic*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"panic\", indicating that a Go application has encountered a runtime panic, which often causes the application to crash or become unstable. The presence of the Kubernetes 'region' label allows for regional aggregation and filtering of these panic events.\n\n**Purpose:**  \nTo detect and monitor critical runtime failures in Go applications across different Kubernetes regions by capturing panic occurrences in logs.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of panic log entries exceeds a defined threshold within a given time window (e.g., more than 5 panic messages in 5 minutes per region), signaling a potential widespread or recurring application failure.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or ongoing panics, suggesting severe instability or crashes in the application that require immediate investigation and remediation.  \n- **Low or zero values:** Indicate stable application behavior with no recent panics detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of panic log entries grouped by `region` over time to identify spikes or trends in application crashes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (region) (count_over_time({message=~\"panic.*\"}[5m])) > 5\n  ```  \n  This rule triggers an alert if more than 5 panic messages occur within 5 minutes in any region."
  },
  {
    "query": {
      "message": "panic*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"panic\" and the `service` label is set, typically indicating that a Go application has encountered a runtime panic. In Loki, this helps identify critical failures that often cause application crashes or instability within specific Kubernetes services.\n\n**Purpose:**  \nTo detect and monitor occurrences of panics in Go applications across services, enabling rapid identification of severe errors that require immediate attention.\n\n**Alert Threshold:**  \nAn alert should trigger if the number of panic log entries exceeds a low threshold (e.g., 1 or more) within a short time window (e.g., 5 minutes), as even a single panic can indicate a critical failure. For sustained issues, a higher threshold (e.g., >5 panics in 5 minutes) may indicate systemic problems needing urgent investigation.\n\n**Impact:**  \n- **High values:** Frequent panics suggest unstable application behavior, potential crashes, and degraded service availability, requiring immediate remediation.  \n- **Low values:** Occasional panics might be isolated incidents but still warrant prompt review to prevent escalation.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"panic.*\", service=~\".*\"}[5m])\n```\nto count panic occurrences per service over the last 5 minutes. An alert can be configured to fire when this count exceeds 0, signaling that a panic has occurred and needs investigation."
  },
  {
    "query": {
      "message": "panic*",
      "job": "*"
    },
    "description": "This pattern **{\"message\": \"panic*\", \"job\": \"*\"}** in Loki captures log entries where the message contains the term \"panic\", typically indicating a Go application panic event within a Kubernetes job. These panics often lead to application crashes or restarts, impacting service availability.\n\n**Purpose:**  \nTo monitor and detect occurrences of application panics across all Kubernetes jobs, enabling early identification of critical failures.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of panic log entries exceeds a defined threshold within a short time window (e.g., more than 1 panic event in 5 minutes), as even a single panic can indicate a severe issue.\n\n**Impact:**  \n- **High values:** Frequent panics suggest unstable application behavior, likely causing service disruptions, degraded user experience, or cascading failures. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable application operation with no recent panics, reflecting healthy service status.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of panic events per job over time, using a query like:  \n  `count_over_time({job=~\".*\"} |= \"panic\" [5m])`  \n- **Alert Rule:** Trigger an alert if panic count per job exceeds 1 in 5 minutes:  \n  ```\n  alert: ApplicationPanicDetected\n  expr: count_over_time({job=~\".*\"} |= \"panic\" [5m]) > 1\n  for: 1m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"Panic detected in job {{ $labels.job }}\"\n    description: \"More than one panic event detected in the last 5 minutes for job {{ $labels.job }}. Immediate investigation required.\"\n  ```"
  },
  {
    "query": {
      "message": "panic*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"panic\" and includes the Kubernetes 'environment' label, typically indicating a Go application panic event that may cause service crashes or instability. In Loki, it helps SREs quickly identify critical runtime failures across different environments. An alert threshold is commonly set to trigger when the count of such panic logs exceeds a low number (e.g., >0 or >1) within a short time window (e.g., 5 minutes), as even a single panic can signify a severe issue requiring immediate attention. High values indicate frequent or repeated panics, suggesting systemic problems or cascading failures, while low or zero values imply stable application behavior. Example alert rule snippet:  \n```\ncount_over_time({message=~\"panic.*\", environment=~\".*\"}[5m]) > 0\n```  \nThis can be visualized in a dashboard as a time series graph showing the rate of panic occurrences per environment, enabling rapid detection and response to critical application errors."
  },
  {
    "query": {
      "message": "panic*",
      "host": "*"
    },
    "description": "This query filters log entries in Loki where the `message` field starts with \"panic\" (e.g., \"panic: runtime error\") and the `host` label is present, typically representing the hostname or pod name in a Kubernetes environment. It captures runtime panic events from Go applications or similar systems that log critical failures leading to abrupt termination or crashes. The data source is application logs ingested into Loki, with each log entry labeled by `host` to identify the origin. The returned results are raw log lines matching the pattern, without aggregation or units. An unusual spike or repeated occurrence of such panic logs within a short timeframe indicates instability or critical failures in the application, warranting immediate alerting to prevent downtime or data loss."
  },
  {
    "query": {
      "message": "panic*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"panic,\" indicating a runtime panic event in a Go application. The \"level\" field captures the severity or log level assigned by Kubernetes. It measures the occurrence count of such panic events as discrete log entries. Each matched entry represents one instance of a panic-related log message."
  },
  {
    "query": {
      "message": "panic*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"panic\"—typically indicating a Go application panic event—and includes the Kubernetes environment label `env`. In Loki, this helps SREs quickly identify critical runtime failures that often cause application crashes or restarts.\n\n**Purpose:**  \nTo detect and monitor panic occurrences across different environments, enabling rapid response to severe application errors.\n\n**Alert Threshold:**  \nTrigger an alert if the number of panic log entries exceeds a defined threshold within a short time window (e.g., >5 panics in 5 minutes), signaling a potential widespread or recurring failure.\n\n**Impact:**  \n- **High values:** Indicate frequent or ongoing panics, likely causing service instability, degraded performance, or downtime. Immediate investigation and remediation are required.  \n- **Low or zero values:** Suggest stable application behavior with no recent panics.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of panic logs per environment over time, enabling trend analysis and environment comparison.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"panic.*\", env=~\".*\"}[5m]) > 5\n  ```  \n  This fires if more than 5 panic logs occur in any environment within 5 minutes.\n\nBy monitoring this pattern, SREs can proactively detect critical Go application failures and maintain system reliability."
  },
  {
    "query": {
      "message": "panic*",
      "app": "*"
    },
    "description": "This pattern **{\"message\": \"panic*\", \"app\": \"*\"}** in Loki is used to identify log entries from any application (`app=\"*\"`) where the message contains the substring \"panic\", typically indicating a Go runtime panic event. Such panics often cause the application to crash or enter an unstable state, making these logs critical for early detection of severe runtime errors.\n\n**Purpose:**  \nTo monitor and alert on application panics across all services, enabling rapid response to crashes or critical failures.\n\n**Alert Threshold:**  \nAn alert should trigger if the number of panic log entries exceeds a defined threshold within a short time window (e.g., > 0 panics in 5 minutes), as even a single panic can indicate a serious issue. For noisy environments, thresholds can be adjusted (e.g., > 3 panics in 5 minutes) to reduce false positives.\n\n**Impact of Values:**  \n- **High values:** Frequent panics suggest systemic instability or recurring bugs, potentially leading to service downtime or degraded user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable application behavior with no detected panics, reflecting healthy runtime conditions.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of panic logs per application over time, highlighting spikes that may correlate with incidents.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"panic.*\", app=~\".*\"}[5m]) > 0\n  ```  \n  This rule fires if any panic logs appear in the last 5 minutes, prompting an alert to the SRE team for investigation."
  },
  {
    "query": {
      "message": "panic*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"panic\" and includes the Kubernetes node label, indicating that a Go application running on that node has encountered a panic event—an unexpected error causing the application to crash or terminate abruptly. In Loki, this pattern helps SREs quickly identify and correlate panic incidents to specific nodes for faster troubleshooting.\n\n**Alerting guidance:**  \nSet a threshold based on the number of panic occurrences within a defined time window (e.g., alert if there are ≥ 1 panic logs per node within 5 minutes). A single panic event often warrants immediate attention due to its potential to cause service disruption.\n\n**Impact:**  \n- **High values:** Frequent panics on a node indicate unstable application behavior, risking downtime and degraded user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation; no panics detected, indicating stable application health.\n\n**Example alert rule snippet (PromQL for Loki):**  \n```\ncount_over_time({message=~\"panic.*\", node=~\".*\"}[5m]) > 0\n```\nThis triggers an alert if any panic log is detected on any node within the last 5 minutes.\n\n**Example dashboard usage:**  \nVisualize the count of panic logs per node over time to identify nodes with recurring panics, enabling targeted debugging and capacity planning."
  },
  {
    "query": {
      "message": "panic*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"panic\" and the entry is labeled with `app_kubernetes_io/name`, identifying the specific Kubernetes application. It is used in Loki to detect Go application panics, which typically indicate critical runtime errors causing the application to crash or become unstable.\n\n**Purpose:**  \nTo monitor and alert on application panics by filtering logs that signal severe failures within Kubernetes workloads, enabling rapid detection and response to service disruptions.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of panic log entries for a given `app_kubernetes_io/name` exceeds a defined threshold within a short time window (e.g., more than 1 panic event within 5 minutes), as even a single panic often requires immediate investigation.\n\n**Impact of Values:**  \n- **High values:** Frequent panics indicate unstable application behavior, likely causing service outages or degraded performance, requiring urgent remediation.  \n- **Low or zero values:** Normal operation with no detected panics, indicating stable application health.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"panic.*\", app_kubernetes_io/name=~\".+\"}[5m])\n```\nto count panic events per application over the last 5 minutes. Configure an alert to fire if this count is greater than 0, signaling that a panic has occurred and needs immediate attention."
  },
  {
    "query": {
      "message": "panic*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"panic\" and the `instance` label identifies the Kubernetes pod or node emitting the log. It is used in Loki to detect Go application panics, which typically indicate critical runtime errors that can cause application crashes or instability.\n\n**Purpose:**  \nTo monitor and alert on occurrences of panics in Go applications running in Kubernetes, enabling rapid detection of severe failures.\n\n**Alert Threshold:**  \nAn alert should trigger if the number of panic log entries exceeds a defined threshold within a short time window (e.g., > 0 panics in 5 minutes), as even a single panic often requires immediate investigation.\n\n**Impact:**  \n- **High values:** Frequent panics suggest unstable application behavior, potential crashes, and degraded service reliability, warranting urgent remediation.  \n- **Low or zero values:** Indicates stable application operation without critical runtime failures.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of panic logs per instance over the last hour to identify affected pods.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"panic.*\", instance=~\".*\"}[5m]) > 0\n  ```  \n  This fires an alert if any panic logs are detected in the last 5 minutes on any instance."
  },
  {
    "query": {
      "message": "panic*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"panic\", typically indicating a Go application panic event that often causes the application to crash or become unstable. The `filename` label corresponds to the Kubernetes log source, helping identify which component or pod generated the panic.\n\n**Purpose:**  \nIn Loki, this pattern is used to detect critical runtime errors in Go applications by filtering logs that signal panics. Monitoring these events helps SREs quickly identify and respond to application crashes or severe faults.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of panic log entries exceeds a defined threshold within a short time window (e.g., more than 1 panic event per 5 minutes per pod), as even a single panic can indicate a serious issue requiring immediate attention.\n\n**Impact of Values:**  \n- **High values:** Frequent panic logs suggest recurring crashes or instability, potentially leading to service downtime or degraded user experience. This requires urgent investigation and remediation.  \n- **Low or zero values:** Indicates stable application behavior with no recent panics, implying normal operation.\n\n**Example Alert Rule (PromQL for Loki):**  \n```promql\ncount_over_time({filename=~\".*\"} |= \"panic\" [5m]) > 1\n```\nThis alert fires if more than one panic message is logged in any pod within 5 minutes.\n\n**Example Dashboard Usage:**  \nA dashboard panel can display a time series graph of panic event counts per pod over time, using a query like:  \n```promql\ncount_over_time({filename=~\".*\"} |= \"panic\" [1m]) by (filename)\n```\nThis helps visualize panic frequency and identify problematic pods quickly."
  },
  {
    "query": {
      "message": "panic*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"panic\" from any application, typically indicating a Go runtime panic that may cause the application to crash or become unstable. In Loki, it helps identify critical failures in Kubernetes applications by filtering for panic events labeled with the application name.\n\n**Purpose:** To detect and monitor panic occurrences in applications, enabling rapid response to crashes or severe errors.\n\n**Alert Threshold:** Trigger an alert if the number of panic log entries exceeds a defined threshold within a short time window (e.g., >5 panics in 5 minutes), signaling a recurring or ongoing critical failure.\n\n**Impact:**  \n- **High values:** Frequent panics suggest unstable application behavior, potential downtime, and degraded user experience requiring immediate investigation.  \n- **Low or zero values:** Normal operation with no recent critical panics detected.\n\n**Example Alert Rule:**  \n```\nsum by (application) (count_over_time({message=~\"panic.*\", application=~\".*\"}[5m])) > 5\n```\nThis alerts when any application logs more than 5 panic messages in 5 minutes.\n\n**Example Dashboard Usage:**  \nVisualize the rate of panic messages per application over time to quickly identify spikes or trends indicating instability, enabling proactive troubleshooting before widespread impact."
  },
  {
    "query": {
      "message": "panic*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"panic,\" indicating a runtime panic event in a Go application. The component field identifies the Kubernetes component generating the log. It measures the count of such panic events per component. Each matched log entry represents one panic occurrence."
  },
  {
    "query": {
      "message": "panic*",
      "source": "*"
    },
    "description": "This pattern **{\"message\": \"panic*\", \"source\": \"*\"}** in Loki captures log entries where the message contains the substring \"panic\", typically indicating a Go application panic event. These panics often cause the application to crash or enter an unstable state, making this pattern critical for early detection of severe runtime failures.\n\n**Purpose:**  \nTo identify and monitor occurrences of application panics across all sources labeled by Kubernetes, enabling rapid response to critical errors that can impact service availability.\n\n**Alert Threshold:**  \nAn alert should be triggered if the count of panic log entries exceeds a defined threshold within a short time window, for example:  \n- More than 1 panic event in 5 minutes for a single source, or  \n- A sudden spike compared to baseline panic rates (e.g., a 5x increase over the average panic count in the last hour).\n\n**Impact of Values:**  \n- **High panic counts:** Indicate frequent or repeated application crashes, likely causing downtime or degraded service. Immediate investigation and remediation are required.  \n- **Low or zero panic counts:** Suggest stable application behavior with no recent critical failures.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: ApplicationPanicDetected\nexpr: sum by (source) (count_over_time({message=~\"panic.*\", source=~\".*\"}[5m])) > 1\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"Panic events detected in application logs (source={{ $labels.source }})\"\n  description: \"More than 1 panic event detected in the last 5 minutes for source {{ $labels.source }}. Immediate investigation recommended.\"\n```\n\n**Example Dashboard Panel Query:**  \n```logql\ncount_over_time({message=~\"panic.*\", source=~\".*\"}[5m])\n```\nThis query shows the number of panic events per source over 5-minute intervals, helping SREs visualize panic frequency and identify problematic components quickly."
  },
  {
    "query": {
      "message": "panic*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the message contains the term \"panic\", typically indicating that a Go application running in the specified Kubernetes pod (`pod_name`) has encountered a runtime panic, which often causes the application to crash or restart. In Loki, this pattern helps SREs quickly identify critical failures at the pod level.\n\n**Purpose:**  \nTo detect and monitor unexpected panics in Go applications by filtering logs for panic-related messages tagged by pod name, enabling rapid identification of unstable or failing pods.\n\n**Alert Threshold:**  \nAn alert should be triggered if the number of panic occurrences exceeds a defined threshold within a short time window (e.g., more than 3 panic messages per pod within 5 minutes), signaling a recurring or unresolved issue.\n\n**Impact:**  \n- **High values:** Frequent panics indicate unstable application behavior, potential service disruption, increased pod restarts, and degraded user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation with no detected panics, indicating stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of panic messages per pod over the last hour, highlighting pods with spikes in panic occurrences.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"panic.*\", pod_name=~\".*\"}[5m]) > 3\n  ```  \n  This rule triggers an alert if more than 3 panic messages are logged by a pod within 5 minutes."
  },
  {
    "query": {
      "message": "panic*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries in Loki where the `message` field begins with the string \"panic\" (case-sensitive), indicating that the application has encountered a runtime panic event. The `container_name` label specifies the Kubernetes container from which the log originated, enabling identification of the affected service or component. The data source is structured log streams collected from Kubernetes pods, with logs typically emitted by Go applications or other services that use \"panic\" to signal critical failures. This pattern returns all log lines signaling a panic event, which often precedes application crashes or restarts. Alerting on this pattern is critical because the presence of panic logs usually indicates severe runtime errors that can impact application availability or stability. An alert should be triggered when panic occurrences exceed a defined threshold within a given time window, signaling potential systemic issues requiring immediate investigation."
  },
  {
    "query": {
      "message": "panic*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"panic\" and includes a Kubernetes cluster label, indicating that a Go application running in that cluster has encountered a panic event. In Loki, this helps SREs quickly identify critical runtime failures that often cause application crashes or instability.\n\n**Purpose:**  \nTo detect and monitor occurrences of Go panics across clusters, enabling rapid response to application crashes or severe errors.\n\n**Alert Threshold:**  \nTrigger an alert if the number of panic log entries exceeds a defined threshold within a short time window (e.g., >5 panics in 5 minutes per cluster), signaling a potential widespread or recurring failure.\n\n**Impact:**  \n- **High values:** Frequent panics suggest unstable application behavior, risking downtime or degraded service quality. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable application operation with no recent panics detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of panic logs per cluster over time, highlighting spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (cluster) (count_over_time({message=~\"panic.*\", cluster=~\".*\"}[5m])) > 5\n  ```  \nThis rule fires when more than 5 panic logs occur in any cluster within 5 minutes, prompting an alert for SREs to investigate."
  },
  {
    "query": {
      "message": "panic*",
      "pod": "*"
    },
    "description": "This pattern matches log entries from Kubernetes pods where the message contains the term \"panic\", typically indicating a Go application panic that may cause the pod to crash or restart. In Loki, **{\"message\": \"panic*\", \"pod\": \"*\"}** helps SREs quickly identify critical runtime failures within specific pods.\n\n**Alerting guidance:**  \n- Trigger an alert if the count of panic messages exceeds a threshold (e.g., more than 5 panics within 5 minutes per pod), signaling unstable or crashing applications.  \n- A high panic rate suggests frequent application crashes, impacting service availability and requiring immediate investigation.  \n- A low or zero panic count indicates stable pod operation.\n\n**Example alert rule snippet:**  \n```\nsum by (pod) (count_over_time({message=~\"panic.*\", pod=~\".*\"}[5m])) > 5\n```\n\n**Dashboard usage:**  \nVisualize the number of panic logs per pod over time to spot trends or sudden spikes, enabling proactive incident response before user impact escalates."
  },
  {
    "query": {
      "message": "panic*",
      "namespace": "*"
    },
    "description": "This query pattern **{\"message\": \"panic*\", \"namespace\": \"*\"}** in Loki filters log entries where the `message` field starts with the string \"panic\" and includes any Kubernetes namespace label. It is used to identify occurrences of panic-level events or runtime panics typically emitted by Go applications or other services that log critical failures starting with \"panic\". The data source is structured log streams ingested into Loki, with each log entry containing metadata such as the Kubernetes `namespace` label. The pattern returns all matching log lines across all namespaces, enabling operators to detect and investigate critical application failures. An alert should be triggered when one or more such panic messages appear within a short time window, as these indicate severe runtime errors that may cause application crashes or instability, requiring immediate attention to prevent service disruption."
  },
  {
    "query": {
      "message": "stacktrace*",
      "k8s_app": "*"
    },
    "description": "This log pattern **{\"message\": \"stacktrace*\", \"k8s_app\": \"*\"}** is used in Loki to identify log entries containing stack traces, which typically indicate runtime errors or failures within Kubernetes applications labeled by `k8s_app`. Its primary purpose is to help SREs quickly detect and investigate critical application errors by filtering logs that include detailed error traces.\n\n**Alert threshold:** An alert should be triggered when the count of stacktrace-containing log entries exceeds a defined threshold within a given time window (e.g., more than 5 stacktrace logs in 5 minutes for a specific `k8s_app`). This threshold can be adjusted based on the normal error rate and criticality of the application.\n\n**Impact of values:**  \n- **High values:** A spike in stacktrace logs usually signals increased application instability or recurring errors that require immediate investigation to prevent outages or degraded service.  \n- **Low or zero values:** Indicates stable application behavior with no recent critical errors detected.\n\n**Example usage in an alert rule:**  \n```yaml\nalert: HighStacktraceErrors\nexpr: sum by (k8s_app) (count_over_time({message=~\"stacktrace.*\", k8s_app=~\".+\"}[5m])) > 5\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High number of stacktrace errors in {{ $labels.k8s_app }}\"\n  description: \"More than 5 stacktrace logs detected in the last 5 minutes for application {{ $labels.k8s_app }}. Investigate for potential runtime errors.\"\n```\n\nThis enables proactive monitoring of application health by surfacing critical error traces promptly."
  },
  {
    "query": {
      "message": "stacktrace*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the `container` label is present, typically indicating error logs with detailed stack traces from a specific Kubernetes container. Its purpose in Loki is to help SREs quickly identify and monitor occurrences of application errors or crashes that generate stack traces. \n\nA common alert threshold might be set on the rate of these stacktrace logs, for example: trigger an alert if more than 5 stacktrace entries occur within 5 minutes for any container, signaling a potential ongoing failure or instability. \n\nHigh values indicate frequent or repeated errors that could impact application reliability and user experience, requiring immediate investigation. Low or zero values suggest normal operation without critical errors.\n\nExample alert rule snippet in PromQL for Loki:\n```\nsum by (container) (rate({message=~\".*stacktrace.*\", container=~\".*\"}[5m])) > 5\n```\nThis triggers when any container logs more than 5 stacktrace messages in 5 minutes.\n\nIn a dashboard, you might graph the count of stacktrace logs per container over time to visualize error spikes and correlate with incidents."
  },
  {
    "query": {
      "message": "stacktrace*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"stacktrace\" and the \"node_name\" field is present, indicating the Kubernetes node that generated the log. It measures the occurrence of error stack traces associated with specific nodes. The unit of measurement is the count of such log entries. This helps identify and quantify error traces per node in a Kubernetes environment."
  },
  {
    "query": {
      "message": "stacktrace*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and includes a `region` label, typically indicating Kubernetes cluster regions. It is used in Loki to identify error logs that contain detailed stack traces, which are critical for diagnosing failures or crashes in specific regions.\n\n**Purpose:**  \nTo monitor and alert on the occurrence of error stack traces across different Kubernetes regions, enabling rapid detection of issues impacting service stability.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 stacktrace logs per 5 minutes per region), indicating a potential spike in errors requiring investigation.\n\n**Impact of Values:**  \n- **High values:** Suggest a surge in errors or failures in the affected region, potentially signaling service degradation or outages that need immediate attention.  \n- **Low or zero values:** Indicate normal operation with no recent critical errors logged.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of `{\"message\": \"stacktrace*\", \"region\": \"*\"}` logs per region over time to identify error trends.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"stacktrace.*\", region=~\".*\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 stacktrace logs are detected in any region within 5 minutes."
  },
  {
    "query": {
      "message": "stacktrace*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the `service` label identifies the Kubernetes service emitting the log. It is used in Loki to detect error conditions that generate stack traces, which typically indicate application failures or critical exceptions.  \n\n**Purpose:**  \nTo surface and monitor occurrences of stack traces across services, enabling rapid identification of runtime errors that may impact service stability or user experience.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 stacktrace logs per 5 minutes), signaling a spike in errors that warrants investigation.\n\n**Impact:**  \n- **High values:** Indicate frequent or widespread application errors, potentially leading to degraded service performance or outages. Immediate attention is required to diagnose and remediate the root cause.  \n- **Low or zero values:** Suggest normal operation with no recent critical exceptions logged.\n\n**Example Usage:**  \nIn a Grafana dashboard, this pattern can be used to create a panel showing the rate of stacktrace logs per service over time, helping SREs spot error trends.  \n\n**Example alert rule snippet:**  \n```\nsum by (service) (count_over_time({message=~\"stacktrace.*\", service=~\".*\"}[5m])) > 5\n```\nThis triggers an alert if any service logs more than 5 stacktrace entries in 5 minutes, indicating a potential incident."
  },
  {
    "query": {
      "message": "stacktrace*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the `job` label is set, typically indicating error logs with detailed stack traces from a specific Kubernetes job. In Loki, it helps identify and aggregate error occurrences tied to particular jobs or services. An alert threshold might be set on the rate of these stacktrace logs, for example, triggering if more than 5 stacktrace entries appear within 5 minutes for a given job, signaling a potential increase in errors or failures. High values indicate frequent or recurring errors that may impact system stability or user experience, while low or zero values suggest normal operation without critical failures. Example alert rule snippet:  \n```\nsum(rate({message=~\".*stacktrace.*\", job=~\".+\"}[5m])) by (job) > 5\n```  \nThis can be visualized in a dashboard as a time series graph showing the count of stacktrace logs per job over time, enabling quick identification of problematic services."
  },
  {
    "query": {
      "message": "stacktrace*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message contains the term \"stacktrace\" from any Kubernetes environment, indicating that an error or failure has occurred and a detailed error trace is available. In Loki, this helps SREs quickly identify and aggregate critical error traces across all environments for faster troubleshooting. A typical alert threshold might be set to trigger when the count of such stacktrace logs exceeds a defined rate (e.g., more than 5 occurrences within 5 minutes), signaling a potential systemic issue or recurring failure. High values suggest frequent or widespread errors that could impact system stability or user experience, while low values indicate normal or infrequent error occurrences. For example, an alert rule could use the query `{message=~\"stacktrace.*\", environment=~\".*\"}` with a `count_over_time` function to monitor error spikes, and a dashboard panel could visualize the rate of stacktrace logs per environment to help prioritize incident response."
  },
  {
    "query": {
      "message": "stacktrace*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"stacktrace\" and the host field is present with any value. It identifies occurrences of error stack traces reported by any Kubernetes host. The measurement is a count of such log entries, representing the number of stacktrace events per host. This helps monitor error frequency and source within the Kubernetes environment."
  },
  {
    "query": {
      "message": "stacktrace*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"stacktrace\"—typically indicating detailed error traces—and includes any Kubernetes log level (e.g., error, warning). Its purpose in Loki is to help identify and monitor occurrences of application errors or failures that generate stack traces, which are critical for diagnosing issues.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"stacktrace*\", \"level\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 stacktrace logs in 5 minutes), signaling a potential spike in errors requiring immediate investigation.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or recurring errors causing stack traces, potentially leading to degraded service reliability or outages. Immediate attention is needed to identify root causes and mitigate impact.  \n- **Low or zero values:** Suggest normal operation with few or no critical errors producing stack traces, indicating system stability.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of stacktrace logs over time, filtered by this pattern, to visualize error trends.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"stacktrace.*\", level=~\".*\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 stacktrace logs appear within 5 minutes, prompting SREs to investigate potential issues promptly."
  },
  {
    "query": {
      "message": "stacktrace*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\"—typically indicating an error or exception trace—and the `env` label identifies the Kubernetes environment (e.g., dev, staging, prod). In Loki, this filter helps isolate critical error traces for faster troubleshooting.\n\n**Purpose:**  \nTo detect and monitor occurrences of error stack traces across all environments, enabling SREs to quickly identify when and where failures happen.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of logs matching **{\"message\": \"stacktrace*\", \"env\": \"*\"}** exceeds a defined threshold within a short time window (e.g., >5 stacktrace logs in 5 minutes), signaling a spike in errors that may impact system stability.\n\n**Impact:**  \n- **High values:** Indicate frequent or widespread errors, potentially causing service degradation or outages. Immediate investigation is required.  \n- **Low or zero values:** Suggest normal operation with no recent critical errors detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of stacktrace logs per environment over time, helping visualize error trends and correlate with deployments or incidents.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"stacktrace.*\", env=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 stacktrace logs appear in any environment within 5 minutes."
  },
  {
    "query": {
      "message": "stacktrace*",
      "app": "*"
    },
    "description": "This query filters log entries in Loki where the `message` field begins with the string \"stacktrace\" and the `app` label is present with any value. It returns detailed error stack traces emitted by applications, typically indicating runtime exceptions or failures captured in logs. The data source is structured log streams ingested from Kubernetes pods or other services labeled by `app`. The unit of analysis is individual log entries containing multi-line stack traces. An unusually high frequency or sudden spike of such stacktrace logs for a given `app` over a short time window may indicate critical application errors or instability, warranting an alert to prompt investigation and remediation."
  },
  {
    "query": {
      "message": "stacktrace*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"stacktrace\" and the node field contains any value. It identifies occurrences of error stack traces associated with specific Kubernetes nodes. The measurement counts the number of such log entries. The unit is a simple count of matching log events."
  },
  {
    "query": {
      "message": "stacktrace*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the log is labeled with any value for `app_kubernetes_io/name`. Its purpose in Loki is to identify error logs that include stack traces, which typically indicate application failures or exceptions within a specific Kubernetes application.  \n\n**Alerting guidance:**  \n- Set a threshold based on the rate of stacktrace occurrences, for example, alert if more than 5 stacktrace logs appear within 5 minutes for a given `app_kubernetes_io/name`.  \n- A high rate of stacktrace logs signals frequent or severe application errors that may impact service stability or availability.  \n- A low or zero rate generally indicates normal operation without critical errors.  \n\n**Example alert rule snippet:**  \n```\nsum by (app_kubernetes_io_name) (rate({message=~\".*stacktrace.*\", app_kubernetes_io_name=~\".+\"}[5m])) > 5\n```\n\n**Example dashboard usage:**  \nVisualize the per-application count of stacktrace logs over time to quickly identify which Kubernetes apps are experiencing errors, enabling targeted investigation and remediation."
  },
  {
    "query": {
      "message": "stacktrace*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"stacktrace\" and includes an `instance` label, typically representing a Kubernetes pod or node. It is used in Loki to identify error logs containing stack traces, which indicate application failures or crashes. Monitoring the frequency of these stacktrace logs per instance helps SREs detect and respond to critical issues.\n\n**Alert threshold:** Trigger an alert if the number of stacktrace logs exceeds a defined threshold (e.g., more than 5 occurrences within 5 minutes per instance), signaling a potential ongoing failure.\n\n**Impact:**  \n- **High values:** A surge in stacktrace logs suggests frequent errors or crashes, potentially impacting application stability and user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates normal operation with no recent critical errors detected.\n\n**Example usage:**  \n- **Dashboard:** A panel showing the rate of stacktrace logs per instance over time, enabling quick identification of problematic pods.  \n- **Alert rule (PromQL):**  \n  ```promql\n  count_over_time({message=~\"stacktrace.*\", instance=~\".*\"}[5m]) > 5\n  ```  \nThis alert fires when more than 5 stacktrace logs occur within 5 minutes on any instance, prompting SREs to investigate the underlying issue."
  },
  {
    "query": {
      "message": "stacktrace*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the `filename` field is present, typically indicating error logs with detailed stack traces from Kubernetes components or applications. Its purpose in Loki is to help identify and aggregate error occurrences that include stack traces, which are critical for diagnosing failures or crashes.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of such stacktrace logs exceeds a defined threshold within a short time window (e.g., more than 5 stacktrace entries in 5 minutes), signaling a spike in errors that may indicate system instability or a critical failure.\n\n**Impact of Values:**  \n- **High values:** A sudden increase in stacktrace logs suggests frequent or severe errors, potentially impacting system reliability and requiring immediate investigation.  \n- **Low or zero values:** Indicates normal operation with few or no critical errors logged.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of logs matching `{\"message\": \"stacktrace*\", \"filename\": \"*\"}` over time to monitor error trends.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"stacktrace.*\", filename=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 stacktrace logs appear in 5 minutes, prompting an SRE to investigate potential issues."
  },
  {
    "query": {
      "message": "stacktrace*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the `application` label is set to any value, typically indicating error logs with detailed stack traces from a specific Kubernetes application. In Loki, this pattern helps identify critical error events that may require immediate attention.\n\n**Purpose:**  \nTo surface logs containing stack traces that often signify application crashes, exceptions, or severe failures within a Kubernetes application, enabling rapid troubleshooting.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 stacktrace logs in 5 minutes), indicating a spike in errors.\n\n**Impact of Values:**  \n- **High values:** Suggest frequent or ongoing critical errors, potentially causing service degradation or outages, requiring urgent investigation.  \n- **Low or zero values:** Indicate normal operation with no recent critical failures logged.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of `{\"message\": \"stacktrace*\", \"application\": \"*\"}` logs over time per application, highlighting error spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"stacktrace.*\", application=~\".*\"}[5m]) > 5\n  ```  \nThis rule triggers an alert if more than 5 stacktrace logs occur within 5 minutes for any application, signaling potential critical issues."
  },
  {
    "query": {
      "message": "stacktrace*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the `component` field identifies the Kubernetes component emitting the log. Its purpose in Loki is to surface error logs that include stack traces, which are critical for diagnosing application or system failures.  \n\n**Alerting guidance:**  \n- Trigger an alert if the count of stacktrace-containing logs from any component exceeds a defined threshold within a short time window (e.g., >5 stacktrace logs per 5 minutes), indicating a spike in errors or failures.  \n- A high rate suggests ongoing or escalating issues requiring immediate investigation, while a low or zero rate indicates normal operation with no recent critical errors.  \n\n**Example usage:**  \n- **Dashboard:** Display a time series graph showing the number of stacktrace logs per component over time to identify error trends.  \n- **Alert rule (PromQL example):**  \n  ```  \n  count_over_time({message=~\".*stacktrace.*\", component=~\".*\"}[5m]) > 5  \n  ```  \nThis alert fires when more than 5 stacktrace logs are detected from any component in 5 minutes, signaling potential system instability."
  },
  {
    "query": {
      "message": "stacktrace*",
      "source": "*"
    },
    "description": "This pattern matches log entries whose message field begins with the word \"stacktrace\". It identifies logs that contain detailed error or failure traces. The source field captures the Kubernetes component or service generating the log. This pattern measures the occurrence count of such stacktrace-related log entries."
  },
  {
    "query": {
      "message": "stacktrace*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"stacktrace\" and the \"pod_name\" field is present. It identifies error logs containing stack traces generated by Kubernetes pods. The measurement is the count of such log entries, representing occurrences of detailed error traces per pod."
  },
  {
    "query": {
      "message": "stacktrace*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and includes the Kubernetes `container_name` label. Its purpose in Loki is to identify error logs that include stack traces, which are critical for diagnosing application failures or crashes within specific containers.  \n\n**Alert Threshold:**  \nAn alert should be triggered when the count of stacktrace-containing log entries exceeds a defined threshold within a short time window (e.g., more than 5 stacktrace logs in 5 minutes), indicating a spike in errors or potential system instability.\n\n**Impact of Values:**  \n- **High values:** A surge in stacktrace logs suggests frequent or severe errors, potentially impacting application availability or performance and requiring immediate investigation.  \n- **Low or zero values:** Indicates normal operation with no recent critical errors logged.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of stacktrace logs per container over time to quickly identify which containers are experiencing errors.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\".*stacktrace.*\", container_name=~\".*\"}[5m]) > 5\n  ```  \nThis rule triggers an alert if more than 5 stacktrace logs are detected in any container within 5 minutes, prompting an SRE to investigate the underlying issue."
  },
  {
    "query": {
      "message": "stacktrace*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the `cluster` label identifies the Kubernetes cluster emitting the log. It is used in Loki to detect occurrences of error stack traces, which typically indicate application failures or critical exceptions requiring investigation.\n\n**Purpose:**  \nTo surface and monitor error stack traces across clusters, enabling rapid identification of problematic services or infrastructure issues.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 stacktrace logs in 5 minutes), signaling a spike in errors that may impact system stability.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or widespread errors, potentially causing service degradation or outages. Immediate investigation and remediation are needed.  \n- **Low or zero values:** Suggest stable operation with no recent critical errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of `stacktrace` logs per cluster to identify error trends and correlate with deployments or incidents.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"stacktrace.*\", cluster=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 stacktrace logs occur in any cluster within 5 minutes."
  },
  {
    "query": {
      "message": "stacktrace*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"stacktrace\" and the log is associated with any Kubernetes `pod`. Its purpose in Loki is to identify error logs that include stack traces, which typically indicate application failures or exceptions requiring immediate attention. \n\n**Alert threshold:** Trigger an alert if the number of stacktrace-containing log entries exceeds a defined threshold (e.g., more than 5 occurrences within 5 minutes) for any pod, signaling a potential ongoing failure or instability.\n\n**Impact:**  \n- **High values:** A spike in stacktrace logs suggests frequent or severe errors in the application, potentially leading to degraded service or outages. Immediate investigation is warranted.  \n- **Low or zero values:** Indicates normal operation with no recent critical errors logged.\n\n**Example usage:**  \n- **Dashboard:** Visualize the count of stacktrace logs per pod over time to quickly identify pods experiencing errors.  \n- **Alert rule (PromQL example):**  \n  ```\n  count_over_time({message=~\".*stacktrace.*\", pod=~\".*\"}[5m]) > 5\n  ```  \nThis alert fires if more than 5 stacktrace logs are detected in any pod within 5 minutes, prompting an SRE to investigate the root cause."
  },
  {
    "query": {
      "message": "stacktrace*",
      "namespace": "*"
    },
    "description": "This pattern captures log entries where the message contains the term \"stacktrace\" from any Kubernetes namespace, indicating the presence of error stack traces that typically signal application failures or exceptions. In Loki, **{\"message\": \"stacktrace*\", \"namespace\": \"*\"}** helps SREs quickly identify and aggregate error occurrences across all namespaces for faster troubleshooting.\n\n**Alerting guidance:**  \nSet a threshold based on the rate of stacktrace logs, for example, alert if more than 5 stacktrace entries occur within 5 minutes in any single namespace, as this may indicate a critical or recurring failure impacting service stability.\n\n**Impact of values:**  \n- **High values:** A spike in stacktrace logs suggests frequent or severe errors, potentially leading to degraded service performance or outages that require immediate investigation.  \n- **Low or zero values:** Indicates normal operation with few or no critical errors detected.\n\n**Example usage:**  \n- **Dashboard:** Create a panel showing the count of stacktrace logs grouped by namespace over time to monitor error trends and identify problematic namespaces.  \n- **Alert rule:**  \n```\nsum by (namespace) (rate({message=~\"stacktrace.*\"}[5m])) > 5\n```\nThis triggers an alert when any namespace logs more than 5 stacktrace entries in 5 minutes, prompting SREs to investigate the underlying issues promptly."
  },
  {
    "query": {
      "message": "oom*",
      "k8s_app": "*"
    },
    "description": "This log pattern **{\"message\": \"oom*\", \"k8s_app\": \"*\"}** is used in Loki to identify occurrences of Out Of Memory (OOM) events within Kubernetes applications, where the log message contains the substring \"oom\" (e.g., \"oom-killer\"). These events indicate that a container or pod was terminated by the system due to exceeding its memory limits.\n\n**Purpose:**  \nTo monitor and detect OOM kill events across all Kubernetes applications (`k8s_app` label), enabling SREs to quickly identify memory pressure issues affecting application stability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of OOM-related log entries exceeds a defined threshold within a given time window (e.g., more than 3 OOM events in 5 minutes for a single `k8s_app`). This threshold can be adjusted based on application criticality and baseline memory usage patterns.\n\n**Impact of Values:**  \n- **High values:** Frequent OOM events indicate persistent memory resource exhaustion, leading to pod restarts, degraded application performance, or downtime. Immediate investigation and remediation (e.g., increasing memory limits, optimizing memory usage) are required.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: KubernetesOOMKillDetected\nexpr: sum by (k8s_app) (count_over_time({message=~\"oom.*\", k8s_app=~\".+\"}[5m])) > 3\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"OOM kill events detected in Kubernetes app {{ $labels.k8s_app }}\"\n  description: \"More than 3 OOM kill events occurred in the last 5 minutes for app {{ $labels.k8s_app }}. Investigate memory usage and pod resource limits.\"\n```\n\n**Example Dashboard Panel:**  \nA time series graph showing the count of OOM events per `k8s_app` over time, enabling trend analysis and quick identification of problematic applications. Query example:  \n```\nsum by (k8s_app) (count_over_time({message=~\"oom.*\", k8s_app=~\".+\"}[1m]))\n```\n\nThis pattern and associated alerts help maintain application reliability by proactively detecting and responding to memory-related failures in Kubernetes environments."
  },
  {
    "query": {
      "message": "oom*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\"), indicating that a container experienced an Out Of Memory (OOM) event. The presence of the `container` label ensures the log is associated with a specific Kubernetes container.\n\n**Purpose:**  \nIn Loki, this pattern helps detect OOM kill events in container logs, which are critical signals of resource exhaustion and stability issues in your workloads.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"oom*\", \"container\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 1 OOM event per container within 5 minutes). This threshold can be adjusted based on workload tolerance and historical baseline.\n\n**Impact:**  \n- **High values:** Frequent OOM events indicate insufficient memory allocation or memory leaks, leading to container restarts, degraded application performance, or downtime. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation with no memory exhaustion detected.\n\n**Example Alert Rule (PromQL for Loki):**  \n```promql\ncount_over_time({message=~\"oom.*\", container=~\".+\"}[5m]) > 1\n```\nThis rule fires if more than one OOM-related log appears per container in 5 minutes.\n\n**Example Dashboard Usage:**  \nVisualize the rate of OOM events per container over time using a graph panel with the query:  \n```promql\nrate({message=~\"oom.*\", container=~\".+\"}[5m])\n```\nThis helps SREs monitor memory pressure trends and proactively adjust resource limits or investigate memory leaks."
  },
  {
    "query": {
      "message": "oom*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\"), indicating that the system has triggered an Out Of Memory (OOM) kill event on a Kubernetes node identified by `node_name`. In Loki, this helps detect when processes are being terminated due to insufficient memory, which can signal resource pressure or misconfiguration.\n\n**Purpose:**  \nTo monitor and alert on OOM kill events across Kubernetes nodes, enabling timely investigation and remediation of memory-related issues.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of OOM events exceeds a defined threshold within a given time window, for example:  \n- More than 1 OOM event per node within 5 minutes, or  \n- A sustained increase in OOM events over 15 minutes indicating worsening memory pressure.\n\n**Impact:**  \n- **High values:** Frequent OOM kills can cause application instability, degraded performance, or downtime, indicating that nodes may be under-provisioned or workloads require tuning.  \n- **Low or zero values:** Normal operation with no memory-related process terminations.\n\n**Example Alert Rule (PromQL for Loki):**  \n```promql\nsum by (node_name) (count_over_time({message=~\".*oom.*\", node_name=~\".*\"}[5m])) > 1\n```\nThis triggers if more than one OOM event occurs on any node within 5 minutes.\n\n**Example Dashboard Usage:**  \nVisualize the count of OOM events per node over time to identify trends and hotspots. Use a graph panel with the query above to track OOM frequency, helping prioritize nodes for memory resource adjustments or workload redistribution."
  },
  {
    "query": {
      "message": "oom*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"oom\" and includes any following characters. It identifies events related to Out Of Memory (OOM) kills, such as those triggered by the Linux OOM killer. Each matched log entry is tagged with a Kubernetes region label. The pattern measures the count of such OOM-related log occurrences per region."
  },
  {
    "query": {
      "message": "oom*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\"), indicating that an Out Of Memory (OOM) event has occurred within a Kubernetes service identified by the `service` label. In Loki, this helps detect when containers or pods are being terminated due to memory exhaustion.\n\n**Purpose:**  \nTo identify and monitor OOM kill events across Kubernetes services, enabling timely detection of memory pressure issues that can cause service instability or downtime.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching `{\"message\": \"oom*\", \"service\": \"*\"}` exceeds a defined threshold within a given time window (e.g., more than 3 OOM events per 5 minutes per service). This threshold can be adjusted based on service criticality and historical baseline.\n\n**Impact of Values:**  \n- **High values:** Frequent OOM events indicate persistent memory shortages, risking pod restarts, degraded performance, or outages. Immediate investigation and remediation (e.g., increasing memory limits, optimizing workloads) are required.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of OOM events per service over time using a Loki query like:  \n  ```\n  count_over_time({message=~\"oom.*\", service=~\".*\"}[5m])\n  ```  \n- **Alert Rule:**  \n  ```\n  alert: OOMKillDetected\n  expr: count_over_time({message=~\"oom.*\", service=~\".*\"}[5m]) > 3\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"OOM kill events detected in service {{ $labels.service }}\"\n    description: \"More than 3 OOM kill events occurred in the last 5 minutes for service {{ $labels.service }}. Investigate memory usage and pod stability.\"\n  ```"
  },
  {
    "query": {
      "message": "oom*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (commonly indicating Out Of Memory kill events, such as \"oom-killer\") and the `job` label identifies the Kubernetes workload emitting the log. In Loki, it is used to detect and monitor OOM events across different jobs or services.\n\n**Purpose:**  \nTo identify occurrences of OOM kills that can cause pod restarts or service disruptions, enabling proactive investigation and remediation.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching `{\"message\": \"oom*\", \"job\": \"*\"}` exceeds a defined threshold within a short time window (e.g., more than 3 OOM events per 5 minutes per job), indicating a potential memory pressure issue.\n\n**Impact:**  \n- **High values:** Frequent OOM events suggest memory resource exhaustion, leading to pod restarts, degraded performance, or downtime. Immediate action is required to adjust resource limits, optimize memory usage, or scale workloads.  \n- **Low or zero values:** Indicates stable memory usage without OOM kills, reflecting healthy application behavior.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of OOM events over time grouped by `job`, helping visualize which workloads are experiencing memory issues.  \n- **Alert Rule (PromQL for Loki):**  \n  ```\n  count_over_time({message=~\"oom.*\", job=~\".*\"}[5m]) > 3\n  ```  \n  This triggers an alert if more than 3 OOM-related log entries occur within 5 minutes for any job."
  },
  {
    "query": {
      "message": "oom*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"oom\" and the environment label is present. It identifies events related to Out Of Memory (OOM) conditions, such as OOM kills triggered by the system. The pattern measures the count of these OOM-related log occurrences within each Kubernetes environment. The unit of measurement is the number of OOM event log entries."
  },
  {
    "query": {
      "message": "oom*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\"), indicating that an Out Of Memory (OOM) event has occurred on a Kubernetes host identified by the `host` label. In Loki, this pattern helps detect OOM kill events that can cause pod or container restarts, impacting application availability and stability.\n\n**Purpose:**  \nTo monitor and alert on OOM kill events across Kubernetes hosts, enabling rapid identification and remediation of memory pressure issues.\n\n**Alert Threshold:**  \nTrigger an alert if the count of logs matching **{\"message\": \"oom*\", \"host\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 3 OOM events per host within 5 minutes). This threshold can be adjusted based on workload sensitivity and baseline behavior.\n\n**Impact:**  \n- **High values:** Frequent OOM events indicate severe memory constraints, potentially causing pod restarts, degraded performance, or downtime. Immediate investigation and resource allocation adjustments are recommended.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of OOM events per host over time using a Loki query like:  \n  `count_over_time({message=~\"oom.*\", host=~\".*\"}[5m]) by (host)`  \n- **Alert Rule:** Fire an alert when the count exceeds 3 per host in 5 minutes:  \n  ```\n  alert: OOMKillHigh\n  expr: count_over_time({message=~\"oom.*\", host=~\".*\"}[5m]) > 3\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High number of OOM kill events on {{ $labels.host }}\"\n    description: \"More than 3 OOM kill events detected on host {{ $labels.host }} in the last 5 minutes, indicating potential memory pressure.\"\n  ```"
  },
  {
    "query": {
      "message": "oom*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\"), indicating that an Out Of Memory (OOM) event has occurred on a Kubernetes node or container. The `level` field captures the log severity or source level, helping to filter or categorize these events.\n\n**Purpose:**  \nTo detect and monitor OOM kill events in your Kubernetes environment, which often signal resource exhaustion and can cause pod restarts or degraded application performance.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching `{\"message\": \"oom*\", \"level\": \"*\"}` exceeds a defined threshold within a given time window (e.g., more than 1 OOM event in 5 minutes), indicating repeated or sustained memory pressure.\n\n**Impact:**  \n- **High values:** Frequent OOM events suggest insufficient memory allocation, potential memory leaks, or resource contention, leading to pod instability and possible downtime. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing the rate of OOM events over time using a Loki query like:  \n  `count_over_time({message=~\"oom.*\", level=~\".*\"}[5m])`  \n- **Alert Rule:** Trigger an alert if the count exceeds 1 in 5 minutes:  \n  ```yaml\n  alert: OOMKillDetected\n  expr: count_over_time({message=~\"oom.*\", level=~\".*\"}[5m]) > 1\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"Multiple OOM kill events detected\"\n    description: \"More than one OOM kill event occurred in the last 5 minutes, indicating potential memory issues.\"\n  ```"
  },
  {
    "query": {
      "message": "oom*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\"), indicating that the system has experienced an Out Of Memory (OOM) event. The `env` label corresponds to the Kubernetes environment where the event occurred, allowing filtering by deployment or cluster environment.\n\n**Purpose:**  \nIn Loki, **{\"message\": \"oom*\", \"env\": \"*\"}** is used to identify and monitor OOM kill events across all environments. These events typically indicate that a container or pod was terminated by the kernel due to memory exhaustion, which can impact application availability and stability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of OOM events exceeds a defined threshold within a given time window. For example, more than 3 OOM events within 5 minutes in a single environment (`env`) may indicate a critical memory pressure issue requiring immediate investigation.\n\n**Impact of Values:**  \n- **High OOM event count:** Suggests persistent memory resource constraints, potentially causing frequent pod restarts, degraded service performance, or downtime. Immediate remediation (e.g., increasing memory limits, optimizing workloads) is recommended.  \n- **Low or zero OOM event count:** Indicates stable memory usage with no recent OOM kills, reflecting healthy resource allocation.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of OOM events per environment over the last hour using a Loki query like:  \n  `count_over_time({message=~\"oom.*\", env=~\".*\"}[5m]) by (env)`  \n- **Alert Rule:** Trigger an alert if the count exceeds 3 in 5 minutes:  \n  ```\n  alert: HighOOMKillRate\n  expr: count_over_time({message=~\"oom.*\", env=~\".*\"}[5m]) > 3\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High OOM kill rate detected in {{ $labels.env }}\"\n    description: \"More than 3 OOM kill events detected in the last 5 minutes in environment {{ $labels.env }}. Investigate memory usage and pod stability.\"\n  ```"
  },
  {
    "query": {
      "message": "oom*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (commonly indicating Out Of Memory events such as \"oom-killer\") and the `app` label identifies the Kubernetes application emitting the log. It is used in Loki to detect and monitor OOM kill events across applications.\n\n**Purpose:**  \nTo identify and track occurrences of OOM kills that can cause application crashes or pod restarts, signaling resource exhaustion issues.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"oom*\", \"app\": \"*\"}** exceeds a defined threshold within a given time window—for example, more than 3 OOM events per 5 minutes for a single app—indicating a recurring memory pressure problem.\n\n**Impact:**  \n- **High values:** Frequent OOM events suggest that the application or pod is consistently running out of memory, potentially leading to instability, degraded performance, or downtime. Immediate investigation and remediation (e.g., increasing memory limits, optimizing memory usage) are required.  \n- **Low or zero values:** Normal operation with no detected OOM kills, indicating stable memory usage.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of OOM events over time per app, using a Loki query like:  \n  `count_over_time({message=~\"oom.*\", app=~\".*\"}[5m]) by (app)`  \n- **Alert Rule:**  \n  ```yaml\n  alert: OOMKillDetected\n  expr: count_over_time({message=~\"oom.*\", app=~\".*\"}[5m]) > 3\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"OOM kill events detected for {{ $labels.app }}\"\n    description: \"More than 3 OOM kill events occurred in the last 5 minutes for app {{ $labels.app }}. This may indicate memory resource exhaustion.\"\n  ```"
  },
  {
    "query": {
      "message": "oom*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"oom\" and includes any additional text. It identifies events related to Out Of Memory (OOM) conditions, such as OOM kills triggered by the system. Each matched log entry is associated with a specific Kubernetes node, indicated by the \"node\" label. This pattern measures the occurrence count of OOM-related events per node."
  },
  {
    "query": {
      "message": "oom*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This log pattern **{\"message\": \"oom*\", \"app_kubernetes_io/name\": \"*\"}** is used in Loki to identify log entries indicating Out Of Memory (OOM) kill events within Kubernetes pods, where the message contains the substring \"oom\" (e.g., \"oom-killer\"). The presence of the label `app_kubernetes_io/name` helps attribute these events to specific applications or services running in the cluster.\n\n**Purpose:**  \nTo detect and monitor OOM kill occurrences that can cause pod restarts or service disruptions, enabling proactive investigation and remediation.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of OOM-related log entries for a given application exceeds a defined threshold within a specified time window (e.g., more than 3 OOM events in 5 minutes). This threshold may vary depending on application criticality and baseline behavior.\n\n**Impact:**  \n- **High values:** Frequent OOM kills indicate insufficient memory allocation or memory leaks, leading to degraded application performance, increased pod restarts, and potential downtime. Immediate action is required to adjust resource limits or fix memory issues.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: OOMKillDetected\nexpr: count_over_time({app_kubernetes_io/name=~\".+\", message=~\"oom.*\"}[5m]) > 3\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"OOM kill events detected for {{ $labels.app_kubernetes_io/name }}\"\n  description: \"More than 3 OOM kill events occurred in the last 5 minutes for application {{ $labels.app_kubernetes_io/name }}. Investigate memory usage and pod resource limits.\"\n```\n\nThis pattern can also be visualized in dashboards by plotting the rate or count of OOM kill log entries per application over time, helping SREs quickly identify memory pressure issues across services."
  },
  {
    "query": {
      "message": "oom*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"oom\" and includes any following characters, indicating an Out Of Memory (OOM) event. The \"instance\" field identifies the specific Kubernetes instance generating the log. It measures occurrences of OOM-related messages per instance as discrete log events. Each matched log entry represents one OOM event recorded by that instance."
  },
  {
    "query": {
      "message": "oom*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"oom\" and the filename field is present with any value. It identifies occurrences related to Out Of Memory (OOM) events, such as OOM killer actions in the system. The measurement is a count of these OOM-related log entries. Units are discrete event counts per log dataset or time interval."
  },
  {
    "query": {
      "message": "oom*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"oom\" and the application field is present with any value. It identifies events related to Out Of Memory (OOM) conditions, such as OOM kills triggered by the system. The pattern measures the occurrence count of these OOM-related log entries per application. The unit of measurement is the number of OOM log events recorded."
  },
  {
    "query": {
      "message": "oom*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\"), indicating Out Of Memory (OOM) kill events within Kubernetes components identified by the `component` label. In Loki, this pattern helps detect and monitor OOM occurrences that can cause pod restarts or service disruptions.\n\n**Purpose:**  \nTo identify and track OOM kill events across Kubernetes components, enabling timely detection of memory pressure issues affecting cluster stability.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"oom*\", \"component\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 3 OOM events per 5 minutes per component), signaling abnormal memory pressure or resource exhaustion.\n\n**Impact:**  \n- **High values:** Frequent OOM events indicate critical memory shortages, leading to pod restarts, degraded application performance, or outages. Immediate investigation and remediation (e.g., resource allocation, memory limits adjustment) are required.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of OOM events over time, grouped by `component`, to visualize trends and identify problematic services.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"oom.*\", component=~\".*\"}[5m]) > 3\n  ```  \n  This fires if more than 3 OOM-related logs occur within 5 minutes for any component.\n\nBy monitoring this pattern, SREs can proactively detect memory-related failures and maintain cluster reliability."
  },
  {
    "query": {
      "message": "oom*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (commonly indicating Out Of Memory events such as \"oom-killer\") and the `source` field is present, typically representing the Kubernetes component or container generating the log. In Loki, this pattern helps identify and aggregate OOM kill events across your cluster.\n\n**Purpose:**  \nTo detect and monitor Out Of Memory kill events that can cause pod restarts or service disruptions, enabling proactive investigation and remediation.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 OOM events in 5 minutes), indicating a potential memory pressure issue affecting stability.\n\n**Impact of Values:**  \n- **High values:** Frequent OOM events suggest insufficient memory allocation, memory leaks, or resource contention, potentially leading to degraded application performance or downtime. Immediate investigation and resource tuning are recommended.  \n- **Low or zero values:** Indicate stable memory usage with no recent OOM kills, reflecting healthy resource management.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the number of OOM events over time by querying:  \n  `count_over_time({message=~\"oom.*\", source=~\".*\"}[5m])`  \n  to track trends and identify spikes.  \n- **Alert Rule (Prometheus-style):**  \n  ```\n  alert: HighOOMKillRate\n  expr: count_over_time({message=~\"oom.*\", source=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High rate of OOM kill events detected\"\n    description: \"More than 5 OOM kill events occurred in the last 5 minutes, indicating potential memory pressure.\"\n  ```"
  },
  {
    "query": {
      "message": "oom*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"oom\", indicating events related to Out Of Memory (OOM) conditions, such as OOM kills. It specifically filters logs labeled with any Kubernetes pod name under the \"pod_name\" field. The pattern measures the occurrence count of OOM-related messages per pod. The unit of measurement is the number of OOM event log entries."
  },
  {
    "query": {
      "message": "oom*",
      "container_name": "*"
    },
    "description": "This query filters log entries in Loki where the `message` field contains the substring \"oom\" (case-sensitive prefix match) and the `container_name` label is present with any value. It is primarily used to identify Out Of Memory (OOM) events or related messages emitted by containers running in Kubernetes environments. The data source is container logs ingested into Loki, typically collected via Promtail or similar agents. The returned entries indicate occurrences of memory exhaustion or OOM kills affecting specific containers, identified by their `container_name`. Alerts should be triggered when the frequency or rate of these OOM-related log entries exceeds a defined threshold within a given time window, as this signals potential memory resource constraints or instability in container workloads requiring immediate investigation."
  },
  {
    "query": {
      "message": "oom*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\", indicating Out Of Memory (OOM) kill events detected by the system or Kubernetes, and includes the `cluster` label to identify the Kubernetes cluster source. In Loki, this pattern helps SREs monitor and alert on OOM occurrences across clusters, which are critical as OOM kills can cause pod restarts, service disruptions, and degraded application performance.\n\n**Purpose:**  \nTo detect and track OOM kill events in Kubernetes clusters by filtering logs with messages containing \"oom\" and associating them with their respective clusters.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of OOM events exceeds a defined threshold within a given time window, for example:  \n- More than 5 OOM events per cluster within 5 minutes, indicating a potential memory pressure issue requiring immediate investigation.\n\n**Impact of Values:**  \n- **High values:** Frequent OOM events suggest insufficient memory allocation, memory leaks, or resource contention, potentially causing instability or downtime. Immediate remediation is advised.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of OOM events per cluster over time using a Loki query like:  \n  ```  \n  count_over_time({message=~\"oom.*\", cluster=~\".*\"}[5m]) by (cluster)  \n  ```  \n- **Alert Rule:** Trigger an alert if the count exceeds 5 in 5 minutes:  \n  ```  \n  alert: HighOOMKillRate  \n  expr: count_over_time({message=~\"oom.*\", cluster=~\".*\"}[5m]) > 5  \n  for: 5m  \n  labels:  \n    severity: critical  \n  annotations:  \n    summary: \"High OOM kill rate detected in cluster {{ $labels.cluster }}\"  \n    description: \"More than 5 OOM kill events detected in the last 5 minutes in cluster {{ $labels.cluster }}. Investigate memory usage and pod stability.\"  \n  ```"
  },
  {
    "query": {
      "message": "oom*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\") and the entry is associated with a Kubernetes `pod` label. It is used in Loki to detect Out Of Memory (OOM) kill events affecting pods, which indicate that a container was terminated by the system due to exceeding its memory limits.\n\n**Purpose:**  \nTo identify and monitor OOM kill events in Kubernetes pods, enabling SREs to quickly detect memory pressure issues that cause pod restarts or failures.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching `{\"message\": \"oom*\", \"pod\": \"*\"}` exceeds a defined threshold within a given time window (e.g., more than 1 OOM event per pod within 5 minutes). This threshold can be adjusted based on the environment’s tolerance for OOM events.\n\n**Impact of Values:**  \n- **High values:** Frequent OOM events indicate pods are consistently exceeding memory limits, leading to instability, degraded service availability, and potential cascading failures. Immediate investigation and remediation (e.g., increasing memory limits, optimizing workloads) are required.  \n- **Low or zero values:** Normal operation with no recent OOM kills, indicating stable memory usage within pods.\n\n**Example Usage:**  \n- **Alert rule:** Trigger an alert if `count_over_time({message=~\"oom.*\", pod=~\".*\"}[5m]) > 1` per pod, signaling repeated OOM kills.  \n- **Dashboard panel:** Display a time series graph showing the rate of OOM events per pod over time, helping visualize trends and identify problematic pods.\n\nThis pattern is essential for proactive memory management and maintaining Kubernetes cluster health."
  },
  {
    "query": {
      "message": "oom*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"oom\" (e.g., \"oom-killer\", \"oom connection failed\"), indicating that a container or pod was terminated due to an Out Of Memory (OOM) event. The `namespace` label identifies the Kubernetes namespace where the event occurred, enabling scoped monitoring.\n\n**Purpose:**  \nTo detect and monitor OOM kill events in Kubernetes clusters by filtering logs that signal memory exhaustion issues affecting pods or containers within specific namespaces.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of OOM-related log entries exceeds a defined threshold within a given time window (e.g., more than 3 OOM events in 5 minutes per namespace). This threshold can be adjusted based on workload sensitivity and historical baseline.\n\n**Impact:**  \n- **High values:** Frequent OOM events indicate memory pressure or misconfiguration, potentially causing pod restarts, degraded application performance, or downtime. Immediate investigation and remediation (e.g., resource limit adjustments, memory leak fixes) are required.  \n- **Low or zero values:** Normal operation with no recent memory kill events, indicating stable memory usage.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of OOM events per namespace over time using a Loki query like:  \n  `count_over_time({message=~\"oom.*\", namespace=~\".*\"}[5m])`  \n- **Alert Rule (Prometheus-style):**  \n  ```\n  alert: OOMKillHighRate\n  expr: sum by(namespace) (count_over_time({message=~\"oom.*\", namespace=~\".*\"}[5m])) > 3\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of OOM kills in namespace {{ $labels.namespace }}\"\n    description: \"More than 3 OOM kill events detected in the last 5 minutes in namespace {{ $labels.namespace }}. Investigate memory usage and pod resource limits.\"\n  ```"
  },
  {
    "query": {
      "message": "fault*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"fault\" and the k8s_app label is present with any value. It identifies occurrences of fault-related events within Kubernetes applications. The measurement is a count of such log entries, representing the number of fault events detected per unit time."
  },
  {
    "query": {
      "message": "fault*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"fault\" (e.g., \"fault connection failed\") and the entry is associated with any Kubernetes `container`. It is used in Loki to identify critical system errors or faults originating from application containers.  \n\n**Purpose:**  \nTo detect and monitor fault-related errors in containerized applications, enabling early identification of issues that may impact system stability or availability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"fault*\", \"container\": \"*\"}** exceeds a defined threshold within a given time window—for example, more than 5 fault logs in 5 minutes—indicating a potential systemic problem requiring immediate attention.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high rate of fault logs suggests recurring or severe errors that could degrade service performance or cause outages. Immediate investigation and remediation are necessary.  \n- **Low or zero values:** Indicates normal operation with no detected fault-level errors, implying system stability.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: HighFaultLogRate\nexpr: sum(rate({container=~\".*\"} |= \"fault\" [5m])) > 5\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"High rate of fault logs detected in containers\"\n  description: \"More than 5 fault-related log entries per 5 minutes detected, indicating potential critical errors in containerized applications.\"\n```\n\n**Example Dashboard Panel Query:**  \n```logql\nsum by (container) (rate({container=~\".*\"} |= \"fault\" [5m]))\n```\nThis query visualizes the rate of fault logs per container over time, helping SREs quickly identify which containers are experiencing critical faults."
  },
  {
    "query": {
      "message": "fault*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"fault\" (e.g., \"fault connection failed\") and includes the Kubernetes `node_name` label, indicating that a critical system error has occurred on a specific node. In Loki, this filter helps isolate node-level faults that may impact cluster stability or application availability.\n\n**Purpose:**  \nTo detect and monitor critical faults reported by nodes, enabling rapid identification of problematic nodes requiring investigation or remediation.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"fault*\", \"node_name\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 5 fault messages per node within 5 minutes), signaling a potential systemic issue or node degradation.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or ongoing critical faults on one or more nodes, potentially leading to service disruption or degraded performance. Immediate investigation is warranted.  \n- **Low or zero values:** Suggest normal operation with no recent critical faults detected on nodes.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the number of fault logs per node over time, using a Loki query like:  \n  `count_over_time({message=~\"fault.*\", node_name=~\".+\"}[5m]) by (node_name)`  \n- **Alert Rule:** Trigger an alert if any node reports more than 5 fault logs in 5 minutes:  \n  ```\n  expr: count_over_time({message=~\"fault.*\", node_name=~\".+\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High fault rate detected on node {{ $labels.node_name }}\"\n    description: \"Node {{ $labels.node_name }} has reported more than 5 fault messages in the last 5 minutes, indicating potential critical issues.\"\n  ```"
  },
  {
    "query": {
      "message": "fault*",
      "region": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'region'. Example log entry: message='fault connection failed', region='example-region'."
  },
  {
    "query": {
      "message": "fault*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"fault\" (e.g., \"fault connection failed\") and the `service` label is set, typically representing a Kubernetes service name. It is used in Loki to identify critical system errors or faults reported by specific services.\n\n**Purpose:**  \nTo detect and monitor occurrences of fault-related errors across services, enabling timely identification of critical issues affecting system stability or availability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the rate of logs matching this pattern exceeds a defined threshold, such as more than 5 fault messages per minute per service, indicating a potential systemic failure or degradation.\n\n**Impact of Values:**  \n- **High values:** A spike in fault logs suggests increasing error rates, signaling urgent investigation to prevent outages or data loss.  \n- **Low or zero values:** Indicates normal operation with no critical faults detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of `{\"message\": \"fault*\", \"service\": \"*\"}` logs per service over time to spot trends or sudden increases.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (service) (rate({message=~\"fault.*\", service=~\".+\"}[1m])) > 5\n  ```  \n  This triggers an alert if any service logs more than 5 fault messages per minute."
  },
  {
    "query": {
      "message": "fault*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"fault\" (indicating a critical system error or failure) and the `job` label identifies the Kubernetes job emitting the log. In Loki, **{\"message\": \"fault*\", \"job\": \"*\"}** helps SREs isolate and monitor fault-related errors across all jobs.\n\n**Purpose:**  \nTo detect and track critical faults in system components by filtering logs that report fault conditions, enabling timely identification of issues affecting service reliability.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of fault-related log entries exceeds a defined threshold within a given time window (e.g., more than 5 fault logs per 5 minutes per job), signaling a potential systemic problem requiring immediate investigation.\n\n**Impact of Values:**  \n- **High fault counts:** Indicate frequent or ongoing critical errors, potentially leading to service degradation or outages. Immediate remediation is needed.  \n- **Low or zero fault counts:** Suggest stable operation with no recent critical faults detected.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of fault logs per job over time using a Loki query like:  \n  `count_over_time({job=~\".*\", message=~\"fault.*\"}[5m])`  \n- **Alert Rule:** Trigger an alert if fault logs exceed 5 in 5 minutes for any job:  \n  ```\n  alert: HighFaultRate\n  expr: count_over_time({job=~\".*\", message=~\"fault.*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High fault rate detected in job {{ $labels.job }}\"\n    description: \"More than 5 fault logs detected in the last 5 minutes for job {{ $labels.job }}. Immediate investigation required.\"\n  ```"
  },
  {
    "query": {
      "message": "fault*",
      "environment": "*"
    },
    "description": "This pattern matches log entries whose message begins with the word \"fault\" and that include any Kubernetes environment label. It identifies occurrences of critical system errors indicated by messages starting with \"fault\". The measurement is a count of such log entries per environment. Each unit represents one log entry matching these criteria within a given environment."
  },
  {
    "query": {
      "message": "fault*",
      "host": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'host'. Example log entry: message='fault connection failed', host='example-host'."
  },
  {
    "query": {
      "message": "fault*",
      "level": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'level'. Example log entry: message='fault connection failed', level='example-level'."
  },
  {
    "query": {
      "message": "fault*",
      "env": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'env'. Example log entry: message='fault connection failed', env='example-env'."
  },
  {
    "query": {
      "message": "fault*",
      "app": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'app'. Example log entry: message='fault connection failed', app='example-app'."
  },
  {
    "query": {
      "message": "fault*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"fault\" and the node field is present with any value. It identifies occurrences of fault-related events reported by specific Kubernetes nodes. The measurement is a count of such log entries, representing the number of fault events per node. This helps monitor critical system errors associated with individual nodes."
  },
  {
    "query": {
      "message": "fault*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'app_kubernetes_io/name'. Example log entry: message='fault connection failed', app_kubernetes_io/name='example-app_kubernetes_io/name'."
  },
  {
    "query": {
      "message": "fault*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"fault\" and the \"instance\" field is present with any value. It identifies occurrences of fault-related events reported by specific Kubernetes instances. The measurement is a count of such log entries, representing the number of fault events per instance. This helps monitor critical system errors tied to individual Kubernetes instances."
  },
  {
    "query": {
      "message": "fault*",
      "filename": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'filename'. Example log entry: message='fault connection failed', filename='example-filename'."
  },
  {
    "query": {
      "message": "fault*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"fault\" (e.g., \"fault connection failed\") and the `application` label is set, typically indicating a critical system error within a specific Kubernetes application. In Loki, this pattern helps SREs quickly identify fault-related errors tied to particular applications.\n\n**Purpose:**  \nTo detect and monitor critical faults in application logs that may indicate system failures or degraded service health.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 fault logs in 5 minutes), signaling a potential systemic issue requiring immediate investigation.\n\n**Impact of Values:**  \n- **High values:** A spike in fault logs suggests increased error rates, possibly leading to service outages or degraded performance. Immediate action is recommended.  \n- **Low or zero values:** Indicates normal operation with no critical faults detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of fault logs per application over time, enabling trend analysis and rapid identification of problematic services.  \n- **Alert Rule (PromQL):**  \n  ```\n  count_over_time({message=~\"fault.*\", application=~\".+\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 fault logs occur within 5 minutes for any application."
  },
  {
    "query": {
      "message": "fault*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"fault\" and the component field contains any value. It identifies occurrences of fault-related events reported by any Kubernetes component. The measurement is the count of such log entries, representing the number of fault events detected. Units are expressed as the total number of matching log entries over a given time period."
  },
  {
    "query": {
      "message": "fault*",
      "source": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'source'. Example log entry: message='fault connection failed', source='example-source'."
  },
  {
    "query": {
      "message": "fault*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"fault\" (e.g., \"fault connection failed\") and includes the Kubernetes `pod_name` label. It is used in Loki to identify critical system errors or faults occurring within specific pods.  \n\n**Purpose:**  \nTo detect and monitor fault-related errors at the pod level, enabling rapid identification of pods experiencing critical issues that may impact application stability or availability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window—for example, more than 5 fault messages per pod within 5 minutes—indicating a potential systemic problem requiring immediate investigation.\n\n**Impact:**  \n- **High values:** A spike in fault logs suggests critical failures or degraded pod health, potentially leading to service disruption or downtime. Immediate remediation is advised.  \n- **Low or zero values:** Indicates normal operation with no detected critical faults in the pods.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of fault logs per pod over time, enabling SREs to spot trends or sudden increases.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"fault.*\", pod_name=~\".*\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 fault messages are logged by any pod within 5 minutes."
  },
  {
    "query": {
      "message": "fault*",
      "container_name": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'container_name'. Example log entry: message='fault connection failed', container_name='example-container_name'."
  },
  {
    "query": {
      "message": "fault*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"fault\" and the cluster field is present with any value. It identifies occurrences of fault-related events within Kubernetes clusters. The measurement is a count of such log entries per cluster. Each entry represents one detected fault event in the specified cluster."
  },
  {
    "query": {
      "message": "fault*",
      "pod": "*"
    },
    "description": "This pattern captures logs where the message contains 'fault', indicating a critical system error, and the log entry is labeled with Kubernetes 'pod'. Example log entry: message='fault connection failed', pod='example-pod'."
  },
  {
    "query": {
      "message": "fault*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"fault\" (e.g., \"fault connection failed\") and includes any Kubernetes `namespace`. It is used in Loki to identify critical system errors or faults occurring within specific namespaces, enabling targeted monitoring of fault conditions in your cluster.\n\n**Purpose:**  \nTo detect and track occurrences of critical faults across all namespaces, helping SREs quickly identify and respond to system errors that may impact application stability or availability.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 fault logs in 5 minutes per namespace). This threshold can be adjusted based on the normal error rate and criticality of the services.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent or ongoing critical faults in one or more namespaces, signaling potential system instability or failure that requires immediate investigation.  \n- **Low or zero values:** Suggest normal operation with no recent critical faults detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the number of \"fault*\" messages per namespace over time, highlighting namespaces with rising fault counts.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"fault.*\", namespace=~\".*\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 fault messages occur in any namespace within 5 minutes, prompting SREs to investigate the underlying issues promptly."
  },
  {
    "query": {
      "message": "crash*",
      "k8s_app": "*"
    },
    "description": "This log pattern **{\"message\": \"crash*\", \"k8s_app\": \"*\"}** is used in Loki to identify log entries from Kubernetes applications where the message contains the keyword \"crash,\" indicating that a service or process has unexpectedly terminated or failed. This pattern helps SREs quickly detect and investigate crash events within specific Kubernetes workloads.\n\n**Purpose:**  \n- To monitor and alert on crash occurrences in Kubernetes applications by filtering logs that include \"crash\" in their message field and are labeled with the relevant `k8s_app` identifier.  \n- Enables proactive detection of stability issues affecting application availability or reliability.\n\n**Alert Threshold:**  \n- A typical alert might trigger if the number of crash-related log entries exceeds a threshold such as 5 crashes within 5 minutes for a given `k8s_app`.  \n- Thresholds should be tuned based on normal application behavior and criticality; frequent crashes indicate a severe problem requiring immediate attention.\n\n**Impact of Values:**  \n- **High values:** A spike in crash logs suggests instability or recurring failures in the application, potentially leading to downtime, degraded user experience, or cascading failures in dependent services.  \n- **Low or zero values:** Indicates stable operation with no recent crash events detected.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: KubernetesAppCrashDetected\nexpr: sum(rate({k8s_app=~\".*\"} |= \"crash\" [5m])) by (k8s_app) > 5\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"High crash rate detected in Kubernetes app {{ $labels.k8s_app }}\"\n  description: \"More than 5 crash events detected in the last 5 minutes for app {{ $labels.k8s_app }}. Immediate investigation recommended.\"\n```\n\n**Example Dashboard Panel:**  \n- A graph showing the count of crash-related log entries over time per `k8s_app`, enabling SREs to visualize trends and correlate crashes with deployments or incidents."
  },
  {
    "query": {
      "message": "crash*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"crash\" and the container field is present with any value. It identifies occurrences of service or process crashes within Kubernetes containers. The measurement is a count of such log entries, representing the number of crash-related events per container. Units are raw event counts without time normalization."
  },
  {
    "query": {
      "message": "crash*",
      "node_name": "*"
    },
    "description": "This pattern captures logs where the message contains 'crash', indicating a service or process crash, and the log entry is labeled with Kubernetes 'node_name'. Example log entry: message='crash connection failed', node_name='example-node_name'."
  },
  {
    "query": {
      "message": "crash*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" (e.g., \"crash connection failed\") and includes the Kubernetes `region` label. It is used in Loki to identify and aggregate crash-related events across different regions, helping SREs monitor service or process failures.\n\n**Purpose:**  \nTo detect and quantify occurrences of crashes within services or processes, segmented by Kubernetes region, enabling targeted investigation and response.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of crash events exceeds a defined threshold within a given time window (e.g., more than 5 crash logs per 5 minutes per region), indicating a potential systemic issue or outage.\n\n**Impact:**  \n- **High values:** A spike in crash logs suggests instability or failure in the service, potentially leading to degraded user experience or downtime. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable operation with no recent crash events detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of crash logs over time, grouped by `region`, to visualize trends and identify problematic regions.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (region) (count_over_time({message=~\"crash.*\", region=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if more than 5 crash-related logs occur in any region within 5 minutes."
  },
  {
    "query": {
      "message": "crash*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" and the `service` label is present, typically indicating that a specific Kubernetes service has experienced a crash or failure event. In Loki, this pattern helps identify and aggregate crash-related logs per service to monitor stability and reliability.\n\n**Purpose:**  \nTo detect and track occurrences of crashes within individual services, enabling rapid identification of failing components in the cluster.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of crash-related log entries for a given service exceeds a defined threshold within a specific time window—for example, more than 5 crash logs in 5 minutes—indicating a potentially unstable or failing service.\n\n**Impact:**  \n- **High values:** A spike in crash logs suggests frequent or ongoing failures, which can lead to degraded service availability, increased error rates, or cascading failures in dependent systems. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable service operation with no recent crash events detected.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the count of crash logs per service over time using a Loki query like:  \n  `count_over_time({service=~\".+\", message=~\"crash.*\"}[5m]) by (service)`  \n  This helps SREs quickly spot services with increasing crash rates.  \n- **Alert Rule:**  \n  ```yaml\n  alert: ServiceCrashSpike\n  expr: sum by (service) (count_over_time({service=~\".+\", message=~\"crash.*\"}[5m])) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High crash rate detected for {{ $labels.service }}\"\n    description: \"More than 5 crash-related log entries detected in the last 5 minutes for service {{ $labels.service }}. Immediate investigation recommended.\"\n  ```"
  },
  {
    "query": {
      "message": "crash*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" (e.g., \"crash connection failed\") and the entry is associated with any Kubernetes `job`. Its purpose in Loki is to identify and aggregate occurrences of service or process crashes within specific jobs, enabling monitoring of stability issues.\n\n**Thresholds and Alerting:**  \nAn alert should be triggered when the count of crash-related log entries exceeds a defined threshold within a given time window (e.g., more than 5 crash messages in 5 minutes), indicating a potential service disruption or instability.\n\n**Impact of Values:**  \n- **High values:** A spike in crash logs suggests frequent failures, which can lead to degraded service availability, increased error rates, and potential downtime. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable operation with no recent crash events, reflecting healthy service behavior.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching `{\"message\": \"crash*\", \"job\": \"*\"}` per job over time to visualize crash frequency trends.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (job) (count_over_time({message=~\"crash.*\", job=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any job logs more than 5 crash messages within 5 minutes."
  },
  {
    "query": {
      "message": "crash*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" (e.g., \"crash connection failed\") and the `environment` label is set to any Kubernetes environment. It is used in Loki to identify occurrences of service or process crashes across different deployment environments.\n\n**Purpose:**  \nTo detect and monitor crash events that may indicate instability or failures in services running within Kubernetes clusters.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of crash-related log entries exceeds a defined threshold within a specific time window—for example, more than 5 crash logs within 5 minutes in a given environment. This threshold can be adjusted based on service criticality and baseline crash rates.\n\n**Impact of Values:**  \n- **High values:** A spike in crash logs suggests increased instability or recurring failures, potentially leading to degraded service availability or outages. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicates stable service operation with no recent crash events detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of crash logs over time, grouped by `environment`, helps visualize trends and identify problematic environments.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (environment) (count_over_time({message=~\"crash.*\", environment=~\".*\"}[5m])) > 5\n  ```  \n  This rule triggers an alert if more than 5 crash messages occur within 5 minutes in any environment."
  },
  {
    "query": {
      "message": "crash*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" and the `host` label is present, typically indicating that a service or process on a specific Kubernetes host has experienced a crash. In Loki, this pattern helps identify and aggregate crash-related events per host to monitor system stability.\n\n**Purpose:**  \nTo detect and track occurrences of crashes across hosts, enabling timely identification of failing services or infrastructure issues.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of crash events exceeds a defined threshold within a given time window—for example, more than 5 crash logs per host within 5 minutes—indicating a potential systemic problem requiring immediate investigation.\n\n**Impact:**  \n- **High values:** Frequent crash logs suggest unstable services or hosts, potentially leading to degraded application performance, downtime, or data loss. Immediate remediation is critical.  \n- **Low or zero values:** Indicate stable operation with no recent crashes detected, reflecting healthy service behavior.\n\n**Example Usage:**  \n- **Dashboard:** Visualize a time series graph showing the count of crash logs per host over time, enabling quick identification of hosts with increasing crash rates.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"crash.*\", host=~\".*\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 crash messages are logged on any host within 5 minutes."
  },
  {
    "query": {
      "message": "crash*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message contains the word \"crash\" (e.g., \"crash connection failed\") and includes any Kubernetes log level. It is used in Loki to identify potential service or process crashes that may indicate instability or failures in your environment.  \n\n**Purpose:** To detect and monitor crash-related events across all log severity levels, enabling early identification of critical issues affecting service availability.\n\n**Alert Threshold:** An alert should be triggered when the number of crash-related log entries exceeds a defined threshold within a specific time window (e.g., more than 5 crash logs in 5 minutes), signaling a potential systemic problem requiring immediate investigation.\n\n**Impact:**  \n- **High values:** A spike in crash logs typically indicates recurring failures or instability, which can lead to service downtime or degraded performance. Immediate action is recommended.  \n- **Low values:** Occasional crash logs may be expected during normal operation or restarts but should be monitored to ensure they do not escalate.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the count of logs matching `{\"message\": \"crash*\", \"level\": \"*\"}` over time to track crash frequency trends.  \n- **Alert Rule (PromQL):**  \n  ```promql\n  count_over_time({message=~\"crash.*\", level=~\".*\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 crash-related logs occur within 5 minutes, prompting an SRE to investigate potential service failures."
  },
  {
    "query": {
      "message": "crash*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" (e.g., \"crash connection failed\") and the `env` label is set to any environment value. It is used in Loki to identify occurrences of service or process crashes within specific Kubernetes environments.\n\n**Purpose:**  \nTo detect and monitor crash events that may indicate instability or failures in services running in different environments.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of crash-related log entries exceeds a defined threshold within a given time window (e.g., more than 5 crash logs in 5 minutes), signaling a potential systemic issue requiring immediate investigation.\n\n**Impact of Values:**  \n- **High values:** A spike in crash logs suggests frequent or ongoing failures, potentially leading to degraded service availability or outages. Immediate remediation is critical.  \n- **Low or zero values:** Indicates stable operation with no recent crash events detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching `{\"message\": \"crash*\", \"env\": \"*\"}` grouped by `env` over the last hour to quickly identify which environments are experiencing crashes.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (env) (count_over_time({message=~\"crash.*\", env=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if more than 5 crash logs occur in any environment within 5 minutes."
  },
  {
    "query": {
      "message": "crash*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"crash\" and the app label is present with any value. It identifies occurrences of service or process crashes within applications running in Kubernetes. The measurement counts the number of such crash-related log entries. The unit is the total count of matching log events."
  },
  {
    "query": {
      "message": "crash*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"crash\" and the node field is present, indicating the Kubernetes node reporting the event. It measures the count of such crash-related log occurrences per node. Each matched entry represents one instance of a service or process crash event recorded on that node. The unit of measurement is the number of crash log entries per Kubernetes node."
  },
  {
    "query": {
      "message": "crash*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern captures logs where the message contains 'crash', indicating a service or process crash, and the log entry is labeled with Kubernetes 'app_kubernetes_io/name'. Example log entry: message='crash connection failed', app_kubernetes_io/name='example-app_kubernetes_io/name'."
  },
  {
    "query": {
      "message": "crash*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" and the `instance` label identifies the Kubernetes instance emitting the log. It is used in Loki to detect and monitor service or process crashes within specific instances.  \n\n**Purpose:** To identify occurrences of crashes in services or processes running on Kubernetes instances, enabling timely detection of stability issues.  \n\n**Alert Threshold:** An alert should be triggered when the number of crash-related log entries exceeds a defined threshold within a given time window (e.g., more than 5 crash logs per instance within 5 minutes), indicating a potential service disruption or instability.  \n\n**Impact:**  \n- **High values:** Frequent crash logs suggest critical service failures or instability, potentially leading to downtime or degraded user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate stable service operation with no recent crashes detected.  \n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of crash logs per instance over time, using a Loki query like:  \n  `count_over_time({message=~\"crash.*\", instance=~\".*\"}[5m]) by (instance)`  \n- **Alert Rule:** Trigger an alert if the count exceeds 5 in 5 minutes:  \n  ```\n  alert: ServiceCrashDetected\n  expr: count_over_time({message=~\"crash.*\", instance=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"Crash spike detected on instance {{ $labels.instance }}\"\n    description: \"More than 5 crash logs detected in the last 5 minutes on instance {{ $labels.instance }}. Immediate investigation recommended.\"\n  ```"
  },
  {
    "query": {
      "message": "crash*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"crash\" (e.g., \"crash connection failed\") and includes any Kubernetes log filename. It is used in Loki to identify occurrences of service or process crashes within Kubernetes pods.  \n\n**Purpose:**  \nTo detect and monitor crash events that may indicate instability or failures in services running on the cluster.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window—for example, more than 5 crash logs in 5 minutes—indicating a potential ongoing issue requiring investigation.\n\n**Impact of Values:**  \n- **High values:** A spike in crash logs suggests frequent or repeated service failures, which can lead to degraded application availability or data loss. Immediate attention is needed to identify root causes and mitigate impact.  \n- **Low or zero values:** Indicates stable service operation with no recent crash events detected.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({message=~\"crash.*\", filename=~\".*\"}[5m])\n```  \nto count crash events over the last 5 minutes. An alert can be configured to fire if this count exceeds 5, signaling abnormal crash frequency. This helps SREs proactively detect and respond to service instability."
  },
  {
    "query": {
      "message": "crash*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"crash\" and the application field is present with any value. It identifies occurrences of service or process crashes reported by any Kubernetes application. The measurement is a count of such log entries, representing the number of crash-related events per application. Units are discrete event counts."
  },
  {
    "query": {
      "message": "crash*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message contains the word \"crash\" and includes the Kubernetes component label, helping identify service or process crashes within specific components. In Loki, it is used to monitor stability by filtering crash-related events per component. An alert should be triggered when the number of crash logs exceeds a defined threshold within a given time window (e.g., more than 5 crash messages per 5 minutes), indicating potential service instability or failure. High values suggest frequent or ongoing crashes that may impact service availability and require immediate investigation, while low or zero values indicate stable operation. \n\nExample alert rule snippet:\n```\nsum by (component) (count_over_time({message=~\"crash.*\", component=~\".*\"}[5m])) > 5\n```\n\nThis can be visualized in a dashboard as a time series graph showing crash counts per component over time, enabling SREs to quickly identify and prioritize components experiencing frequent crashes."
  },
  {
    "query": {
      "message": "crash*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" (e.g., \"crash connection failed\") and includes any Kubernetes `source` label. It is used in Loki to identify occurrences of service or process crashes within your cluster.\n\n**Purpose:**  \nTo detect and monitor crash events from various sources, enabling timely identification of instability or failures in services.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of crash-related log entries exceeds a defined threshold within a given time window—for example, more than 5 crash logs per 5 minutes from any single source or across the cluster, indicating a potential systemic issue.\n\n**Impact of Values:**  \n- **High values:** A spike in crash logs suggests frequent or ongoing failures, which may lead to degraded service availability or outages requiring immediate investigation.  \n- **Low or zero values:** Indicates stable operation with no recent crash events detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching `{\"message\": \"crash*\", \"source\": \"*\"}` grouped by `source` over the last hour to identify which components are experiencing crashes.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\"crash.*\", source=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 crash logs occur within 5 minutes, signaling a potential problem requiring SRE attention."
  },
  {
    "query": {
      "message": "crash*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" (e.g., \"crash connection failed\") and the entry is associated with a specific Kubernetes pod via the `pod_name` label. Its purpose in Loki is to identify and aggregate crash-related events per pod, enabling SREs to monitor service stability and detect pods experiencing failures.\n\n**Threshold for alerting:** An alert should trigger when the number of crash-related log entries for a given pod exceeds a defined threshold within a set time window (e.g., more than 5 crash messages in 5 minutes), indicating a potentially unstable or failing pod.\n\n**Impact of values:**  \n- **High values:** Frequent crash messages suggest recurring failures or instability in the pod, which may lead to degraded service availability or require immediate investigation and remediation.  \n- **Low or zero values:** Indicate normal operation without recent crash events, implying stable pod behavior.\n\n**Example usage in a dashboard or alert rule:**  \nA PromQL-style Loki query to count crash events per pod over 5 minutes:  \n```\ncount_over_time({message=~\"crash.*\", pod_name=~\".*\"}[5m])\n```\nAn alert rule could be:  \n```\nalert: PodCrashHigh\nexpr: count_over_time({message=~\"crash.*\", pod_name=~\".*\"}[5m]) > 5\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"High crash rate detected in pod {{ $labels.pod_name }}\"\n  description: \"Pod {{ $labels.pod_name }} has logged more than 5 crash messages in the last 5 minutes, indicating potential instability.\"\n```\nThis enables proactive detection and response to pod crashes, improving system reliability."
  },
  {
    "query": {
      "message": "crash*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"crash\" and the container_name field is present with any value. It identifies occurrences of service or process crashes within Kubernetes containers. The measurement is a count of such log entries, representing the number of crash-related events per container. Units are discrete event counts."
  },
  {
    "query": {
      "message": "crash*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message contains the term \"crash\" and includes the Kubernetes cluster label, helping identify service or process crashes within specific clusters. In Loki, **{\"message\": \"crash*\", \"cluster\": \"*\"}** is used to filter and aggregate crash-related events per cluster, enabling targeted monitoring of stability issues. An alert threshold might be set to trigger when the number of crash logs exceeds a defined count (e.g., more than 5 crashes within 5 minutes) in any cluster, signaling potential service degradation or outages. High values indicate frequent or ongoing crashes that could impact availability and require immediate investigation, while low or zero values suggest stable operation. Example alert rule snippet:\n\n```\nsum by (cluster) (count_over_time({message=~\"crash.*\", cluster=~\".*\"}[5m])) > 5\n```\n\nThis can be visualized in a dashboard as a time series graph showing crash counts per cluster, helping SREs quickly identify and respond to clusters experiencing elevated crash rates."
  },
  {
    "query": {
      "message": "crash*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"crash\" (e.g., \"crash connection failed\") and the entry is associated with any Kubernetes `pod`. It is used in Loki to identify and monitor crash-related events within pods, which often indicate service instability or failures.\n\n**Purpose:**  \nTo detect and track occurrences of crashes in pod logs, enabling early identification of service disruptions or failures.\n\n**Alert Threshold:**  \nAn alert should be triggered when the number of crash-related log entries exceeds a defined threshold within a given time window—for example, more than 5 crash messages per pod within 5 minutes—indicating a potential ongoing issue requiring investigation.\n\n**Impact of Values:**  \n- **High values:** Frequent crash messages suggest unstable pods or services, potentially leading to degraded application performance or downtime. Immediate remediation is recommended.  \n- **Low or zero values:** Indicate stable pod operation with no recent crash events, reflecting healthy service behavior.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching `{\"message\": \"crash*\", \"pod\": \"*\"}` per pod over time to visualize crash frequency and identify problematic pods.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (pod) (count_over_time({message=~\"crash.*\", pod=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any pod logs more than 5 crash-related messages in 5 minutes."
  },
  {
    "query": {
      "message": "crash*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"crash\" and the namespace field is any Kubernetes namespace. It identifies occurrences of service or process crashes within specific Kubernetes namespaces. The measurement is a count of such log entries, representing the number of crash-related events per namespace."
  },
  {
    "query": {
      "message": "start*",
      "k8s_app": "*"
    },
    "description": "This log pattern **{\"message\": \"start*\", \"k8s_app\": \"*\"}** in Loki captures entries where the log message begins with \"start\" (e.g., \"start connection failed\", \"start process\") and is associated with any Kubernetes application (`k8s_app` label). It is primarily used to detect events related to application restarts, startup failures, or crash loops within Kubernetes pods.\n\n**Purpose:**  \nTo monitor and identify frequent or repeated start-related events that may indicate instability, such as pods repeatedly restarting or failing to initialize properly.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 \"start*\" messages per 5 minutes per `k8s_app`). This threshold can be adjusted based on normal application behavior.\n\n**Impact of Values:**  \n- **High values:** A surge in \"start*\" messages typically signals instability, such as crash loops or failed startups, which can lead to degraded service availability or downtime. Immediate investigation is warranted.  \n- **Low or zero values:** Indicates stable application startups with no recent restart or startup failures, suggesting healthy pod lifecycle behavior.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({k8s_app=~\".+\", message=~\"start.*\"}[5m])\n```\nto count occurrences over 5 minutes per application. An alert rule could fire if this count exceeds 5, signaling potential restart issues for that `k8s_app`. This helps SREs proactively detect and respond to pod instability before it impacts users."
  },
  {
    "query": {
      "message": "start*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), and the `container` label is present, typically indicating events related to container restarts or crash loops in Kubernetes. In Loki, this helps identify frequent container restarts or failures by filtering logs that signal the start of a process or connection attempt.\n\n**Purpose:**  \nTo detect and monitor restart or crash loop events by capturing logs that indicate a container is repeatedly starting, which often signals instability or failure in the application or environment.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching **{\"message\": \"start*\", \"container\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 5 occurrences within 5 minutes). This threshold can be adjusted based on normal restart frequency for the service.\n\n**Impact:**  \n- **High values:** Indicate frequent container restarts or crash loops, which can lead to service downtime, degraded performance, or resource exhaustion. Immediate investigation is required.  \n- **Low or zero values:** Suggest stable container operation with no recent restart-related issues.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing the rate of logs matching **{\"message\": \"start*\", \"container\": \"*\"}** over time, segmented by container name, to visualize restart patterns.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"start.*\", container=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 \"start*\" messages occur in any container within 5 minutes, signaling potential crash loops."
  },
  {
    "query": {
      "message": "start*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), and the entry includes a Kubernetes `node_name` label. It is used in Loki to identify events related to pod or container restarts, crashes, or initialization failures on specific nodes.  \n\n**Purpose:**  \nTo detect and monitor restart or crash loop events that may indicate instability or failures in workloads running on Kubernetes nodes.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"start*\" messages per node within 5 minutes), signaling frequent restarts or crash loops.\n\n**Impact:**  \n- **High values:** Indicate potential instability or repeated failures on the node, which can lead to degraded service availability or performance issues. Immediate investigation is recommended.  \n- **Low or zero values:** Suggest normal operation without frequent restarts or crashes.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"start*\" messages per `node_name` over time, highlighting nodes with increasing restart events.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"start.*\", node_name=~\".+\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 restart-related logs occur on a node within 5 minutes."
  },
  {
    "query": {
      "message": "start*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"start\" and includes any subsequent text. It specifically captures events indicating the initiation of processes, such as restarts or crash loops, within Kubernetes environments. The \"region\" field identifies the Kubernetes region associated with each log entry. The measurement unit is the count of such log entries per region."
  },
  {
    "query": {
      "message": "start*",
      "service": "*"
    },
    "description": "This log pattern **{\"message\": \"start*\", \"service\": \"*\"}** is used in Loki to identify log entries where the message begins with \"start\", typically indicating service restarts or crash loop events within Kubernetes services. Monitoring these entries helps detect instability or failures in services labeled by the `service` field.\n\n**Purpose:**  \nTo track and alert on frequent service restarts or crash loops by capturing logs that signal the start of a process or connection attempt, which often precedes or follows a failure.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a specific time window—for example, more than 5 \"start*\" messages per service within 5 minutes—indicating abnormal restart frequency.\n\n**Impact of Values:**  \n- **High values:** Suggest frequent restarts or crash loops, potentially causing service downtime, degraded performance, or instability. Immediate investigation is required.  \n- **Low or zero values:** Indicate stable service operation with no recent restarts detected.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({service=~\".+\", message=~\"start.*\"}[5m])\n```\nto count \"start*\" messages per service over the last 5 minutes. An alert can be configured to fire if this count exceeds 5 for any service, signaling a potential crash loop or restart storm."
  },
  {
    "query": {
      "message": "start*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"start\" and includes any Kubernetes job label. It identifies events related to the initiation of processes, such as restarts or crash loops, within a specific job. The measurement counts the number of such log entries per job. Units are expressed as the total count of matching log events."
  },
  {
    "query": {
      "message": "start*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), filtered by any Kubernetes `environment` label. It is used in Loki to identify events related to service restarts or crash loops, which often indicate instability or failures in the application or infrastructure.\n\n**Purpose:**  \nTo detect and monitor restart or crash loop events by capturing logs that signal the start of a process or connection failure, helping SREs quickly identify and respond to potential service disruptions.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a specific time window (e.g., more than 5 \"start*\" messages within 5 minutes), indicating frequent restarts or failures.\n\n**Impact:**  \n- **High values:** Suggest frequent restarts or crash loops, potentially leading to service downtime, degraded performance, or cascading failures. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate stable service operation with no recent restart-related issues.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of logs matching `{\"message\": \"start*\", \"environment\": \"*\"}` per environment to visualize restart trends over time.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"start.*\", environment=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 \"start*\" messages occur within 5 minutes in any environment, signaling potential instability."
  },
  {
    "query": {
      "message": "start*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), and the entry includes a `host` label identifying the Kubernetes node. It is used in Loki to detect events related to pod restarts or crash loops, as these often emit logs starting with \"start\".  \n\n**Purpose:**  \nTo monitor and alert on potential instability in pods or nodes by tracking the frequency of \"start*\" messages per host, which typically indicate restarts or failures.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of log entries matching **{\"message\": \"start*\", \"host\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 5 occurrences per host within 5 minutes), signaling frequent restarts or crash loops.\n\n**Impact:**  \n- **High values:** Indicate repeated pod restarts or crash loops on a host, potentially causing service disruption or degraded performance. Immediate investigation is required.  \n- **Low or zero values:** Suggest stable pod operation with no recent restart-related events.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"start*\" messages per host over time, highlighting hosts with increasing restart events.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"start.*\", host=~\".*\"}[5m]) > 5\n  ```  \nThis triggers an alert if more than 5 \"start*\" messages are logged on a single host within 5 minutes, indicating potential instability."
  },
  {
    "query": {
      "message": "start*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"start\" and the log includes any Kubernetes severity level. It identifies events related to the initiation of processes, such as restarts or crash loops. The pattern measures the occurrence count of such log entries, with no specific unit beyond the number of matching logs. Example: message='start connection failed', level='warning'."
  },
  {
    "query": {
      "message": "start*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\", typically indicating events related to service restarts, initialization attempts, or crash loops within a Kubernetes environment identified by the `env` label. In Loki, **{\"message\": \"start*\", \"env\": \"*\"}** helps SREs monitor the frequency of restart-related logs across all environments.\n\n**Purpose:** Detect potential instability or repeated restarts of services by tracking how often \"start\"-prefixed messages occur, which may signal crash loops or failed initialization.\n\n**Alert Threshold:** An alert should be triggered if the count of these log entries exceeds a defined threshold within a set time window (e.g., more than 5 occurrences in 5 minutes), indicating abnormal restart activity.\n\n**Impact:**  \n- **High values:** Suggest frequent restarts or crash loops, potentially causing service downtime or degraded performance, requiring immediate investigation.  \n- **Low or zero values:** Indicate stable service operation with no recent restart events.\n\n**Example Usage:**  \n- **Dashboard:** A graph showing the rate of logs matching **{\"message\": \"start*\", \"env\": \"$env\"}** over time, segmented by environment, to quickly identify spikes in restart events.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (env) (count_over_time({message=~\"start.*\", env=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if more than 5 \"start\"-related log entries occur in any environment within 5 minutes."
  },
  {
    "query": {
      "message": "start*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), and the `app` label identifies the Kubernetes application emitting the log. It is used in Loki to detect events related to application restarts or crash loops, which often manifest as repeated \"start\" messages indicating instability.\n\n**Purpose:**  \nTo monitor and alert on frequent application restarts or crash loops by tracking how often logs with messages starting with \"start\" occur per application.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of such \"start*\" log entries for a given `app` exceeds a defined threshold within a short time window (e.g., more than 5 occurrences in 5 minutes), signaling potential instability or failure.\n\n**Impact:**  \n- **High values:** Indicate frequent restarts or crash loops, which can lead to degraded service availability, increased resource consumption, and potential downtime. Immediate investigation is required.  \n- **Low or zero values:** Suggest stable application behavior with no recent restart-related issues.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"start.*\", app=~\".*\"}[5m])\n```\nto count occurrences of \"start*\" messages per app over the last 5 minutes. Set an alert to fire if this count exceeds 5 for any `app`, enabling proactive detection of crash loops or restart storms."
  },
  {
    "query": {
      "message": "start*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"start\" (e.g., \"start connection failed\"), tagged by the Kubernetes node emitting the log. It is primarily used to detect events related to pod or container restarts, crashes, or initialization failures on specific nodes.  \n\n**Purpose:**  \nTo monitor and identify restart or crash loop events that may indicate instability or failures in workloads running on Kubernetes nodes.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"start*\", \"node\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 5 occurrences per node within 5 minutes), signaling potential crash loops or repeated restarts.\n\n**Impact:**  \n- **High values:** Suggest frequent restarts or failures on a node, potentially leading to degraded application availability, increased resource consumption, or cascading failures. Immediate investigation is recommended.  \n- **Low or zero values:** Indicate normal operation with no recent restart-related issues detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of \"start*\" messages per node over time to quickly identify nodes experiencing frequent restarts.  \n- **Alert Rule (PromQL example):**  \n  ```  \n  count_over_time({message=~\"start.*\"}[5m]) by (node) > 5  \n  ```  \n  This triggers an alert if more than 5 \"start*\" messages are logged on any node within 5 minutes."
  },
  {
    "query": {
      "message": "start*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This log pattern matches entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), combined with any value for the Kubernetes label `app_kubernetes_io/name`. It is designed to detect events related to application restarts or crash loops within Kubernetes pods, as these often emit logs starting with \"start\" during initialization or failure sequences.\n\n**Purpose:**  \nIn Loki, this pattern helps identify pods that are repeatedly restarting or failing to start properly by filtering logs indicative of start-related errors or restarts, tagged by the specific application (`app_kubernetes_io/name`).\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of such \"start*\" log entries exceeds a defined threshold within a given time window (e.g., more than 5 occurrences in 5 minutes) for a particular application. This threshold indicates abnormal restart frequency or instability.\n\n**Impact:**  \n- **High values:** Suggest frequent restarts or crash loops, potentially leading to degraded service availability, increased resource consumption, and user impact. Immediate investigation is warranted.  \n- **Low or zero values:** Indicate stable application start behavior with no recent restart-related errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of `{\"message\": \"start*\", \"app_kubernetes_io/name\": \"*\"}` logs over time per application, highlighting spikes that may correlate with incidents.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"start.*\", app_kubernetes_io/name=~\".+\"}[5m]) > 5\n  ```  \n  This rule fires if more than 5 \"start*\" messages occur within 5 minutes for any app, signaling potential restart issues."
  },
  {
    "query": {
      "message": "start*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"start\" and includes any following text. It specifically captures events related to restarts or crash loops in Kubernetes pods, identified by the presence of the \"instance\" label. The pattern measures the count of such log occurrences per instance. Each matched log entry corresponds to one event instance."
  },
  {
    "query": {
      "message": "start*",
      "filename": "*"
    },
    "description": "This query pattern **{\"message\": \"start*\", \"filename\": \"*\"}** in Loki filters log entries where the `message` field begins with the string \"start\" and the `filename` label is present with any value. It is primarily used to identify log events related to the initiation of processes, services, or connections as recorded in Kubernetes pod logs or other containerized environments. The data source is structured log streams ingested by Loki, typically from Kubernetes nodes or containers, where `filename` corresponds to the source log file or container log identifier.\n\nThe pattern returns all log entries indicating the start of an operation or event, such as \"start connection\", \"start process\", or similar messages. These entries are useful for monitoring system behavior, tracking service restarts, or diagnosing startup issues.\n\nAn unusually high frequency or sudden spike in logs matching this pattern may indicate repeated restarts, crash loops, or instability in the system. Such anomalies should trigger alerts to prompt investigation, as they can signify degraded service health or failures in initialization sequences. Alerts should be configured based on thresholds relevant to the expected startup frequency and operational context."
  },
  {
    "query": {
      "message": "start*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), and the `application` label is set, typically indicating events related to application restarts or crash loops in Kubernetes environments. In Loki, this helps identify frequent or repeated start events that may signal instability or failures in the application lifecycle.\n\n**Purpose:**  \nTo detect and monitor restart or crash loop events by capturing logs that indicate the application is starting or attempting to start, which often precedes or follows failure conditions.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 \"start*\" messages per 5 minutes), signaling potential instability or crash loops.\n\n**Impact of Values:**  \n- **High values:** Frequent \"start*\" messages suggest repeated restarts or crash loops, indicating application instability, degraded service availability, or underlying infrastructure issues requiring immediate investigation.  \n- **Low or zero values:** Normal operation with no recent restarts detected, implying stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching `{message=~\"start.*\", application=~\".*\"}` over time, grouped by `application`, to visualize restart frequency per service.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"start.*\", application=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 start-related log entries occur within 5 minutes for any application, prompting SREs to investigate potential crash loops or restarts."
  },
  {
    "query": {
      "message": "start*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), and includes any Kubernetes `component` label. It is used in Loki to identify events related to service restarts or crash loops, which often indicate instability or failures in components.\n\n**Purpose:**  \nTo detect and monitor restart or crash loop events by capturing logs that signal the start of a process or connection attempts that may fail repeatedly.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of log entries matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 occurrences in 5 minutes), indicating frequent restarts or failures.\n\n**Impact:**  \n- **High values:** Suggest persistent instability or crash loops in the component, potentially causing service degradation or outages. Immediate investigation is required.  \n- **Low or zero values:** Indicate normal operation with no recent restart-related issues.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of `{\"message\": \"start*\", \"component\": \"*\"}` logs per component over time to identify spikes in restarts.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"start.*\", component=~\".*\"}[5m]) > 5\n  ```\n  This triggers an alert if more than 5 \"start\" messages appear for any component within 5 minutes, signaling potential crash loops."
  },
  {
    "query": {
      "message": "start*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\"), and the `source` label is present, typically indicating events related to service restarts or crash loops in Kubernetes environments. In Loki, this helps identify frequent or repeated start events that may signal instability.\n\n**Purpose:** To detect and monitor restart or crash loop events by capturing logs that indicate a service or component is starting, often after failure.\n\n**Alert Threshold:** An alert could be triggered if the count of such log entries exceeds a defined threshold within a short time window (e.g., more than 5 \"start*\" messages per minute), suggesting abnormal restart frequency.\n\n**Impact:**  \n- **High values:** Indicate frequent restarts or crash loops, potentially causing service instability, degraded performance, or downtime. Immediate investigation is warranted.  \n- **Low or zero values:** Suggest normal operation without frequent restarts.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching `{\"message\": \"start*\", \"source\": \"*\"}` over time, highlighting spikes that correlate with service restarts.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"start.*\", source=~\".*\"}[1m]) > 5\n  ```  \n  This triggers an alert if more than 5 start-related log entries occur within one minute, indicating potential crash loops."
  },
  {
    "query": {
      "message": "start*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field begins with the word \"start\" and the \"pod_name\" field is present. It identifies events related to pod restarts or crash loops in Kubernetes environments. The pattern measures the count of such log occurrences, representing the number of restart-related messages per pod. The unit of measurement is the number of log entries matching these criteria."
  },
  {
    "query": {
      "message": "start*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field begins with \"start\" (e.g., \"start connection failed\") and includes any Kubernetes `container_name`. It is used in Loki to identify events related to container restarts or crash loops, which often indicate instability or failures in application pods.\n\n**Purpose:**  \nTo detect and monitor frequent container restarts or crash loop events by capturing logs signaling the start of such events.\n\n**Alert Threshold:**  \nAn alert should be triggered if the count of log entries matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 occurrences in 5 minutes), indicating potential instability or repeated failures.\n\n**Impact:**  \n- **High values:** Frequent \"start*\" messages suggest containers are repeatedly restarting, which can lead to service disruption, degraded performance, or downtime. Immediate investigation is required.  \n- **Low or zero values:** Normal operation, indicating containers are stable without frequent restarts.\n\n**Example Usage:**  \nIn a Loki alert rule, you might count the number of \"start*\" messages per container over 5 minutes:\n\n```\ncount_over_time({container_name=~\".*\"} |= \"start\" [5m]) > 5\n```\n\nThis triggers an alert when a container logs more than 5 \"start*\" messages in 5 minutes, signaling a potential crash loop.\n\nIn a dashboard, you can graph the rate of these messages per container to visualize restart trends and quickly identify problematic containers."
  },
  {
    "query": {
      "message": "start*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"start\", typically indicating events related to service restarts, initialization attempts, or crash loops within a Kubernetes cluster (identified by the \"cluster\" label). In Loki, this helps SREs monitor the frequency of restart-related events across clusters.\n\n**Purpose:** To detect and quantify restart or crash loop events that may signal instability or failures in services running in the cluster.\n\n**Alert Threshold:** An alert should be triggered if the count of logs matching **{\"message\": \"start*\", \"cluster\": \"*\"}** exceeds a defined threshold within a short time window (e.g., more than 5 restart events per 5 minutes), indicating potential service instability.\n\n**Impact:**  \n- **High values:** Suggest frequent restarts or crash loops, which can lead to degraded service availability, increased latency, or downtime. Immediate investigation is warranted.  \n- **Low or zero values:** Indicate stable service operation with no recent restart-related issues.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"start*\" messages per cluster over time, enabling quick identification of clusters experiencing frequent restarts.  \n- **Alert Rule (PromQL example):**  \n  ```  \n  count_over_time({message=~\"start.*\", cluster=~\".*\"}[5m]) > 5  \n  ```  \n  This triggers an alert if more than 5 restart-related log entries occur within 5 minutes in any cluster."
  },
  {
    "query": {
      "message": "start*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field begins with the word \"start\" and the \"pod\" field is present. It identifies events related to the initiation of processes or connections within Kubernetes pods. The pattern measures the count of such log entries, representing occurrences of start-related events per pod. The unit of measurement is the number of matching log entries."
  },
  {
    "query": {
      "message": "start*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"start\" and includes any Kubernetes namespace label. It identifies events related to the initiation of processes, such as restarts or crash loops. The measurement is a count of matching log entries, grouped by their respective namespaces. Units are discrete log occurrences per namespace."
  },
  {
    "query": {
      "message": "deploy*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"deploy\" and the \"k8s_app\" label is present with any value. It identifies deployment-related events or errors within Kubernetes applications. The measurement is a count of such log entries, representing the number of deployment-related logs per unit of time. For example, a log with message=\"deploy connection failed\" and k8s_app=\"example-k8s_app\" would be included."
  },
  {
    "query": {
      "message": "deploy*",
      "container": "*"
    },
    "description": "This pattern captures logs where the message contains 'deploy', indicating a deployment event or related log, and the log entry is labeled with Kubernetes 'container'. Example log entry: message='deploy connection failed', container='example-container'."
  },
  {
    "query": {
      "message": "deploy*",
      "node_name": "*"
    },
    "description": "This log pattern **{\"message\": \"deploy*\", \"node_name\": \"*\"}** is used in Loki to filter and identify log entries related to deployment events across Kubernetes nodes. It captures any log message beginning with or containing the term \"deploy\" (e.g., \"deploy started\", \"deploy failed\") and associates it with the specific Kubernetes node where the event occurred via the `node_name` label.\n\n**Purpose:**  \nThis pattern helps SREs monitor deployment activities and detect deployment-related issues on individual nodes, enabling targeted troubleshooting and operational awareness.\n\n**Alert Thresholds:**  \n- **Warning alert:** Trigger if the number of deployment failure messages (e.g., containing \"deploy failed\" or similar error keywords) exceeds 5 occurrences within 5 minutes on any single node.  \n- **Critical alert:** Trigger if deployment failures exceed 10 occurrences within 5 minutes on any node, indicating a potentially widespread or persistent deployment problem.\n\n**Impact:**  \n- **High values:** A surge in deployment failure logs suggests deployment instability or automation issues, potentially causing service downtime or delayed rollouts. Immediate investigation is required to prevent cascading failures.  \n- **Low or zero values:** Normal state indicating deployments are proceeding without logged errors, reflecting stable deployment pipelines.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing a time series graph of deployment failure counts per node, using a Loki query like:  \n  `count_over_time({message=~\"deploy.*failed\", node_name=~\".*\"}[5m]) by (node_name)`  \n- **Alert Rule:**  \n  ```yaml\n  alert: DeploymentFailuresHigh\n  expr: count_over_time({message=~\"deploy.*failed\", node_name=~\".*\"}[5m]) > 5\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High deployment failure rate on node {{ $labels.node_name }}\"\n    description: \"More than 5 deployment failure logs detected in the last 5 minutes on node {{ $labels.node_name }}.\"\n  ```"
  },
  {
    "query": {
      "message": "deploy*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"deploy\" (e.g., \"deploy started\", \"deploy failed\") and includes any Kubernetes `region` label. It is used in Loki to monitor deployment-related events across different regions. \n\n**Purpose:**  \nTo track deployment activities and detect potential issues such as failed or stalled deployments by aggregating deployment logs per region.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of deployment failure messages (e.g., containing \"deploy failed\" or similar error indicators) exceeds a defined threshold within a given time window (e.g., more than 5 failures in 10 minutes) for any region.\n\n**Impact:**  \n- **High values:** A spike in deployment failure logs indicates problems in the deployment pipeline or infrastructure in that region, potentially causing service downtime or delayed feature releases.  \n- **Low or zero values:** Normal operation, indicating deployments are proceeding without logged errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of deployment-related logs per region, highlighting failure messages to quickly identify problematic regions.  \n- **Alert Rule (pseudo-expression):**  \n  ```\n  count_over_time({message=~\"deploy.*failed\", region=~\".*\"}[10m]) > 5\n  ```  \nThis triggers an alert if more than 5 deployment failure logs occur in any region within 10 minutes."
  },
  {
    "query": {
      "message": "deploy*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"deploy\" and includes any subsequent characters. It specifically filters logs labeled with any Kubernetes service name. The pattern measures the count of deployment-related log events per service. The unit of measurement is the number of matching log entries."
  },
  {
    "query": {
      "message": "deploy*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"deploy\" (e.g., \"deploy started\", \"deploy failed\") and includes any Kubernetes `job` label. It is used in Loki to track deployment-related events across jobs, helping SREs monitor deployment success and failures.\n\n**Purpose:**  \nTo identify and aggregate deployment events and errors from various Kubernetes jobs, enabling timely detection of deployment issues.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of deployment failure messages (e.g., containing \"deploy failed\" or similar error indicators) exceeds a defined threshold within a given time window (e.g., >5 failures in 5 minutes), signaling potential deployment instability.\n\n**Impact of Values:**  \n- **High count:** Indicates frequent deployment failures or issues, potentially causing service downtime or delayed releases. Immediate investigation is required.  \n- **Low or zero count:** Suggests deployments are proceeding smoothly without errors.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the number of deployment events per job over time, filtering logs with `{message=~\"deploy.*\", job=~\".*\"}` to spot trends or spikes in deployment failures.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (job) (count_over_time({message=~\"deploy.*\", job=~\".*\"} |= \"failed\"[5m])) > 5\n  ```  \n  This triggers an alert if more than 5 deployment failure logs occur within 5 minutes for any job."
  },
  {
    "query": {
      "message": "deploy*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"deploy\" (e.g., \"deploy started\", \"deploy failed\") and includes any Kubernetes environment label. It is used in Loki to monitor deployment-related events across all environments. An alert based on this pattern typically triggers when the count of deployment failure messages (e.g., containing \"deploy failed\" or similar error indicators) exceeds a defined threshold within a given time window (e.g., more than 5 failures in 10 minutes), signaling potential issues in the deployment process. High values indicate frequent deployment problems that may impact application availability or stability, requiring immediate investigation, while low or zero values suggest normal deployment operations. Example alert rule snippet:  \n```\ncount_over_time({message=~\"deploy.*\", environment=~\".*\"} |= \"failed\"[10m]) > 5\n```\nThis can be visualized in a dashboard as a time series graph showing the rate of deployment failures per environment, enabling SREs to quickly identify and respond to deployment issues."
  },
  {
    "query": {
      "message": "deploy*",
      "host": "*"
    },
    "description": "This pattern captures logs where the message contains 'deploy', indicating a deployment event or related log, and the log entry is labeled with Kubernetes 'host'. Example log entry: message='deploy connection failed', host='example-host'."
  },
  {
    "query": {
      "message": "deploy*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"deploy\" (e.g., deployment-related events or errors) and the `level` field captures the severity or log level (such as info, warning, error). In Loki, this filter helps isolate deployment-related logs to monitor the health and status of deployment processes.\n\n**Purpose:**  \nTo track deployment events and detect issues during deployments by filtering logs that mention \"deploy\" at any severity level.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"deploy*\", \"level\": \"error\"}** or **{\"message\": \"deploy*\", \"level\": \"warning\"}** exceeds a defined threshold (e.g., more than 5 error or warning deployment logs within 5 minutes), indicating potential deployment failures or instability.\n\n**Impact of Values:**  \n- **High count of error/warning logs:** Suggests deployment problems that may cause service disruption or rollback. Immediate investigation is required.  \n- **Low or zero error/warning logs:** Indicates deployments are proceeding smoothly without critical issues.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of deployment-related logs grouped by `level` over time, highlighting spikes in errors or warnings during deployment windows.  \n- **Alert Rule (PromQL):**  \n  ```\n  count_over_time({message=~\"deploy.*\", level=~\"error|warning\"}[5m]) > 5\n  ```  \nThis rule triggers an alert if more than 5 deployment-related error or warning logs occur within 5 minutes, signaling potential deployment failures."
  },
  {
    "query": {
      "message": "deploy*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"deploy\" and includes any subsequent characters. It specifically captures deployment-related events labeled with any Kubernetes environment value in the \"env\" field. The measurement counts the number of such deployment log entries. The unit is a simple count of matching log occurrences."
  },
  {
    "query": {
      "message": "deploy*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"deploy\" (e.g., \"deploy started\", \"deploy failed\") and the `app` label identifies the Kubernetes application emitting the log. It is used in Loki to track deployment-related events across applications.\n\n**Purpose:**  \nTo monitor deployment activities and detect failures or anomalies during deployment processes by filtering logs that indicate deployment events.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of deployment failure messages (e.g., containing \"deploy failed\" or similar error indicators) exceeds a defined threshold within a given time window (e.g., more than 5 failures in 10 minutes). This threshold helps identify problematic deployments that may impact application availability or stability.\n\n**Impact of Values:**  \n- **High values:** A spike in deployment failure logs suggests repeated or ongoing deployment issues, potentially causing downtime or degraded service. Immediate investigation is required.  \n- **Low or zero values:** Normal state indicating deployments are proceeding without logged errors.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of deployment-related logs per application over time to spot trends or spikes in deployment failures.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\"deploy.*failed\", app=~\".*\"}[10m]) > 5\n  ```  \n  This rule triggers if more than 5 deployment failure logs occur within 10 minutes for any app, alerting SREs to investigate deployment issues promptly."
  },
  {
    "query": {
      "message": "deploy*",
      "node": "*"
    },
    "description": "This pattern captures logs where the message contains 'deploy', indicating a deployment event or related log, and the log entry is labeled with Kubernetes 'node'. Example log entry: message='deploy connection failed', node='example-node'."
  },
  {
    "query": {
      "message": "deploy*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This log query pattern **{\"message\": \"deploy*\", \"app_kubernetes_io/name\": \"*\"}** in Loki is designed to capture all log entries related to deployment events across Kubernetes applications, identified by the presence of the substring \"deploy\" in the message and the Kubernetes label `app_kubernetes_io/name` specifying the application name. It helps SREs monitor deployment activities and detect potential deployment failures or issues.\n\n**Purpose:**  \n- Track deployment-related logs for all applications labeled with `app_kubernetes_io/name`.  \n- Identify deployment errors, warnings, or unusual deployment frequency that could impact application stability or availability.\n\n**Alert Threshold Guidance:**  \n- Trigger an alert if the count of deployment failure messages (e.g., containing keywords like \"failed\", \"error\", or \"timeout\" alongside \"deploy\") exceeds a defined threshold within a short time window (e.g., >5 failed deploy logs in 5 minutes).  \n- Alternatively, alert if deployment events spike abnormally, indicating possible repeated or failed deployment attempts.\n\n**Impact of Values:**  \n- **High values:** A surge in deployment-related error logs may indicate ongoing deployment failures, risking application downtime or degraded service. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal or no deployment activity, indicating stable deployment processes or no recent deployments.\n\n**Example Usage in Dashboard or Alert Rule:**  \n- **Dashboard Panel:** A time series graph showing the count of logs matching **{\"message\": \"deploy*\", \"app_kubernetes_io/name\": \"*\"}** grouped by `app_kubernetes_io/name`, highlighting spikes or error rates during deployments.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"deploy.*\", app_kubernetes_io/name=~\".*\"} |= \"failed\"[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 deployment failure logs occur within 5 minutes for any application.\n\nThis pattern enables proactive monitoring of deployment health and rapid detection of deployment-related issues across Kubernetes applications."
  },
  {
    "query": {
      "message": "deploy*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"deploy\" (e.g., deployment-related events or errors) and includes any Kubernetes `instance` label, helping to isolate deployment activity per instance in Loki. It is primarily used to monitor deployment success and failures across your cluster.\n\n**Purpose:**  \nTo track deployment-related logs in real-time, enabling detection of deployment issues such as failures or repeated deployment attempts on specific instances.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of logs matching this pattern with error keywords (e.g., \"failed\", \"error\") exceeds a defined threshold within a short time window (e.g., >5 error logs in 5 minutes), indicating potential deployment instability or failures.\n\n**Impact of Values:**  \n- **High count:** May indicate frequent deployment failures or repeated deployment attempts, potentially causing service disruption or instability. Immediate investigation is recommended.  \n- **Low or zero count:** Suggests normal deployment activity or no recent deployments, indicating stable deployment processes.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of deployment-related logs per instance over time, highlighting spikes in errors.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"deploy.*\", instance=~\".*\"} |= \"failed\"[5m]) > 5\n  ```\n  This triggers an alert if more than 5 deployment failure logs occur within 5 minutes on any instance."
  },
  {
    "query": {
      "message": "deploy*",
      "filename": "*"
    },
    "description": "This pattern matches log entries whose message starts with the word \"deploy\" and have any filename label from Kubernetes logs. It identifies deployment-related events or errors recorded in the logs. The pattern measures the count of such log entries, representing the number of deployment occurrences or issues. The unit of measurement is the total number of matching log entries."
  },
  {
    "query": {
      "message": "deploy*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"deploy\" and the application field is present with any value. It identifies deployment-related events recorded by Kubernetes applications. The pattern measures the count of such deployment log occurrences. Each matched log entry represents one deployment event instance."
  },
  {
    "query": {
      "message": "deploy*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"deploy\" and includes any additional text. It specifically captures logs labeled with any Kubernetes component name. The pattern measures the count of deployment-related log events generated by each component. The unit of measurement is the number of matching log entries."
  },
  {
    "query": {
      "message": "deploy*",
      "source": "*"
    },
    "description": "This pattern matches log entries whose message begins with the word \"deploy\" followed by any characters. It captures events related to deployment activities recorded in logs. The \"source\" field indicates the Kubernetes component or service generating the log. The measurement is a count of such matching log entries."
  },
  {
    "query": {
      "message": "deploy*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"deploy\" (e.g., \"deploy started\", \"deployment failed\") and includes the Kubernetes `pod_name` label. It is used in Loki to track deployment-related events across pods, helping SREs monitor deployment success or failure signals in real time.\n\n**Purpose:**  \nTo identify and aggregate deployment-related logs per pod, enabling detection of deployment issues or trends that may affect application stability.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of logs matching this pattern exceeds a defined threshold within a short time window (e.g., >5 deployment failure messages in 5 minutes per pod), indicating repeated deployment problems.\n\n**Impact of Values:**  \n- **High count:** Suggests frequent deployment errors or retries, potentially causing downtime or degraded service. Immediate investigation is warranted.  \n- **Low or zero count:** Indicates normal deployment activity or no deployment issues detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the number of \"deploy*\" messages per `pod_name` over time, highlighting spikes in deployment errors.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"deploy.*\", pod_name=~\".*\"}[5m]) > 5\n  ```  \nThis triggers an alert if more than 5 deployment-related log entries occur within 5 minutes for any pod, signaling potential deployment instability."
  },
  {
    "query": {
      "message": "deploy*",
      "container_name": "*"
    },
    "description": "This pattern captures logs where the message contains 'deploy', indicating a deployment event or related log, and the log entry is labeled with Kubernetes 'container_name'. Example log entry: message='deploy connection failed', container_name='example-container_name'."
  },
  {
    "query": {
      "message": "deploy*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"deploy\" and the \"cluster\" field contains any value. It identifies deployment-related events recorded within a Kubernetes cluster. The measurement is a count of such log entries, representing the frequency of deployment events per cluster. Units are expressed as the number of matching log entries."
  },
  {
    "query": {
      "message": "deploy*",
      "pod": "*"
    },
    "description": "This log selector **{\"message\": \"deploy*\", \"pod\": \"*\"}** in Loki captures all log entries from any Kubernetes pod where the message field starts with \"deploy\". It is primarily used to monitor deployment-related events such as deployment starts, successes, failures, or errors.  \n\n**Purpose:**  \n- Enables SREs to track deployment activity and detect issues during deployment processes across all pods.  \n- Helps identify failed or problematic deployments quickly by filtering logs that mention deployment events.\n\n**Alert Thresholds:**  \n- An alert could be triggered if the count of logs matching this pattern with error keywords (e.g., \"failed\", \"error\") exceeds a defined threshold within a short time window (e.g., >5 error deployment logs in 5 minutes).  \n- Alternatively, absence of deployment logs during expected deployment windows might also trigger alerts indicating potential pipeline issues.\n\n**Impact of Values:**  \n- **High count of deployment error logs:** Indicates frequent deployment failures or instability, potentially causing service disruption or delayed rollouts. Immediate investigation is required.  \n- **Low or zero deployment logs during deployment windows:** May suggest deployment pipeline failures or monitoring gaps, risking unnoticed failed deployments.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of deployment-related logs over time, split by pod, highlighting error vs. success messages.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"deploy.*\", pod=~\".*\"} |= \"failed\"[5m]) > 5\n  ```  \n  This alert fires if more than 5 deployment failure logs occur in 5 minutes, signaling deployment issues needing immediate attention."
  },
  {
    "query": {
      "message": "deploy*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"deploy\" (e.g., \"deploy started\", \"deploy failed\") and includes any Kubernetes namespace label. It is used in Loki to track deployment-related events across all namespaces, helping SREs monitor deployment activity and detect failures or anomalies.\n\n**Purpose:**  \nTo identify and aggregate deployment events and errors in real time, enabling quick detection of deployment issues that could impact application availability or stability.\n\n**Alert Threshold:**  \nAn alert could be triggered if the count of deployment failure messages (e.g., containing \"deploy failed\" or similar error indicators) exceeds a defined threshold within a short time window (e.g., >5 failures in 5 minutes), signaling potential widespread deployment problems.\n\n**Impact of Values:**  \n- **High values:** A spike in deployment failure logs may indicate systemic issues with deployment pipelines, configuration errors, or infrastructure problems, potentially causing service downtime or degraded performance.  \n- **Low or zero values:** Normal operation, indicating deployments are proceeding without logged errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the number of deployment events per namespace over time, highlighting failure counts to quickly identify problematic namespaces.  \n- **Alert Rule (PromQL example):**  \n  ```  \n  sum by (namespace) (count_over_time({message=~\"deploy.*failed\", namespace=~\".*\"}[5m])) > 5  \n  ```  \n  This triggers an alert if more than 5 deployment failure logs occur in any namespace within 5 minutes."
  },
  {
    "query": {
      "message": "auth*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"auth\" (e.g., authentication attempts or failures) and the log is labeled with any Kubernetes application (`k8s_app`). It is used in Loki to monitor authentication-related events across all Kubernetes apps, helping identify potential login issues or security incidents.\n\n**Purpose:**  \nTo detect and track authentication-related log messages that may indicate failed login attempts, authentication errors, or suspicious access patterns within Kubernetes applications.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 auth failure messages in 5 minutes), signaling a potential authentication problem or attack.\n\n**Impact:**  \n- **High values:** A spike in auth-related logs often indicates widespread authentication failures, which could mean service outages, misconfigurations, or brute-force attacks, requiring immediate investigation.  \n- **Low or zero values:** Normal operation, indicating no recent authentication issues detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of `auth*` messages per `k8s_app` over time to identify trends or spikes in authentication failures.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"auth.*\", k8s_app=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 auth-related log entries occur within 5 minutes for any Kubernetes app."
  },
  {
    "query": {
      "message": "auth*",
      "container": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes container where the message contains the substring \"auth\", typically indicating authentication-related events such as login attempts or failures. In Loki, **{\"message\": \"auth*\", \"container\": \"*\"}** is used to filter and monitor authentication activity across all containers.\n\n**Purpose:**  \nTo detect and track authentication failures or suspicious login behavior that may indicate security issues or service access problems.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 authentication failure messages in 5 minutes), signaling potential brute-force attacks or systemic authentication issues.\n\n**Impact of Values:**  \n- **High values:** A spike in \"auth*\" messages often reflects repeated failed login attempts, possible credential misuse, or authentication service outages, requiring immediate investigation.  \n- **Low or zero values:** Normal operation or no authentication issues detected.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({container=~\".*\"} |= \"auth\" [5m])\n```\nto count authentication-related log entries over the last 5 minutes. An alert can be configured to fire if this count exceeds the threshold, enabling proactive response to authentication failures."
  },
  {
    "query": {
      "message": "auth*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"auth\"—typically indicating authentication-related events such as login attempts or failures—and includes the Kubernetes `node_name` label to identify the source node. In Loki, this filter helps SREs monitor authentication issues across cluster nodes by isolating relevant logs.\n\n**Purpose:**  \nTo detect and analyze authentication-related errors or warnings (e.g., failed logins, token validation errors) on specific Kubernetes nodes, enabling timely identification of security or access problems.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of `auth*` messages exceeds a defined threshold within a short time window (e.g., >10 authentication failures per node within 5 minutes), signaling potential brute force attacks, misconfigurations, or service disruptions.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent authentication failures or suspicious activity on a node, potentially leading to service outages or security breaches. Immediate investigation is required.  \n- **Low or zero values:** Suggest normal authentication behavior with no detected issues.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of `auth*` messages per `node_name` over time, highlighting spikes in authentication failures.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (node_name) (count_over_time({message=~\"auth.*\", node_name=~\".*\"}[5m])) > 10\n  ```  \nThis rule triggers an alert if any node logs more than 10 authentication-related messages in 5 minutes, prompting SREs to investigate node-specific authentication problems."
  },
  {
    "query": {
      "message": "auth*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"auth\"—typically indicating authentication-related events such as login attempts or failures—and includes the Kubernetes 'region' label to identify the source cluster or data center. In Loki, **{\"message\": \"auth*\", \"region\": \"*\"}** is used to monitor authentication activity across regions, helping SREs detect spikes in authentication failures or anomalies.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 50 auth failure messages in 5 minutes), signaling potential widespread authentication issues or attacks.\n\n**Impact:**  \n- **High values:** May indicate a surge in failed login attempts, possible credential stuffing, misconfigurations, or service outages affecting authentication services, potentially leading to user access problems or security incidents.  \n- **Low or zero values:** Suggest normal authentication activity or no recent authentication failures, indicating stable system behavior.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of auth-related log entries per region over time, enabling quick identification of regions experiencing authentication issues.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"auth.*\", region=~\".*\"}[5m]) > 50\n  ```  \nThis rule triggers an alert if more than 50 auth-related log entries occur in any region within 5 minutes, prompting investigation."
  },
  {
    "query": {
      "message": "auth*",
      "service": "*"
    },
    "description": "This log pattern **{\"message\": \"auth*\", \"service\": \"*\"}** is used in Loki to capture all log entries from any Kubernetes service where the message contains the substring \"auth\", typically indicating authentication-related events such as login attempts or failures. Its primary purpose is to monitor authentication issues across services.\n\n**Purpose:**  \n- Detect authentication failures or anomalies that may indicate security issues or service access problems.  \n- Provide visibility into the frequency and distribution of authentication-related errors per service.\n\n**Alert Threshold:**  \n- An alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 authentication failures per 5 minutes per service).  \n- Thresholds can be adjusted based on normal traffic patterns and service criticality.\n\n**Impact of Values:**  \n- **High values:** A spike in \"auth*\" messages often signals potential security incidents (e.g., brute force attacks), misconfigurations, or service outages affecting user authentication. Immediate investigation is recommended.  \n- **Low or zero values:** Normal operation or no authentication issues detected.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: HighAuthFailureRate\nexpr: sum by (service) (count_over_time({message=~\"auth.*\"}[5m])) > 10\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High authentication failure rate detected in {{ $labels.service }}\"\n  description: \"More than 10 authentication-related log entries detected in the last 5 minutes for service {{ $labels.service }}.\"\n```\n\n**Example Dashboard Panel:**  \n- A time series graph showing the count of \"auth*\" log entries per service over time, enabling SREs to quickly identify spikes or trends in authentication failures."
  },
  {
    "query": {
      "message": "auth*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"auth\" and the job field is present with any value. It identifies authentication-related events, such as login attempts or failures, recorded by Kubernetes jobs. The pattern measures the count of these log entries as discrete occurrences. Each matched entry corresponds to one authentication-related log event."
  },
  {
    "query": {
      "message": "auth*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"auth\" and the environment label is present. It identifies authentication-related events, such as login attempts or failures, within any Kubernetes environment. The measurement counts the number of these log entries. Units are expressed as the total count of matching log events."
  },
  {
    "query": {
      "message": "auth*",
      "host": "*"
    },
    "description": "This log query pattern **{\"message\": \"auth*\", \"host\": \"*\"}** in Loki is designed to capture all log entries from any Kubernetes host where the message starts with \"auth\", typically indicating authentication-related events such as login attempts or failures. Its primary purpose is to monitor authentication issues across your cluster by aggregating logs that may signal security problems or service access disruptions.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of these authentication-related log entries exceeds a defined threshold within a given time window (e.g., more than 10 \"auth*\" messages per minute per host). This threshold helps detect abnormal spikes in authentication failures that could indicate brute force attacks, misconfigurations, or service outages.\n\n**Impact of Values:**  \n- **High values:** A sudden increase in \"auth*\" messages often signals widespread authentication failures, potentially leading to service unavailability or security breaches. Immediate investigation is required to identify root causes such as credential issues, expired tokens, or malicious activity.  \n- **Low or zero values:** Normal operation typically shows low or zero authentication failure logs. Consistently low values indicate stable authentication processes and healthy service access.\n\n**Example Usage:**  \nIn a Grafana dashboard, you might create a panel with the query:  \n```\ncount_over_time({message=~\"auth.*\", host=~\".*\"}[5m])\n```\nto visualize the number of authentication-related logs per host over the last 5 minutes.\n\nFor alerting, a Prometheus-style alert rule could be:  \n```\nalert: HighAuthFailureRate\nexpr: count_over_time({message=~\"auth.*\", host=~\".*\"}[5m]) > 10\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"High authentication failure rate detected on {{ $labels.host }}\"\n  description: \"More than 10 authentication failure logs detected in the last 5 minutes on host {{ $labels.host }}. Investigate potential security or configuration issues.\"\n```\nThis setup enables SREs to proactively detect and respond to authentication problems impacting system reliability and security."
  },
  {
    "query": {
      "message": "auth*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"auth\" (e.g., authentication attempts or failures) and includes any Kubernetes log level. It is used in Loki to identify and monitor authentication-related events, such as login successes, failures, or errors. An alert threshold might be set on the count or rate of these log entries—for example, triggering an alert if more than 10 authentication failure messages appear within 5 minutes—indicating potential security issues or service disruptions. High values suggest frequent authentication problems that could impact user access or signal attacks, while low values indicate normal authentication activity. Example alert rule snippet:\n\n```\ncount_over_time({message=~\"auth.*\", level=~\".*\"}[5m]) > 10\n```\n\nThis can be visualized in a dashboard as a time series graph showing the rate of authentication-related logs, helping SREs quickly detect and respond to authentication anomalies."
  },
  {
    "query": {
      "message": "auth*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"auth\" and the environment label (env) is present with any value. It identifies authentication-related events, such as login attempts or failures. The measurement counts the number of these matching log entries. The unit is a simple event count per specified time interval."
  },
  {
    "query": {
      "message": "auth*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"auth\" (e.g., \"auth failed\", \"authentication error\") and the `app` label is any application. It is used in Loki to identify authentication-related events such as login failures or authorization errors across all applications.  \n\n**Purpose:** To monitor authentication issues that may indicate security problems or user access disruptions.  \n\n**Alert Threshold:** An alert should trigger if the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 auth failure logs in 5 minutes), signaling a potential authentication outage or attack.  \n\n**Impact:**  \n- **High values:** Suggest widespread or repeated authentication failures, possibly due to misconfiguration, credential issues, or brute-force attacks, requiring immediate investigation.  \n- **Low or zero values:** Indicate normal authentication behavior with no detected failures.  \n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"auth.*\", app=~\".*\"}[5m])\n```  \nSet an alert to fire if this count exceeds 10 within 5 minutes, helping SREs quickly detect and respond to authentication problems across all applications."
  },
  {
    "query": {
      "message": "auth*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"auth\"—typically indicating authentication-related events such as login attempts or failures—and includes the Kubernetes node label to identify the source node. In Loki, **{\"message\": \"auth*\", \"node\": \"*\"}** helps SREs monitor authentication issues across cluster nodes by aggregating relevant logs.\n\n**Purpose:**  \nTo detect and track authentication failures or anomalies per node, enabling timely identification of potential security issues or service disruptions caused by failed logins.\n\n**Alert Threshold:**  \nAn alert could be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window—for example, more than 10 authentication failure messages per node within 5 minutes—indicating a possible brute force attack or systemic authentication problem.\n\n**Impact of Values:**  \n- **High values:** A spike in \"auth*\" messages suggests increased authentication failures, which may lead to user access issues, security breaches, or degraded service availability. Immediate investigation is warranted.  \n- **Low or zero values:** Normal operation, indicating authentication processes are functioning without notable errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"auth*\" messages per node over time, highlighting nodes with elevated authentication failures.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"auth.*\", node=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if any node logs more than 10 authentication-related messages in 5 minutes.\n\nBy monitoring **{\"message\": \"auth*\", \"node\": \"*\"}**, SREs gain actionable insights into authentication health across nodes, enabling proactive response to security and availability issues."
  },
  {
    "query": {
      "message": "auth*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This log pattern **{\"message\": \"auth*\", \"app_kubernetes_io/name\": \"*\"}** is used in Loki to filter log entries related to authentication events within Kubernetes applications, where the log message contains the substring \"auth\" (e.g., \"auth failed\", \"authentication error\") and the log is tagged with the Kubernetes label `app_kubernetes_io/name` identifying the specific application. Its primary purpose is to monitor authentication-related issues such as login failures or authorization errors per application.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of these authentication-related log entries exceeds a defined threshold within a given time window (e.g., more than 5 auth failure logs per minute per application). This threshold can be adjusted based on normal traffic patterns and security posture.\n\n**Impact of Values:**  \n- **High values:** A spike or sustained high rate of auth-related errors may indicate potential security incidents such as brute-force attacks, misconfigurations causing authentication failures, or service outages affecting user access. Immediate investigation and remediation are recommended.  \n- **Low or zero values:** Indicates normal operation with no recent authentication issues detected, suggesting stable and secure authentication flows.\n\n**Example Usage in Dashboard or Alert Rule:**  \n- **Dashboard:** A panel showing a time series graph of the count of logs matching `{\"message\": \"auth*\", \"app_kubernetes_io/name\": \"*\"}` grouped by `app_kubernetes_io/name` to visualize authentication error trends per application.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (app_kubernetes_io_name) (\n    count_over_time({message=~\"auth.*\", app_kubernetes_io_name=~\".*\"}[1m])\n  ) > 5\n  ```  \n  This rule triggers an alert if any application logs more than 5 authentication-related errors within one minute."
  },
  {
    "query": {
      "message": "auth*",
      "instance": "*"
    },
    "description": "This query pattern **{\"message\": \"auth*\", \"instance\": \"*\"}** in Loki filters log entries where the `message` field starts with the substring \"auth\" and includes any value for the `instance` label, typically representing the Kubernetes pod or node instance emitting the log. It is used to retrieve authentication-related log events such as login attempts, authentication successes, failures, or related security messages. The data source is the centralized Loki log store aggregating logs from Kubernetes clusters or other containerized environments. The returned logs are unstructured text entries with timestamps, enabling analysis of authentication activity over time. Unusual spikes or sustained increases in the volume of these \"auth*\" messages—such as repeated authentication failures—may indicate security incidents like brute-force attacks or misconfigurations and should trigger alerts for immediate investigation."
  },
  {
    "query": {
      "message": "auth*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"auth\" and the filename field is present with any value. It identifies authentication-related events, such as login attempts or failures, recorded in Kubernetes log files. The measurement is the count of such log entries, representing the number of authentication-related messages detected."
  },
  {
    "query": {
      "message": "auth*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"auth\" (e.g., \"auth failed\", \"authentication error\") and the `application` label is set, typically indicating authentication-related events within a specific Kubernetes application. In Loki, this filter helps isolate authentication failures or issues such as login errors, token validation problems, or unauthorized access attempts.\n\n**Purpose:**  \nTo monitor authentication-related errors or warnings across applications, enabling early detection of potential security incidents or service access problems.\n\n**Alert Threshold:**  \nAn alert should be triggered if the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 auth failure logs in 5 minutes), indicating a possible authentication outage or attack.\n\n**Impact of Values:**  \n- **High values:** May indicate widespread authentication failures, potential brute-force attacks, misconfigured authentication services, or degraded user access, requiring immediate investigation.  \n- **Low or zero values:** Suggest normal operation with no recent authentication errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of `{\"message\": \"auth*\", \"application\": \"*\"}` logs per application to identify spikes in authentication errors.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (application) (count_over_time({message=~\"auth.*\", application=~\".*\"}[5m])) > 10\n  ```  \n  This triggers an alert if any application logs more than 10 authentication-related errors in 5 minutes."
  },
  {
    "query": {
      "message": "auth*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"auth\" and the component field is present with any value. It identifies authentication-related events, such as login attempts or failures, recorded by various Kubernetes components. The measurement is the count of these matching log entries over a specified time interval. This helps monitor authentication activity and potential issues within the cluster."
  },
  {
    "query": {
      "message": "auth*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"auth\" (e.g., \"authentication failed\", \"auth connection error\") and the `source` field is any value, typically representing the Kubernetes component or service emitting the log. It is used in Loki to identify authentication-related events, such as login failures or authorization errors, which may indicate security issues or service access problems.\n\n**Purpose:**  \nTo monitor authentication failures or issues across all sources, enabling early detection of potential security breaches or misconfigurations affecting user or service access.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 auth failure logs in 5 minutes). Thresholds should be tuned based on normal baseline activity to reduce false positives.\n\n**Impact of Values:**  \n- **High values:** A sudden spike or sustained high rate of auth-related errors may indicate brute force attacks, credential issues, or service outages impacting user authentication, requiring immediate investigation.  \n- **Low or zero values:** Normal operation, indicating no recent authentication failures detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching `{\"message\": \"auth*\", \"source\": \"*\"}` over time, broken down by source, helps identify which components are experiencing authentication issues.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (source) (count_over_time({message=~\"auth.*\"}[5m])) > 10\n  ```  \n  This triggers an alert if any source emits more than 10 auth-related error logs in 5 minutes."
  },
  {
    "query": {
      "message": "auth*",
      "pod_name": "*"
    },
    "description": "This log pattern **{\"message\": \"auth*\", \"pod_name\": \"*\"}** in Loki captures all log entries from any Kubernetes pod where the message contains the substring \"auth\", typically indicating authentication-related events such as login attempts or failures. Its primary purpose is to monitor authentication issues across your cluster by filtering logs that may signal failed logins, expired tokens, or other auth errors.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of these auth-related log entries exceeds a defined threshold within a given time window (e.g., more than 10 auth failure messages in 5 minutes). This threshold depends on your environment’s normal authentication traffic but should be set low enough to catch unusual spikes that may indicate security incidents or service disruptions.\n\n**Impact of Values:**  \n- **High values:** A sudden increase in auth-related errors often signals potential security threats (e.g., brute force attacks), misconfigurations, or service outages affecting user authentication. Immediate investigation is recommended.  \n- **Low or zero values:** Normal operation or no authentication issues detected. However, a sudden drop to zero in expected auth logs might also indicate logging failures or service downtime.\n\n**Example Usage:**  \nIn a Grafana dashboard, you might create a panel with a Loki query like:  \n```\ncount_over_time({message=~\"auth.*\", pod_name=~\".*\"}[5m])\n```  \nto visualize the number of auth-related log entries per 5-minute interval across pods.\n\nFor alerting, a Prometheus alert rule using Loki’s query results could be:  \n```\nalert: HighAuthFailureRate\nexpr: sum by (pod_name) (count_over_time({message=~\"auth.*\", pod_name=~\".*\"}[5m])) > 10\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High authentication failure rate detected on pod {{ $labels.pod_name }}\"\n  description: \"More than 10 auth-related errors in the last 5 minutes on pod {{ $labels.pod_name }}. Investigate potential security or service issues.\"\n```"
  },
  {
    "query": {
      "message": "auth*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"auth\" and the container_name field is present with any value. It identifies authentication-related events, such as login attempts or failures, recorded by any Kubernetes container. The measurement is a count of matching log entries, representing the number of authentication events detected. Units are raw log entry counts."
  },
  {
    "query": {
      "message": "auth*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"auth\"—typically indicating authentication-related events such as login attempts or failures—and includes the `cluster` label identifying the Kubernetes cluster source. Its primary purpose in Loki is to monitor authentication activity and detect potential security issues like repeated login failures or unauthorized access attempts.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of `auth*` messages exceeds a defined threshold within a short time window (e.g., more than 50 authentication failures within 5 minutes), signaling possible brute-force attacks or systemic authentication problems.\n\n**Impact of Values:**  \n- **High values:** A spike in `auth*` messages often indicates authentication failures or suspicious activity, which can lead to service disruptions or security breaches if not addressed promptly.  \n- **Low or normal values:** Typical authentication traffic with no immediate cause for concern.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of `auth*` messages per cluster over time to identify trends or sudden spikes.  \n- **Alert Rule (PromQL example):**  \n  ```\n  sum by (cluster) (count_over_time({message=~\"auth.*\"}[5m])) > 50\n  ```  \nThis rule triggers an alert if any cluster logs more than 50 authentication-related messages in 5 minutes, enabling rapid response to potential security incidents."
  },
  {
    "query": {
      "message": "auth*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"auth\" (e.g., \"authentication failed\", \"auth connection error\") and the entry is associated with any Kubernetes `pod`. It is used in Loki to identify authentication-related events, such as login failures or authorization errors, across all pods.\n\n**Purpose:**  \nTo monitor authentication issues within the cluster by capturing all relevant logs indicating failed or problematic auth attempts.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 auth failure logs in 5 minutes), signaling a potential security incident or service disruption.\n\n**Impact of Values:**  \n- **High values:** A spike in auth-related errors may indicate brute force attacks, misconfigured authentication services, or widespread login failures affecting user access and system security. Immediate investigation is required.  \n- **Low or zero values:** Normal operation, indicating no recent authentication errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of `auth*` log entries per pod over time to identify trends or sudden spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"auth.*\", pod=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 auth-related log entries occur within 5 minutes across any pod."
  },
  {
    "query": {
      "message": "auth*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"auth\" (e.g., \"auth failure\", \"authentication error\") and includes any Kubernetes namespace label. It is used in Loki to identify authentication-related events such as login failures or authorization errors across all namespaces. Monitoring the frequency of these logs helps detect potential security issues or service disruptions caused by authentication problems.\n\n**Alert threshold:** Trigger an alert if the count of matching log entries exceeds a defined threshold (e.g., more than 10 auth failures within 5 minutes) in any namespace, indicating a possible attack or misconfiguration.\n\n**Impact:**  \n- **High values:** A spike in auth-related errors may signal brute force attacks, credential issues, or service outages affecting user access. Immediate investigation is required to prevent security breaches or downtime.  \n- **Low or zero values:** Normal operation with no authentication errors detected.\n\n**Example usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"auth.*\", namespace=~\".*\"}[5m])\n```\nto track the number of auth-related log entries per namespace over the last 5 minutes. Set an alert to fire if this count exceeds 10, enabling proactive response to authentication failures."
  },
  {
    "query": {
      "message": "token*",
      "k8s_app": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"k8s_app\": \"*\"}** is used in Loki to filter log entries from Kubernetes applications where the message contains the substring \"token\". It helps identify issues related to API authentication tokens, OAuth flows, or JWT validation failures within any Kubernetes app labeled by `k8s_app`.  \n\n**Purpose:**  \n- Detect authentication or authorization problems indicated by token-related errors or warnings in application logs.  \n- Provide early visibility into token issuance, validation, or connectivity failures that could impact service access or security.  \n\n**Alert Threshold Guidance:**  \n- Trigger an alert if the count of log entries matching this pattern exceeds a defined threshold (e.g., > 5 errors within 5 minutes) to indicate a potential authentication outage or security incident.  \n- Adjust thresholds based on baseline token error rates for your environment to reduce noise.  \n\n**Impact of Values:**  \n- **High values:** Suggest widespread or persistent token-related failures, potentially causing service disruptions, failed API calls, or security vulnerabilities. Immediate investigation is warranted.  \n- **Low or zero values:** Indicate normal operation with no detected token issues.  \n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: TokenErrorsHigh\nexpr: sum by (k8s_app) (count_over_time({message=~\"token.*\", k8s_app=~\".*\"}[5m])) > 5\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High token error rate in {{ $labels.k8s_app }}\"\n  description: \"More than 5 token-related errors detected in the last 5 minutes for app {{ $labels.k8s_app }}. Investigate authentication or authorization issues.\"\n```\n\nThis alert can be visualized in dashboards by plotting the rate of token-related log entries per Kubernetes app over time, enabling SREs to monitor authentication health and respond proactively."
  },
  {
    "query": {
      "message": "token*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the container field is present with any value. It identifies occurrences related to token-related issues, such as API, OAuth, or JWT token errors, within Kubernetes containers. The measurement is a count of these matching log entries. Each unit represents one log entry containing a token-related message from a specific container."
  },
  {
    "query": {
      "message": "token*",
      "node_name": "*"
    },
    "description": "This pattern filters logs where the `message` field contains the substring \"token\" and the log entry includes a Kubernetes `node_name` label. It is used to detect issues related to API authentication tokens, OAuth flows, or JWT validation failures occurring on specific nodes.  \n\n**Purpose:**  \nTo identify and monitor authentication or authorization problems tied to token handling that may impact service availability or security on individual Kubernetes nodes.\n\n**Alert Threshold:**  \nTrigger an alert if the count of logs matching this pattern exceeds a defined threshold (e.g., >10 errors per 5 minutes per node), indicating a potential widespread or persistent token-related failure.\n\n**Impact:**  \n- **High values:** Suggest frequent token errors that could lead to service disruptions, failed API requests, or security vulnerabilities on affected nodes. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate normal operation with no detected token-related issues.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\n{message=~\"token.*\", node_name=~\".*\"}\n| count_over_time({message=~\"token.*\", node_name=~\".*\"}[5m])\n```\nSet an alert to fire when the count exceeds 10 within 5 minutes for any `node_name`, enabling targeted troubleshooting of token errors per node."
  },
  {
    "query": {
      "message": "token*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"token\" and includes a Kubernetes 'region' label, helping identify issues related to API authentication tokens such as OAuth or JWT failures. It is used in Loki to monitor authentication-related errors that may impact service access or security.\n\n**Purpose:** Detect and aggregate token-related errors across regions to quickly identify authentication problems affecting API access.\n\n**Alert Threshold:** Trigger an alert if the count of logs matching **{\"message\": \"token*\", \"region\": \"*\"}** exceeds a defined threshold (e.g., 10 errors) within a 5-minute window per region, indicating a potential widespread authentication failure.\n\n**Impact:**  \n- **High values:** Suggest a significant or ongoing authentication issue that could lead to service outages or security risks in the affected region. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate normal operation with no detected token-related authentication errors.\n\n**Example Usage:**  \nIn a Grafana dashboard, display a panel with a Loki query like:  \n```\ncount_over_time({message=~\"token.*\", region=~\".*\"}[5m]) by (region)\n```\nto visualize token error rates per region.  \n\nFor alerting, define a rule:  \n```\nWHEN count_over_time({message=~\"token.*\", region=~\".*\"}[5m]) BY region > 10\nFOR 5m\nTHEN alert \"High token error rate in {{ $labels.region }}\"\n```\nThis setup enables proactive detection and response to authentication token issues by region."
  },
  {
    "query": {
      "message": "token*",
      "service": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"service\": \"*\"}** in Loki is designed to capture log entries where the message contains the substring \"token\"—typically indicating issues related to API authentication tokens, OAuth flows, or JWT validation—and is associated with a specific Kubernetes service. Its primary purpose is to help SREs detect and investigate authentication or authorization failures that could disrupt service access or user sessions.\n\n**Alert Threshold:**  \nTrigger an alert if the count of such log entries exceeds a defined threshold within a short time window (e.g., more than 5 token-related errors per 5 minutes per service). This threshold should be tuned based on normal baseline error rates for each service.\n\n**Impact of Values:**  \n- **High values:** A spike in token-related errors often signals authentication failures, expired or malformed tokens, or issues with identity providers, potentially causing service outages or degraded user experience. Immediate investigation is warranted.  \n- **Low or zero values:** Indicates normal operation with no detected token-related authentication issues.\n\n**Example Usage in Dashboard or Alert Rule:**  \n- **Dashboard:** Display a time series panel showing the count of logs matching **{\"message\": \"token*\", \"service\": \"*\"}** grouped by `service` over the last hour to identify which services are experiencing token errors.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum by (service) (count_over_time({message=~\"token.*\", service=~\".*\"}[5m])) > 5\n  ```  \n  This fires if any service logs more than 5 token-related errors in 5 minutes, indicating a potential authentication issue requiring SRE attention."
  },
  {
    "query": {
      "message": "token*",
      "job": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"job\": \"*\"}** in Loki filters for log entries where the message contains the substring \"token\"—typically indicating issues related to API authentication tokens, OAuth flows, or JWT validation—and associates them with a specific Kubernetes job label. Its purpose is to help SREs quickly identify and monitor authentication or authorization failures that could disrupt service access or security.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of such token-related error logs exceeds a defined threshold within a given time window (e.g., more than 5 token error messages in 5 minutes), signaling a potential authentication system degradation or attack.\n\n**Impact of Values:**  \n- **High values:** A spike in token-related errors often indicates widespread authentication failures, which can lead to service outages, degraded user experience, or security vulnerabilities. Immediate investigation is required.  \n- **Low or zero values:** Normal operation, indicating that token handling is functioning correctly.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series panel showing the count of logs matching **{\"message\": \"token*\", \"job\": \"*\"}** over the last hour, segmented by job, to visualize trends and identify problematic services.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({job=~\".*\", message=~\"token.*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 token-related error logs occur within 5 minutes for any job."
  },
  {
    "query": {
      "message": "token*",
      "environment": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"environment\": \"*\"}** is used in Loki to identify log entries where the message contains the keyword \"token\"—typically indicating issues related to API tokens, OAuth authentication, or JWT token validation failures—across all Kubernetes environments. This helps SREs quickly detect authentication or authorization problems that may impact service availability or security.\n\n**Purpose:**  \nTo monitor and surface token-related errors or warnings that could cause failed API requests, user authentication issues, or service disruptions.\n\n**Alert Threshold:**  \nTrigger an alert if the count of log entries matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 token-related errors in 5 minutes). This threshold should be tuned based on normal traffic and error rates for your environment.\n\n**Impact of Values:**  \n- **High values:** Indicate a surge in token-related failures, potentially signaling authentication service outages, expired or malformed tokens, or security incidents. This can lead to degraded user experience or service downtime.  \n- **Low or zero values:** Suggest normal operation with no token-related errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching **{\"message\": \"token*\", \"environment\": \"$env\"}** over time, segmented by environment, to quickly identify spikes in token errors.  \n- **Alert Rule:**  \n  ```\n  sum by (environment) (count_over_time({message=~\"token.*\", environment=~\".*\"}[5m])) > 10\n  ```  \n  This rule fires if more than 10 token-related log entries occur in any environment within 5 minutes, prompting investigation into authentication issues."
  },
  {
    "query": {
      "message": "token*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the host field is present with any value. It identifies events related to API, OAuth, or JWT token issues occurring on Kubernetes hosts. The measurement is a count of such log entries, representing the frequency of token-related errors per host. Units are raw log entry counts."
  },
  {
    "query": {
      "message": "token*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"token\" and includes any Kubernetes log level (e.g., error, warning, info). It is used in Loki to identify issues related to API authentication tokens, OAuth flows, or JWT validation failures that may impact service availability or security.  \n\n**Purpose:** To detect and monitor authentication token-related errors or warnings that could indicate failed API requests, expired tokens, or misconfigurations affecting user access or service communication.\n\n**Alert Threshold:** An alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"token\"-related errors in 5 minutes), signaling a potential authentication outage or security incident.\n\n**Impact:**  \n- **High values:** A spike in these logs often correlates with authentication failures, leading to service disruptions, degraded user experience, or security vulnerabilities. Immediate investigation is required.  \n- **Low or zero values:** Indicates normal operation with no token-related errors detected.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\n{message=~\"token.*\", level=~\"error|warning\"}\n| count_over_time({message=~\"token.*\", level=~\"error|warning\"}[5m])\n```\nSet an alert to fire if the count exceeds 5 within 5 minutes, enabling proactive response to authentication issues."
  },
  {
    "query": {
      "message": "token*",
      "env": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"env\": \"*\"}** in Loki is designed to capture log entries where the message contains the substring \"token\"—typically indicating issues related to API tokens, OAuth authentication, or JWT token validation failures—across all Kubernetes environments labeled by `env`.  \n\n**Purpose:**  \nIt helps SREs monitor authentication and authorization problems that may cause service disruptions, such as failed token refreshes, invalid tokens, or token expiration errors.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 token-related errors in 5 minutes), signaling a potential authentication outage or security issue.\n\n**Impact of Values:**  \n- **High values:** Indicate frequent token-related failures, which can lead to service unavailability, degraded user experience, or security vulnerabilities. Immediate investigation and remediation are required.  \n- **Low or zero values:** Suggest normal operation with no token-related errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of logs matching **{\"message\": \"token*\", \"env\": \"$env\"}** grouped by environment to quickly identify which environment is experiencing token issues.  \n- **Alert Rule (PromQL example):**  \n  ```  \n  sum by (env) (count_over_time({message=~\"token.*\", env=~\".*\"}[5m])) > 5  \n  ```  \n  This triggers an alert if more than 5 token-related errors occur in any environment within 5 minutes."
  },
  {
    "query": {
      "message": "token*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the app label is present. It identifies occurrences related to token issues, such as API, OAuth, or JWT token errors. The measurement counts the number of such log entries per application (app label). The unit is the count of matching log entries."
  },
  {
    "query": {
      "message": "token*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the node field is present with any value. It identifies occurrences of token-related issues, such as API, OAuth, or JWT token errors, reported on specific Kubernetes nodes. The measurement is a count of these log entries per node. This helps monitor authentication or authorization problems tied to individual nodes."
  },
  {
    "query": {
      "message": "token*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"token\" and the entry includes any value for the Kubernetes label \"app_kubernetes_io/name.\" It measures the count of such log occurrences, indicating potential issues related to API, OAuth, or JWT tokens within specific Kubernetes applications. The unit of measurement is the number of matching log entries. For example, a log with message=\"token connection failed\" and app_kubernetes_io/name=\"example-app\" would be included."
  },
  {
    "query": {
      "message": "token*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the instance field is present. It identifies occurrences of token-related issues, such as failures or errors involving API, OAuth, or JWT tokens. Each matched log entry corresponds to one event, counted as a single occurrence. The unit of measurement is the number of such log events per instance."
  },
  {
    "query": {
      "message": "token*",
      "filename": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"filename\": \"*\"}** in Loki filters log entries where the message field starts with \"token\" (e.g., \"token expired\", \"token validation failed\") from any Kubernetes log file. It is used to detect issues related to API authentication tokens, OAuth flows, or JWT handling that may cause service disruptions or security risks.\n\n**Purpose:**  \nTo identify and monitor authentication token-related errors or warnings that could impact service availability or security.\n\n**Alert Threshold:**  \nTrigger an alert if the count of matching log entries exceeds a defined threshold within a short time window (e.g., > 5 errors in 5 minutes), indicating a potential authentication failure spike.\n\n**Impact:**  \n- **High values:** Suggest widespread or persistent token-related failures, potentially causing user authentication issues, service outages, or security vulnerabilities. Immediate investigation is required.  \n- **Low or zero values:** Indicate normal operation with no recent token-related errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching **{\"message\": \"token*\", \"filename\": \"*\"}** over time, highlighting spikes in token errors.  \n- **Alert Rule (PromQL):**  \n  ```\n  count_over_time({message=~\"token.*\", filename=~\".*\"}[5m]) > 5\n  ```  \nThis triggers an alert if more than 5 token-related log entries occur within 5 minutes, prompting SREs to investigate authentication issues promptly."
  },
  {
    "query": {
      "message": "token*",
      "application": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"application\": \"*\"}** in Loki filters log entries where the message field contains the substring \"token\"—typically indicating issues related to API tokens, OAuth authentication, or JWT validation—and associates these entries with a specific Kubernetes application. This helps SREs quickly identify authentication or authorization failures impacting application access.\n\n**Purpose:**  \nTo detect and monitor token-related errors or warnings that may cause authentication failures, degraded service availability, or security risks within applications.\n\n**Alert Threshold:**  \nTrigger an alert if the count of log entries matching this pattern exceeds a defined threshold (e.g., > 5 errors within 5 minutes) for a given application, indicating a potential authentication outage or attack.\n\n**Impact of Values:**  \n- **High values:** Suggest frequent token-related failures, potentially causing user login issues, API access denials, or security breaches. Immediate investigation is required.  \n- **Low or zero values:** Indicate normal operation with no token-related errors detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of token-related errors per application over time, enabling trend analysis and rapid detection of spikes.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (application) (count_over_time({message=~\"token.*\", application=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if more than 5 token-related log entries occur within 5 minutes for any application."
  },
  {
    "query": {
      "message": "token*",
      "component": "*"
    },
    "description": "This log pattern matches entries where the message begins with \"token\" and includes any Kubernetes component label, helping identify issues related to API authentication tokens such as OAuth or JWT failures. In Loki, it is used to monitor authentication-related errors that may cause service disruptions or degraded security. An alert should be triggered when the count of these log entries exceeds a threshold (e.g., more than 5 occurrences within 5 minutes), indicating a potential widespread token validation failure or attack. High values suggest persistent authentication problems impacting service availability or user access, while low or zero values indicate normal operation. For example, an alert rule in Loki could use a query like `{component=~\".*\", message=~\"token.*\"}` to count matching logs over time, triggering a PagerDuty notification if the count surpasses the threshold. Similarly, a dashboard panel can visualize the frequency of token-related errors per component to quickly identify affected services."
  },
  {
    "query": {
      "message": "token*",
      "source": "*"
    },
    "description": "This pattern matches log entries whose message field starts with the word \"token\" and that include any value in the source field. It identifies occurrences of token-related issues, such as problems with API, OAuth, or JWT tokens. Each matched log entry represents one instance of a token-related event. The unit measured is the count of these log entries."
  },
  {
    "query": {
      "message": "token*",
      "pod_name": "*"
    },
    "description": "This log pattern **{\"message\": \"token*\", \"pod_name\": \"*\"}** is used in Loki to filter and identify log entries where the message contains the keyword \"token\"—typically indicating issues related to API authentication tokens, OAuth flows, or JWT validation failures within specific Kubernetes pods. Its purpose is to surface authentication or authorization errors that may impact service availability or security.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of these token-related error logs exceeds a defined threshold within a given time window (e.g., more than 5 occurrences in 5 minutes), signaling a potential widespread authentication failure or token service degradation.\n\n**Impact of Values:**  \n- **High values:** A spike in token-related errors suggests authentication problems affecting multiple requests or pods, potentially causing service outages or user access issues. Immediate investigation is required.  \n- **Low or zero values:** Indicates normal operation with no detected token-related authentication errors.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching **{\"message\": \"token*\", \"pod_name\": \"*\"}** over time, grouped by pod_name, helps identify which pods are experiencing token issues.  \n- **Alert Rule:**  \n  ```\n  sum by (pod_name) (count_over_time({message=~\"token.*\", pod_name=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any pod logs more than 5 token-related errors in 5 minutes, enabling rapid response to authentication failures."
  },
  {
    "query": {
      "message": "token*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the container_name field is present. It identifies occurrences related to issues with API, OAuth, or JWT tokens within Kubernetes containers. The measurement is a count of such log entries, grouped by container_name. This helps monitor token-related errors per container."
  },
  {
    "query": {
      "message": "token*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the cluster field is present. It identifies events related to issues involving API, OAuth, or JWT tokens within a specific Kubernetes cluster. The pattern measures the count of such log entries per cluster. Units are expressed as the number of matching log events."
  },
  {
    "query": {
      "message": "token*",
      "pod": "*"
    },
    "description": "This log query pattern **{\"message\": \"token*\", \"pod\": \"*\"}** in Loki filters for log entries where the message contains the substring \"token\" and the entry is associated with any Kubernetes pod. It is primarily used to detect issues related to API authentication tokens, OAuth flows, or JWT validation failures that may cause service disruptions or security risks.\n\n**Purpose:**  \nTo identify and monitor authentication token-related errors or warnings across pods, enabling early detection of authorization failures or token expiration problems that could impact service availability or user access.\n\n**Alert Threshold:**  \nTrigger an alert if the count of log entries matching this pattern exceeds a defined threshold (e.g., > 10 errors within 5 minutes per pod), indicating a potential widespread or persistent token-related failure.\n\n**Impact:**  \n- **High values:** Suggest frequent token errors, which may lead to failed API requests, degraded user experience, or security vulnerabilities. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate normal operation with no token-related issues detected.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"token.*\", pod=~\".*\"}[5m])\n```\nSet an alert to fire if this count exceeds 10 within 5 minutes for any pod, signaling abnormal token error rates that need attention."
  },
  {
    "query": {
      "message": "token*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"token\" and the namespace field is present with any value. It identifies events related to API, OAuth, or JWT token issues within Kubernetes namespaces. The pattern measures the count of such log entries, grouped by their namespace labels. The unit of measurement is the number of matching log entries."
  },
  {
    "query": {
      "message": "stop*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes application (`k8s_app=\"*\"`) where the `message` field starts with \"stop\" (e.g., \"stop connection failed\"). It is used in Loki to identify potentially critical operational events such as service shutdowns, connection terminations, or error conditions that begin with \"stop\".  \n\n**Purpose:**  \nTo monitor and alert on occurrences of \"stop\"-related events that may indicate service interruptions or failures within Kubernetes applications.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"stop*\" messages in 5 minutes), signaling a potential issue requiring investigation.\n\n**Impact:**  \n- **High values:** A surge in \"stop*\" messages often indicates systemic problems such as frequent service stops, failed connections, or cascading failures, potentially impacting application availability and user experience.  \n- **Low or zero values:** Normal operation, indicating no recent stop-related errors or shutdowns.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of `{\"message\": \"stop*\", \"k8s_app\": \"*\"}` logs over time per application to quickly identify spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({k8s_app=~\".*\", message=~\"stop.*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 \"stop*\" messages occur within 5 minutes for any Kubernetes app."
  },
  {
    "query": {
      "message": "stop*",
      "container": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes container where the message begins with \"stop\", typically indicating that a process, connection, or service has been stopped or terminated. In Loki, **{\"message\": \"stop*\", \"container\": \"*\"}** helps SREs identify potentially significant operational events such as service shutdowns, connection drops, or intentional stops that may affect system availability or performance.\n\n**Alert Threshold:**  \nAn alert could be triggered if the count of \"stop*\" messages exceeds a defined threshold within a short time window (e.g., more than 5 stop events per minute), signaling abnormal or frequent stoppages that may indicate instability or failures.\n\n**Impact of Values:**  \n- **High values:** A surge in \"stop*\" messages may reflect repeated service interruptions, failed connection teardowns, or cascading shutdowns, potentially leading to degraded service or outages. Immediate investigation is warranted.  \n- **Low or zero values:** Normal operation, indicating stable services without unexpected stops.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching **{\"message\": \"stop*\", \"container\": \"*\"}** over time, helping visualize spikes in stop events.  \n- **Alert Rule:**  \n```yaml\nalert: FrequentStopEvents  \nexpr: sum(rate({message=~\"stop.*\", container=~\".*\"}[1m])) > 5  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"High number of stop events detected\"  \n  description: \"More than 5 stop-related log messages per minute in Kubernetes containers, indicating potential service instability.\"  \n```"
  },
  {
    "query": {
      "message": "stop*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"stop\" and includes any following characters. It specifically filters logs that contain a Kubernetes node identifier under the field \"node_name.\" The pattern measures the occurrence count of such log messages within the system. Each matched log entry corresponds to one instance of a \"stop\"-related event on the specified node."
  },
  {
    "query": {
      "message": "stop*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"stop\" and includes any Kubernetes region label, capturing events related to stopping operations or services. In Loki, it helps identify occurrences of stop-related issues across different regions. An alert should be triggered when the count of such log entries exceeds a defined threshold within a set time window (e.g., more than 5 \"stop*\" messages in 5 minutes), indicating potential operational disruptions. A high rate suggests frequent or widespread stop events that may impact service availability, while a low rate indicates normal or infrequent stop occurrences. For example, an alert rule could use a query like `{message=~\"stop.*\", region=~\".*\"}` to count matching logs per region, triggering an alert if the count surpasses the threshold. This pattern can also be visualized in dashboards to monitor stop event trends by region, enabling proactive incident response."
  },
  {
    "query": {
      "message": "stop*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and the service field is present with any value. It identifies log messages related to stopping operations within any Kubernetes service. The measurement is a count of such log entries, representing the number of stop-related events per service. Units are expressed as the total number of matching log entries."
  },
  {
    "query": {
      "message": "stop*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"stop\" (e.g., \"stop connection failed\") and includes any Kubernetes `job` label. It is used in Loki to identify potentially critical operational events indicating that a process or connection has stopped or failed within a specific job.  \n\n**Purpose:**  \nTo monitor and alert on occurrences of stop-related events that may signal service interruptions or failures in Kubernetes jobs.\n\n**Alert Threshold:**  \nTrigger an alert if the count of logs matching **{\"message\": \"stop*\", \"job\": \"*\"}** exceeds a defined threshold (e.g., more than 5 occurrences within 5 minutes), indicating a recurring or systemic issue.\n\n**Impact:**  \n- **High values:** Suggest frequent or widespread stoppages, potentially leading to degraded service availability or failures requiring immediate investigation.  \n- **Low or zero values:** Indicate normal operation with no recent stop events detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of \"stop*\" messages per job over time to identify trends or spikes.  \n- **Alert Rule:**  \n```yaml\nalert: StopMessageHighFrequency  \nexpr: count_over_time({message=~\"stop.*\", job=~\".*\"}[5m]) > 5  \nfor: 5m  \nlabels:  \n  severity: critical  \nannotations:  \n  summary: \"High frequency of stop messages in job {{ $labels.job }}\"  \n  description: \"More than 5 stop-related log entries detected in the last 5 minutes for job {{ $labels.job }}, indicating potential service issues.\"\n```"
  },
  {
    "query": {
      "message": "stop*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and the environment field is present with any value. It identifies occurrences of operational events or errors indicated by messages starting with \"stop.\" The measurement is a count of such log entries, grouped by the Kubernetes environment label. For example, a log with message=\"stop connection failed\" and environment=\"example-environment\" would be included."
  },
  {
    "query": {
      "message": "stop*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and the host field is present. It identifies events related to stopping operations or processes on any Kubernetes host. The measurement is a count of such log entries, representing the number of stop-related messages per host. Units are expressed as the total number of matching log events."
  },
  {
    "query": {
      "message": "stop*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" followed by any characters. It captures log events indicating termination or interruption actions within the system. The \"level\" field represents the severity or importance of the log, as defined by Kubernetes logging conventions. This pattern does not measure a numeric value but categorizes logs based on message content and severity level."
  },
  {
    "query": {
      "message": "stop*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"stop\" (e.g., \"stop connection failed\") across all Kubernetes environments (`env`). It is used in Loki to identify potentially critical operational events such as service interruptions or failed shutdowns.  \n\n**Purpose:**  \nTo monitor and alert on occurrences of \"stop\"-related events that may indicate service disruptions or failures requiring immediate attention.\n\n**Alert Threshold:**  \nAn alert should be triggered if the count of logs matching **{\"message\": \"stop*\", \"env\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 occurrences in 5 minutes), signaling abnormal or repeated stop events.\n\n**Impact:**  \n- **High values:** Suggest frequent or widespread service interruptions or failures, potentially impacting system availability or stability. Immediate investigation is warranted.  \n- **Low or zero values:** Indicate normal operation with no recent stop-related issues detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"stop*\" messages per environment over time, helping SREs quickly identify spikes or trends.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"stop.*\", env=~\".*\"}[5m]) > 5\n  ```  \nThis rule triggers an alert if more than 5 \"stop\" messages occur in any environment within 5 minutes."
  },
  {
    "query": {
      "message": "stop*",
      "app": "*"
    },
    "description": "This query pattern **{\"message\": \"stop*\", \"app\": \"*\"}** in Loki filters log entries where the `message` field begins with the string \"stop\" (case-sensitive prefix match) and the `app` label is present with any value. It returns all logs from any application (`app` label) that start with \"stop\", typically indicating events related to stopping processes, services, or connections within Kubernetes-managed applications. The data source is structured log streams ingested into Loki, where each log entry includes labels such as `app` for application identification and a `message` field containing the log text. Monitoring these logs is critical because entries starting with \"stop\" often signify shutdowns, failures, or interruptions. An unusual increase in the frequency or occurrence of such logs—especially those indicating errors or unexpected stops—should trigger alerts to prompt investigation into potential service disruptions or operational issues."
  },
  {
    "query": {
      "message": "stop*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and includes any subsequent characters. It specifically captures logs labeled with any Kubernetes node identifier. The measurement is a count of such log entries, representing occurrences of stop-related events per node. This helps track operational issues or shutdown events associated with each Kubernetes node."
  },
  {
    "query": {
      "message": "stop*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and includes any additional text. It specifically filters logs that have the Kubernetes label \"app_kubernetes_io/name\" set to any value. The pattern measures the count of such log entries, representing occurrences of stop-related events within Kubernetes applications. The unit of measurement is the number of matching log entries."
  },
  {
    "query": {
      "message": "stop*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"stop\" and the instance field is present with any value. It identifies occurrences of operational events or errors beginning with \"stop\" in Kubernetes instances. The measurement is a count of such log entries, aggregated per instance. Units are number of log entries."
  },
  {
    "query": {
      "message": "stop*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" followed by any characters. It specifically captures logs labeled with any Kubernetes filename. The measurement is the count of such log entries, representing occurrences of stop-related events. Example: message=\"stop connection failed\", filename=\"example-filename\"."
  },
  {
    "query": {
      "message": "stop*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and the application field is present with any value. It identifies events related to stopping operations within any Kubernetes application. The measurement is a count of such log entries, representing the number of stop-related messages recorded. Units are raw log entry counts."
  },
  {
    "query": {
      "message": "stop*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"stop\" (e.g., \"stop connection failed\") and includes any Kubernetes `component` label. It is used in Loki to identify potentially critical operational events indicating that a process, connection, or service has stopped or encountered a failure.  \n\n**Purpose:**  \nTo monitor and alert on occurrences of \"stop\"-related messages that may signal service interruptions or failures within specific Kubernetes components.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"stop*\" messages per 5 minutes), indicating a potential systemic issue or repeated failures.\n\n**Impact:**  \n- **High values:** A spike in \"stop*\" messages suggests frequent or ongoing service disruptions, which could lead to degraded system availability or performance. Immediate investigation is warranted.  \n- **Low or zero values:** Normal operation, indicating no recent stop-related errors or interruptions.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching `{message=~\"stop.*\", component=~\".*\"}` grouped by `component` over time, highlighting components with increasing stop events.  \n- **Alert Rule:**  \n  ```\n  sum by (component) (count_over_time({message=~\"stop.*\", component=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any component logs more than 5 \"stop*\" messages in 5 minutes, signaling potential operational issues requiring SRE attention."
  },
  {
    "query": {
      "message": "stop*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"stop\" (e.g., \"stop connection failed\") and the `source` label is present, typically indicating Kubernetes component or application sources. It is used in Loki to identify potentially critical operational events related to stopping processes, connections, or services.\n\n**Purpose:**  \nTo monitor and alert on occurrences of \"stop\"-related events that may indicate failures, interruptions, or shutdowns within the system.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching **{\"message\": \"stop*\", \"source\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 occurrences in 5 minutes), signaling abnormal or frequent stop events that could impact system stability.\n\n**Impact of Values:**  \n- **High values:** Frequent \"stop\" messages may indicate recurring failures, service interruptions, or cascading issues requiring immediate investigation.  \n- **Low or zero values:** Normal operation, indicating no recent stop-related errors or interruptions.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"stop*\" messages over time, grouped by `source`, helps identify which components are experiencing stop events.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"stop.*\", source=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 stop-related log entries occur within 5 minutes, prompting SREs to investigate potential service disruptions."
  },
  {
    "query": {
      "message": "stop*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and includes any following text. It specifically filters logs that have a Kubernetes pod label named \"pod_name.\" The pattern measures the occurrence count of such log entries within the system. For example, it captures logs like message=\"stop connection failed\" from pod_name=\"example-pod_name.\""
  },
  {
    "query": {
      "message": "stop*",
      "container_name": "*"
    },
    "description": "This log query pattern **{\"message\": \"stop*\", \"container_name\": \"*\"}** in Loki is designed to capture all log entries where the message starts with or contains the word \"stop\" from any Kubernetes container. This typically indicates events related to stopping processes, connections, or services, which can be critical for identifying operational issues such as unexpected shutdowns, failed terminations, or service interruptions.\n\n**Purpose:**  \nTo monitor and detect occurrences of stop-related events across all containers, enabling early identification of potential failures or disruptions in the system.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"stop*\" messages in 5 minutes). This threshold should be tuned based on normal operational baselines to reduce noise.\n\n**Impact of Values:**  \n- **High values:** A spike in \"stop*\" messages may indicate widespread service interruptions, failed shutdowns, or cascading failures, requiring immediate investigation.  \n- **Low or zero values:** Normal operation, indicating no recent stop-related issues detected.\n\n**Example Usage in Alert Rule:**  \n```yaml\nalert: HighStopMessages\nexpr: count_over_time({container_name=~\".*\", message=~\"stop.*\"}[5m]) > 5\nfor: 2m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High number of stop-related log messages detected\"\n  description: \"More than 5 stop-related messages have been logged in the last 5 minutes across containers, indicating potential operational issues.\"\n```\n\n**Example Dashboard Panel:**  \nA time-series graph showing the count of \"stop*\" messages per container over time, helping SREs quickly identify which containers are experiencing stop-related events and correlate with other metrics or incidents."
  },
  {
    "query": {
      "message": "stop*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"stop\" and includes any subsequent characters. It specifically captures logs labeled with any Kubernetes cluster identifier. The pattern measures the count of such log entries, representing notable operational events related to stopping processes or connections. The unit of measurement is the number of matching log entries."
  },
  {
    "query": {
      "message": "stop*",
      "pod": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod where the message begins with \"stop\", typically indicating that a process, connection, or service has been stopped or terminated. In Loki, this helps identify potentially disruptive events such as service shutdowns, connection drops, or aborted operations that may affect system stability or availability.\n\n**Purpose:**  \nTo monitor and alert on occurrences of \"stop\"-related events across pods, enabling early detection of unexpected or frequent terminations that could signal underlying issues.\n\n**Alert Threshold:**  \nAn alert should be triggered if the count of logs matching **{\"message\": \"stop*\", \"pod\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 stop-related messages per pod within 5 minutes). This threshold can be adjusted based on normal operational baselines.\n\n**Impact of Values:**  \n- **High values:** A surge in \"stop\" messages may indicate frequent service interruptions, connection failures, or pod restarts, potentially leading to degraded application performance or downtime. Immediate investigation is warranted.  \n- **Low or zero values:** Normal operation, indicating stable services without unexpected stops.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of \"stop*\" messages per pod over time, highlighting pods with increasing stop events.  \n- **Alert Rule (PromQL for Loki):**  \n  ```\n  count_over_time({message=~\"stop.*\", pod=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 stop-related logs occur in any pod within 5 minutes, signaling potential issues requiring SRE attention."
  },
  {
    "query": {
      "message": "stop*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"stop\" (e.g., \"stop connection failed\") and includes any Kubernetes `namespace`. It is used in Loki to identify potentially critical operational events indicating that a process, connection, or service has stopped or encountered a stop-related error.  \n\n**Purpose:**  \nTo monitor and alert on occurrences of stop-related events that may signal failures or interruptions in services within specific Kubernetes namespaces.\n\n**Alert Threshold:**  \nTrigger an alert if the count of logs matching **{\"message\": \"stop*\", \"namespace\": \"*\"}** exceeds a defined threshold (e.g., more than 5 occurrences within 5 minutes) in any namespace, indicating a potential service disruption.\n\n**Impact:**  \n- **High values:** A spike in stop-related messages suggests increased failures or service interruptions, requiring immediate investigation to prevent downtime or degraded performance.  \n- **Low or zero values:** Normal operation, indicating no recent stop events detected.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"stop.*\", namespace=~\".*\"}[5m])\n```\nSet an alert to fire if this count exceeds 5 in any namespace, helping SREs quickly identify and respond to critical stop events affecting cluster services."
  },
  {
    "query": {
      "message": "conn*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the substring \"conn\" and the \"k8s_app\" label is present with any value. It identifies log messages related to connection events within Kubernetes applications. The measurement counts the number of such log entries. The unit is the total count of matching log messages."
  },
  {
    "query": {
      "message": "conn*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"conn\" and the entry includes any Kubernetes container label. It identifies log messages related to connection events within containers. The measurement is a count of such log entries, representing the number of connection-related messages per container. Units are simple log entry counts without additional scaling."
  },
  {
    "query": {
      "message": "conn*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"conn\" and the node_name field is present. It identifies log messages related to connection events occurring on specific Kubernetes nodes. The measurement is a count of such log entries per node_name, representing the frequency of connection-related messages. Units are raw log entry counts indexed by node_name."
  },
  {
    "query": {
      "message": "conn*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"conn\" and the region field is present with any value. It measures the count of such log entries, indicating occurrences of connection-related events within different Kubernetes regions. The unit of measurement is the number of log entries. For example, a matching log might have message=\"conn connection failed\" and region=\"example-region\"."
  },
  {
    "query": {
      "message": "conn*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"conn\" (e.g., \"connection failed\", \"conn timeout\"), filtered by any Kubernetes `service` label. It is used in Loki to identify and monitor connection-related events or errors across services, which are often critical indicators of network or service health issues.\n\n**Purpose:**  \nTo detect and track connection-related log messages that may signify connectivity problems, failures, or degraded service performance within Kubernetes services.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 \"conn*\" messages per 5 minutes), indicating a potential connectivity issue requiring investigation.\n\n**Impact:**  \n- **High values:** A surge in \"conn*\" messages typically signals widespread or persistent connection problems, potentially causing service outages or degraded user experience. Immediate attention is needed to identify root causes such as network failures, misconfigurations, or resource exhaustion.  \n- **Low or zero values:** Normal operation, indicating stable connectivity without notable connection errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of logs matching `{\"message\": \"conn*\", \"service\": \"*\"}` per service, highlighting spikes or trends in connection-related errors.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"conn.*\", service=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 connection-related log entries occur within 5 minutes for any service, prompting SREs to investigate connectivity issues promptly."
  },
  {
    "query": {
      "message": "conn*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"conn\" (e.g., \"connection failed\", \"conn timeout\"), and the log is associated with any Kubernetes `job`. It is used in Loki to identify connection-related events that may indicate network issues, service disruptions, or failures in establishing connections within or between services.\n\n**Purpose:**  \nTo monitor connection-related log messages across all jobs, helping SREs detect and respond to connectivity problems early.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching **{\"message\": \"conn*\", \"job\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 10 connection-related errors in 5 minutes), signaling a potential systemic connectivity issue.\n\n**Impact:**  \n- **High values:** A spike in connection-related logs often indicates network instability, service outages, or degraded performance, potentially affecting application availability and user experience.  \n- **Low or zero values:** Normal operation with no significant connection errors detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of connection-related logs per job over time, enabling quick identification of jobs experiencing connectivity issues.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({job=~\".*\", message=~\"conn.*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 connection-related log entries occur within 5 minutes for any job."
  },
  {
    "query": {
      "message": "conn*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"conn\" and the environment label is present with any value. It identifies log messages related to connection events within Kubernetes environments. The measurement is a count of such log entries, with no specific unit beyond the number of occurrences. For example, a matching log might have message=\"conn connection failed\" and environment=\"production\"."
  },
  {
    "query": {
      "message": "conn*",
      "host": "*"
    },
    "description": "This pattern **{\"message\": \"conn*\", \"host\": \"*\"}** in Loki matches log entries where the message field starts with \"conn\" (e.g., \"connection failed\", \"conn timeout\") and includes any Kubernetes host label. It is used to identify connection-related events across all hosts in the cluster, which are critical for monitoring network connectivity and service availability.\n\n**Purpose:**  \nTo detect and aggregate connection-related log messages that may indicate network issues, service disruptions, or failures in establishing connections between components.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 connection errors per 5 minutes per host), signaling potential connectivity problems.\n\n**Impact:**  \n- **High values:** A spike in connection-related logs often indicates network instability, service outages, or degraded performance, potentially impacting application availability and user experience.  \n- **Low or zero values:** Normal operation, indicating stable connectivity without connection errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of logs matching **{\"message\": \"conn*\", \"host\": \"*\"}** per host to quickly identify hosts experiencing connection issues.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  sum by (host) (count_over_time({message=~\"conn.*\", host=~\".*\"}[5m])) > 10\n  ```  \n  This triggers an alert if any host logs more than 10 connection-related messages in 5 minutes."
  },
  {
    "query": {
      "message": "conn*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message starts with the text \"conn\" and the log has any Kubernetes-defined severity level. It identifies log messages related to connection events or issues. The pattern measures the count of such log entries, with the unit being the number of occurrences. Example: message=\"conn connection failed\", level=\"error\"."
  },
  {
    "query": {
      "message": "conn*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the substring \"conn\" and the env field is present with any value. It identifies log messages related to connection events within a Kubernetes environment. The pattern measures the count of such log entries, representing the number of connection-related logs recorded. The unit of measurement is the total number of matching log entries."
  },
  {
    "query": {
      "message": "conn*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the `message` field starts with \"conn\" (e.g., \"connection failed\", \"conn timeout\") and includes any Kubernetes `app` label. It is used in Loki to identify connection-related events across applications, which often indicate network or service connectivity issues.\n\n**Purpose:**  \nTo monitor connection-related log messages that may signal failures or degraded service health within any application (`app=*`).\n\n**Alert Threshold:**  \nAn alert should trigger when the count of log entries matching this pattern exceeds a defined threshold within a short time window (e.g., more than 10 \"conn*\" messages in 5 minutes), indicating a potential connectivity problem.\n\n**Impact:**  \n- **High values:** Suggest frequent connection errors or timeouts, potentially causing service disruptions or degraded user experience. Immediate investigation is recommended.  \n- **Low or zero values:** Indicate normal operation with no notable connection issues detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of `{\"message\": \"conn*\", \"app\": \"*\"}` log entries per application to quickly identify spikes in connection errors.  \n- **Alert Rule (PromQL):**  \n  ```  \n  count_over_time({message=~\"conn.*\", app=~\".*\"}[5m]) > 10  \n  ```  \n  This triggers an alert if more than 10 connection-related log messages occur within 5 minutes for any app."
  },
  {
    "query": {
      "message": "conn*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"conn\" (e.g., \"connection failed\", \"conn timeout\"), and the entry is associated with a specific Kubernetes `node`. It is used in Loki to identify connection-related events or errors occurring on individual nodes, which can indicate network issues, service connectivity problems, or resource exhaustion affecting node stability.\n\n**Purpose:**  \nTo monitor connection-related log messages per node, enabling detection of connectivity failures or anomalies that may impact application availability or cluster health.\n\n**Alert Threshold:**  \nAn alert should be triggered if the count of logs matching this pattern exceeds a defined threshold within a short time window (e.g., > 10 connection-related errors per node within 5 minutes), signaling a potential connectivity problem requiring investigation.\n\n**Impact of Values:**  \n- **High values:** A spike in \"conn*\" messages often indicates persistent connection failures or network instability on the affected node, potentially leading to degraded service performance or outages. Immediate attention is needed to diagnose and resolve underlying issues.  \n- **Low or zero values:** Normal operation, indicating stable connectivity without notable connection errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of `{\"message\": \"conn*\", \"node\": \"*\"}` log entries per node to quickly identify nodes experiencing connection issues.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"conn.*\", node=~\".*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 connection-related log entries occur on any node within 5 minutes."
  },
  {
    "query": {
      "message": "conn*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains strings starting with \"conn\" (e.g., \"connection failed\", \"conn timeout\"), and the log is associated with any Kubernetes application identified by the `app_kubernetes_io/name` label. It is used in Loki to surface connection-related events—such as connection attempts, failures, or timeouts—that may indicate networking or service availability issues within specific Kubernetes applications.\n\n**Purpose:**  \nTo monitor connection-related log messages per application, helping SREs detect and investigate connectivity problems impacting service reliability.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 \"conn*\" messages in 5 minutes), signaling a potential spike in connection errors or instability.\n\n**Impact:**  \n- **High values:** A surge in \"conn*\" messages often indicates widespread connection failures or degraded network conditions, potentially causing service outages or degraded user experience. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation, indicating stable connectivity without notable connection-related errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of \"conn*\" log entries grouped by `app_kubernetes_io/name` to identify which applications are experiencing connection issues.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({app_kubernetes_io/name=~\".*\", message=~\"conn.*\"}[5m]) > 10\n  ```  \n  This triggers an alert if more than 10 connection-related log messages occur within 5 minutes for any application, prompting SREs to investigate connectivity problems."
  },
  {
    "query": {
      "message": "conn*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"conn\" (e.g., \"connection failed\", \"conn timeout\"), and the log is tagged with a Kubernetes instance label. It is used in Loki to identify connection-related events that may indicate network or service connectivity issues on specific instances.\n\n**Purpose:**  \nTo monitor connection-related log messages that could signal failures or degraded connectivity impacting service availability or performance on individual Kubernetes instances.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of logs matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 \"conn*\" messages per instance within 5 minutes), indicating a potential connectivity problem.\n\n**Impact:**  \n- **High values:** A spike in \"conn*\" messages suggests frequent connection failures or timeouts, potentially leading to service disruptions or degraded user experience. Immediate investigation is warranted.  \n- **Low or zero values:** Normal operation with no significant connection issues detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the rate of \"conn*\" log messages per instance to quickly identify instances experiencing connection problems.  \n- **Alert Rule (PromQL example):**  \n  ```  \n  count_over_time({message=~\"conn.*\", instance=~\".*\"}[5m]) > 5  \n  ```  \n  This triggers an alert if more than 5 connection-related log messages occur on any instance within 5 minutes."
  },
  {
    "query": {
      "message": "conn*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"conn\" and the filename field contains any value. It identifies log messages related to connection events or operations. The pattern measures the count of such log entries, with no specific unit beyond the number of occurrences. Example: message=\"conn connection failed\", filename=\"example-filename\"."
  },
  {
    "query": {
      "message": "conn*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"conn\" (e.g., \"connection failed\", \"conn timeout\"), and the `application` label identifies the Kubernetes application emitting the log. It is used in Loki to monitor connection-related events such as connection failures, timeouts, or drops that may indicate network or service connectivity issues.\n\n**Purpose:**  \nTo detect and track connection-related errors or warnings within specific applications, enabling early identification of connectivity problems that could impact service availability or performance.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a given time window (e.g., more than 10 \"conn*\" messages in 5 minutes), signaling a potential connectivity issue requiring investigation.\n\n**Impact:**  \n- **High values:** A spike in \"conn*\" messages often indicates persistent connection failures or instability, which can lead to degraded service, increased latency, or outages. Immediate attention is needed to prevent user impact.  \n- **Low or zero values:** Normal operation with stable connections; no action required.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching `{message=~\"conn.*\", application=~\".*\"}` grouped by `application`, helping SREs quickly identify which applications are experiencing connection issues.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (application) (count_over_time({message=~\"conn.*\", application=~\".*\"}[5m])) > 10\n  ```  \n  This triggers an alert if any application logs more than 10 connection-related messages in 5 minutes."
  },
  {
    "query": {
      "message": "conn*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"conn\"—typically indicating connection-related events such as connection attempts, failures, or drops—and includes the Kubernetes component label identifying the source component emitting the log. In Loki, this filter helps surface connection issues across components for monitoring and troubleshooting.\n\n**Purpose:**  \nTo detect and monitor connection-related log events that may signal network or service connectivity problems within Kubernetes components.\n\n**Alert Threshold:**  \nAn alert should trigger when the rate of \"conn*\" log messages exceeds a defined threshold (e.g., more than 5 connection failure messages per minute per component), indicating a potential connectivity degradation or outage.\n\n**Impact:**  \n- **High values:** A surge in \"conn*\" messages often reflects widespread or persistent connection failures, which can lead to service unavailability, increased latency, or cascading failures. Immediate investigation is warranted.  \n- **Low values:** Occasional \"conn*\" messages may be normal (e.g., transient connection retries) and typically do not require action.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of logs matching `{message=~\"conn.*\", component=~\".*\"}` grouped by `component` over time, highlighting spikes in connection-related errors.  \n- **Alert Rule (PromQL):**  \n  ```promql\n  sum by (component) (rate({message=~\"conn.*\", component=~\".*\"}[1m])) > 5\n  ```  \n  This triggers an alert if any component logs more than 5 connection-related messages per minute, signaling potential connectivity issues."
  },
  {
    "query": {
      "message": "conn*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the string \"conn\" and the source field contains any value. It identifies log messages related to connection events or operations. The measurement is a count of such log entries, with no specific unit beyond the number of occurrences. For example, a log with message=\"conn connection failed\" and source=\"example-source\" would be included."
  },
  {
    "query": {
      "message": "conn*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"conn\" and the entry includes any Kubernetes pod_name label. It identifies log messages related to connection events or operations within pods. The measurement is a count of such log occurrences, with no specific unit beyond the number of matching entries. For example, a log with message=\"conn connection failed\" and pod_name=\"example-pod_name\" would be included."
  },
  {
    "query": {
      "message": "conn*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"conn\" and the container_name field is present with any value. It identifies log messages related to connection events within Kubernetes containers. The pattern measures the occurrence count of such log entries. The unit of measurement is the number of matching log records."
  },
  {
    "query": {
      "message": "conn*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"conn\" and the cluster field is present with any value. It identifies log messages related to connection events within a specific Kubernetes cluster. The measurement is a count of such log entries, representing the number of connection-related logs per cluster. Units are raw log entry counts."
  },
  {
    "query": {
      "message": "conn*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the message field starts with \"conn\" and the pod label is present. It identifies logs related to connection events within Kubernetes pods. The measurement is a count of such log entries, representing the number of connection-related messages per pod. Units are raw log entry counts."
  },
  {
    "query": {
      "message": "conn*",
      "namespace": "*"
    },
    "description": "This log query pattern **{\"message\": \"conn*\", \"namespace\": \"*\"}** in Loki is designed to capture all log entries where the message field contains strings starting with \"conn\" (e.g., \"connection failed\", \"conn reset\"), across any Kubernetes namespace. Its primary purpose is to surface connection-related events that may indicate network issues, service connectivity problems, or failures in establishing or maintaining connections within your cluster.\n\n**Thresholds and Alerting:**  \nAn alert should be triggered when the rate of these \"conn*\" messages exceeds a defined threshold within a given time window, for example, more than 10 connection-related errors per minute per namespace. This threshold can be adjusted based on normal traffic patterns and historical baselines.\n\n**Impact of Values:**  \n- **High values:** A spike in \"conn*\" messages typically signals widespread or persistent connectivity problems, which can lead to service degradation, increased latency, or outages. Immediate investigation is warranted to identify root causes such as network partitions, misconfigurations, or resource exhaustion.  \n- **Low or zero values:** Indicates stable connectivity with no recent connection-related errors, suggesting normal operation.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the count of logs matching **{\"message\": \"conn*\", \"namespace\": \"*\"}** over time, grouped by namespace, to quickly identify namespaces experiencing connection issues.  \n- **Alert Rule (PromQL example):**  \n  ```  \n  sum by (namespace) (rate({message=~\"conn.*\", namespace=~\".*\"}[1m])) > 10  \n  ```  \n  This alert fires if any namespace logs more than 10 connection-related messages per minute, signaling potential connectivity problems requiring SRE attention."
  },
  {
    "query": {
      "message": "disk*",
      "k8s_app": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"disk\" and includes any Kubernetes application label under \"k8s_app.\" It identifies log messages related to disk operations or issues within Kubernetes applications. The measurement is a count of such log entries, representing the number of disk-related events recorded per application. Units are expressed as the total number of matching log entries."
  },
  {
    "query": {
      "message": "disk*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"disk\" and any container name is specified. It identifies log messages related to disk operations or issues within Kubernetes containers. The measurement is a count of such log entries, with no specific unit beyond the number of occurrences. For example, a log with message=\"disk connection failed\" and container=\"example-container\" would be included."
  },
  {
    "query": {
      "message": "disk*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"disk\" and the entry is associated with a specific Kubernetes `node_name`. It is used in Loki to identify disk-related events such as errors, warnings, or status messages on individual nodes.  \n\n**Purpose:**  \nTo monitor disk-related issues (e.g., failures, latency, or capacity warnings) on Kubernetes nodes, enabling early detection of hardware or filesystem problems that could impact node stability or application performance.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of disk-related error or warning messages (e.g., containing keywords like \"fail\", \"error\", \"timeout\") exceeds a defined threshold within a given time window (e.g., >5 errors in 5 minutes) for any single node. This indicates a persistent or recurring disk problem requiring investigation.\n\n**Impact:**  \n- **High values:** Frequent disk-related errors suggest potential hardware failure, disk saturation, or filesystem corruption, which can lead to degraded node performance, pod eviction, or data loss. Immediate remediation is critical.  \n- **Low or zero values:** Normal operation with no significant disk issues detected.\n\n**Example Usage:**  \nIn a Loki query for an alert rule or dashboard panel:  \n```\ncount_over_time({message=~\"disk.*\", node_name=~\".*\"} |= \"error\" [5m]) > 5\n```\nThis query counts disk-related error messages per node over the last 5 minutes and triggers an alert if more than 5 occur, helping SREs quickly identify and respond to disk problems on specific nodes."
  },
  {
    "query": {
      "message": "disk*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"disk\" and includes any additional characters. It specifically captures logs related to disk operations or issues. Each matched log entry must have a Kubernetes \"region\" label indicating the cluster region. The pattern measures the count of such log entries, with no specific unit beyond the number of occurrences."
  },
  {
    "query": {
      "message": "disk*",
      "service": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"disk\" and the service field is present with any value. It identifies log messages related to disk operations or issues within any Kubernetes service. The measurement is a count of such log entries, representing the number of disk-related events recorded. Units are simply the number of occurrences (count)."
  },
  {
    "query": {
      "message": "disk*",
      "job": "*"
    },
    "description": "This pattern **{\"message\": \"disk*\", \"job\": \"*\"}** in Loki matches log entries where the message contains the keyword \"disk\" from any Kubernetes job. It is used to monitor disk-related events such as errors, warnings, or status updates that may indicate disk health or performance issues.  \n\n**Purpose:**  \nTo detect and track disk-related anomalies or failures across all jobs, enabling early identification of storage problems that could impact application stability or data integrity.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of log entries matching this pattern exceeds a defined threshold within a short time window—for example, more than 5 disk-related error or warning messages within 5 minutes—indicating a potential disk failure or degradation.\n\n**Impact:**  \n- **High values:** A spike in disk-related logs often signals disk errors, I/O bottlenecks, or hardware failures, which can lead to application crashes, data loss, or degraded performance. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation with no disk-related issues detected.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series panel showing the rate of logs matching **{\"message\": \"disk*\", \"job\": \"*\"}** grouped by job, highlighting spikes or trends in disk-related events.  \n- **Alert Rule (PromQL example):**  \n  ```  \n  count_over_time({job=~\".*\", message=~\"disk.*\"}[5m]) > 5  \n  ```  \n  This triggers an alert if more than 5 disk-related log entries occur within 5 minutes for any job."
  },
  {
    "query": {
      "message": "disk*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"disk\" from any Kubernetes environment, helping to identify disk-related events such as errors, warnings, or performance issues. In Loki, it is used to filter and aggregate disk-related logs across all environments for monitoring storage health and availability.\n\n**Purpose:** To detect and analyze disk-related issues (e.g., disk failures, I/O errors, or capacity warnings) that could impact application stability or data integrity.\n\n**Alert Thresholds:**  \n- Trigger an alert if the count of disk-related error or warning messages exceeds a defined threshold (e.g., > 5 errors within 5 minutes), indicating a potential disk subsystem failure or degradation.  \n- Low counts typically indicate normal operation, while sustained high counts suggest escalating disk problems requiring immediate investigation.\n\n**Impact:**  \n- High frequency of disk-related errors can lead to application crashes, data loss, or degraded performance.  \n- Early detection allows proactive remediation, minimizing downtime and data corruption risks.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of disk-related error logs over time using a query like `{message=~\"disk.*\", environment=~\".*\"}` to monitor disk health trends across environments.  \n- **Alert Rule:**  \n  ```\n  sum by (environment) (count_over_time({message=~\"disk.*\", environment=~\".*\"} |= \"error\"[5m])) > 5\n  ```  \n  This triggers an alert when more than 5 disk-related error logs occur within 5 minutes in any environment, prompting SREs to investigate disk subsystem issues promptly."
  },
  {
    "query": {
      "message": "disk*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the message contains the keyword \"disk\" from any Kubernetes host, helping to identify disk-related events such as errors, warnings, or status updates. It is used in Loki to monitor disk health and performance issues across hosts. An alert should be triggered when the count of such log entries exceeds a threshold indicating potential disk failures or degradation—for example, more than 5 \"disk*\" messages within 5 minutes per host. High volumes of these logs may signal disk I/O problems, hardware faults, or capacity issues that could impact application stability and data integrity, while low or zero counts generally indicate normal disk operation. \n\nExample alert rule snippet:\n```\ncount_over_time({message=~\"disk.*\", host=~\".*\"}[5m]) > 5\n```\n\nExample dashboard usage:\n- A graph showing the rate of \"disk*\" log messages per host over time to quickly spot spikes.\n- A table listing hosts with the highest number of disk-related logs to prioritize investigation."
  },
  {
    "query": {
      "message": "disk*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"disk\" and includes any Kubernetes log level. It identifies events related to disk operations or errors. The \"message\" field contains the log text, and the \"level\" field specifies the severity (e.g., info, warning, error). This pattern measures the occurrence count of such disk-related log messages."
  },
  {
    "query": {
      "message": "disk*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"disk\" (e.g., \"disk failure\", \"disk connection failed\") across all Kubernetes environments (`env=\"*\"`). It is used in Loki to identify disk-related issues such as disk errors, failures, or performance problems that could impact system stability or application availability.\n\n**Purpose:**  \nTo monitor disk-related log messages indicating potential hardware or I/O problems that require immediate attention.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of log entries matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 disk-related error messages within 5 minutes), signaling a possible disk subsystem failure or degradation.\n\n**Impact:**  \n- **High values:** A surge in disk-related errors often indicates critical storage issues that can lead to application crashes, data loss, or degraded performance. Immediate investigation and remediation are required.  \n- **Low or zero values:** Normal operation with no detected disk-related errors.\n\n**Example Usage:**  \nIn a Grafana dashboard, a panel could display the rate of disk-related error logs filtered by environment:  \n```\ncount_over_time({message=~\"disk.*\", env=~\".*\"}[5m])\n```\nAn alert rule might be configured as:  \n- **Condition:** When the count of disk-related logs in any environment exceeds 5 within 5 minutes.  \n- **Action:** Notify the SRE team to investigate potential disk failures."
  },
  {
    "query": {
      "message": "disk*",
      "app": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"disk\" and the log is associated with any Kubernetes application (app label). It identifies events related to disk operations or issues within Kubernetes-managed applications. The measurement counts the number of such log entries. The unit is the total count of matching log messages."
  },
  {
    "query": {
      "message": "disk*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"disk\" and includes any following text. It specifically filters logs that are tagged with a Kubernetes node identifier. The pattern measures the count of such log entries, representing the number of disk-related events recorded per node. The unit of measurement is the number of log occurrences."
  },
  {
    "query": {
      "message": "disk*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"disk\" and the \"app_kubernetes_io/name\" label is present with any value. It identifies log messages related to disk operations within Kubernetes applications. The pattern measures the count of such log entries, representing the number of disk-related events recorded per unit of time."
  },
  {
    "query": {
      "message": "disk*",
      "instance": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"disk\" and includes the Kubernetes `instance` label, helping identify disk-related events such as errors or warnings on specific instances. In Loki, it is used to filter and aggregate disk-related logs for monitoring storage health and performance.\n\n**Purpose:**  \nTo detect and analyze disk-related issues (e.g., failures, latency, or capacity warnings) on individual Kubernetes instances, enabling timely identification of storage problems.\n\n**Alert Thresholds:**  \nAn alert could be triggered when the count of disk-related error or warning messages exceeds a defined threshold within a given time window (e.g., more than 5 disk error logs per 5 minutes per instance), indicating potential disk failures or degraded performance.\n\n**Impact:**  \n- **High values:** Frequent disk-related errors or warnings suggest underlying hardware issues, disk saturation, or I/O bottlenecks, potentially leading to application slowdowns or data loss. Immediate investigation and remediation are required.  \n- **Low or zero values:** Indicate normal disk operation with no detected disk-related anomalies.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the rate of disk-related log messages per instance over time to spot trends or spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"disk.*\", instance=~\".*\"} |= \"error\"[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 disk error messages occur within 5 minutes on any instance."
  },
  {
    "query": {
      "message": "disk*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"disk\" and the filename field is present with any value. It identifies log messages related to disk operations or issues within Kubernetes-managed containers. The measurement is a count of such log entries, representing the number of disk-related events recorded. Units are simply the number of matching log occurrences."
  },
  {
    "query": {
      "message": "disk*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"disk\" and the application field is present with any value. It identifies log messages related to disk operations or issues within any Kubernetes application. The pattern measures the count of such log entries, representing the number of disk-related events recorded. The unit of measurement is the total number of matching log entries."
  },
  {
    "query": {
      "message": "disk*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"disk\" and the component field has any value. It identifies log messages related to disk operations or issues within any Kubernetes component. The measurement is a count of such log entries, representing the number of disk-related events recorded. Units are simply the number of matching log occurrences."
  },
  {
    "query": {
      "message": "disk*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"disk\" and the source field contains any value. It identifies log messages related to disk operations or issues within Kubernetes components. The pattern does not measure a numeric value or unit but filters logs based on textual content. Example: message=\"disk connection failed\", source=\"example-source\"."
  },
  {
    "query": {
      "message": "disk*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod (`pod_name=\"*\"`) where the log message contains the substring \"disk\", typically indicating disk-related events such as errors, warnings, or status updates. In Loki, it helps SREs monitor disk health and performance issues across pods by filtering relevant disk-related logs.\n\n**Purpose:**  \nTo detect and investigate disk-related anomalies (e.g., disk failures, I/O errors, or capacity warnings) that could impact application stability or data integrity.\n\n**Alert Threshold:**  \nAn alert should trigger if the count of log entries matching this pattern exceeds a defined threshold within a given time window—for example, more than 5 disk-related error messages in 5 minutes—indicating a potential disk subsystem problem requiring immediate attention.\n\n**Impact:**  \n- **High volume:** A surge in disk-related logs often signals underlying hardware issues, degraded performance, or imminent failures, potentially causing application downtime or data loss.  \n- **Low or no occurrences:** Normal operation, indicating disk subsystems are functioning without notable errors.\n\n**Example Usage:**  \n- **Dashboard:** Display a panel showing the rate of `{\"message\": \"disk*\", \"pod_name\": \"*\"}` logs over time, segmented by pod, to quickly identify pods experiencing disk issues.  \n- **Alert Rule (PromQL for Loki):**  \n  ```\n  count_over_time({message=~\"disk.*\", pod_name=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 disk-related log entries occur in any pod within 5 minutes, prompting investigation and remediation."
  },
  {
    "query": {
      "message": "disk*",
      "container_name": "*"
    },
    "description": "This pattern matches log entries where the message field contains the substring \"disk\" and the container_name field is present with any value. It identifies log messages related to disk operations or issues within any Kubernetes container. The pattern does not measure a numeric value or unit but filters relevant log events for monitoring or troubleshooting purposes. Example: message=\"disk connection failed\", container_name=\"example-container_name\"."
  },
  {
    "query": {
      "message": "disk*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"disk\" and the cluster field contains any value. It measures the count of such log entries related to disk operations within Kubernetes clusters. The unit of measurement is the number of log entries. Example: message=\"disk connection failed\", cluster=\"example-cluster\"."
  },
  {
    "query": {
      "message": "disk*",
      "pod": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"disk\" and includes any Kubernetes pod label. It identifies events related to disk operations or errors within pods. The measurement is the count of such log entries, representing the frequency of disk-related messages per pod. Units are number of log entries."
  },
  {
    "query": {
      "message": "disk*",
      "namespace": "*"
    },
    "description": "This pattern matches log entries where the `message` field contains the substring \"disk\" (e.g., \"disk full\", \"disk error\", \"disk connection failed\") across any Kubernetes `namespace`. It is used in Loki to identify disk-related events that may indicate storage issues, such as disk failures, capacity problems, or I/O errors.\n\n**Purpose:**  \nTo monitor disk-related log messages across all namespaces, enabling early detection of storage problems that could impact application stability or performance.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a short time window (e.g., more than 5 disk-related error messages in 5 minutes), indicating a potential disk issue requiring investigation.\n\n**Impact:**  \n- **High values:** A spike in disk-related errors often signals critical storage problems such as disk failures, full disks, or degraded I/O performance, which can lead to application crashes, data loss, or degraded service availability. Immediate action is recommended.  \n- **Low or zero values:** Indicates normal operation with no detected disk-related errors.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\n{namespace=~\".*\"} |= \"disk\"\n```\nand create a Prometheus-style alert rule:  \n```\ncount_over_time({namespace=~\".*\"} |= \"disk\"[5m]) > 5\n```\nThis triggers an alert if more than 5 disk-related log messages occur within 5 minutes across any namespace, prompting SREs to investigate disk health and capacity issues."
  },
  {
    "query": {
      "message": "kill*",
      "k8s_app": "*"
    },
    "description": "This log pattern **{\"message\": \"kill*\", \"k8s_app\": \"*\"}** is used in Loki to identify log entries from any Kubernetes application (`k8s_app`) where the message starts with or contains the term \"kill\". This typically indicates that a process, connection, or resource was forcibly terminated or failed to terminate gracefully, which can be a critical operational event.\n\n**Purpose:**  \nTo monitor and detect occurrences of forced terminations or kill signals within Kubernetes applications, helping SREs quickly identify potential stability or performance issues caused by unexpected kills.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"kill\" messages in 5 minutes). The exact threshold depends on the normal behavior of your applications but should be set low enough to catch abnormal spikes without causing noise.\n\n**Impact of Values:**  \n- **High values:** A sudden increase in \"kill\" messages may indicate underlying problems such as resource exhaustion, application crashes, or misconfigured shutdown procedures, potentially leading to degraded service availability or data loss.  \n- **Low or zero values:** Normal operation, indicating that no forced terminations are occurring.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the count of \"kill\" messages over time per `k8s_app`, enabling quick identification of which applications are experiencing forced terminations.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({k8s_app=~\".*\", message=~\"kill.*\"}[5m]) > 5\n  ```  \n  This rule triggers an alert if more than 5 kill-related log entries occur within 5 minutes for any Kubernetes app.\n\nBy monitoring this pattern, SREs can proactively investigate and mitigate issues causing forced terminations, improving system reliability."
  },
  {
    "query": {
      "message": "kill*",
      "container": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"kill\" and the container field is present with any value. It identifies log messages related to termination or interruption events within Kubernetes containers. The measurement is a count of such log entries, representing the number of kill-related messages recorded per container. This helps monitor occurrences of kill operations or failures in containerized environments."
  },
  {
    "query": {
      "message": "kill*",
      "node_name": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"kill\" and the node_name field is present. It identifies events related to termination or killing operations recorded on specific Kubernetes nodes. Each matched log entry represents one occurrence of such an event on the given node. The unit of measurement is the count of these log entries per node."
  },
  {
    "query": {
      "message": "kill*",
      "region": "*"
    },
    "description": "This pattern matches log entries where the message starts with the word \"kill\" and includes any subsequent characters. It specifically captures logs labeled with any Kubernetes region value. The measurement counts the number of such log entries occurring within the specified region. The unit of measurement is the count of matching log entries."
  },
  {
    "query": {
      "message": "kill*",
      "service": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes service where the message begins with \"kill\", typically indicating that a process, connection, or operation was forcibly terminated. In Loki, **{\"message\": \"kill*\", \"service\": \"*\"}** helps SREs identify potentially disruptive events such as killed connections, terminated pods, or aborted tasks that may impact system stability or user experience.\n\n**Purpose:**  \nTo detect and monitor occurrences of forced terminations or kills across all services, enabling early identification of issues like resource exhaustion, crashes, or misconfigurations.\n\n**Alert Threshold:**  \nAn alert should trigger when the count of \"kill*\" messages exceeds a defined threshold within a short time window (e.g., more than 5 kill events per minute per service), signaling abnormal termination rates that warrant investigation.\n\n**Impact:**  \n- **High values:** Indicate frequent forced terminations, which can lead to service instability, degraded performance, or outages. Immediate attention is required to diagnose root causes.  \n- **Low or zero values:** Normal operation, indicating that forced kills are rare or absent.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"kill*\" messages per service over time, highlighting spikes that correlate with incidents.  \n- **Alert Rule (PromQL):**  \n  ```  \n  sum by (service) (rate({message=~\"kill.*\", service=~\".*\"}[1m])) > 5  \n  ```  \nThis rule triggers when any service logs more than 5 kill-related messages per minute, prompting an alert for potential service disruption."
  },
  {
    "query": {
      "message": "kill*",
      "job": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the word \"kill\" and the job field is present with any value. It identifies log messages related to termination or failure events associated with Kubernetes jobs. The measurement is a count of such log entries, representing the number of kill-related events per job. Units are raw log entry counts."
  },
  {
    "query": {
      "message": "kill*",
      "environment": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"kill\" (e.g., \"kill connection failed\") across any Kubernetes environment, helping identify potentially disruptive events such as terminated processes or killed connections. In Loki, it is used to monitor occurrences of these critical operational events that may indicate instability or failures.\n\n**Alert Threshold:** Trigger an alert if the count of \"kill*\" messages exceeds a defined threshold (e.g., more than 5 occurrences within 5 minutes) in a given environment, signaling abnormal termination activity.\n\n**Impact:**  \n- **High values:** Frequent \"kill\" messages may indicate systemic issues causing process or connection terminations, potentially leading to service degradation or outages.  \n- **Low or zero values:** Normal operation with no unexpected terminations detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"kill*\" messages per environment over time, enabling quick identification of spikes.  \n- **Alert Rule:**  \n```\nsum by (environment) (count_over_time({message=~\"kill.*\", environment=~\".*\"}[5m])) > 5\n```\nThis alerts SREs to investigate environments with elevated kill events promptly."
  },
  {
    "query": {
      "message": "kill*",
      "host": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"kill\" and the host field is present. It identifies events related to termination or killing operations recorded on any Kubernetes host. The measurement is a count of such log entries, representing the number of kill-related messages per host. Units are raw log entry counts."
  },
  {
    "query": {
      "message": "kill*",
      "level": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"kill\" and the log includes any Kubernetes-defined severity level. It identifies events related to termination or interruption operations recorded in the logs. The measurement is a count of such log entries, categorized by their severity level. Units are discrete log occurrences labeled by the Kubernetes log level."
  },
  {
    "query": {
      "message": "kill*",
      "env": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"kill\" (e.g., \"kill connection failed\") across any Kubernetes environment (`env`). It is used in Loki to identify potentially critical events involving terminated processes or connections that may impact system stability or service availability.\n\n**Purpose:**  \nTo detect and monitor occurrences of kill-related events that could indicate failures, resource contention, or abnormal terminations within any environment.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"kill*\" messages in 5 minutes), signaling a potential issue requiring investigation.\n\n**Impact:**  \n- **High values:** Frequent kill events may indicate systemic problems such as resource exhaustion, misbehaving processes, or network instability, potentially leading to degraded service or outages.  \n- **Low or zero values:** Normal operation, indicating no recent kill-related errors detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"kill*\" messages over time per environment, helping SREs quickly spot spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"kill.*\", env=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 kill-related log entries occur within 5 minutes in any environment."
  },
  {
    "query": {
      "message": "kill*",
      "app": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes application (`app=\"*\"`) where the log message begins with \"kill\" (e.g., \"kill connection failed\"). It is used in Loki to identify potentially critical events involving termination or failure of processes, connections, or resources.  \n\n**Purpose:**  \nTo detect and monitor occurrences of kill-related events that may indicate underlying issues such as forced termination of processes, failed connection kills, or resource cleanup failures within applications.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of logs matching `{\"message\": \"kill*\", \"app\": \"*\"}` exceeds a defined threshold within a given time window (e.g., more than 5 kill-related messages in 5 minutes), signaling abnormal or frequent termination events that could impact system stability.\n\n**Impact:**  \n- **High values:** Frequent kill messages may indicate systemic problems like resource exhaustion, application crashes, or network instability, potentially leading to degraded service or outages.  \n- **Low or zero values:** Normal operation, indicating that kill events are rare or absent, suggesting stable application behavior.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of kill-related log entries per application over time, helping SREs quickly identify spikes or trends.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  count_over_time({message=~\"kill.*\", app=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 kill-related messages occur within 5 minutes across any app, prompting investigation."
  },
  {
    "query": {
      "message": "kill*",
      "node": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"kill\" and includes any subsequent characters. It specifically captures events related to termination or interruption operations recorded on Kubernetes nodes. The \"node\" field identifies the Kubernetes node where the log originated. Each matched entry represents a single occurrence of such an event."
  },
  {
    "query": {
      "message": "kill*",
      "app_kubernetes_io/name": "*"
    },
    "description": "This log query pattern **{\"message\": \"kill*\", \"app_kubernetes_io/name\": \"*\"}** is used in Loki to filter log entries where the message starts with \"kill\" (e.g., \"kill connection failed\") and the log is associated with a specific Kubernetes application identified by the `app_kubernetes_io/name` label. This pattern helps SREs detect potentially critical operational events such as process terminations, connection kills, or forced shutdowns within application pods.\n\n**Purpose:**  \n- To surface logs indicating that a \"kill\" event occurred, which may signal abnormal behavior, resource contention, or manual intervention in the application lifecycle.  \n- To correlate these events with specific Kubernetes applications for targeted troubleshooting.\n\n**Alert Threshold Guidance:**  \n- An alert should be triggered if the count of \"kill*\" messages exceeds a defined threshold within a given time window (e.g., more than 5 kill events in 5 minutes), as frequent kill events often indicate instability or resource exhaustion.  \n- Thresholds should be tuned based on normal application behavior; a sudden spike or sustained high rate is cause for investigation.\n\n**Impact of Values:**  \n- **High values:** May indicate recurring failures, pod evictions, or aggressive resource limits causing processes to be killed, potentially leading to degraded service or outages.  \n- **Low or zero values:** Typically expected under normal operation; absence of kill events suggests stable pod behavior.\n\n**Example Usage in Dashboard or Alert Rule:**  \n- **Dashboard:** A panel showing the rate of logs matching `{\"message\": \"kill*\", \"app_kubernetes_io/name\": \"$app\"}` over time, enabling quick identification of spikes in kill events per application.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({message=~\"kill.*\", app_kubernetes_io/name=~\".+\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 kill-related log entries occur within 5 minutes for any application, prompting immediate investigation."
  },
  {
    "query": {
      "message": "kill*",
      "instance": "*"
    },
    "description": "This pattern **{\"message\": \"kill*\", \"instance\": \"*\"}** in Loki matches log entries where the message starts with or contains the term \"kill\"—typically indicating that a process, connection, or operation was forcibly terminated on a specific Kubernetes instance. It helps SREs detect potentially disruptive events such as killed connections, pods, or processes that may impact system stability or availability.\n\n**Purpose:**  \nTo monitor and alert on occurrences of forced terminations or kills within the system, which often signal underlying issues like resource exhaustion, misconfigurations, or failures.\n\n**Alert Threshold:**  \nAn alert should be triggered when the count of log entries matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"kill\" messages per 5 minutes per instance), indicating an abnormal spike in forced terminations.\n\n**Impact:**  \n- **High values:** Suggest frequent forced terminations that could degrade service reliability, cause downtime, or indicate cascading failures requiring immediate investigation.  \n- **Low or zero values:** Indicate normal operation with no recent forced kills, implying system stability.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, you might use a Loki query like:  \n```\ncount_over_time({message=~\"kill.*\", instance=~\".*\"}[5m])\n```\nto count occurrences over 5 minutes per instance. An alert rule could fire if this count exceeds 5, prompting the SRE team to investigate potential issues causing forced terminations on that instance."
  },
  {
    "query": {
      "message": "kill*",
      "filename": "*"
    },
    "description": "This pattern matches log entries where the message begins with the word \"kill\" and the filename field is present. It identifies log messages related to termination or kill operations within Kubernetes components. The measurement is the count of such log entries, representing occurrences of kill-related events. Each matched entry corresponds to one event instance."
  },
  {
    "query": {
      "message": "kill*",
      "application": "*"
    },
    "description": "This pattern matches log entries where the message contains the substring \"kill\" and the log is associated with any Kubernetes application label. It is used in Loki to identify potentially disruptive events such as terminated processes, killed connections, or aborted operations that may indicate underlying issues affecting application stability or performance.  \n\n**Purpose:**  \nTo detect and monitor occurrences of \"kill\" events that could signal failures, resource contention, or abnormal terminations within applications running in the cluster.\n\n**Alert Threshold:**  \nAn alert should be triggered if the count of logs matching **{\"message\": \"kill*\", \"application\": \"*\"}** exceeds a defined threshold within a given time window (e.g., more than 5 kill-related messages in 5 minutes), indicating a possible systemic problem requiring investigation.\n\n**Impact:**  \n- **High values:** Frequent \"kill\" messages may reflect critical issues such as process crashes, forced terminations, or network disruptions, potentially leading to degraded service availability or data loss.  \n- **Low or zero values:** Normal operation, indicating no recent kill-related events detected.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"kill\" messages per application over time, helping SREs quickly identify spikes or trends.  \n- **Alert Rule (PromQL):**  \n  ```\n  count_over_time({message=~\"kill.*\", application=~\".*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 kill-related log entries occur within 5 minutes for any application."
  },
  {
    "query": {
      "message": "kill*",
      "component": "*"
    },
    "description": "This pattern matches log entries where the message starts with \"kill\" and includes any Kubernetes component label. It identifies events related to termination or killing operations within Kubernetes components. The measurement is a count of such log entries, representing the frequency of kill-related messages per component. Units are number of log entries."
  },
  {
    "query": {
      "message": "kill*",
      "source": "*"
    },
    "description": "This pattern matches log entries where the message field starts with the text \"kill\" and the source field contains any value. It identifies log messages related to kill operations or events within Kubernetes components. The measurement is a count of such log entries, representing the frequency of kill-related messages. Units are raw log entry counts without additional normalization."
  },
  {
    "query": {
      "message": "kill*",
      "pod_name": "*"
    },
    "description": "This pattern matches log entries where the \"message\" field starts with the word \"kill\" and the \"pod_name\" field is present with any value. It identifies events related to termination or failure actions recorded by Kubernetes pods. The measurement is a count of such log entries, representing the number of kill-related messages per pod. Units are discrete log occurrences without a time dimension unless specified elsewhere."
  },
  {
    "query": {
      "message": "kill*",
      "container_name": "*"
    },
    "description": "This log pattern **{\"message\": \"kill*\", \"container_name\": \"*\"}** in Loki is used to identify log entries where the message contains the term \"kill\" from any container in a Kubernetes environment. It helps SREs detect potentially critical events such as process terminations, connection kills, or forced shutdowns that may indicate underlying issues affecting application stability or performance.\n\n**Purpose:**  \n- To monitor and surface events where processes or connections are being killed, which can signal resource contention, crashes, or abnormal behavior in containers.  \n- Enables early detection of operational problems that might require investigation or remediation.\n\n**Alert Threshold:**  \n- An alert could be triggered when the count of log entries matching this pattern exceeds a defined threshold within a given time window (e.g., more than 5 \"kill\" messages per 5 minutes per container).  \n- Thresholds should be tuned based on baseline behavior; a sudden spike or sustained high rate often indicates a problem.\n\n**Impact of Values:**  \n- **High values:** May indicate frequent process or connection terminations, potentially causing service disruption, degraded performance, or cascading failures. Immediate investigation is recommended.  \n- **Low or zero values:** Typically expected under normal operation; no immediate concern.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of \"kill\" messages per container over time, highlighting spikes or trends.  \n- **Alert Rule (PromQL example):**  \n  ```\n  count_over_time({container_name=~\".*\", message=~\"kill.*\"}[5m]) > 5\n  ```  \n  This triggers an alert if more than 5 \"kill\" messages occur in any container within 5 minutes.\n\nBy monitoring this pattern, SREs gain visibility into critical kill events that may impact system reliability and can proactively respond before user-facing issues escalate."
  },
  {
    "query": {
      "message": "kill*",
      "cluster": "*"
    },
    "description": "This pattern matches log entries where the message begins with \"kill\", typically indicating termination events such as killed processes, connections, or pods, within a specific Kubernetes cluster (identified by the \"cluster\" label). In Loki, this helps SREs monitor unexpected or forced terminations that may impact system stability or availability.\n\n**Purpose:** Detect and track occurrences of kill-related events across clusters to identify potential issues like resource exhaustion, pod evictions, or network disruptions.\n\n**Alert Threshold:** Trigger an alert if the count of \"kill*\" messages exceeds a defined threshold (e.g., more than 5 kill events within 5 minutes per cluster), signaling abnormal termination activity that requires investigation.\n\n**Impact:**  \n- **High values:** May indicate systemic problems such as frequent pod restarts, failing connections, or resource contention, potentially leading to degraded service or outages.  \n- **Low or zero values:** Normal operation, indicating stable system behavior without unexpected terminations.\n\n**Example Usage:**  \nIn a Grafana dashboard or alert rule, use a Loki query like:  \n```\ncount_over_time({message=~\"kill.*\", cluster=~\".+\"}[5m])\n```\nto visualize or alert on the number of kill events per cluster over the last 5 minutes. Set alert conditions to notify when this count surpasses the acceptable threshold, enabling proactive response to termination-related issues."
  },
  {
    "query": {
      "message": "kill*",
      "pod": "*"
    },
    "description": "This pattern matches log entries from any Kubernetes pod where the message contains the substring \"kill\", typically indicating that a process, connection, or resource was forcibly terminated. In Loki, **{\"message\": \"kill*\", \"pod\": \"*\"}** helps identify potentially disruptive events such as killed connections, terminated processes, or pods being evicted.\n\n**Purpose:**  \nTo monitor and alert on occurrences of forced terminations or kills within pods, which may signal underlying issues like resource exhaustion, application crashes, or network problems.\n\n**Alert Threshold:**  \nAn alert could be triggered if the count of \"kill\" messages exceeds a defined threshold within a given time window (e.g., more than 5 kill events in 5 minutes per pod), indicating abnormal or frequent terminations that warrant investigation.\n\n**Impact:**  \n- **High values:** Frequent kill messages may lead to degraded service availability, increased error rates, or cascading failures. Immediate attention is required to identify root causes such as memory pressure, OOM kills, or network instability.  \n- **Low or zero values:** Normal operation, indicating stable pod behavior without forced terminations.\n\n**Example Usage:**  \n- **Dashboard:** A panel showing the rate of logs matching **{\"message\": \"kill*\", \"pod\": \"*\"}** grouped by pod, highlighting pods with spikes in kill events.  \n- **Alert Rule:**  \n  ```\n  sum by (pod) (count_over_time({message=~\"kill.*\", pod=~\".*\"}[5m])) > 5\n  ```  \n  This triggers an alert if any pod logs more than 5 kill-related messages in 5 minutes, prompting an SRE to investigate potential stability or resource issues."
  },
  {
    "query": {
      "message": "kill*",
      "namespace": "*"
    },
    "description": "Namespace kill operation logs capture events related to process terminations and resource cleanup activities occurring across all Kubernetes namespaces. In Loki, the **{\"message\": \"kill*\", \"namespace\": \"*\"}** filter helps SREs monitor these critical lifecycle events to ensure proper resource management and detect abnormal termination patterns.\n\n**Alert Thresholds:**  \n- Trigger an alert if the count of kill events exceeds a baseline threshold (e.g., more than 5 kills per minute per namespace), indicating potential instability or resource exhaustion.  \n- Conversely, an unusually low number of kill events over an extended period may suggest missing logs or suppressed cleanup operations.\n\n**Impact:**  \n- **High values:** May signal frequent pod or container crashes, resource contention, or misbehaving workloads requiring immediate investigation to prevent cascading failures.  \n- **Low values:** Could indicate stable workloads or potential logging gaps that might mask underlying issues.\n\n**Example Usage:**  \n- **Dashboard:** Visualize kill event counts over time per namespace to identify spikes or trends.  \n- **Alert Rule (Prometheus-style):**  \n  ```\n  sum by (namespace) (count_over_time({message=~\"kill.*\", namespace=~\".*\"}[1m])) > 5\n  ```  \n  This rule fires when more than 5 kill events occur within one minute in any namespace, prompting an SRE to investigate potential resource or application issues."
  },
  {
    "query": {
      "message": "authentication*",
      "k8s_app": "*"
    },
    "description": "Kubernetes authentication logs collected from all k8s applications, capturing login attempts, credential validations, and identity verifications to monitor access control and detect potential security breaches. In Loki, **{\"message\": \"authentication*\", \"k8s_app\": \"*\"}** filters these logs for centralized analysis. Set alert thresholds based on the rate of failed authentication attempts—e.g., trigger an alert if failures exceed 10 per minute—to quickly identify possible brute-force attacks or compromised credentials. High values indicate increased security risk and potential unauthorized access, while low values suggest normal authentication activity. Example alert rule snippet:  \n```\nsum(rate({message=~\"authentication.*\", k8s_app=~\".*\"} |= \"failed\" [1m])) > 10\n```\nThis can be visualized in dashboards showing authentication success vs failure trends per app to aid in proactive security monitoring."
  },
  {
    "query": {
      "message": "authentication*",
      "namespace": "*"
    },
    "description": "Logs capturing all authentication-related events across every Kubernetes namespace. This includes user login attempts, service account authentications, and access control decisions. Each event is recorded as an individual log entry with timestamps. These logs measure the count and details of authentication activities for security monitoring and compliance auditing."
  },
  {
    "query": {
      "message": "authorization*",
      "k8s_app": "*"
    },
    "description": "Logs capturing authorization events from all Kubernetes applications. Each entry records permission checks, access control decisions, and policy evaluations performed during user or service requests. Measures the occurrence and outcome of authorization attempts as discrete log events. Used for security monitoring and compliance auditing across Kubernetes workloads."
  },
  {
    "query": {
      "message": "authorization*",
      "namespace": "*"
    },
    "description": "Namespace authorization logs capture detailed records of Role-Based Access Control (RBAC) permission checks, access control decisions, and policy evaluations occurring across all Kubernetes namespaces. In Loki, the **{\"message\": \"authorization*\", \"namespace\": \"*\"}** selector helps SREs monitor authorization events to detect unusual access patterns or potential security breaches. \n\n**Alert Thresholds:**  \n- Trigger an alert if the rate of authorization failures (e.g., denied RBAC checks) exceeds a defined threshold such as 5 failures per minute, indicating possible misconfigurations or unauthorized access attempts.  \n- Conversely, an unusually low volume of authorization logs may suggest logging issues or disabled auditing, which could mask security incidents.\n\n**Impact:**  \n- High authorization failure rates can signal attempted privilege escalations, misconfigured permissions, or compromised accounts, requiring immediate investigation.  \n- Low or absent authorization logs reduce visibility into access control enforcement, increasing security risk.\n\n**Example Usage:**  \n- Dashboard: Visualize the count of authorization successes vs failures over time using a Loki query like:  \n  `count_over_time({message=~\"authorization.*\", namespace=~\".*\"} |= \"denied\" [5m])`  \n- Alert Rule: Trigger when denied authorization events exceed 5 per minute:  \n  `sum(rate({message=~\"authorization.*\", namespace=~\".*\"} |= \"denied\" [1m])) > 5`  \n\nThis enables proactive detection of access control anomalies and supports compliance auditing across Kubernetes namespaces."
  },
  {
    "query": {
      "message": "latency*",
      "k8s_app": "*"
    },
    "description": "This log captures latency metrics for all Kubernetes applications, measuring response times in milliseconds. It tracks the time taken for requests to be processed, enabling detection of performance degradation. The data supports monitoring user experience and helps ensure applications meet defined SLA targets."
  },
  {
    "query": {
      "message": "latency*",
      "service": "*"
    },
    "description": "Service latency logs capture API response times, service performance metrics, and throughput data for all Kubernetes services, enabling monitoring of system responsiveness and capacity. Alerts should trigger when latency exceeds defined thresholds (e.g., 95th percentile response time > 500ms sustained over 5 minutes), indicating potential performance degradation or resource bottlenecks. High latency values can lead to poor user experience and increased error rates, while consistently low latency suggests healthy service performance. Example alert rule: `sum(rate({message=~\"latency.*\", service=~\".*\"}[5m])) by (service) > 500` triggers when average latency per service exceeds 500ms. This data can be visualized in dashboards showing latency percentiles over time per service to quickly identify trends and anomalies."
  },
  {
    "query": {
      "message": "throughput*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application throughput logs capture real-time request rates, capacity utilization, and performance metrics for all k8s applications, enabling SREs to monitor application scalability and plan capacity effectively. Alerts should be triggered when throughput drops below a defined minimum threshold (indicating potential service degradation) or exceeds maximum capacity limits (signaling overload risk). High throughput values may indicate healthy demand or potential resource saturation, while low throughput could reflect service outages or reduced user activity. For example, an alert rule might trigger if the average request rate for **{\"k8s_app\": \"*\"}** falls below 100 requests per minute for 5 consecutive minutes, or if CPU utilization exceeds 80% alongside sustained high throughput, indicating scaling is needed. Dashboards can visualize throughput trends over time per application to correlate performance with resource usage and detect anomalies early."
  },
  {
    "query": {
      "message": "throughput*",
      "service": "*"
    },
    "description": "These logs measure the request processing rate of each Kubernetes service, expressed in requests per second (RPS). They track capacity utilization as a percentage of allocated resources used during processing. Performance metrics include latency and error rates to assess service responsiveness and reliability. This data supports monitoring service scalability and informs capacity planning decisions."
  },
  {
    "query": {
      "message": "audit*",
      "k8s_app": "*"
    },
    "description": "Kubernetes audit logs capturing security-related events for all Kubernetes applications. This includes user access attempts, configuration changes, and permission modifications. Events are recorded as discrete log entries, each representing a single security or compliance action. Data is measured by the count of audit log events generated per application."
  },
  {
    "query": {
      "message": "audit*",
      "namespace": "*"
    },
    "description": "Namespace audit logs. Security events, access tracking, and change monitoring across all Kubernetes namespaces. Security auditing and compliance reporting."
  },
  {
    "query": {
      "message": "compliance*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application compliance logs capture audit events, regulatory compliance checks, and governance monitoring across all k8s apps. These logs help SREs ensure that applications meet security and regulatory standards by tracking compliance-related activities. Alerts should be triggered when compliance violations or audit failures exceed a defined threshold (e.g., more than 5 violations within 10 minutes), indicating potential security risks or policy breaches. High volumes of compliance errors suggest increased risk of non-compliance and potential operational impact, while low or zero violations indicate healthy adherence to policies. Example alert rule: trigger if `count_over_time({message=~\"compliance.*\", k8s_app=~\".*\"}[10m]) > 5`. Dashboards can visualize trends in compliance events over time to proactively manage governance."
  },
  {
    "query": {
      "message": "compliance*",
      "namespace": "*"
    },
    "description": "Logs capturing compliance-related events across all Kubernetes namespaces. These include regulatory adherence checks, audit trails, and governance monitoring activities. Measurements reflect the count of compliance events recorded per namespace. This data supports tracking regulatory compliance and generating audit reports."
  },
  {
    "query": {
      "message": "memory*",
      "k8s_app": "*"
    },
    "description": "Logs capturing memory-related events from all Kubernetes applications. This includes memory usage measured in bytes, allocation rates, and garbage collection occurrences. Data supports resource monitoring and helps optimize application performance."
  },
  {
    "query": {
      "message": "memory*",
      "container": "*"
    },
    "description": "Logs capturing memory usage metrics for all containers, including total memory allocated, used, and freed. Measurements are reported in bytes and include details on garbage collection events. These logs enable monitoring of container memory consumption and help identify performance bottlenecks."
  },
  {
    "query": {
      "message": "cpu*",
      "k8s_app": "*"
    },
    "description": "This metric captures CPU-related log entries from all Kubernetes applications. It measures CPU usage and throttling events, reported in CPU cores and percentage utilization. The data helps monitor resource consumption and identify performance bottlenecks across k8s apps."
  },
  {
    "query": {
      "message": "cpu*",
      "container": "*"
    },
    "description": "Container CPU logs capture detailed metrics on CPU usage, throttling events, and overall performance for all containers. In Loki, **{\"message\": \"cpu*\", \"container\": \"*\"}** filters logs related to CPU activity across containers, enabling monitoring of resource consumption and identifying performance bottlenecks. Alerts should be triggered when CPU usage exceeds 80% for sustained periods (e.g., 5 minutes) or when throttling events increase, indicating resource contention. High CPU usage can lead to degraded application performance and increased latency, while unusually low CPU usage might signal underutilization or stalled processes. For example, a Grafana alert rule could use a Loki query like `sum(rate({message=~\"cpu.*\", container=~\".*\"}[1m])) by (container) > 0.8` to notify when container CPU usage surpasses 80%, helping SREs proactively manage container performance."
  },
  {
    "query": {
      "message": "database*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application database logs capturing database connections, queries, and transaction events across all k8s apps. Used in Loki to monitor database health and performance by tracking query latency, error rates, and connection counts. Alerts should trigger when query error rates exceed 5% over 5 minutes or average query latency surpasses 500ms, indicating potential database issues impacting application responsiveness. High values may signal database overload or failures causing slowdowns or outages; low values typically indicate normal operation. Example alert rule: `sum(rate({message=~\"database.*\", k8s_app=~\".*\"} |= \"error\"[5m])) / sum(rate({message=~\"database.*\", k8s_app=~\".*\"}[5m])) > 0.05` triggers when error rate exceeds 5%. Dashboards can visualize query latency trends and error counts using these logs to proactively detect and resolve database performance degradations."
  },
  {
    "query": {
      "message": "database*",
      "service": "*"
    },
    "description": "Service database logs capturing database connections, queries, and transaction events from all Kubernetes services. This log stream helps SREs monitor database performance and detect anomalies such as slow queries, connection failures, or transaction errors. Alerts should be triggered when error rates exceed 5% of total queries within a 5-minute window, or when average query latency surpasses 500ms, indicating potential service degradation or database bottlenecks. High error rates or latency can lead to application slowdowns or outages, while unusually low activity may signal connectivity issues or service downtime. Example alert rule: trigger if `rate({message=~\"database.*\", service=~\".*\"} |= \"error\"[5m]) > 0.05` or if average query duration extracted from logs exceeds 500ms. Dashboards can visualize query error rates and latency trends over time using this log filter to proactively identify and resolve database performance issues."
  },
  {
    "query": {
      "message": "api*",
      "k8s_app": "*"
    },
    "description": "Logs capturing API-related events from all Kubernetes applications. This includes details on API calls, accessed endpoints, and request processing times measured in milliseconds. It enables monitoring of API usage patterns and performance metrics across the Kubernetes environment."
  },
  {
    "query": {
      "message": "api*",
      "service": "*"
    },
    "description": "Service API logs capture detailed information about API calls, endpoints accessed, and request processing across all Kubernetes services. In Loki, the **{\"message\": \"api*\", \"service\": \"*\"}** selector aggregates these logs to monitor API usage patterns, latency, error rates, and throughput. \n\n**Alert thresholds:**  \n- Trigger an alert if the error rate exceeds 5% of total API calls within 5 minutes.  \n- Trigger an alert if the 95th percentile API response latency exceeds 500ms over 5 minutes.  \n- Trigger an alert if API call volume drops below 10% of the baseline, indicating potential service disruption.\n\n**Impact:**  \n- High error rates or latency indicate degraded API performance, potentially affecting service reliability and user experience.  \n- Low API call volume may signal service outages or connectivity issues.\n\n**Example usage:**  \n- Dashboard: Visualize API call count, error rate, and latency trends using queries like:  \n  `sum(rate({message=~\"api.*\", service=~\".*\"} |= \"error\" [5m])) by (service)`  \n- Alert rule:  \n  ```\n  alert: HighApiErrorRate\n  expr: sum(rate({message=~\"api.*\", service=~\".*\"} |= \"error\" [5m])) / sum(rate({message=~\"api.*\", service=~\".*\"} [5m])) > 0.05\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High API error rate detected for {{ $labels.service }}\"\n    description: \"API error rate exceeded 5% over the last 5 minutes.\"\n  ```"
  },
  {
    "query": {
      "message": "health*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application health logs filtered by **{\"message\": \"health*\", \"k8s_app\": \"*\"}** capture detailed events from health checks, liveness probes, and readiness probes across all Kubernetes applications. These logs help SREs monitor application availability and responsiveness by tracking probe success and failure rates. Alerts should be triggered when the failure rate of these probes exceeds a defined threshold (e.g., >5% failures over 5 minutes), indicating potential application instability or downtime. High failure rates often signal degraded service health or readiness issues, potentially leading to user impact, while consistently low failure rates confirm stable and responsive applications. For example, a Grafana dashboard panel can use this filter to display the ratio of failed to total health probes over time, and an alert rule can be configured to fire when the failure ratio surpasses the threshold, enabling proactive incident response."
  },
  {
    "query": {
      "message": "health*",
      "service": "*"
    },
    "description": "Service health logs capture detailed information about the status and performance of Kubernetes services, including health checks, liveness probes, and readiness probes. In Loki, the **{\"message\": \"health*\", \"service\": \"*\"}** query filters logs related to these probes across all services, enabling SREs to monitor service availability and responsiveness in real time.  \n\n**Alerting thresholds:**  \n- Trigger an alert if the failure rate of health checks exceeds 5% over a 5-minute window.  \n- Trigger an alert if readiness or liveness probes fail consecutively 3 times within 1 minute.  \n\n**Impact:**  \n- High failure rates or frequent probe failures indicate potential service instability or downtime, affecting application availability and user experience.  \n- Low or zero failures confirm healthy service operation and readiness to serve traffic.  \n\n**Example usage:**  \n- Dashboard panel query:  \n  ```  \n  sum by (service) (rate({message=~\"health.*\", service=~\".*\"} |= \"fail\" [5m]))  \n  ```  \n  This shows the failure rate of health checks per service over the last 5 minutes.  \n\n- Alert rule example (Prometheus-style):  \n  ```  \n  alert: ServiceHealthCheckFailures  \n  expr: sum by (service) (rate({message=~\"health.*\", service=~\".*\"} |= \"fail\" [5m])) > 0.05  \n  for: 5m  \n  labels: { severity=\"critical\" }  \n  annotations: { summary=\"High health check failure rate for {{ $labels.service }}\" }  \n  ```"
  },
  {
    "query": {
      "message": "deployment*",
      "k8s_app": "*"
    },
    "description": "Logs capturing deployment-related events for all Kubernetes applications identified by the label **k8s_app**. This includes records of deployment creations, rolling updates, and version changes. Each log entry measures event occurrences as discrete timestamped records without aggregated units. These logs enable monitoring of deployment status and tracking of release activities across Kubernetes apps."
  },
  {
    "query": {
      "message": "deployment*",
      "namespace": "*"
    },
    "description": "Namespace deployment logs capture events related to Kubernetes deployments across all namespaces, including deployment creations, rolling updates, version changes, and rollout statuses. This log stream helps SREs monitor deployment health and track release progress in real time.\n\n**Purpose:**  \nTo provide visibility into deployment lifecycle events, enabling detection of failed or stalled rollouts, frequent redeployments, or unexpected version changes that may impact application stability.\n\n**Alert Thresholds:**  \n- Alert if the number of failed deployment events exceeds a defined threshold (e.g., >3 failures within 5 minutes).  \n- Alert on unusually high deployment frequency (e.g., >5 deployments per namespace per hour), which may indicate instability or automation issues.  \n- Alert if no deployment events occur for a critical service within an expected timeframe, signaling potential pipeline or monitoring gaps.\n\n**Impact:**  \n- High failure counts or frequent rollouts can indicate instability, deployment pipeline issues, or regressions affecting service availability.  \n- Low or absent deployment events may suggest stalled releases or monitoring blind spots, risking delayed detection of issues.\n\n**Example Usage:**  \n- **Dashboard:** Visualize deployment event counts by namespace and status (success/failure) over time to identify trends or anomalies.  \n- **Alert Rule (PromQL example):**  \n  ```promql\n  count_over_time({message=~\"deployment.*\", namespace=~\".*\"} |= \"failed\" [5m]) > 3\n  ```  \n  This triggers an alert if more than 3 deployment failures occur within 5 minutes across all namespaces."
  },
  {
    "query": {
      "message": "scaling*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application scaling logs capturing auto-scaling events, replica count changes, and capacity adjustments for all k8s apps. In Loki, this stream helps SREs monitor scaling behavior to ensure applications meet demand without overprovisioning. Alerts should trigger when scaling events exceed predefined thresholds, such as rapid consecutive scale-ups or scale-downs within a short period (e.g., more than 3 scaling events in 5 minutes), indicating instability or resource pressure. High scaling activity may signal fluctuating load or misconfigured autoscalers, potentially impacting application availability or cost; low scaling activity during high load could indicate insufficient capacity. Example alert rule: trigger if count_over_time({message=~\"scaling.*\", k8s_app=~\".*\"}[5m]) > 3. Dashboards can visualize scaling event frequency over time to correlate with application performance metrics."
  },
  {
    "query": {
      "message": "scaling*",
      "namespace": "*"
    },
    "description": "Namespace scaling logs capture events related to auto-scaling activities, including replica count changes and capacity adjustments, across all Kubernetes namespaces. These logs help SREs monitor scaling behavior to ensure application availability and resource efficiency. Alerts should be triggered when scaling events occur too frequently within a short period (e.g., more than 5 scaling actions per 10 minutes), indicating potential instability or misconfiguration. High scaling event rates may signal resource pressure or flapping, risking service disruption, while low or no scaling activity during load spikes could indicate insufficient responsiveness.  \nExample alert rule snippet:  \n```\ncount_over_time({message=~\"scaling.*\", namespace=~\".*\"}[10m]) > 5\n```  \nThis can be visualized in dashboards as a time series graph showing scaling event counts per namespace, aiding capacity planning and proactive incident response."
  },
  {
    "query": {
      "message": "network*",
      "k8s_app": "*"
    },
    "description": "Kubernetes application network logs capturing network connections, routing, and connectivity events across all k8s apps. In Loki, **{\"message\": \"network*\", \"k8s_app\": \"*\"}** helps SREs monitor real-time network health and diagnose connectivity issues within the cluster. Alerts should trigger when network error rates (e.g., connection failures, timeouts) exceed a defined threshold—commonly above 5% of total network events over 5 minutes—indicating potential service disruption. High values suggest degraded network performance or outages impacting application availability, while low values indicate stable connectivity. Example alert rule: trigger if `count_over_time({message=~\"network.*\", k8s_app=~\".*\"} |= \"error\"[5m]) / count_over_time({message=~\"network.*\", k8s_app=~\".*\"}[5m]) > 0.05`. This query can also populate dashboards showing error rates and connection counts per app to quickly identify problematic services."
  },
  {
    "query": {
      "message": "network*",
      "service": "*"
    },
    "description": "Service network logs capture detailed information about network connections, routing, and connectivity events across all Kubernetes services. In Loki, **{\"message\": \"network*\", \"service\": \"*\"}** helps SREs monitor the health and performance of service-to-service communication within the cluster. Key metrics derived from these logs include connection success rates, latency, error rates, and dropped packets.\n\n**Alert thresholds:**  \n- Trigger an alert if connection error rates exceed 5% over a 5-minute window, indicating potential network instability.  \n- Alert if average network latency surpasses 200ms consistently for 5 minutes, suggesting degraded service responsiveness.  \n- High volumes of dropped packets or frequent routing failures should also raise immediate alerts.\n\n**Impact:**  \n- High error rates or latency can cause service disruptions, increased request timeouts, and degraded user experience.  \n- Low or stable values indicate healthy network connectivity and reliable service communication.\n\n**Example usage:**  \n- Dashboard: Visualize connection error rate and latency trends filtered by service to quickly identify problematic services.  \n- Alert rule (PromQL example):  \n  ```  \n  sum(rate({message=~\"network.*\", service=~\".*\"} |= \"error\" [5m])) / sum(rate({message=~\"network.*\", service=~\".*\"} [5m])) > 0.05  \n  ```  \nThis rule triggers when more than 5% of network events are errors in the last 5 minutes, prompting investigation."
  },
  {
    "query": {
      "message": "security*",
      "k8s_app": "*"
    },
    "description": "Kubernetes security logs capturing events, vulnerabilities, and threat detections across all k8s applications. This stream helps SREs monitor security posture by tracking the frequency and severity of security-related messages. Alerts should trigger when the count of security events exceeds a defined threshold (e.g., >10 events in 5 minutes), indicating potential active threats or breaches. High values suggest increased security risks requiring immediate investigation, while low or zero values indicate normal operation with no detected security issues. Example alert rule: `sum by (k8s_app) (count_over_time({message=~\"security.*\", k8s_app=~\".*\"}[5m])) > 10` triggers a critical alert for suspicious activity spikes. Dashboards can visualize event trends per app to prioritize incident response."
  },
  {
    "query": {
      "message": "security*",
      "namespace": "*"
    },
    "description": "Namespace security logs capturing security-related events, vulnerabilities, and threat detections across all Kubernetes namespaces. This log stream helps SREs monitor the security posture of the cluster by identifying suspicious activities, policy violations, or potential breaches. Alerts should be triggered when the frequency of security events exceeds a defined threshold (e.g., more than 10 critical security events within 5 minutes), indicating a possible ongoing attack or vulnerability exploitation. High volumes of security logs may signal active threats requiring immediate investigation, while low volumes typically indicate normal operation or absence of detected security incidents.  \nExample alert rule: trigger if count_over_time({message=~\"security.*\", namespace=~\".*\"}[5m]) > 10.  \nExample dashboard use: visualize the rate of security events per namespace over time to quickly identify namespaces with unusual security activity."
  },
  {
    "query": {
      "message": "config*",
      "k8s_app": "*"
    },
    "description": "Logs capturing configuration-related events for all Kubernetes applications, including changes to settings, environment variables, and deployment parameters. Each log entry records a discrete configuration update or modification event. Measurements are event counts, representing the number of configuration changes detected per application. This data enables tracking and auditing of configuration management activities across Kubernetes workloads."
  },
  {
    "query": {
      "message": "config*",
      "namespace": "*"
    },
    "description": "Namespace configuration logs capture all changes related to configuration settings, environment variables, and other configuration management activities across every Kubernetes namespace. In Loki, **{\"message\": \"config*\", \"namespace\": \"*\"}** helps SREs monitor and audit configuration changes in real time, enabling quick detection of unauthorized or erroneous updates that could impact system stability or security.\n\n**Alert Threshold:**  \nTrigger an alert if the rate of configuration change logs exceeds a predefined baseline (e.g., more than 5 changes within 5 minutes per namespace), indicating potential misconfigurations, automated deployment issues, or security incidents.\n\n**Impact of Values:**  \n- **High values:** A sudden spike in configuration changes may signal a misconfiguration rollout, automation errors, or malicious activity, potentially leading to service disruptions or security vulnerabilities.  \n- **Low values:** Normal or zero changes indicate stable configuration states, reducing risk but also possibly missing needed updates.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the count of configuration change logs over time per namespace to identify unusual spikes.  \n- **Alert Rule (PromQL-like):**  \n  ```\n  sum(rate({message=~\"config.*\", namespace=~\".*\"}[5m])) by (namespace) > 5\n  ```  \nThis rule alerts when more than 5 configuration changes occur within 5 minutes in any namespace, prompting investigation."
  }
]
