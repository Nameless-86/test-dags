{
    "test_cases_lookup_map": {
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"s3.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are spikes in S3 get operations causing increased latency or errors?\", \"retrieval_context\": [\" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - S3 bucket object deletion events. Data loss prevention, backup verification, ransomware detection, and cleanup operations. Critical for data governance, compliance, and disaster recovery monitoring.\", \" - S3 get operations. Object downloads, data retrieval, access pattern analysis, and usage monitoring. Data access auditing, performance optimization, and cost analysis.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.27310585844894975,
                        "reason": "The retrieval context mentions S3 get operations and related activities but does not directly address spikes, latency, or errors as asked in the input. The description is somewhat clear but lacks specific metrics, units, or implications related to performance or error monitoring, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question.\",\n    \"Evaluate Clarity of the Retrieval Context: Check if the description is understandable and concise in relation to the terminology and intent expressed in the Input.\",\n    \"Assess Coverage by verifying if the description explains what is measured, including units, context, and implications relevant to the Input question.\",\n    \"Determine Utility by judging whether the description contains enough information to enable a user to confidently answer the Input question based on the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.27310585844894975"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question.",
                            "Evaluate Clarity of the Retrieval Context: Check if the description is understandable and concise in relation to the terminology and intent expressed in the Input.",
                            "Assess Coverage by verifying if the description explains what is measured, including units, context, and implications relevant to the Input question.",
                            "Determine Utility by judging whether the description contains enough information to enable a user to confidently answer the Input question based on the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ec2.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Create* events for EC2 resources exceeding ten per minute?\", \"retrieval_context\": [\" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - This pattern captures EC2 resource creation events. It identifies when EC2 resources are created, including security groups, network interfaces, and other EC2 components.\", \" - This pattern captures ECS creation events. It identifies when ECS resources are created, including clusters, services, task definitions, and capacity providers.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.42406354818608394,
                        "reason": "The retrieval context partially addresses the question by identifying that the pattern captures EC2 resource creation events, including relevant components. However, it lacks clarity and conciseness regarding the metric of 'exceeding ten per minute,' does not specify units or measurement details, and omits implications or thresholds needed to effectively answer the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input question with the Retrieval Context description to assess if the description clearly addresses the question being asked.\",\n    \"Evaluate the description's clarity and conciseness to determine if the information is easily understandable in relation to the metric queried.\",\n    \"Check for coverage by confirming the description includes what is measured, units, context, and implications relevant to the input question.\",\n    \"Assess the utility by verifying if the description provides sufficient information to enable the user to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.42406354818608394"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input question with the Retrieval Context description to assess if the description clearly addresses the question being asked.",
                            "Evaluate the description's clarity and conciseness to determine if the information is easily understandable in relation to the metric queried.",
                            "Check for coverage by confirming the description includes what is measured, units, context, and implications relevant to the input question.",
                            "Assess the utility by verifying if the description provides sufficient information to enable the user to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ec2.amazonaws.com\\\", \\\"EventName\\\": \\\"Run*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the RunInstances events triggering a spike in CPU utilization on the affected hosts?\", \"retrieval_context\": [\" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - Lambda function update events. Code deployments, configuration changes, environment variable modifications, memory scaling, timeout adjustments, and function versioning. Application lifecycle management and deployment tracking.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.03208213073183839,
                        "reason": "The retrieval context does not address RunInstances events or CPU utilization; it only describes Lambda function events and ElastiCache failover events, which are unrelated to the input question about CPU spikes triggered by RunInstances events. The description lacks relevance, measurement details, units, and operational context needed to answer the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly and concisely explains key elements such as what is measured, units, and relevance to the question.\",\n    \"Evaluate the Clarity of the description by determining if it is easily understandable and free of ambiguity when related to the input question.\",\n    \"Assess Coverage by checking if the description adequately includes measurement details, units, operational context, and implications relevant to the monitoring question.\",\n    \"Judge Utility by verifying if the description provides sufficient information to effectively answer the input question; if deficiencies exist, identify specific areas for enhancement.\"\n] \n \nRubric:\nNone \n \nScore: 0.03208213073183839"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly and concisely explains key elements such as what is measured, units, and relevance to the question.",
                            "Evaluate the Clarity of the description by determining if it is easily understandable and free of ambiguity when related to the input question.",
                            "Assess Coverage by checking if the description adequately includes measurement details, units, operational context, and implications relevant to the monitoring question.",
                            "Judge Utility by verifying if the description provides sufficient information to effectively answer the input question; if deficiencies exist, identify specific areas for enhancement."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ec2.amazonaws.com\\\", \\\"EventName\\\": \\\"Terminate*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Terminate* events caused by a malicious actor attempting to compromise our infrastructure?\", \"retrieval_context\": [\" - CloudFormation stack deletion events. Infrastructure cleanup, resource decommissioning, and environment teardown. Cost optimization and resource lifecycle management.\", \" - This pattern captures CloudFront distribution deletion events. It identifies when CloudFront distributions are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2,
                        "reason": "The retrieval context describes deletion events related to infrastructure components but does not address whether Terminate* events are caused by malicious actors, failing to directly answer the monitoring question. The description is clear and concise but lacks coverage of the cause or implications related to security threats, limiting its utility for assessing malicious activity.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description addresses the question directly.\",\n    \"Evaluate Clarity in the description by checking if it is concise and easily understandable given the terminology and detail expected from the input question.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications that align with the monitoring question.\",\n    \"Determine Utility by judging whether the description provides actionable insights or sufficient detail enabling the user to answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.2"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description addresses the question directly.",
                            "Evaluate Clarity in the description by checking if it is concise and easily understandable given the terminology and detail expected from the input question.",
                            "Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications that align with the monitoring question.",
                            "Determine Utility by judging whether the description provides actionable insights or sufficient detail enabling the user to answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ec2.amazonaws.com\\\", \\\"EventName\\\": \\\"Modify*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any EC2 modifications that could have compromised the security posture of our instances?\", \"retrieval_context\": [\" - API Gateway update events. API modifications, configuration changes, and deployment updates. API versioning, performance tuning, and security policy updates.\", \" - IAM update events. Permission modifications, credential changes, policy updates, and access control adjustments. Privilege escalation detection and security policy enforcement monitoring.\", \" - This pattern captures EC2 modification events. It identifies when EC2 resources are modified, including security group rules, instance attributes, and network configurations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7499999999999999,
                        "reason": "The retrieval context directly addresses EC2 modification events relevant to security posture by mentioning changes to security group rules, instance attributes, and network configurations, which are key to assessing security impact. The language is clear and concise, making it understandable. However, the description lacks explicit details on units, measurement methods, or implications of these modifications, limiting full coverage and utility for confidently answering the question about potential security compromises.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) against the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description uses concise, unambiguous language understandable within the context of the question.\",\n    \"Assess Coverage by verifying the description explains what is measured, units used, relevant context, and implications, ensuring it fully supports the question.\",\n    \"Judge Utility by determining whether the description provides enough information to confidently answer the input question, considering both completeness and relevance.\"\n] \n \nRubric:\nNone \n \nScore: 0.7499999999999999"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) against the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description uses concise, unambiguous language understandable within the context of the question.",
                            "Assess Coverage by verifying the description explains what is measured, units used, relevant context, and implications, ensuring it fully supports the question.",
                            "Judge Utility by determining whether the description provides enough information to confidently answer the input question, considering both completeness and relevance."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"lambda.amazonaws.com\\\", \\\"EventName\\\": \\\"Put*\\\"}\", \"hyperparameters\": null, \"input\": \"Are concurrency settings for the lambda function being exceeded, causing potential performance issues?\", \"retrieval_context\": [\" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - Lambda function update events. Code deployments, configuration changes, environment variable modifications, memory scaling, timeout adjustments, and function versioning. Application lifecycle management and deployment tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.1989013057855165,
                        "reason": "The input question about concurrency settings exceeding limits and causing performance issues is not addressed by the retrieval context, which focuses on creation, invocation, and update events without mentioning concurrency or performance implications. The description lacks clarity and coverage regarding what is measured, units, or implications related to concurrency, limiting its utility in answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) to the Retrieval Context (monitoring question) to ensure the description addresses the user's query.\",\n    \"Evaluate the Input for clarity by checking if the description is concise, free of ambiguity, and easy to understand.\",\n    \"Assess the description's coverage by verifying it explains what is measured, includes units, provides context, and describes implications relevant to the question.\",\n    \"Judge the utility by determining if the description enables the user to confidently answer the monitoring question; if not, identify gaps between the Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.1989013057855165"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) to the Retrieval Context (monitoring question) to ensure the description addresses the user's query.",
                            "Evaluate the Input for clarity by checking if the description is concise, free of ambiguity, and easy to understand.",
                            "Assess the description's coverage by verifying it explains what is measured, includes units, provides context, and describes implications relevant to the question.",
                            "Judge the utility by determining if the description enables the user to confidently answer the monitoring question; if not, identify gaps between the Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"iam.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any IAM update events indicating privilege escalation or security policy changes?\", \"retrieval_context\": [\" - IAM policy attachment events. Permission assignment, access grant operations, and privilege escalation. Critical security monitoring for policy distribution and access control changes.\", \" - IAM update events. Permission modifications, credential changes, policy updates, and access control adjustments. Privilege escalation detection and security policy enforcement monitoring.\", \" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7268941409651012,
                        "reason": "The Retrieval Context addresses the Input question by listing relevant IAM update events related to privilege escalation and security policy changes, including permission modifications, policy updates, and access control adjustments. It is fairly clear and concise, highlighting key event types and their security implications. However, it lacks explicit details on measurement units, specific event counts or thresholds, and contextual implications for interpreting these events, which limits full coverage and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question\u2019s focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is easy to understand and concise, particularly in how it explains the metric related to the Input question.\",\n    \"Assess Coverage by verifying that the Retrieval Context includes all necessary details: what is measured, units, context, and implications relevant to the Input.\",\n    \"Determine Utility by judging whether the Retrieval Context provides sufficient information to confidently answer the Input question; if gaps exist, consider improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.7268941409651012"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question\u2019s focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is easy to understand and concise, particularly in how it explains the metric related to the Input question.",
                            "Assess Coverage by verifying that the Retrieval Context includes all necessary details: what is measured, units, context, and implications relevant to the Input.",
                            "Determine Utility by judging whether the Retrieval Context provides sufficient information to confidently answer the Input question; if gaps exist, consider improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"s3.amazonaws.com\\\", \\\"EventName\\\": \\\"Put*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the S3 Put operations exceeding their normal rate, indicating a potential issue with data ingestion or object uploads?\", \"retrieval_context\": [\" - S3 bucket object deletion events. Data loss prevention, backup verification, ransomware detection, and cleanup operations. Critical for data governance, compliance, and disaster recovery monitoring.\", \" - S3 get operations. Object downloads, data retrieval, access pattern analysis, and usage monitoring. Data access auditing, performance optimization, and cost analysis.\", \" - S3 put operations. Object uploads, bucket policy changes, configuration modifications, and data ingestion. Data pipeline monitoring, backup operations, and content management tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.35322443433526596,
                        "reason": "The description mentions S3 put operations and related activities but lacks clarity and detail on what exactly is measured, such as the rate or units of put operations. It does not explicitly address whether the put operations are exceeding normal rates or provide thresholds or implications related to data ingestion issues. The retrieval context includes relevant topics but is too general to directly answer the monitoring question about abnormal put operation rates.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the metric description to determine if it is easily understandable and concise in relation to the monitoring question provided.\",\n    \"Assess whether the description adequately covers what is measured, including units, relevant context, and possible implications, ensuring it aligns with the user's monitoring intent.\",\n    \"Determine the utility of the description by checking if it provides sufficient information to directly answer the monitoring question based on the retrieval context provided.\",\n    \"Compare input and retrieval context to identify any gaps or mismatches that could affect accurate interpretation and usefulness of the metric description for the given question.\"\n] \n \nRubric:\nNone \n \nScore: 0.35322443433526596"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the metric description to determine if it is easily understandable and concise in relation to the monitoring question provided.",
                            "Assess whether the description adequately covers what is measured, including units, relevant context, and possible implications, ensuring it aligns with the user's monitoring intent.",
                            "Determine the utility of the description by checking if it provides sufficient information to directly answer the monitoring question based on the retrieval context provided.",
                            "Compare input and retrieval context to identify any gaps or mismatches that could affect accurate interpretation and usefulness of the metric description for the given question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"lambda.amazonaws.com\\\", \\\"EventName\\\": \\\"Invoke*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Lambda function invocation events that could indicate a resource-intensive process or cold start issue?\", \"retrieval_context\": [\" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - This pattern captures RDS start events. It identifies when RDS instances are started, which can indicate cost optimization or service restoration.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.407300894822107,
                        "reason": "The Retrieval Context mentions Lambda function invocation events and their relevance to performance monitoring and cold start detection, which aligns with the Input question about spikes indicating resource-intensive processes or cold starts. However, the description lacks specific details on what is measured, units, or how spikes are identified, limiting clarity and completeness. The context is somewhat concise but could be improved by explicitly explaining how invocation event spikes are detected and their implications, enhancing utility for the user.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate whether the Input (monitoring question) and Retrieval Context (metric description) are clearly connected and relevant to each other.\",\n    \"Assess the clarity of the Retrieval Context in explaining the metric, ensuring it is concise and understandable in relation to the Input question.\",\n    \"Check the coverage of the Retrieval Context to verify it includes what is measured, units, context, and implications necessary to answer the Input question.\",\n    \"Determine the utility of the Retrieval Context for effectively helping a user address the Input question, providing improvement suggestions if the description scores below 8.\"\n] \n \nRubric:\nNone \n \nScore: 0.407300894822107"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate whether the Input (monitoring question) and Retrieval Context (metric description) are clearly connected and relevant to each other.",
                            "Assess the clarity of the Retrieval Context in explaining the metric, ensuring it is concise and understandable in relation to the Input question.",
                            "Check the coverage of the Retrieval Context to verify it includes what is measured, units, context, and implications necessary to answer the Input question.",
                            "Determine the utility of the Retrieval Context for effectively helping a user address the Input question, providing improvement suggestions if the description scores below 8."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"s3.amazonaws.com\\\", \\\"EventName\\\": \\\"List*\\\"}\", \"hyperparameters\": null, \"input\": \"Are S3 list operations being used for data exploration or security reconnaissance?\", \"retrieval_context\": [\" - S3 bucket object deletion events. Data loss prevention, backup verification, ransomware detection, and cleanup operations. Critical for data governance, compliance, and disaster recovery monitoring.\", \" - S3 get operations. Object downloads, data retrieval, access pattern analysis, and usage monitoring. Data access auditing, performance optimization, and cost analysis.\", \" - S3 list operations. Bucket enumeration, object discovery, directory browsing, and inventory management. Data exploration, security reconnaissance detection, and access pattern analysis.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6777299871492541,
                        "reason": "The description mentions S3 list operations and their association with bucket enumeration, object discovery, data exploration, and security reconnaissance detection, which aligns well with the monitoring question. However, it lacks explicit details on units, measurement context, and practical implications that would better support answering whether these operations are specifically used for data exploration or security reconnaissance. Adding clearer distinctions or examples of how to differentiate these uses would improve utility and clarity.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Retrieval Context) with the monitoring question (Input) to evaluate if the description clearly and concisely explains what is being measured and is understandable in the context of the question.\",\n    \"Assess whether the description provides adequate coverage, including details on units, measurement context, and practical implications relevant to answering the question.\",\n    \"Determine if the description\u2019s content directly supports the user's ability to answer the monitoring question effectively, focusing on its utility.\",\n    \"Evaluate both Input and Retrieval Context together to ensure the description is aligned with the question\u2019s intent; if there are gaps or ambiguities, suggest specific improvements to clarify or expand the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.6777299871492541"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Retrieval Context) with the monitoring question (Input) to evaluate if the description clearly and concisely explains what is being measured and is understandable in the context of the question.",
                            "Assess whether the description provides adequate coverage, including details on units, measurement context, and practical implications relevant to answering the question.",
                            "Determine if the description\u2019s content directly supports the user's ability to answer the monitoring question effectively, focusing on its utility.",
                            "Evaluate both Input and Retrieval Context together to ensure the description is aligned with the question\u2019s intent; if there are gaps or ambiguities, suggest specific improvements to clarify or expand the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"iam.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CreateRolePolicyGroup events indicating a potential security risk or unauthorized access?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - Security Hub findings and compliance events. Security posture assessment, compliance monitoring, security control evaluation, and risk management. Centralized security governance and audit reporting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3320821300824607,
                        "reason": "The retrieval context partially addresses the input by mentioning IAM user role policy group creation events and their importance for security and compliance, which relates to CreateRolePolicyGroup events. However, it lacks specific details on how these events indicate potential security risks or unauthorized access, and does not clarify measurement parameters, units, or the exact implications for monitoring. The description is somewhat ambiguous and does not provide sufficient information for a user to confidently assess security risks from these events.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent and scope.\",\n    \"Evaluate clarity by checking if the description uses concise language and avoids ambiguity while adequately detailing what is measured and how.\",\n    \"Assess coverage by verifying the description includes measurement parameters, units, context of measurement, and implications relevant to the input question.\",\n    \"Determine utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; suggest improvements if any element is missing or unclear.\"\n] \n \nRubric:\nNone \n \nScore: 0.3320821300824607"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent and scope.",
                            "Evaluate clarity by checking if the description uses concise language and avoids ambiguity while adequately detailing what is measured and how.",
                            "Assess coverage by verifying the description includes measurement parameters, units, context of measurement, and implications relevant to the input question.",
                            "Determine utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; suggest improvements if any element is missing or unclear."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"iam.amazonaws.com\\\", \\\"EventName\\\": \\\"Detach*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there a large number of IAM policy detachments indicating potential security or compliance issues?\", \"retrieval_context\": [\" - IAM policy attachment events. Permission assignment, access grant operations, and privilege escalation. Critical security monitoring for policy distribution and access control changes.\", \" - IAM policy detachment events. Permission revocation, access removal, and privilege reduction. Security incident response and compliance enforcement monitoring.\", \" - IAM update events. Permission modifications, credential changes, policy updates, and access control adjustments. Privilege escalation detection and security policy enforcement monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6883687942182337,
                        "reason": "The description addresses IAM policy detachment events relevant to the question about detachments indicating security or compliance issues, covering permission revocation and access removal. The language is mostly clear and concise, though some jargon like 'privilege reduction' may be less accessible. It explains the metric (detachment events) and its security context but lacks explicit mention of units or quantitative thresholds. The description implies security implications but does not provide actionable insights or guidance for interpreting the data to answer the monitoring question confidently.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description addresses the specific monitoring question asked.\",\n    \"Evaluate the description\u2019s clarity by checking if the language is concise, free of jargon, and easy to understand in relation to the question.\",\n    \"Assess coverage by verifying if the description explains what metric is measured, the units used, the relevant context, and the implications related to the question.\",\n    \"Determine utility by judging whether the description provides actionable insights or enough information for a user to confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6883687942182337"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description addresses the specific monitoring question asked.",
                            "Evaluate the description\u2019s clarity by checking if the language is concise, free of jargon, and easy to understand in relation to the question.",
                            "Assess coverage by verifying if the description explains what metric is measured, the units used, the relevant context, and the implications related to the question.",
                            "Determine utility by judging whether the description provides actionable insights or enough information for a user to confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"iam.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a legitimate identity cleanup or a potential security incident?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - Lambda function deletion events. Application cleanup, cost optimization, security incident response, and resource decommissioning. Critical for audit trails and compliance monitoring.\", \" - This pattern captures Secrets Manager deletion events. It identifies when secrets are removed, which can indicate cleanup operations or potential credential loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.785195280196831,
                        "reason": "The Retrieval Context directly addresses the Input question by describing Delete* events related to IAM resources, Secrets Manager, and Lambda functions, linking them to identity cleanup and security incident response. The descriptions are concise and use terminology consistent with the Input, such as 'identity cleanup' and 'security incident response.' However, while the context mentions the types of events and their implications, it lacks detailed explanation of units, specific measurement metrics, or explicit guidance on distinguishing legitimate cleanup from incidents, slightly limiting full coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if the description is concise and easy to understand in relation to the terminology used in the Input.\",\n    \"Assess the coverage of the Retrieval Context by verifying if it explains what is measured, units, context, and implications needed to answer the Input question.\",\n    \"Determine the utility by judging whether the Retrieval Context provides sufficient information to effectively help a user respond to the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.785195280196831"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question.",
                            "Evaluate the clarity of the Retrieval Context by checking if the description is concise and easy to understand in relation to the terminology used in the Input.",
                            "Assess the coverage of the Retrieval Context by verifying if it explains what is measured, units, context, and implications needed to answer the Input question.",
                            "Determine the utility by judging whether the Retrieval Context provides sufficient information to effectively help a user respond to the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"lambda.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CreateFunction events indicating a recent serverless compute deployment?\", \"retrieval_context\": [\" - CloudFormation stack creation events. Infrastructure provisioning, resource deployment, and environment setup. Infrastructure as Code monitoring and deployment tracking.\", \" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - Step Functions workflow events. Serverless workflow execution, state machine operations, and business process automation. Workflow orchestration and process monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7898565800120066,
                        "reason": "The description clearly outlines Lambda function creation events relevant to serverless compute deployment, including key aspects like function code uploads and configuration, which aligns well with the monitoring question about CreateFunction events. It provides contextual relevance by linking these events to application deployment monitoring and cost tracking. However, it does not explicitly mention the specific event name 'CreateFunction' or the timing aspect ('recent'), which slightly limits clarity and completeness for directly answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Step 1: Evaluate the clarity of the metric description (Retrieval Context) ensuring it is understandable and concise relative to the monitoring question (Input).\",\n    \"Step 2: Assess the coverage of the description by verifying it explains what is measured, units, contextual relevance, and implications directly related to the user's question.\",\n    \"Step 3: Determine the utility by checking if the description enables the user to effectively answer the given monitoring question based on the provided context.\",\n    \"Step 4: Compare the description and question for alignment, ensuring the context sufficiently supports the input to avoid ambiguity or missing information.\"\n] \n \nRubric:\nNone \n \nScore: 0.7898565800120066"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Step 1: Evaluate the clarity of the metric description (Retrieval Context) ensuring it is understandable and concise relative to the monitoring question (Input).",
                            "Step 2: Assess the coverage of the description by verifying it explains what is measured, units, contextual relevance, and implications directly related to the user's question.",
                            "Step 3: Determine the utility by checking if the description enables the user to effectively answer the given monitoring question based on the provided context.",
                            "Step 4: Compare the description and question for alignment, ensuring the context sufficiently supports the input to avoid ambiguity or missing information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"lambda.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a recent spike in UnHealthyHostCount?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - Lambda function deletion events. Application cleanup, cost optimization, security incident response, and resource decommissioning. Critical for audit trails and compliance monitoring.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3339054986440246,
                        "reason": "The description identifies different Delete* events and their contexts but does not explicitly link these events to the UnHealthyHostCount spike, limiting its utility in answering the monitoring question. While the description is clear and provides relevant context about deletion events, it lacks coverage of how these events impact or correlate with UnHealthyHostCount, missing critical implications needed for full alignment.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description against the retrieval context to ensure the description clearly aligns with and supports the monitoring question provided.\",\n    \"Evaluate clarity by checking if the description is understandable and concise, considering how well it complements the context and question together.\",\n    \"Assess coverage by verifying that the description explains what is measured, including units, context, and implications, in relation to the retrieval context and input question.\",\n    \"Judge utility by determining whether the description can effectively help answer the monitoring question within the given retrieval context; if not, identify specific information gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.3339054986440246"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description against the retrieval context to ensure the description clearly aligns with and supports the monitoring question provided.",
                            "Evaluate clarity by checking if the description is understandable and concise, considering how well it complements the context and question together.",
                            "Assess coverage by verifying that the description explains what is measured, including units, context, and implications, in relation to the retrieval context and input question.",
                            "Judge utility by determining whether the description can effectively help answer the monitoring question within the given retrieval context; if not, identify specific information gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"lambda.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Lambda function update events that could be causing a disruption to the application?\", \"retrieval_context\": [\" - Lambda function deletion events. Application cleanup, cost optimization, security incident response, and resource decommissioning. Critical for audit trails and compliance monitoring.\", \" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - Lambda function update events. Code deployments, configuration changes, environment variable modifications, memory scaling, timeout adjustments, and function versioning. Application lifecycle management and deployment tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6969171281507195,
                        "reason": "The retrieval context addresses Lambda function update events by listing relevant types of updates and their relation to application lifecycle and deployment tracking, which aligns with the question about disruptions caused by update events. However, the description lacks explicit mention of how these updates might cause disruptions or how to detect such disruptions, limiting clarity and utility for confidently answering the question. Including specific metrics or indicators of disruption would improve alignment.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to assess if the description clearly addresses the question\u2019s intent.\",\n    \"Evaluate the clarity of the metric description, ensuring it is concise and understandable in the context of the question provided.\",\n    \"Check coverage by verifying the description explains what is measured, including units, relevant context, and possible implications related to the question.\",\n    \"Assess utility by determining whether the description provides sufficient information to confidently answer the monitoring question; if not, suggest specific details to improve alignment.\"\n] \n \nRubric:\nNone \n \nScore: 0.6969171281507195"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to assess if the description clearly addresses the question\u2019s intent.",
                            "Evaluate the clarity of the metric description, ensuring it is concise and understandable in the context of the question provided.",
                            "Check coverage by verifying the description explains what is measured, including units, relevant context, and possible implications related to the question.",
                            "Assess utility by determining whether the description provides sufficient information to confidently answer the monitoring question; if not, suggest specific details to improve alignment."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"iam.amazonaws.com\\\", \\\"EventName\\\": \\\"Attach*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any AttachUserPolicy events indicating a sudden change in permissions?\", \"retrieval_context\": [\" - IAM policy attachment events. Permission assignment, access grant operations, and privilege escalation. Critical security monitoring for policy distribution and access control changes.\", \" - IAM policy detachment events. Permission revocation, access removal, and privilege reduction. Security incident response and compliance enforcement monitoring.\", \" - IAM update events. Permission modifications, credential changes, policy updates, and access control adjustments. Privilege escalation detection and security policy enforcement monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.41067139104722994,
                        "reason": "The Retrieval Context broadly covers IAM policy attachment, update, and detachment events related to permissions and privilege changes, which partially addresses the question about AttachUserPolicy events and sudden permission changes. However, it lacks specific mention of 'AttachUserPolicy' events or indicators of sudden changes, reducing clarity and completeness. The context is somewhat clear but not sufficiently detailed to confidently answer the question about sudden permission changes in AttachUserPolicy events.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.\",\n    \"Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the terminology and intent expressed in the Input.\",\n    \"Assess if the Retrieval Context covers all necessary elements: what is measured, units, context, and implications, ensuring these aspects relate directly to the Input's question.\",\n    \"Determine the utility of the Retrieval Context by verifying if it provides sufficient information to accurately and confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.41067139104722994"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.",
                            "Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the terminology and intent expressed in the Input.",
                            "Assess if the Retrieval Context covers all necessary elements: what is measured, units, context, and implications, ensuring these aspects relate directly to the Input's question.",
                            "Determine the utility of the Retrieval Context by verifying if it provides sufficient information to accurately and confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ec2.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there multiple Delete* events occurring in a short period, indicating potential resource removal issues?\", \"retrieval_context\": [\" - This pattern captures ECS deletion events. It identifies when ECS resources are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures KMS deletion events. It identifies when KMS keys are removed, which can indicate cleanup operations or potential data access loss incidents.\", \" - This pattern captures Systems Manager deletion events. It identifies when SSM resources are removed, which can indicate cleanup operations or potential configuration loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5536099276519941,
                        "reason": "The Retrieval Context addresses the presence of multiple Delete* events by describing deletion patterns for KMS, ECS, and Systems Manager resources, which relates to the Input question about resource removal issues. However, it lacks explicit mention of timing or frequency ('in a short period'), units, or metrics to measure event occurrence rates, limiting clarity and utility for confidently assessing potential issues. The language is generally clear but could be more concise and directly tied to the question's focus on multiple events and their implications.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the key components of the question clearly and directly.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it uses concise language and is easy to understand, ensuring it relates well to the Input.\",\n    \"Assess the coverage in the Retrieval Context by confirming it explains what is being measured, includes units, provides relevant context, and implications directly tied to the Input question.\",\n    \"Determine the utility by verifying if the Retrieval Context enables a user to confidently answer the Input question; if any gaps exist, note concrete improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.5536099276519941"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the key components of the question clearly and directly.",
                            "Evaluate the clarity of the Retrieval Context by checking if it uses concise language and is easy to understand, ensuring it relates well to the Input.",
                            "Assess the coverage in the Retrieval Context by confirming it explains what is being measured, includes units, provides relevant context, and implications directly tied to the Input question.",
                            "Determine the utility by verifying if the Retrieval Context enables a user to confidently answer the Input question; if any gaps exist, note concrete improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"s3.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in S3 bucket object deletion events indicating potential data loss or malicious activity?\", \"retrieval_context\": [\" - S3 bucket object deletion events. Data loss prevention, backup verification, ransomware detection, and cleanup operations. Critical for data governance, compliance, and disaster recovery monitoring.\", \" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5220021217168154,
                        "reason": "The retrieval context partially addresses the input question by mentioning S3 bucket object deletion events and their relevance to data loss prevention and ransomware detection, which aligns with the concern about spikes indicating potential data loss or malicious activity. However, it lacks specific details on what is measured (e.g., frequency or volume of deletion events), units, or how spikes are detected, limiting clarity and utility. The inclusion of DynamoDB and ELB deletion events is less relevant and may cause ambiguity. Overall, the description provides some context but does not fully enable confident assessment of spikes in S3 deletion events.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly and concisely addresses the question's focus.\",\n    \"Evaluate coverage by checking if the description explains what is measured, including units, relevant context, and implications, demonstrating relevance to the input question.\",\n    \"Assess the clarity of the description by verifying it is understandable without ambiguity and uses concise language to support answering the input question.\",\n    \"Determine utility by confirming that the metric description enables a user to confidently answer the input question using the information provided; if not, identify specific missing details or ambiguities.\"\n] \n \nRubric:\nNone \n \nScore: 0.5220021217168154"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly and concisely addresses the question's focus.",
                            "Evaluate coverage by checking if the description explains what is measured, including units, relevant context, and implications, demonstrating relevance to the input question.",
                            "Assess the clarity of the description by verifying it is understandable without ambiguity and uses concise language to support answering the input question.",
                            "Determine utility by confirming that the metric description enables a user to confidently answer the input question using the information provided; if not, identify specific missing details or ambiguities."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"s3.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are S3 bucket creation events exceeding ten per minute, indicating a potential data storage provisioning issue?\", \"retrieval_context\": [\" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - S3 bucket creation events. Data storage provisioning, bucket configurations, region selection, versioning settings, and access control setup. Data governance and compliance monitoring.\", \" - S3 bucket object deletion events. Data loss prevention, backup verification, ransomware detection, and cleanup operations. Critical for data governance, compliance, and disaster recovery monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.30373268852023205,
                        "reason": "The retrieval context mentions S3 bucket creation events and related aspects like data storage provisioning and configurations, which aligns with the input question. However, it lacks specific details on what is measured (e.g., event counts per minute), units, or thresholds such as 'exceeding ten per minute.' The description is somewhat vague and does not clearly explain the implications or how to interpret the metric in relation to potential provisioning issues, limiting its utility for directly answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (monitoring question) and retrieval context (metric description) to ensure the description addresses the specific question clearly and directly.\",\n    \"Evaluate clarity by checking if the retrieval context uses concise language that a user familiar with cloud telemetry can easily understand in relation to the input question.\",\n    \"Assess coverage by verifying that the description explains what is measured, units, context, and implications necessary to answer the input question.\",\n    \"Judge utility by determining if the retrieval context equips the user with enough information to resolve the input monitoring question, and suggest improvements when the link is weak or incomplete.\"\n] \n \nRubric:\nNone \n \nScore: 0.30373268852023205"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (monitoring question) and retrieval context (metric description) to ensure the description addresses the specific question clearly and directly.",
                            "Evaluate clarity by checking if the retrieval context uses concise language that a user familiar with cloud telemetry can easily understand in relation to the input question.",
                            "Assess coverage by verifying that the description explains what is measured, units, context, and implications necessary to answer the input question.",
                            "Judge utility by determining if the retrieval context equips the user with enough information to resolve the input monitoring question, and suggest improvements when the link is weak or incomplete."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"rds.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Create* events for RDS databases exceeding ten per minute?\", \"retrieval_context\": [\" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - Redshift cluster creation events. Data warehouse provisioning, cluster configuration, and analytics infrastructure setup. Business intelligence infrastructure deployment.\", \" - This pattern captures RDS database creation events. It identifies when RDS instances are created, including database configurations and initial settings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.24378234991142017,
                        "reason": "The Retrieval Context identifies that the pattern captures RDS database creation events, which aligns with the Input's focus on Create* events for RDS databases. However, it lacks any quantitative information such as event rates, units (per minute), or thresholds like 'exceeding ten per minute.' The context is somewhat clear but does not provide sufficient coverage or actionable data to determine if the events exceed the specified rate, limiting its utility in answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent and focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise and understandable when considered alongside the Input; ambiguous or overly technical language reduces score.\",\n    \"Assess Coverage by verifying that the Retrieval Context explains what is measured, units, context, and implications sufficiently to answer the Input's question.\",\n    \"Judge Utility by determining if the Retrieval Context provides actionable or relevant information that directly helps resolve the monitoring question posed in the Input.\"\n] \n \nRubric:\nNone \n \nScore: 0.24378234991142017"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent and focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise and understandable when considered alongside the Input; ambiguous or overly technical language reduces score.",
                            "Assess Coverage by verifying that the Retrieval Context explains what is measured, units, context, and implications sufficiently to answer the Input's question.",
                            "Judge Utility by determining if the Retrieval Context provides actionable or relevant information that directly helps resolve the monitoring question posed in the Input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sns.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CreateTopic events that indicate a new SNS topic was created with an unusual configuration?\", \"retrieval_context\": [\" - This pattern captures SNS subscription events. It identifies when subscriptions are created for SNS topics, including endpoint configurations and subscription settings.\", \" - This pattern captures SNS topic creation events. It identifies when SNS topics are created, including topic configurations and initial settings.\", \" - This pattern captures SNS topic deletion events. It identifies when SNS topics are removed, which can indicate cleanup operations or potential notification loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6971465032357322,
                        "reason": "The description clearly addresses the question by identifying SNS topic creation events and mentioning topic configurations and initial settings, which relates to detecting unusual configurations. It is concise and understandable, providing relevant context about what is measured. However, it lacks explicit details on what constitutes 'unusual configuration' and does not specify units or implications, limiting full coverage and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description addresses the specific monitoring question clearly and directly.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise in relation to the question asked.\",\n    \"Assess Coverage by verifying if the description explains what is measured, including units, relevant context, and implications necessary to answer the question.\",\n    \"Judge Utility by determining if the description provides enough information for a user to confidently answer the given monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6971465032357322"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description addresses the specific monitoring question clearly and directly.",
                            "Evaluate Clarity by checking if the description is understandable and concise in relation to the question asked.",
                            "Assess Coverage by verifying if the description explains what is measured, including units, relevant context, and implications necessary to answer the question.",
                            "Judge Utility by determining if the description provides enough information for a user to confidently answer the given monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sqs.amazonaws.com\\\", \\\"EventName\\\": \\\"Send*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of SQS message sends that could indicate a performance issue or resource overload?\", \"retrieval_context\": [\" - This pattern captures SQS message receive events. It identifies when messages are received from SQS queues, useful for monitoring message processing and consumer activity.\", \" - This pattern captures SQS message send events. It identifies when messages are sent to SQS queues, useful for monitoring message flow and application activity.\", \" - This pattern captures SQS queue deletion events. It identifies when SQS queues are removed, which can indicate cleanup operations or potential message loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.36120574105772646,
                        "reason": "The retrieval context describes patterns capturing SQS message send, receive, and queue deletion events, which relate to monitoring message flow and activity. However, it does not explicitly address detecting spikes or performance/resource overload issues, nor does it provide information on units, thresholds, or implications of such spikes. The description is somewhat clear but lacks sufficient detail and utility to confidently answer the monitoring question about spikes indicating performance problems.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.\",\n    \"Evaluate clarity by checking if the metric description is understandable and concise, avoiding ambiguity in relation to the monitoring question.\",\n    \"Assess coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the question.\",\n    \"Determine utility by confirming if the description provides sufficient information to help a user confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.36120574105772646"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.",
                            "Evaluate clarity by checking if the metric description is understandable and concise, avoiding ambiguity in relation to the monitoring question.",
                            "Assess coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the question.",
                            "Determine utility by confirming if the description provides sufficient information to help a user confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"dynamodb.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events in DynamoDB indicative of a cleanup operation or a potential data loss incident?\", \"retrieval_context\": [\" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures KMS deletion events. It identifies when KMS keys are removed, which can indicate cleanup operations or potential data access loss incidents.\", \" - This pattern captures RDS database deletion events. It identifies when RDS instances are removed, which can indicate cleanup operations or potential data loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8622459331201855,
                        "reason": "The retrieval context directly addresses the question by explaining that DynamoDB deletion events capture table removals, which can indicate either cleanup operations or potential data loss incidents, showing strong relevance. The language is clear and concise, suitable for users familiar with cloud telemetry. However, the description lacks details on units, measurement specifics, or further implications, slightly limiting coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) for relevance: ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description uses clear, concise language understandable to users familiar with cloud telemetry.\",\n    \"Assess Coverage by confirming the description explains what is measured, its units, relevant context, and any implications related to the question.\",\n    \"Determine Utility by verifying if the description provides sufficient information to effectively answer the question, noting gaps or ambiguities.\"\n] \n \nRubric:\nNone \n \nScore: 0.8622459331201855"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) for relevance: ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description uses clear, concise language understandable to users familiar with cloud telemetry.",
                            "Assess Coverage by confirming the description explains what is measured, its units, relevant context, and any implications related to the question.",
                            "Determine Utility by verifying if the description provides sufficient information to effectively answer the question, noting gaps or ambiguities."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"dynamodb.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of DynamoDB Get operations that could indicate a performance issue?\", \"retrieval_context\": [\" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures DynamoDB get operations. It identifies when items are retrieved from DynamoDB tables, useful for monitoring data access patterns.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2777299861174691,
                        "reason": "The retrieval context partially addresses the question by identifying DynamoDB get operations and their use in monitoring data access patterns, which relates to detecting spikes. However, it lacks clarity and detail on what is specifically measured (e.g., counts or rates), units, or how spikes would indicate performance issues. The description is somewhat relevant but insufficiently detailed to confidently answer the monitoring question about performance-related spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to verify that the description directly addresses the question posed.\",\n    \"Evaluate Clarity by ensuring the description is understandable and concise without extraneous information, maintaining relevance to the input question.\",\n    \"Assess Coverage by checking if the description thoroughly explains what is measured, includes units, provides contextual background, and indicates implications related to the input question.\",\n    \"Determine Utility by confirming whether the description enables the user to confidently answer the monitoring question based on the provided context.\"\n] \n \nRubric:\nNone \n \nScore: 0.2777299861174691"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to verify that the description directly addresses the question posed.",
                            "Evaluate Clarity by ensuring the description is understandable and concise without extraneous information, maintaining relevance to the input question.",
                            "Assess Coverage by checking if the description thoroughly explains what is measured, includes units, provides contextual background, and indicates implications related to the input question.",
                            "Determine Utility by confirming whether the description enables the user to confidently answer the monitoring question based on the provided context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sqs.amazonaws.com\\\", \\\"EventName\\\": \\\"Receive*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Receive events indicating a potential issue with message processing?\", \"retrieval_context\": [\" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - This pattern captures SQS message receive events. It identifies when messages are received from SQS queues, useful for monitoring message processing and consumer activity.\", \" - This pattern captures SQS message send events. It identifies when messages are sent to SQS queues, useful for monitoring message flow and application activity.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5975872309114812,
                        "reason": "The description directly addresses the monitoring question by explaining that the pattern captures SQS message receive events relevant to message processing and consumer activity. However, it lacks specific details on what is measured (e.g., event counts or rates), units, or how spikes would be identified or interpreted. While the description is clear and concise, it does not fully enable a user to confidently detect spikes or understand their implications, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate the clarity of the description by checking if it is concise, understandable, and free of ambiguity in the context of the given question.\",\n    \"Assess coverage by verifying the description includes what is measured, units, context, and potential implications relevant to the monitoring question.\",\n    \"Determine the utility by judging whether the description enables a user to confidently answer the monitoring question based on the information provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.5975872309114812"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate the clarity of the description by checking if it is concise, understandable, and free of ambiguity in the context of the given question.",
                            "Assess coverage by verifying the description includes what is measured, units, context, and potential implications relevant to the monitoring question.",
                            "Determine the utility by judging whether the description enables a user to confidently answer the monitoring question based on the information provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"dynamodb.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any DynamoDB update events indicating a change to table capacity or indexing that could impact performance?\", \"retrieval_context\": [\" - Lambda function update events. Code deployments, configuration changes, environment variable modifications, memory scaling, timeout adjustments, and function versioning. Application lifecycle management and deployment tracking.\", \" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures DynamoDB update events. It identifies when DynamoDB tables are modified, including capacity changes, index updates, and configuration modifications.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7962673109309294,
                        "reason": "The Retrieval Context directly addresses DynamoDB update events related to capacity changes and index updates, aligning well with the Input's monitoring question. The description is clear and concise, explaining what is captured and its relevance to table modifications. However, it lacks explicit mention of units, detailed implications on performance, or examples, which would enhance coverage and utility for fully understanding the impact.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input's metric description with the Retrieval Context to verify they address the same metric and monitoring question.\",\n    \"Evaluate clarity by checking if the description in the Retrieval Context is concise and easy to understand in relation to the Input question.\",\n    \"Assess coverage by ensuring the Retrieval Context explains what is measured, including units, context, and implications relevant to the Input question.\",\n    \"Determine utility by confirming the description in the Retrieval Context sufficiently aids in answering the Input monitoring question, suggesting improvements if unclear or incomplete.\"\n] \n \nRubric:\nNone \n \nScore: 0.7962673109309294"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input's metric description with the Retrieval Context to verify they address the same metric and monitoring question.",
                            "Evaluate clarity by checking if the description in the Retrieval Context is concise and easy to understand in relation to the Input question.",
                            "Assess coverage by ensuring the Retrieval Context explains what is measured, including units, context, and implications relevant to the Input question.",
                            "Determine utility by confirming the description in the Retrieval Context sufficiently aids in answering the Input monitoring question, suggesting improvements if unclear or incomplete."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"rds.amazonaws.com\\\", \\\"EventName\\\": \\\"Modify*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any RDS modifications that could be causing a performance issue or data loss?\", \"retrieval_context\": [\" - API Gateway update events. API modifications, configuration changes, and deployment updates. API versioning, performance tuning, and security policy updates.\", \" - This pattern captures RDS database deletion events. It identifies when RDS instances are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures RDS modification events. It identifies when RDS instances are modified, including configuration changes, parameter group updates, and scaling operations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6622459331201853,
                        "reason": "The description directly addresses RDS modifications by identifying configuration changes, parameter updates, and scaling operations, which relate to potential performance issues. It also mentions RDS deletions linked to data loss, covering key aspects of the monitoring question. However, it lacks explicit mention of measurement units, detailed implications, and how these modifications concretely impact performance or data loss, limiting clarity and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question being asked.\",\n    \"Evaluate Clarity by checking if the description uses straightforward language and is concise enough to be easily understood in relation to the input question.\",\n    \"Assess Coverage by verifying the description includes what is measured, the measurement units, the relevant context, and potential implications, all aligned with the input question's focus.\",\n    \"Determine Utility by judging if the description provides enough relevant information to effectively answer the monitoring question posed in the input.\"\n] \n \nRubric:\nNone \n \nScore: 0.6622459331201853"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question being asked.",
                            "Evaluate Clarity by checking if the description uses straightforward language and is concise enough to be easily understood in relation to the input question.",
                            "Assess Coverage by verifying the description includes what is measured, the measurement units, the relevant context, and potential implications, all aligned with the input question's focus.",
                            "Determine Utility by judging if the description provides enough relevant information to effectively answer the monitoring question posed in the input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"dynamodb.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the CreateTable events in this metric indicative of a recent change to an existing production database?\", \"retrieval_context\": [\" - This pattern captures DynamoDB table creation events. It identifies when DynamoDB tables are created, including table configurations and initial settings.\", \" - This pattern captures DynamoDB update events. It identifies when DynamoDB tables are modified, including capacity changes, index updates, and configuration modifications.\", \" - This pattern captures RDS database creation events. It identifies when RDS instances are created, including database configurations and initial settings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3485846142279364,
                        "reason": "The description identifies that the metric captures table creation events for DynamoDB and RDS, which relates to the question about CreateTable events. However, it lacks clarity on whether these events indicate recent changes to existing production databases, does not specify units or monitoring environment details, and does not address implications for production changes, limiting its utility in fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's intent.\",\n    \"Evaluate the clarity of the description by assessing whether it uses simple, precise language relevant to the question's topic.\",\n    \"Examine coverage by verifying if the description explains what is measured, the units involved, the monitoring environment, and implications related to the question.\",\n    \"Assess utility by determining if the description provides sufficient information to effectively answer the monitoring question; if not, identify specific gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.3485846142279364"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's intent.",
                            "Evaluate the clarity of the description by assessing whether it uses simple, precise language relevant to the question's topic.",
                            "Examine coverage by verifying if the description explains what is measured, the units involved, the monitoring environment, and implications related to the question.",
                            "Assess utility by determining if the description provides sufficient information to effectively answer the monitoring question; if not, identify specific gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sqs.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Create* events for SQS queues exceeding ten per minute?\", \"retrieval_context\": [\" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - This pattern captures SQS queue creation events. It identifies when SQS queues are created, including queue configurations and initial settings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6904601896300646,
                        "reason": "The description clearly identifies that the pattern captures SQS queue creation events, which aligns with the question about Create* events for SQS queues. It explains what is measured (queue creation events) and includes relevant context such as queue configurations and initial settings. However, it lacks explicit mention of the measurement units (e.g., events per minute) and does not directly address whether the events exceed ten per minute, limiting its utility for answering the question fully.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Description (Retrieval Context) against the Question (Input) to verify that the description clearly addresses the monitoring question posed.\",\n    \"Assess Clarity by checking if the description is easily understandable and concise when read alongside the question, ensuring no ambiguity.\",\n    \"Evaluate Coverage by confirming that the description explains what is measured, including units, relevant context, and implications needed to answer the question.\",\n    \"Determine Utility by verifying that the description provides sufficient and relevant information to effectively answer the monitoring question, identifying any gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.6904601896300646"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Description (Retrieval Context) against the Question (Input) to verify that the description clearly addresses the monitoring question posed.",
                            "Assess Clarity by checking if the description is easily understandable and concise when read alongside the question, ensuring no ambiguity.",
                            "Evaluate Coverage by confirming that the description explains what is measured, including units, relevant context, and implications needed to answer the question.",
                            "Determine Utility by verifying that the description provides sufficient and relevant information to effectively answer the monitoring question, identifying any gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"rds.amazonaws.com\\\", \\\"EventName\\\": \\\"Stop*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the RDS stop events related to a larger-than-usual number of UnHealthyHostCount spikes?\", \"retrieval_context\": [\" - This pattern captures RDS database deletion events. It identifies when RDS instances are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures RDS start events. It identifies when RDS instances are started, which can indicate cost optimization or service restoration.\", \" - This pattern captures RDS stop events. It identifies when RDS instances are stopped, which can indicate cost optimization or maintenance operations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3562176500885798,
                        "reason": "The Input is clear and concise, asking about the relationship between RDS stop events and UnHealthyHostCount spikes. However, the Retrieval Context only covers RDS stop, start, and deletion events without any information on UnHealthyHostCount metrics, their measurement, units, or implications. This lack of coverage prevents effective answering of the monitoring question, indicating poor alignment and utility. To improve, the Retrieval Context should include details on UnHealthyHostCount spikes, their measurement, and how they relate to RDS events.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input description to ensure it is understandable, concise, and free from ambiguity.\",\n    \"Assess the Retrieval Context to verify it provides comprehensive coverage including what is measured, units, relevant context, and implications.\",\n    \"Determine the alignment between the Input and Retrieval Context by checking if the description enables answering the monitoring question effectively (utility).\",\n    \"If any criteria score below 8, identify specific gaps in clarity, coverage, or utility and suggest concrete improvements targeting integration between Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3562176500885798"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input description to ensure it is understandable, concise, and free from ambiguity.",
                            "Assess the Retrieval Context to verify it provides comprehensive coverage including what is measured, units, relevant context, and implications.",
                            "Determine the alignment between the Input and Retrieval Context by checking if the description enables answering the monitoring question effectively (utility).",
                            "If any criteria score below 8, identify specific gaps in clarity, coverage, or utility and suggest concrete improvements targeting integration between Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sns.amazonaws.com\\\", \\\"EventName\\\": \\\"Subscribe*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there multiple SNS subscriptions being created in a short period of time?\", \"retrieval_context\": [\" - This pattern captures SNS subscription events. It identifies when subscriptions are created for SNS topics, including endpoint configurations and subscription settings.\", \" - This pattern captures SNS topic creation events. It identifies when SNS topics are created, including topic configurations and initial settings.\", \" - This pattern captures SNS unsubscription events. It identifies when subscriptions are removed from SNS topics, which can indicate cleanup operations or endpoint changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4761635280030803,
                        "reason": "The description addresses SNS subscription events, including creation, which relates to the question about multiple subscriptions being created. However, it lacks specifics on frequency, time intervals, or how to detect multiple subscriptions in a short period, limiting clarity and utility for directly answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.\",\n    \"Evaluate the description's clarity by determining if it is concise and easy to understand in relation to the terminology used in the question.\",\n    \"Assess coverage by verifying if the description adequately explains what is measured, including units, context, and possible implications relevant to the question.\",\n    \"Judge utility by confirming whether the description provides sufficient information to directly help answer the monitoring question; if not, identify specific missing elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.4761635280030803"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.",
                            "Evaluate the description's clarity by determining if it is concise and easy to understand in relation to the terminology used in the question.",
                            "Assess coverage by verifying if the description adequately explains what is measured, including units, context, and possible implications relevant to the question.",
                            "Judge utility by confirming whether the description provides sufficient information to directly help answer the monitoring question; if not, identify specific missing elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"dynamodb.amazonaws.com\\\", \\\"EventName\\\": \\\"Put*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Put* events in DynamoDB causing an unusual spike in write traffic?\", \"retrieval_context\": [\" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures DynamoDB put operations. It identifies when items are written to DynamoDB tables, including conditional writes and batch operations.\", \" - This pattern captures DynamoDB update events. It identifies when DynamoDB tables are modified, including capacity changes, index updates, and configuration modifications.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.44803569700606777,
                        "reason": "The Retrieval Context identifies that the pattern captures DynamoDB put operations, including conditional writes and batch operations, which aligns with the Input's focus on Put* events. However, it lacks explicit information about measuring write traffic volume, units, or what constitutes an unusual spike, limiting clarity, coverage, and utility in answering whether these events cause an unusual spike.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description explicitly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable and concise, relative to the terminology and expectations set by the Input.\",\n    \"Assess Coverage by verifying that the Retrieval Context explains what is measured, including units, relevant context, and implications, directly supporting the Input question.\",\n    \"Determine Utility by confirming whether the Retrieval Context provides sufficient information to answer the Input question effectively; if not, identify gaps linking the two.\"\n] \n \nRubric:\nNone \n \nScore: 0.44803569700606777"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description explicitly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable and concise, relative to the terminology and expectations set by the Input.",
                            "Assess Coverage by verifying that the Retrieval Context explains what is measured, including units, relevant context, and implications, directly supporting the Input question.",
                            "Determine Utility by confirming whether the Retrieval Context provides sufficient information to answer the Input question effectively; if not, identify gaps linking the two."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sns.amazonaws.com\\\", \\\"EventName\\\": \\\"Publish*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of SNS publish events indicating a potential issue with notification flow?\", \"retrieval_context\": [\" - This pattern captures SNS publish events. It identifies when messages are published to SNS topics, useful for monitoring notification flow and application activity.\", \" - This pattern captures SNS topic deletion events. It identifies when SNS topics are removed, which can indicate cleanup operations or potential notification loss incidents.\", \" - This pattern captures SNS unsubscription events. It identifies when subscriptions are removed from SNS topics, which can indicate cleanup operations or endpoint changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6953350320227171,
                        "reason": "The retrieval context clearly defines the metric related to SNS publish events, which is directly relevant to detecting spikes in notification flow. The description is concise and understandable, mentioning that it captures when messages are published to SNS topics and its usefulness for monitoring notification flow. However, it lacks specific details on units, how spikes might be identified, or implications of such spikes, limiting full coverage and utility for answering the question comprehensively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly defines the metric relevant to answering the question.\",\n    \"Evaluate Clarity by checking if the description is concise and understandable in the context of the question asked.\",\n    \"Assess Coverage by verifying if the description includes what is measured, units, relevant context, and potential implications that relate directly to the question.\",\n    \"Determine Utility by confirming if the description effectively supports the user in answering the monitoring question based on the provided context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6953350320227171"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly defines the metric relevant to answering the question.",
                            "Evaluate Clarity by checking if the description is concise and understandable in the context of the question asked.",
                            "Assess Coverage by verifying if the description includes what is measured, units, relevant context, and potential implications that relate directly to the question.",
                            "Determine Utility by confirming if the description effectively supports the user in answering the monitoring question based on the provided context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sns.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a specific SNS topic that was previously used for critical notifications?\", \"retrieval_context\": [\" - S3 bucket object deletion events. Data loss prevention, backup verification, ransomware detection, and cleanup operations. Critical for data governance, compliance, and disaster recovery monitoring.\", \" - This pattern captures SNS topic deletion events. It identifies when SNS topics are removed, which can indicate cleanup operations or potential notification loss incidents.\", \" - This pattern captures SNS unsubscription events. It identifies when subscriptions are removed from SNS topics, which can indicate cleanup operations or endpoint changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31480472055495773,
                        "reason": "The retrieval context identifies SNS topic deletion events but does not specify if these deletions relate to a specific SNS topic used for critical notifications, which is the focus of the input question. While it explains what the metric measures (SNS topic deletions) and some implications, it lacks clarity on topic specificity and critical notification relevance, limiting its actionability and direct alignment with the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate if the Retrieval Context clearly defines what metric is measured, including units and relevant context, to align with the Input's inquiry.\",\n    \"Check for clarity and conciseness in the description to determine if the user can easily understand and use it to answer the monitoring question.\",\n    \"Assess whether the description provides sufficient information and implications that make it actionable and useful relative to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.31480472055495773"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate if the Retrieval Context clearly defines what metric is measured, including units and relevant context, to align with the Input's inquiry.",
                            "Check for clarity and conciseness in the description to determine if the user can easily understand and use it to answer the monitoring question.",
                            "Assess whether the description provides sufficient information and implications that make it actionable and useful relative to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"dynamodb.amazonaws.com\\\", \\\"EventName\\\": \\\"Query*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of DynamoDB query operations that could indicate a performance issue?\", \"retrieval_context\": [\" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures DynamoDB get operations. It identifies when items are retrieved from DynamoDB tables, useful for monitoring data access patterns.\", \" - This pattern captures DynamoDB query operations. It identifies when queries are executed against DynamoDB tables, including scan operations and index queries.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5413033368737772,
                        "reason": "The retrieval context is relevant as it addresses DynamoDB query operations, which aligns with the monitoring question about spikes in query operations. However, the description lacks detail on what the metric specifically measures (e.g., counts, frequency), units, or how to interpret spikes for performance issues, limiting clarity and utility for confidently guiding action.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure relevance; the description must address aspects raised by the question.\",\n    \"Evaluate Clarity by checking if the description is concise and understandable without ambiguity, considering the terminology used in both Input and Retrieval Context.\",\n    \"Assess Coverage by verifying that the description explains what the metric measures, units, operational context, and implications in a way that aligns with the user's monitoring question.\",\n    \"Judge Utility by determining if the description, given the context of the monitoring question, provides sufficient information to confidently answer or guide action on that question.\"\n] \n \nRubric:\nNone \n \nScore: 0.5413033368737772"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure relevance; the description must address aspects raised by the question.",
                            "Evaluate Clarity by checking if the description is concise and understandable without ambiguity, considering the terminology used in both Input and Retrieval Context.",
                            "Assess Coverage by verifying that the description explains what the metric measures, units, operational context, and implications in a way that aligns with the user's monitoring question.",
                            "Judge Utility by determining if the description, given the context of the monitoring question, provides sufficient information to confidently answer or guide action on that question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sqs.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a specific SQS queue that's being used by a critical application?\", \"retrieval_context\": [\" - Lambda function deletion events. Application cleanup, cost optimization, security incident response, and resource decommissioning. Critical for audit trails and compliance monitoring.\", \" - S3 bucket object deletion events. Data loss prevention, backup verification, ransomware detection, and cleanup operations. Critical for data governance, compliance, and disaster recovery monitoring.\", \" - This pattern captures SQS queue deletion events. It identifies when SQS queues are removed, which can indicate cleanup operations or potential message loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3437823499114202,
                        "reason": "The Retrieval Context identifies that the Delete* events include SQS queue deletion events and explains their significance, which partially addresses the question about Delete* events related to SQS queues. However, it does not specify whether these events are tied to a specific SQS queue or if that queue is used by a critical application, leaving the key aspect of the Input unaddressed. The context is clear and concise but lacks sufficient detail and coverage to fully answer the question without additional information.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the specific question being asked.\",\n    \"Evaluate the clarity and conciseness of the Retrieval Context, ensuring it is easy to understand and free from ambiguity in relation to the terms used in the Input.\",\n    \"Assess whether the Retrieval Context provides sufficient coverage by explaining what is measured, including units, relevant context, and potential implications that directly help answer the Input question.\",\n    \"Determine the overall utility by verifying if the Retrieval Context enables a user to effectively answer the Input question without needing additional information.\"\n] \n \nRubric:\nNone \n \nScore: 0.3437823499114202"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the specific question being asked.",
                            "Evaluate the clarity and conciseness of the Retrieval Context, ensuring it is easy to understand and free from ambiguity in relation to the terms used in the Input.",
                            "Assess whether the Retrieval Context provides sufficient coverage by explaining what is measured, including units, relevant context, and potential implications that directly help answer the Input question.",
                            "Determine the overall utility by verifying if the Retrieval Context enables a user to effectively answer the Input question without needing additional information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"rds.amazonaws.com\\\", \\\"EventName\\\": \\\"Start*\\\"}\", \"hyperparameters\": null, \"input\": \"Are these RDS start events indicative of a recent instance failure or maintenance?\", \"retrieval_context\": [\" - This pattern captures RDS database deletion events. It identifies when RDS instances are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures RDS start events. It identifies when RDS instances are started, which can indicate cost optimization or service restoration.\", \" - This pattern captures RDS stop events. It identifies when RDS instances are stopped, which can indicate cost optimization or maintenance operations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6075858188378258,
                        "reason": "The Input question is clear and concise, asking if RDS start events indicate recent instance failure or maintenance. The Retrieval Context clearly describes what RDS start events capture and their possible implications, such as cost optimization or service restoration, but it does not explicitly address failure or maintenance. Coverage is partial since it lacks mention of failure or maintenance in relation to start events. Utility is limited because the description does not directly support answering whether start events indicate failure or maintenance, reducing alignment with the Input question. To improve, the context should explicitly link start events to failure or maintenance scenarios or clarify their absence.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate clarity by assessing if the Input (monitoring question) and Retrieval Context (metric description) are both clearly and concisely phrased without ambiguity.\",\n    \"Assess coverage by verifying that the Retrieval Context comprehensively explains what the metric measures, including units, relevant context, and possible implications, relative to the Input question.\",\n    \"Determine utility by checking if the metric description directly supports answering the Input question effectively, ensuring the description and question are well-aligned.\",\n    \"If any criteria score below 8, provide actionable suggestions focused on improving clarity, coverage, or utility with respect to the relationship between the Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6075858188378258"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate clarity by assessing if the Input (monitoring question) and Retrieval Context (metric description) are both clearly and concisely phrased without ambiguity.",
                            "Assess coverage by verifying that the Retrieval Context comprehensively explains what the metric measures, including units, relevant context, and possible implications, relative to the Input question.",
                            "Determine utility by checking if the metric description directly supports answering the Input question effectively, ensuring the description and question are well-aligned.",
                            "If any criteria score below 8, provide actionable suggestions focused on improving clarity, coverage, or utility with respect to the relationship between the Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"rds.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are these RDS database deletion events indicative of a deliberate cleanup operation or an unexpected data loss incident?\", \"retrieval_context\": [\" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures KMS deletion events. It identifies when KMS keys are removed, which can indicate cleanup operations or potential data access loss incidents.\", \" - This pattern captures RDS database deletion events. It identifies when RDS instances are removed, which can indicate cleanup operations or potential data loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6622459331201854,
                        "reason": "The retrieval context directly addresses RDS database deletion events and mentions that these can indicate either cleanup operations or potential data loss incidents, aligning well with the monitoring question. The description is clear and concise, explaining what is measured and the possible implications. However, it lacks detailed criteria or additional context to distinguish deliberate cleanup from unexpected data loss, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly addresses the key terms and intent of the question.\",\n    \"Evaluate the clarity of the metric description in relation to the monitoring question, checking for concise language and understandable explanations that align with the query's focus.\",\n    \"Assess the coverage of the description to ensure it includes what is measured, units, relevant context, and implications that are necessary to effectively respond to the monitoring question.\",\n    \"Determine the utility by verifying if the description sufficiently equips the user with information to confidently answer the monitoring question; if not, identify specific gaps or ambiguities.\"\n] \n \nRubric:\nNone \n \nScore: 0.6622459331201854"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly addresses the key terms and intent of the question.",
                            "Evaluate the clarity of the metric description in relation to the monitoring question, checking for concise language and understandable explanations that align with the query's focus.",
                            "Assess the coverage of the description to ensure it includes what is measured, units, relevant context, and implications that are necessary to effectively respond to the monitoring question.",
                            "Determine the utility by verifying if the description sufficiently equips the user with information to confidently answer the monitoring question; if not, identify specific gaps or ambiguities."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sns.amazonaws.com\\\", \\\"EventName\\\": \\\"Unsubscribe*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Unsubscribe events indicating a large-scale removal of SNS topic subscribers?\", \"retrieval_context\": [\" - This pattern captures SNS topic deletion events. It identifies when SNS topics are removed, which can indicate cleanup operations or potential notification loss incidents.\", \" - This pattern captures SNS unsubscription events. It identifies when subscriptions are removed from SNS topics, which can indicate cleanup operations or endpoint changes.\", \" - This pattern captures SQS queue deletion events. It identifies when SQS queues are removed, which can indicate cleanup operations or potential message loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7105315857338039,
                        "reason": "The description clearly addresses the monitoring question by explaining that the pattern captures SNS unsubscription events and identifies when subscriptions are removed from SNS topics, which relates directly to large-scale removal. It is concise and understandable, using relevant terminology like 'SNS unsubscription events' and 'subscriptions removed.' However, it lacks specific details on units of measurement, frequency, or scale, and does not explicitly discuss implications or how to interpret the data to determine if the removal is large-scale, limiting its utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the specific question asked.\",\n    \"Evaluate the clarity of the description by assessing if it is concise and understandable in relation to the terminology and focus of the monitoring question.\",\n    \"Assess coverage by verifying that the description explains what is measured, units of measurement, relevant context, and implications, as these details help answer the monitoring question.\",\n    \"Determine the utility by checking whether the description provides sufficient and relevant information that enables the user to effectively respond to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7105315857338039"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the specific question asked.",
                            "Evaluate the clarity of the description by assessing if it is concise and understandable in relation to the terminology and focus of the monitoring question.",
                            "Assess coverage by verifying that the description explains what is measured, units of measurement, relevant context, and implications, as these details help answer the monitoring question.",
                            "Determine the utility by checking whether the description provides sufficient and relevant information that enables the user to effectively respond to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticache.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events from ElastiCache indicating a deliberate cleanup operation or an unexpected cache loss incident?\", \"retrieval_context\": [\" - This pattern captures DynamoDB deletion events. It identifies when DynamoDB tables are removed, which can indicate cleanup operations or potential data loss incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ElastiCache cluster deletion events. It identifies when ElastiCache clusters are removed, which can indicate cleanup operations or potential cache loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7817678693815618,
                        "reason": "The retrieval context directly addresses the input question by explaining that the ElastiCache deletion events indicate either cleanup operations or potential cache loss incidents, aligning well with the monitoring question. The language is clear and concise, making it easy to understand. However, the description could improve coverage by including more details on metrics, units, or specific implications to fully support confident interpretation of the events.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (Question) with the Retrieval Context (Description) to ensure the description addresses the monitoring question directly and relevantly.\",\n    \"Evaluate Clarity by assessing if the description uses clear, concise language that a user can easily understand in the context of the question asked.\",\n    \"Assess Coverage by verifying the description includes details on what metric is measured, units, context, and implications, ensuring it fully supports the input question.\",\n    \"Determine Utility by judging whether the description enables the user to confidently answer the monitoring question, reflecting adequacy of context and explanations.\"\n] \n \nRubric:\nNone \n \nScore: 0.7817678693815618"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (Question) with the Retrieval Context (Description) to ensure the description addresses the monitoring question directly and relevantly.",
                            "Evaluate Clarity by assessing if the description uses clear, concise language that a user can easily understand in the context of the question asked.",
                            "Assess Coverage by verifying the description includes details on what metric is measured, units, context, and implications, ensuring it fully supports the input question.",
                            "Determine Utility by judging whether the description enables the user to confidently answer the monitoring question, reflecting adequacy of context and explanations."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticloadbalancing.amazonaws.com\\\", \\\"EventName\\\": \\\"Modify*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Modify* events in the Elastic Load Balancer that could be causing unintended configuration changes?\", \"retrieval_context\": [\" - This pattern captures ELB creation events. It identifies when load balancers are created, including load balancer configurations and initial settings.\", \" - This pattern captures ELB modification events. It identifies when load balancers are modified, including configuration changes, target group updates, and listener modifications.\", \" - This pattern captures ElastiCache modification events. It identifies when ElastiCache clusters are modified, including scaling operations, parameter group updates, and configuration changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.699197656390826,
                        "reason": "The description directly addresses the question by identifying ELB modification events, including configuration changes relevant to unintended changes. It is clear and concise, explaining what types of modifications are captured. However, it lacks specific measurement details, units, or implications about how these events might impact the system, limiting its utility for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (monitoring question) and retrieval context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate the clarity of the metric description by checking if it is concise and understandable in relation to the question asked.\",\n    \"Assess the coverage by verifying that the description includes measurement details, units, context, and implications relevant to the question.\",\n    \"Judge the utility by determining whether the description provides sufficient information to effectively answer the monitoring question; if not, specify improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.699197656390826"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (monitoring question) and retrieval context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate the clarity of the metric description by checking if it is concise and understandable in relation to the question asked.",
                            "Assess the coverage by verifying that the description includes measurement details, units, context, and implications relevant to the question.",
                            "Judge the utility by determining whether the description provides sufficient information to effectively answer the monitoring question; if not, specify improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudfront.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are these CloudFront distribution update events causing a spike in unhealthy hosts?\", \"retrieval_context\": [\" - This pattern captures CloudFront distribution deletion events. It identifies when CloudFront distributions are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures CloudFront distribution update events. It identifies when CloudFront distributions are modified, including configuration changes, cache behavior updates, and origin modifications.\", \" - This pattern captures CloudFront invalidation events. It identifies when CloudFront cache invalidations are performed, which can indicate content updates or troubleshooting activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31192029204575916,
                        "reason": "The retrieval context identifies CloudFront distribution update events and their nature, addressing the input question's focus on update events. However, it lacks any information on unhealthy hosts, metrics, units, or implications linking these events to spikes in unhealthy hosts, limiting clarity, coverage, and utility for answering the question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description addresses the specific monitoring question.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable, concise, and free from ambiguity in relation to the Input question.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications that relate directly to answering the Input question.\",\n    \"Determine Utility by confirming the Retrieval Context sufficiently empowers the user to answer the Input question effectively; if not, identify gaps or missing details.\"\n] \n \nRubric:\nNone \n \nScore: 0.31192029204575916"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description addresses the specific monitoring question.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable, concise, and free from ambiguity in relation to the Input question.",
                            "Assess Coverage by verifying the description includes what is measured, units, relevant context, and implications that relate directly to answering the Input question.",
                            "Determine Utility by confirming the Retrieval Context sufficiently empowers the user to answer the Input question effectively; if not, identify gaps or missing details."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticache.amazonaws.com\\\", \\\"EventName\\\": \\\"Failover*\\\"}\", \"hyperparameters\": null, \"input\": \"Are ElastiCache failover events occurring at a rate that exceeds ten per minute?\", \"retrieval_context\": [\" - This pattern captures ElastiCache cluster deletion events. It identifies when ElastiCache clusters are removed, which can indicate cleanup operations or potential cache loss incidents.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\", \" - This pattern captures ElastiCache reboot events. It identifies when ElastiCache nodes are rebooted, which can indicate maintenance operations or troubleshooting activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.40209489013807814,
                        "reason": "The description identifies ElastiCache failover events and their significance but lacks specific information about the rate of occurrence or units such as 'per minute.' It does not address the threshold of ten events per minute or provide operational context to determine if the rate exceeds this limit, limiting its utility in answering the monitoring question fully.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's focus.\",\n    \"Evaluate the clarity of the description to ensure it is understandable and concise, directly relating to the terminology and scope of the question.\",\n    \"Check the coverage in the description to confirm it explains what is measured, units involved, the operational context, and possible implications relevant to the question.\",\n    \"Assess the utility by determining if the description provides sufficient information to answer the monitoring question effectively, considering any gaps between the question and description.\"\n] \n \nRubric:\nNone \n \nScore: 0.40209489013807814"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's focus.",
                            "Evaluate the clarity of the description to ensure it is understandable and concise, directly relating to the terminology and scope of the question.",
                            "Check the coverage in the description to confirm it explains what is measured, units involved, the operational context, and possible implications relevant to the question.",
                            "Assess the utility by determining if the description provides sufficient information to answer the monitoring question effectively, considering any gaps between the question and description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudfront.amazonaws.com\\\", \\\"EventName\\\": \\\"Tag*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any unusual patterns in CloudFront distribution tagging events that could indicate misconfigured resources?\", \"retrieval_context\": [\" - This pattern captures CloudFront distribution deletion events. It identifies when CloudFront distributions are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures CloudFront distribution update events. It identifies when CloudFront distributions are modified, including configuration changes, cache behavior updates, and origin modifications.\", \" - This pattern captures CloudFront tagging events. It identifies when CloudFront distributions are tagged or untagged, useful for resource management and cost allocation.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.36423339960952267,
                        "reason": "The retrieval context describes CloudFront tagging, deletion, and update events but does not specifically address unusual patterns or indicators of misconfigured resources as asked in the input. While it provides some clarity on event types, it lacks coverage on how these events relate to misconfiguration detection or unusual patterns, limiting its utility for effectively answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.\",\n    \"Evaluate Clarity by assessing if the Retrieval Context is easily understandable and concise, relative to the complexity posed by the Input.\",\n    \"Assess Coverage by verifying the extent to which the Retrieval Context explains what is measured, including units, relevant context, and implications, in a way that aligns with the Input.\",\n    \"Judge Utility based on whether the Retrieval Context enables a user to effectively answer the Input question, highlighting any gaps between the two.\"\n] \n \nRubric:\nNone \n \nScore: 0.36423339960952267"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.",
                            "Evaluate Clarity by assessing if the Retrieval Context is easily understandable and concise, relative to the complexity posed by the Input.",
                            "Assess Coverage by verifying the extent to which the Retrieval Context explains what is measured, including units, relevant context, and implications, in a way that aligns with the Input.",
                            "Judge Utility based on whether the Retrieval Context enables a user to effectively answer the Input question, highlighting any gaps between the two."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticloadbalancing.amazonaws.com\\\", \\\"EventName\\\": \\\"Register*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of ELB registrations, indicating a potential scaling issue?\", \"retrieval_context\": [\" - This pattern captures ELB deregistration events. It identifies when targets are removed from load balancers, which can indicate scaling operations or maintenance activities.\", \" - This pattern captures ELB registration events. It identifies when targets are registered with load balancers, including instance registrations and target group updates.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6622459338205511,
                        "reason": "The description addresses ELB registration events relevant to the input question about spikes in ELB registrations, explaining what is captured and its context. However, it lacks explicit mention of measuring spikes or units, and does not clearly link the implications to potential scaling issues, limiting clarity and utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.\",\n    \"Evaluate Clarity by assessing if the description is concise, understandable, and free of ambiguous terms in relation to the input question.\",\n    \"Assess Coverage by verifying if the description details what is measured, includes units, contextual information, and implications relevant to the input question.\",\n    \"Determine Utility by checking if the description provides sufficient information for a user to confidently answer the monitoring question based on the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.6622459338205511"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.",
                            "Evaluate Clarity by assessing if the description is concise, understandable, and free of ambiguous terms in relation to the input question.",
                            "Assess Coverage by verifying if the description details what is measured, includes units, contextual information, and implications relevant to the input question.",
                            "Determine Utility by checking if the description provides sufficient information for a user to confidently answer the monitoring question based on the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"route53.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Route 53 get operations that could indicate a resource lookup issue?\", \"retrieval_context\": [\" - This pattern captures Route 53 change events. It identifies when Route 53 resources are modified, including record set changes, health check updates, and routing policy modifications.\", \" - This pattern captures Route 53 deletion events. It identifies when Route 53 resources are removed, which can indicate cleanup operations or potential DNS disruption incidents.\", \" - This pattern captures Route 53 get operations. It identifies when Route 53 resources are queried, including hosted zone information, record set lookups, and health check status.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5527301688373327,
                        "reason": "The description addresses Route 53 get operations relevant to resource lookups, aligning with the monitoring question about spikes indicating lookup issues. It uses clear language and explains what is captured (hosted zone info, record set lookups, health check status). However, it lacks explicit mention of spikes or how these metrics indicate issues, limiting coverage and utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Input) against the monitoring question (Retrieval Context) to ensure the description is relevant and directly addresses the user's query.\",\n    \"Evaluate clarity by checking if the description uses clear, concise language without ambiguity, making it easily understandable in the context of the question.\",\n    \"Assess coverage by verifying that the description fully explains what is being measured, including units, context, and possible implications related to the retrieval question.\",\n    \"Determine utility by judging whether the description provides sufficient information to effectively answer the monitoring question; if not, identify specific gaps between Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.5527301688373327"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Input) against the monitoring question (Retrieval Context) to ensure the description is relevant and directly addresses the user's query.",
                            "Evaluate clarity by checking if the description uses clear, concise language without ambiguity, making it easily understandable in the context of the question.",
                            "Assess coverage by verifying that the description fully explains what is being measured, including units, context, and possible implications related to the retrieval question.",
                            "Determine utility by judging whether the description provides sufficient information to effectively answer the monitoring question; if not, identify specific gaps between Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticloadbalancing.amazonaws.com\\\", \\\"EventName\\\": \\\"Deregister*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Deregister* events triggered by a large number of instances being removed from load balancers?\", \"retrieval_context\": [\" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ELB deregistration events. It identifies when targets are removed from load balancers, which can indicate scaling operations or maintenance activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6985936370890583,
                        "reason": "The Input question is clear and concise, and the Retrieval Context is understandable, explaining deregistration events as targets being removed from load balancers. However, the context lacks explicit mention of whether deregistration events are triggered specifically by a large number of instances being removed, limiting its direct utility in fully answering the question. Including information on the scale or volume of instance removals triggering these events would improve coverage and relevance.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input (question) and Retrieval Context (description) to determine if both are understandable and concise.\",\n    \"Assess the coverage of the Retrieval Context by checking if it explains what metric is measured, its units, relevant context, and implications related to the Input.\",\n    \"Judge the utility of the Retrieval Context in directly helping to answer the Input question, ensuring alignment between the question and description.\",\n    \"Combine the above assessments to assign a final score; if the score is below 8, specify concrete improvements focusing on enhancing clarity, coverage, or relevance between Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6985936370890583"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input (question) and Retrieval Context (description) to determine if both are understandable and concise.",
                            "Assess the coverage of the Retrieval Context by checking if it explains what metric is measured, its units, relevant context, and implications related to the Input.",
                            "Judge the utility of the Retrieval Context in directly helping to answer the Input question, ensuring alignment between the question and description.",
                            "Combine the above assessments to assign a final score; if the score is below 8, specify concrete improvements focusing on enhancing clarity, coverage, or relevance between Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"route53.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events from Route 53 indicating a removal of critical DNS resources?\", \"retrieval_context\": [\" - This pattern captures Route 53 change events. It identifies when Route 53 resources are modified, including record set changes, health check updates, and routing policy modifications.\", \" - This pattern captures Route 53 creation events. It identifies when Route 53 resources are created, including hosted zones, record sets, and health checks.\", \" - This pattern captures Route 53 deletion events. It identifies when Route 53 resources are removed, which can indicate cleanup operations or potential DNS disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7777299866333616,
                        "reason": "The description directly addresses the Delete* events by explaining they capture Route 53 deletion events and identify when resources are removed, which aligns with the question about removal of critical DNS resources. It is clear and concise, mentioning potential implications like cleanup or DNS disruption incidents. However, it lacks explicit detail on which specific resources are considered critical and does not quantify or specify units, slightly limiting coverage and utility for fully assessing criticality.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question posed.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise in relation to the complexity of the question and the terms used in the input.\",\n    \"Assess Coverage by verifying the description includes details on what is measured, units, context, and implications relevant to the input question.\",\n    \"Determine Utility by judging whether the description provides sufficient information to help a user effectively answer the specific monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7777299866333616"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question posed.",
                            "Evaluate Clarity by checking if the description is understandable and concise in relation to the complexity of the question and the terms used in the input.",
                            "Assess Coverage by verifying the description includes details on what is measured, units, context, and implications relevant to the input question.",
                            "Determine Utility by judging whether the description provides sufficient information to help a user effectively answer the specific monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"route53.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Route 53 resource creations that could indicate a configuration issue?\", \"retrieval_context\": [\" - This pattern captures Route 53 change events. It identifies when Route 53 resources are modified, including record set changes, health check updates, and routing policy modifications.\", \" - This pattern captures Route 53 creation events. It identifies when Route 53 resources are created, including hosted zones, record sets, and health checks.\", \" - This pattern captures Route 53 deletion events. It identifies when Route 53 resources are removed, which can indicate cleanup operations or potential DNS disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.605954274702826,
                        "reason": "The Retrieval Context clearly describes patterns capturing Route 53 creation, deletion, and change events, which aligns with the Input question about spikes in resource creations. However, it lacks explicit mention of detecting spikes or configuration issues, and does not provide units, frequency, or implications of these events, limiting its utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly aligns with and addresses the question.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it is understandable and concise in relation to the Input provided.\",\n    \"Assess the coverage of the Retrieval Context to ensure it explains what is measured, the units, context, and implications relevant to the Input.\",\n    \"Determine the utility by verifying if the Retrieval Context provides enough information to help answer the Input question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.605954274702826"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly aligns with and addresses the question.",
                            "Evaluate the clarity of the Retrieval Context by checking if it is understandable and concise in relation to the Input provided.",
                            "Assess the coverage of the Retrieval Context to ensure it explains what is measured, the units, context, and implications relevant to the Input.",
                            "Determine the utility by verifying if the Retrieval Context provides enough information to help answer the Input question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudfront.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the CreateDistribution events triggering a spike in AnomalousHostCount?\", \"retrieval_context\": [\" - IAM policy attachment events. Permission assignment, access grant operations, and privilege escalation. Critical security monitoring for policy distribution and access control changes.\", \" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.11192029282802232,
                        "reason": "The retrieval context describes various creation events related to Lambda functions and IAM policies but does not address the specific metric 'AnomalousHostCount' or its relationship to 'CreateDistribution' events. The description lacks clarity and relevance to the monitoring question, providing no information on what is measured, units, or implications for detecting spikes in anomalous hosts, thus offering minimal utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity: Check that the description is concise and understandable without ambiguity when considered alongside the question.\",\n    \"Assess Coverage: Verify that the description includes what is measured, units, context, and implications relevant to answering the question.\",\n    \"Determine Utility: Confirm that the description provides enough detail and relevance to effectively help a user answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.11192029282802232"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity: Check that the description is concise and understandable without ambiguity when considered alongside the question.",
                            "Assess Coverage: Verify that the description includes what is measured, units, context, and implications relevant to answering the question.",
                            "Determine Utility: Confirm that the description provides enough detail and relevance to effectively help a user answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudfront.amazonaws.com\\\", \\\"EventName\\\": \\\"Invalidate*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the number of CloudFront invalidate events unusually high, potentially indicating a content update or troubleshooting issue?\", \"retrieval_context\": [\" - This pattern captures CloudFront distribution deletion events. It identifies when CloudFront distributions are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures CloudFront distribution update events. It identifies when CloudFront distributions are modified, including configuration changes, cache behavior updates, and origin modifications.\", \" - This pattern captures CloudFront invalidation events. It identifies when CloudFront cache invalidations are performed, which can indicate content updates or troubleshooting activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7090205889182889,
                        "reason": "The retrieval context directly addresses CloudFront invalidation events, linking them to content updates and troubleshooting, which aligns well with the input question. It is clear and concise, explaining what the metric captures and its implications. However, it lacks specific details on the number of events, units, or thresholds to determine if the count is unusually high, limiting full coverage and utility for answering the question definitively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise and easily understandable without ambiguity when read alongside the Input.\",\n    \"Assess Coverage by confirming that the Retrieval Context explains what the metric measures, its units, relevant context, and implications relevant to the Input's question.\",\n    \"Determine Utility by judging whether the Retrieval Context provides enough information for a user to answer the Input question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.7090205889182889"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise and easily understandable without ambiguity when read alongside the Input.",
                            "Assess Coverage by confirming that the Retrieval Context explains what the metric measures, its units, relevant context, and implications relevant to the Input's question.",
                            "Determine Utility by judging whether the Retrieval Context provides enough information for a user to answer the Input question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticache.amazonaws.com\\\", \\\"EventName\\\": \\\"Modify*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Modify* events related to a recent spike in ElastiCache errors or performance degradation?\", \"retrieval_context\": [\" - This pattern captures ELB modification events. It identifies when load balancers are modified, including configuration changes, target group updates, and listener modifications.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\", \" - This pattern captures ElastiCache modification events. It identifies when ElastiCache clusters are modified, including scaling operations, parameter group updates, and configuration changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.474870801681149,
                        "reason": "The retrieval context identifies ElastiCache modification events, which partially relates to the question about Modify* events. However, it lacks direct information linking these events to spikes in errors or performance degradation, limiting its utility. The description is clear and concise but does not cover implications or metrics related to errors or performance, reducing its effectiveness in fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (monitoring question) and retrieval context (metric description) to ensure the description directly addresses and supports answering the question.\",\n    \"Evaluate clarity by checking if the metric description is concise and easy to understand without ambiguity, especially in the context of the given question.\",\n    \"Assess coverage by verifying that the description explains what is measured, units, relevant context, and implications, all aligned with the question's focus.\",\n    \"Determine utility by confirming that the description provides sufficient and relevant information that would help a user effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.474870801681149"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (monitoring question) and retrieval context (metric description) to ensure the description directly addresses and supports answering the question.",
                            "Evaluate clarity by checking if the metric description is concise and easy to understand without ambiguity, especially in the context of the given question.",
                            "Assess coverage by verifying that the description explains what is measured, units, relevant context, and implications, all aligned with the question's focus.",
                            "Determine utility by confirming that the description provides sufficient and relevant information that would help a user effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticloadbalancing.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a specific application or service being decommissioned, or do they indicate an unexpected removal of load balancers?\", \"retrieval_context\": [\" - This pattern captures CloudFront distribution deletion events. It identifies when CloudFront distributions are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ELB deregistration events. It identifies when targets are removed from load balancers, which can indicate scaling operations or maintenance activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6985936374764654,
                        "reason": "The input question is clear and the retrieval context describes relevant deletion events with implications, but it lacks explicit linkage to specific applications or services being decommissioned. The context focuses on load balancer and CloudFront deletion events and their potential operational meanings but does not clarify whether these deletions are expected or unexpected, limiting utility in fully answering the question. Adding explicit information about application/service decommissioning versus unexpected removals would improve clarity and coverage.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Assess the Input for clarity: Determine if the monitoring question is clearly stated and if the metric description is understandable and concise.\",\n    \"Evaluate the Retrieval Context for coverage: Check if the description fully explains what is measured, including units, context, and implications relevant to the question.\",\n    \"Analyze the relationship between Input and Retrieval Context for utility: Verify whether the description provides sufficient information to answer the monitoring question effectively.\",\n    \"If the combined evaluation of Input and Retrieval Context results in a score below 8, identify specific areas lacking clarity, coverage, or utility and suggest concrete improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6985936374764654"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Assess the Input for clarity: Determine if the monitoring question is clearly stated and if the metric description is understandable and concise.",
                            "Evaluate the Retrieval Context for coverage: Check if the description fully explains what is measured, including units, context, and implications relevant to the question.",
                            "Analyze the relationship between Input and Retrieval Context for utility: Verify whether the description provides sufficient information to answer the monitoring question effectively.",
                            "If the combined evaluation of Input and Retrieval Context results in a score below 8, identify specific areas lacking clarity, coverage, or utility and suggest concrete improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticache.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Create* events for ElastiCache clusters exceeding ten per minute?\", \"retrieval_context\": [\" - This pattern captures ElastiCache cluster creation events. It identifies when ElastiCache clusters are created, including cluster configurations and initial settings.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\", \" - This pattern captures ElastiCache modification events. It identifies when ElastiCache clusters are modified, including scaling operations, parameter group updates, and configuration changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.45,
                        "reason": "The retrieval context partially addresses the input by describing ElastiCache cluster creation events, which aligns with the 'Create*' events in the question. However, it lacks explicit mention of event frequency or units (per minute), and does not clarify how to determine if events exceed ten per minute. Additionally, the inclusion of failover and modification events is irrelevant to the question, reducing clarity and focus. To improve, the description should explicitly define the metric's measurement interval, units, and guidance on interpreting event counts relative to the threshold.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) for clarity, ensuring the description is concise and understandable in relation to the question.\",\n    \"Evaluate coverage by verifying that the description fully explains what the metric measures, includes units, provides relevant context, and highlights implications related to the monitoring question.\",\n    \"Assess utility by determining if the description sufficiently informs or guides the user to answer the monitoring question accurately.\",\n    \"If any of the above aspects score below 8, identify specific deficiencies in the description relative to the input question and suggest targeted improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.45"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) for clarity, ensuring the description is concise and understandable in relation to the question.",
                            "Evaluate coverage by verifying that the description fully explains what the metric measures, includes units, provides relevant context, and highlights implications related to the monitoring question.",
                            "Assess utility by determining if the description sufficiently informs or guides the user to answer the monitoring question accurately.",
                            "If any of the above aspects score below 8, identify specific deficiencies in the description relative to the input question and suggest targeted improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"route53.amazonaws.com\\\", \\\"EventName\\\": \\\"List*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any unusual Route 53 list operations that could indicate a misconfigured or compromised resource?\", \"retrieval_context\": [\" - This pattern captures Route 53 change events. It identifies when Route 53 resources are modified, including record set changes, health check updates, and routing policy modifications.\", \" - This pattern captures Route 53 deletion events. It identifies when Route 53 resources are removed, which can indicate cleanup operations or potential DNS disruption incidents.\", \" - This pattern captures Route 53 list operations. It identifies when Route 53 resources are enumerated, including hosted zone listings, record set listings, and health check listings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.65,
                        "reason": "The description directly addresses Route 53 list operations relevant to the question about unusual list operations, providing what is measured (enumeration of resources) and examples (hosted zones, record sets, health checks). However, it lacks clarity on what constitutes 'unusual' activity or how to identify misconfiguration or compromise, limiting its utility for answering the question fully. The description is concise and mostly clear but could improve by including context on anomaly detection or implications of unusual list operations.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (monitoring question) with the retrieval context (metric description) to assess if the description directly addresses the question asked.\",\n    \"Evaluate the clarity of the description by checking if it is understandable and concise, without ambiguity or excessive jargon, in relation to the monitoring question.\",\n    \"Assess coverage by verifying whether the description includes what is measured, units, relevant context, and potential implications, ensuring it supports answering the question.\",\n    \"Determine utility by judging if the description sufficiently equips a user to answer the provided question; if not, suggest specific improvements to fill gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.65"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (monitoring question) with the retrieval context (metric description) to assess if the description directly addresses the question asked.",
                            "Evaluate the clarity of the description by checking if it is understandable and concise, without ambiguity or excessive jargon, in relation to the monitoring question.",
                            "Assess coverage by verifying whether the description includes what is measured, units, relevant context, and potential implications, ensuring it supports answering the question.",
                            "Determine utility by judging if the description sufficiently equips a user to answer the provided question; if not, suggest specific improvements to fill gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticache.amazonaws.com\\\", \\\"EventName\\\": \\\"Reboot*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the ElastiCache reboot events frequent enough to be causing performance issues for dependent services?\", \"retrieval_context\": [\" - This pattern captures ElastiCache cluster deletion events. It identifies when ElastiCache clusters are removed, which can indicate cleanup operations or potential cache loss incidents.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\", \" - This pattern captures ElastiCache reboot events. It identifies when ElastiCache nodes are rebooted, which can indicate maintenance operations or troubleshooting activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3026350752873229,
                        "reason": "The retrieval context identifies ElastiCache reboot events and their nature but lacks information on frequency, measurement units, or how these events impact performance. It is somewhat clear but does not provide sufficient coverage or actionable details to determine if reboot events are frequent enough to cause performance issues.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate the clarity of the description by checking if it is easily understandable and concise without unnecessary jargon, in relation to the question's complexity.\",\n    \"Assess coverage by verifying if the description includes what is measured, units, relevant context, and implications needed to answer the Input question accurately.\",\n    \"Determine utility by judging whether the description provides actionable information that enables a user to confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3026350752873229"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate the clarity of the description by checking if it is easily understandable and concise without unnecessary jargon, in relation to the question's complexity.",
                            "Assess coverage by verifying if the description includes what is measured, units, relevant context, and implications needed to answer the Input question accurately.",
                            "Determine utility by judging whether the description provides actionable information that enables a user to confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudfront.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a specific CloudFront distribution that was previously serving traffic?\", \"retrieval_context\": [\" - This pattern captures CloudFront distribution deletion events. It identifies when CloudFront distributions are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ElastiCache cluster deletion events. It identifies when ElastiCache clusters are removed, which can indicate cleanup operations or potential cache loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.45092782151719285,
                        "reason": "The description identifies that the pattern captures CloudFront distribution deletion events and hints at implications like cleanup or service disruption, which partially addresses the question about Delete* events related to CloudFront distributions. However, it lacks clarity and detail on whether these deletions are tied to specific distributions previously serving traffic, missing explicit context or units. The description is somewhat concise but does not provide enough information for a user to confidently determine the relationship to specific distributions, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question being asked.\",\n    \"Evaluate the Clarity of the metric description by determining if it is concise and easy to understand in relation to the question to avoid ambiguity.\",\n    \"Assess the Coverage by verifying that the description explains what is measured, the units involved, relevant context, and possible implications aligned with the input question.\",\n    \"Measure the Utility by judging whether the description provides sufficient information to enable a user to confidently answer the monitoring question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.45092782151719285"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question being asked.",
                            "Evaluate the Clarity of the metric description by determining if it is concise and easy to understand in relation to the question to avoid ambiguity.",
                            "Assess the Coverage by verifying that the description explains what is measured, the units involved, relevant context, and possible implications aligned with the input question.",
                            "Measure the Utility by judging whether the description provides sufficient information to enable a user to confidently answer the monitoring question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticloadbalancing.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are new load balancers being created at an excessive rate?\", \"retrieval_context\": [\" - Auto-scaling operations across all services. Capacity management, performance optimization, and cost control. Dynamic resource allocation and performance monitoring.\", \" - This pattern captures ELB creation events. It identifies when load balancers are created, including load balancer configurations and initial settings.\", \" - This pattern captures ELB deregistration events. It identifies when targets are removed from load balancers, which can indicate scaling operations or maintenance activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5508857442983692,
                        "reason": "The description partially explains the metric by identifying ELB creation events and configurations, which relates to monitoring new load balancer creation. However, it lacks clarity on the measurement units, the rate calculation, and what constitutes an 'excessive' rate. The context mentions related patterns like deregistration and auto-scaling but does not connect these explicitly to assessing creation rates. To improve, the description should specify how creation events are quantified over time, define thresholds for excessiveness, and clarify implications for load balancer management to better answer the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description and monitoring question) to ensure the description is clear and concise in explaining the metric relevant to the question.\",\n    \"Evaluate Coverage by checking if the description fully explains what is measured, including units, the context of measurement, and the implications related to the question.\",\n    \"Assess Utility by determining whether the description enables a user to effectively answer the monitoring question posed in the Retrieval Context.\",\n    \"If the description scores below 8, identify specific gaps where clarity, coverage, or utility is lacking, and suggest concrete improvements that address both Input and Retrieval Context alignment.\"\n] \n \nRubric:\nNone \n \nScore: 0.5508857442983692"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description and monitoring question) to ensure the description is clear and concise in explaining the metric relevant to the question.",
                            "Evaluate Coverage by checking if the description fully explains what is measured, including units, the context of measurement, and the implications related to the question.",
                            "Assess Utility by determining whether the description enables a user to effectively answer the monitoring question posed in the Retrieval Context.",
                            "If the description scores below 8, identify specific gaps where clarity, coverage, or utility is lacking, and suggest concrete improvements that address both Input and Retrieval Context alignment."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"route53.amazonaws.com\\\", \\\"EventName\\\": \\\"Change*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Route 53 resource modifications that could be causing a disruption to service?\", \"retrieval_context\": [\" - This pattern captures Route 53 change events. It identifies when Route 53 resources are modified, including record set changes, health check updates, and routing policy modifications.\", \" - This pattern captures Route 53 deletion events. It identifies when Route 53 resources are removed, which can indicate cleanup operations or potential DNS disruption incidents.\", \" - This pattern captures Route 53 get operations. It identifies when Route 53 resources are queried, including hosted zone information, record set lookups, and health check status.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8777299866333615,
                        "reason": "The retrieval context directly addresses the monitoring question by describing patterns that capture Route 53 resource modifications, including changes and deletions that could cause service disruption. The description is clear, concise, and explains what is measured (change, deletion, and get operations) with relevant context. It effectively supports identifying potential disruptions, though it could slightly improve by explicitly mentioning units or frequency metrics.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input and Retrieval Context to ensure they are clearly related, checking if the metric description directly addresses the monitoring question.\",\n    \"Evaluate Clarity by verifying that the metric description is understandable and concise when read alongside the question\u2019s intent.\",\n    \"Assess Coverage by confirming the description explains what is measured, including units and relevant context, as these details support answering the question accurately.\",\n    \"Judge Utility by determining if the description provides sufficient information to effectively address the monitoring question, highlighting any gaps between them.\"\n] \n \nRubric:\nNone \n \nScore: 0.8777299866333615"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input and Retrieval Context to ensure they are clearly related, checking if the metric description directly addresses the monitoring question.",
                            "Evaluate Clarity by verifying that the metric description is understandable and concise when read alongside the question\u2019s intent.",
                            "Assess Coverage by confirming the description explains what is measured, including units and relevant context, as these details support answering the question accurately.",
                            "Judge Utility by determining if the description provides sufficient information to effectively address the monitoring question, highlighting any gaps between them."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudwatch.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events indicative of a resource removal that could impact monitoring capabilities?\", \"retrieval_context\": [\" - This pattern captures CloudWatch deletion events. It identifies when CloudWatch resources are removed, which can indicate cleanup operations or potential monitoring loss incidents.\", \" - This pattern captures KMS deletion events. It identifies when KMS keys are removed, which can indicate cleanup operations or potential data access loss incidents.\", \" - This pattern captures Systems Manager deletion events. It identifies when SSM resources are removed, which can indicate cleanup operations or potential configuration loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8989013055031695,
                        "reason": "The description clearly addresses the question by explaining that Delete* events capture deletion of CloudWatch, Systems Manager, and KMS resources, indicating resource removal. It uses clear and concise language relevant to the input question and specifies the implications such as potential monitoring loss, configuration loss, and data access loss, which directly relate to the impact on monitoring capabilities. The coverage is comprehensive and provides sufficient information to answer the question effectively, though it could slightly improve by explicitly linking all deletions to monitoring impact.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the question.\",\n    \"Evaluate the description\u2019s clarity by checking if it uses understandable and concise language relevant to the input question.\",\n    \"Assess the coverage by verifying if the description explains what is measured, including units, context, and implications that relate directly to the input question.\",\n    \"Determine the utility by confirming if the description provides sufficient information to answer the input question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.8989013055031695"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the question.",
                            "Evaluate the description\u2019s clarity by checking if it uses understandable and concise language relevant to the input question.",
                            "Assess the coverage by verifying if the description explains what is measured, including units, context, and implications that relate directly to the input question.",
                            "Determine the utility by confirming if the description provides sufficient information to answer the input question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudwatch.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of CloudWatch get operations that could indicate a resource-intensive query?\", \"retrieval_context\": [\" - This pattern captures CloudWatch deletion events. It identifies when CloudWatch resources are removed, which can indicate cleanup operations or potential monitoring loss incidents.\", \" - This pattern captures CloudWatch get operations. It identifies when CloudWatch resources are queried, including metric data retrieval, log event queries, and alarm status checks.\", \" - This pattern captures CloudWatch list operations. It identifies when CloudWatch resources are enumerated, including metric listings, log group listings, and alarm listings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6194658615575999,
                        "reason": "The description addresses CloudWatch get operations relevant to the question about spikes in get operations, specifying the types of queries captured. However, it lacks explicit mention of measurement units, how spikes are detected, or implications of resource intensity, limiting clarity and utility for directly answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description addresses the exact measurement needed by the question.\",\n    \"Evaluate Clarity by verifying that the description is concise and easily understandable without ambiguous terminology in relation to the question.\",\n    \"Assess Coverage by confirming the description includes what is measured, units, relevant context, and the implications tied to the monitoring question.\",\n    \"Judge Utility by determining if the description provides sufficient information to directly support answering the monitoring question; if gaps exist, note specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6194658615575999"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description addresses the exact measurement needed by the question.",
                            "Evaluate Clarity by verifying that the description is concise and easily understandable without ambiguous terminology in relation to the question.",
                            "Assess Coverage by confirming the description includes what is measured, units, relevant context, and the implications tied to the monitoring question.",
                            "Judge Utility by determining if the description provides sufficient information to directly support answering the monitoring question; if gaps exist, note specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"kms.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a specific KMS key that was previously used for encryption?\", \"retrieval_context\": [\" - This pattern captures KMS decryption events. It identifies when data is decrypted using KMS keys, useful for monitoring data access patterns and security compliance.\", \" - This pattern captures KMS deletion events. It identifies when KMS keys are removed, which can indicate cleanup operations or potential data access loss incidents.\", \" - This pattern captures KMS encryption events. It identifies when data is encrypted using KMS keys, useful for monitoring data protection activities and compliance requirements.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4069246655944063,
                        "reason": "The Retrieval Context explains what Delete* events capture (KMS deletion events) and their implications but does not specify if these events relate to a specific KMS key previously used for encryption, which is the core of the Input question. The context is clear and concise but lacks coverage on linking deletion events to specific keys or prior encryption usage, limiting its utility in directly answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.\",\n    \"Evaluate the clarity of the Retrieval Context to ensure it is concise and understandable without ambiguity.\",\n    \"Check the coverage in the Retrieval Context to confirm it explains what is measured, the units involved, relevant context, and implications related to the Input question.\",\n    \"Assess the utility by determining if the Retrieval Context provides sufficient information to directly answer or guide solving the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.4069246655944063"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.",
                            "Evaluate the clarity of the Retrieval Context to ensure it is concise and understandable without ambiguity.",
                            "Check the coverage in the Retrieval Context to confirm it explains what is measured, the units involved, relevant context, and implications related to the Input question.",
                            "Assess the utility by determining if the Retrieval Context provides sufficient information to directly answer or guide solving the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"kms.amazonaws.com\\\", \\\"EventName\\\": \\\"Encrypt*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Encrypt events indicating a potential issue with sensitive data encryption?\", \"retrieval_context\": [\" - GuardDuty threat detection events. Security alerts, threat intelligence, compromised credentials, unusual activity detection, and security incident response. Critical for security monitoring and threat hunting.\", \" - This pattern captures KMS decryption events. It identifies when data is decrypted using KMS keys, useful for monitoring data access patterns and security compliance.\", \" - This pattern captures KMS encryption events. It identifies when data is encrypted using KMS keys, useful for monitoring data protection activities and compliance requirements.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7119202926715696,
                        "reason": "The description addresses the monitoring question by focusing on KMS encryption events relevant to detecting spikes in Encrypt events, aligning with the retrieval context. It is generally clear and concise, avoiding ambiguity. However, it lacks explicit mention of units or how spikes are quantified, and does not discuss potential implications in detail, limiting full coverage and utility for answering the question comprehensively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description against the retrieval context to check if the description clearly addresses the monitoring question posed.\",\n    \"Evaluate clarity by ensuring the description is concise and understandable within the context given, confirming it avoids ambiguity or excessive complexity.\",\n    \"Assess coverage by verifying the description includes essential elements such as what is measured, units, relevant context, and potential implications aligning with both input and retrieval context.\",\n    \"Judge utility by determining if the combined input and retrieval context enable a user to effectively answer the monitoring question; if not, suggest improvements targeting missing or unclear elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.7119202926715696"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description against the retrieval context to check if the description clearly addresses the monitoring question posed.",
                            "Evaluate clarity by ensuring the description is concise and understandable within the context given, confirming it avoids ambiguity or excessive complexity.",
                            "Assess coverage by verifying the description includes essential elements such as what is measured, units, relevant context, and potential implications aligning with both input and retrieval context.",
                            "Judge utility by determining if the combined input and retrieval context enable a user to effectively answer the monitoring question; if not, suggest improvements targeting missing or unclear elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"secretsmanager.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CreateSecretsManager events indicating a potential misconfiguration of sensitive credentials?\", \"retrieval_context\": [\" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - This pattern captures Secrets Manager creation events. It identifies when secrets are created, including secret configurations, rotation settings, and access policies.\", \" - This pattern captures Secrets Manager deletion events. It identifies when secrets are removed, which can indicate cleanup operations or potential credential loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7927814871287578,
                        "reason": "The description clearly addresses the question by explaining that the pattern captures Secrets Manager creation events, including secret configurations and access policies, which relate to potential misconfigurations of sensitive credentials. The language is clear and concise, covering what is measured and the context. However, it lacks explicit mention of implications or how to interpret these events as indicators of misconfiguration, slightly limiting utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.\",\n    \"Evaluate Clarity by checking if the description uses understandable language and concise explanations relevant to the question.\",\n    \"Assess Coverage by verifying if the description includes what is measured, the units, context, and implications as needed to answer the question.\",\n    \"Determine Utility by judging whether the description enables the user to effectively answer the monitoring question based on the context provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.7927814871287578"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.",
                            "Evaluate Clarity by checking if the description uses understandable language and concise explanations relevant to the question.",
                            "Assess Coverage by verifying if the description includes what is measured, the units, context, and implications as needed to answer the question.",
                            "Determine Utility by judging whether the description enables the user to effectively answer the monitoring question based on the context provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudwatch.amazonaws.com\\\", \\\"EventName\\\": \\\"Put*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any unusual patterns in the number of CloudWatch metrics being created or updated?\", \"retrieval_context\": [\" - This pattern captures CloudWatch creation events. It identifies when CloudWatch resources are created, including alarms, dashboards, and log groups.\", \" - This pattern captures CloudWatch list operations. It identifies when CloudWatch resources are enumerated, including metric listings, log group listings, and alarm listings.\", \" - This pattern captures CloudWatch put operations. It identifies when CloudWatch resources are created or updated, including metric data, log events, and alarm configurations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6679178692681615,
                        "reason": "The input question is clear and concise, directly asking about unusual patterns in CloudWatch metrics creation or updates, which aligns with the retrieval context describing patterns capturing CloudWatch put, list, and creation operations. However, the input lacks explicit mention of what is measured (e.g., specific metrics or event types), units, or implications, limiting coverage. While the retrieval context provides detailed event types and operations, the input could improve utility by specifying the scope or types of metrics monitored to better leverage the context.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input description with the Retrieval Context to assess if the description is clear and concise, ensuring it is easily understandable in relation to the question asked.\",\n    \"Evaluate whether the Input provides sufficient coverage by checking if it explains what is measured, includes units, offers context, and states implications relevant to the Retrieval Context.\",\n    \"Determine the Utility by analyzing if the Input description effectively supports answering the monitoring question when considered alongside the Retrieval Context.\",\n    \"If the description scores below 8 based on clarity, coverage, or utility, identify specific gaps between the Input and Retrieval Context and suggest precise improvements to bridge these gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.6679178692681615"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input description with the Retrieval Context to assess if the description is clear and concise, ensuring it is easily understandable in relation to the question asked.",
                            "Evaluate whether the Input provides sufficient coverage by checking if it explains what is measured, includes units, offers context, and states implications relevant to the Retrieval Context.",
                            "Determine the Utility by analyzing if the Input description effectively supports answering the monitoring question when considered alongside the Retrieval Context.",
                            "If the description scores below 8 based on clarity, coverage, or utility, identify specific gaps between the Input and Retrieval Context and suggest precise improvements to bridge these gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudwatch.amazonaws.com\\\", \\\"EventName\\\": \\\"List*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any unusual patterns in the number of CloudWatch list operations that could indicate a security or performance issue?\", \"retrieval_context\": [\" - This pattern captures CloudWatch deletion events. It identifies when CloudWatch resources are removed, which can indicate cleanup operations or potential monitoring loss incidents.\", \" - This pattern captures CloudWatch get operations. It identifies when CloudWatch resources are queried, including metric data retrieval, log event queries, and alarm status checks.\", \" - This pattern captures CloudWatch list operations. It identifies when CloudWatch resources are enumerated, including metric listings, log group listings, and alarm listings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.600379772445391,
                        "reason": "The retrieval context directly addresses the question by describing CloudWatch list operations, which are relevant to identifying unusual patterns. It clearly explains what list operations include (metric listings, log group listings, alarm listings), aiding clarity. However, it lacks details on what constitutes unusual patterns, how to measure or detect anomalies, and the implications for security or performance, limiting coverage and utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description directly addresses the question asked.\",\n    \"Evaluate Clarity by checking if the description is concise, free of jargon, and easily understood in relation to the question.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to the question.\",\n    \"Judge Utility by determining if the description provides enough information to confidently answer the monitoring question; if not, identify gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.600379772445391"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description directly addresses the question asked.",
                            "Evaluate Clarity by checking if the description is concise, free of jargon, and easily understood in relation to the question.",
                            "Assess Coverage by verifying the description includes what is measured, units, context, and implications relevant to the question.",
                            "Judge Utility by determining if the description provides enough information to confidently answer the monitoring question; if not, identify gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sts.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Get* operations from sts.amazonaws.com exceeding their normal rate?\", \"retrieval_context\": [\" - S3 get operations. Object downloads, data retrieval, access pattern analysis, and usage monitoring. Data access auditing, performance optimization, and cost analysis.\", \" - S3 list operations. Bucket enumeration, object discovery, directory browsing, and inventory management. Data exploration, security reconnaissance detection, and access pattern analysis.\", \" - This pattern captures STS get operations. It identifies when temporary credentials are retrieved, including session token requests and caller identity queries.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.670572355779566,
                        "reason": "The description includes a relevant segment about STS get operations, explaining that it captures temporary credential retrievals and session token requests, which directly relates to the monitoring question about Get* operations from sts.amazonaws.com. However, the description lacks explicit information on normal rate baselines, units, or thresholds, limiting its utility in determining if the operations are exceeding normal rates. The language is generally clear but could be more concise and focused on the specific metric queried.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) against the Retrieval Context (metric description) to check if the description clearly relates to and supports answering the question.\",\n    \"Evaluate the description's clarity by verifying if the language is concise and understandable within the context of the monitoring question.\",\n    \"Assess whether the description covers all necessary aspects\u2014what is measured, units, context, and implications\u2014in a way that aligns with the question's focus.\",\n    \"Determine the utility of the description by judging if it provides enough relevant information to directly help the user answer the given monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.670572355779566"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) against the Retrieval Context (metric description) to check if the description clearly relates to and supports answering the question.",
                            "Evaluate the description's clarity by verifying if the language is concise and understandable within the context of the monitoring question.",
                            "Assess whether the description covers all necessary aspects\u2014what is measured, units, context, and implications\u2014in a way that aligns with the question's focus.",
                            "Determine the utility of the description by judging if it provides enough relevant information to directly help the user answer the given monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudwatch.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any unusual patterns in the creation of CloudWatch resources that could indicate a misconfiguration or security issue?\", \"retrieval_context\": [\" - This pattern captures CloudWatch creation events. It identifies when CloudWatch resources are created, including alarms, dashboards, and log groups.\", \" - This pattern captures CloudWatch deletion events. It identifies when CloudWatch resources are removed, which can indicate cleanup operations or potential monitoring loss incidents.\", \" - This pattern captures CloudWatch put operations. It identifies when CloudWatch resources are created or updated, including metric data, log events, and alarm configurations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6679178699175393,
                        "reason": "The input question is clear and relevant to the retrieval context, which describes patterns capturing CloudWatch creation, deletion, and update events. However, the input lacks specificity about what constitutes 'unusual patterns' or how to interpret the creation events in terms of misconfiguration or security issues. The retrieval context provides event types but does not explicitly link them to anomaly detection or security implications, limiting the utility for answering the monitoring question fully.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input metric description against the Retrieval Context to check clarity: verify that the description is easy to understand and directly relevant to the monitoring question.\",\n    \"Evaluate coverage by ensuring the Input describes what is measured, units, context, and implications as supported by or consistent with the Retrieval Context.\",\n    \"Assess utility by determining if the Input description, in combination with the Retrieval Context, effectively enables a user to answer the monitoring question.\",\n    \"If any aspect scores below 8, suggest specific improvements focused on missing details, ambiguous language, or lack of actionable information within the relationship between Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6679178699175393"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input metric description against the Retrieval Context to check clarity: verify that the description is easy to understand and directly relevant to the monitoring question.",
                            "Evaluate coverage by ensuring the Input describes what is measured, units, context, and implications as supported by or consistent with the Retrieval Context.",
                            "Assess utility by determining if the Input description, in combination with the Retrieval Context, effectively enables a user to answer the monitoring question.",
                            "If any aspect scores below 8, suggest specific improvements focused on missing details, ambiguous language, or lack of actionable information within the relationship between Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"secretsmanager.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events in Secrets Manager indicative of a security incident or routine cleanup?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - This pattern captures Secrets Manager deletion events. It identifies when secrets are removed, which can indicate cleanup operations or potential credential loss incidents.\", \" - This pattern captures Systems Manager deletion events. It identifies when SSM resources are removed, which can indicate cleanup operations or potential configuration loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8011331994795112,
                        "reason": "The description directly addresses the question by explaining that Secrets Manager deletion events can indicate either cleanup operations or potential credential loss incidents, aligning well with the input query about security incidents versus routine cleanup. It is clear and concise, providing relevant context about what is measured (secrets removal) and its implications. However, it could improve coverage by explicitly mentioning units or frequency and offering more guidance on distinguishing between routine and incident-related deletions to enhance utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question asked.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise in the context of the Input question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the Input question.\",\n    \"Judge Utility by determining if the description provides sufficient information to help a user effectively answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.8011331994795112"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question asked.",
                            "Evaluate Clarity by checking if the description is understandable and concise in the context of the Input question.",
                            "Assess Coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the Input question.",
                            "Judge Utility by determining if the description provides sufficient information to help a user effectively answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sts.amazonaws.com\\\", \\\"EventName\\\": \\\"Assume*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in AssumeRole events indicating unauthorized or unexpected credential assumption?\", \"retrieval_context\": [\" - IAM update events. Permission modifications, credential changes, policy updates, and access control adjustments. Privilege escalation detection and security policy enforcement monitoring.\", \" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - This pattern captures STS assume role events. It identifies when temporary credentials are assumed, including cross-account access and role switching activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.586959176308697,
                        "reason": "The input question is clear and specific about detecting spikes in AssumeRole events indicating unauthorized credential use. The retrieval context partially addresses this by describing the pattern capturing STS assume role events and their relevance to temporary credentials and cross-account access. However, it lacks explicit information on how spikes or anomalies are detected, measured, or interpreted, limiting its utility in fully answering the question. The context also includes unrelated event types, which may reduce focus.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to ensure it can be directly addressed by the metric description.\",\n    \"Assess the Retrieval Context (metric description) for clarity, checking if it is understandable and concise relative to the question asked.\",\n    \"Check Coverage in the Retrieval Context to confirm it explains what is measured, units, context, and potential implications relevant to the Input.\",\n    \"Evaluate Utility by determining if the Retrieval Context provides sufficient information to effectively answer the Input question; consider how well both align.\"\n] \n \nRubric:\nNone \n \nScore: 0.586959176308697"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to ensure it can be directly addressed by the metric description.",
                            "Assess the Retrieval Context (metric description) for clarity, checking if it is understandable and concise relative to the question asked.",
                            "Check Coverage in the Retrieval Context to confirm it explains what is measured, units, context, and potential implications relevant to the Input.",
                            "Evaluate Utility by determining if the Retrieval Context provides sufficient information to effectively answer the Input question; consider how well both align."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"logs.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Create* events for new log groups or streams causing a spike in storage usage?\", \"retrieval_context\": [\" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - This pattern captures CloudWatch Logs creation events. It identifies when log groups and log streams are created, including log group configurations and retention settings.\", \" - This pattern captures CloudWatch creation events. It identifies when CloudWatch resources are created, including alarms, dashboards, and log groups.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.40315811154077635,
                        "reason": "The description partially addresses the question by identifying that the pattern captures CloudWatch Logs creation events, including log groups and streams, which relates to the 'Create* events' in the question. However, it lacks clarity and coverage regarding whether these events cause a spike in storage usage, as it does not mention storage metrics, units, or implications. The description is somewhat understandable but does not provide sufficient context or utility to effectively answer the monitoring question about storage spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input question with the retrieval context to assess if the description directly addresses the monitoring question.\",\n    \"Evaluate the clarity of the description by checking if it is understandable, concise, and free from ambiguity with respect to the question.\",\n    \"Check the coverage of the description to see if it explains what is measured, includes units, context, and implications relevant to the question.\",\n    \"Determine utility by verifying whether the description enables a user to effectively answer the provided question, considering both input and retrieval context alignment.\"\n] \n \nRubric:\nNone \n \nScore: 0.40315811154077635"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input question with the retrieval context to assess if the description directly addresses the monitoring question.",
                            "Evaluate the clarity of the description by checking if it is understandable, concise, and free from ambiguity with respect to the question.",
                            "Check the coverage of the description to see if it explains what is measured, includes units, context, and implications relevant to the question.",
                            "Determine utility by verifying whether the description enables a user to effectively answer the provided question, considering both input and retrieval context alignment."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"sts.amazonaws.com\\\", \\\"EventName\\\": \\\"Decode*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Decode events related to failed or denied authentication attempts?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - This pattern captures STS decode operations. It identifies when authorization messages are decoded, useful for troubleshooting authentication and authorization issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5377540675801801,
                        "reason": "The Input is clear and concise, asking specifically about Decode events related to failed or denied authentication attempts. The Retrieval Context partially covers Decode events by mentioning STS decode operations and their use in troubleshooting authentication and authorization issues, but it lacks explicit information on whether these events indicate failed or denied authentication attempts. The other context entries are unrelated to Decode events, reducing overall relevance. To improve, the Retrieval Context should explicitly address the relationship between Decode events and authentication failures or denials to better support answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input description to ensure it is understandable and concise without ambiguity.\",\n    \"Assess the Retrieval Context for coverage, checking if it explains what is measured, units, relevant context, and implications.\",\n    \"Compare the Input and Retrieval Context to verify that the description supports answering the monitoring question effectively (utility).\",\n    \"If any aspect scores below 8, identify specific gaps or unclear parts in either Input or Retrieval Context and suggest targeted improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.5377540675801801"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input description to ensure it is understandable and concise without ambiguity.",
                            "Assess the Retrieval Context for coverage, checking if it explains what is measured, units, relevant context, and implications.",
                            "Compare the Input and Retrieval Context to verify that the description supports answering the monitoring question effectively (utility).",
                            "If any aspect scores below 8, identify specific gaps or unclear parts in either Input or Retrieval Context and suggest targeted improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"logs.amazonaws.com\\\", \\\"EventName\\\": \\\"Filter*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Filter* events indicating a high volume of log filtering or querying?\", \"retrieval_context\": [\" - This pattern captures CloudWatch Logs deletion events. It identifies when log groups and log streams are removed, which can indicate cleanup operations or potential log loss incidents.\", \" - This pattern captures CloudWatch Logs filter operations. It identifies when log events are filtered or queried, including metric filter operations and log insights queries.\", \" - This pattern captures CloudWatch Logs get operations. It identifies when log events are retrieved from log streams, useful for monitoring log access patterns and troubleshooting activities.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6903510163919606,
                        "reason": "The retrieval context directly addresses the question by describing the pattern capturing CloudWatch Logs filter operations, including filtering and querying events. It is clear and understandable, specifying the types of operations involved. However, it lacks details on units, volume thresholds, or implications of high volume, limiting its coverage and utility for assessing whether there is a high volume of such events.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate Clarity by checking if the description is succinct and understandable, avoiding ambiguity when related to the question.\",\n    \"Assess Coverage by confirming the description explains what is measured, including units, context, and implications relevant to the question.\",\n    \"Judge Utility by determining if the description provides enough actionable information to help a user effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6903510163919606"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate Clarity by checking if the description is succinct and understandable, avoiding ambiguity when related to the question.",
                            "Assess Coverage by confirming the description explains what is measured, including units, context, and implications relevant to the question.",
                            "Judge Utility by determining if the description provides enough actionable information to help a user effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"logs.amazonaws.com\\\", \\\"EventName\\\": \\\"Put*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of Put operations on CloudWatch Logs indicating a potential logging issue?\", \"retrieval_context\": [\" - This pattern captures CloudWatch Logs deletion events. It identifies when log groups and log streams are removed, which can indicate cleanup operations or potential log loss incidents.\", \" - This pattern captures CloudWatch Logs put operations. It identifies when log events are written to log streams, including application logs, system logs, and custom log data.\", \" - This pattern captures CloudWatch put operations. It identifies when CloudWatch resources are created or updated, including metric data, log events, and alarm configurations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6148047192393273,
                        "reason": "The description is somewhat clear in relating to Put operations on CloudWatch Logs, specifically identifying when log events are written to log streams. However, it lacks explicit mention of spikes or how these operations are measured or quantified, missing units or metrics. The context about potential logging issues is implied but not directly addressed, limiting utility for answering the monitoring question fully. Including details on how spikes are detected or the implications of increased Put operations would improve coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess clarity: check if the description is understandable and directly relates to the question's terminology and intent.\",\n    \"Evaluate coverage by verifying if the description explains what is measured, the units involved, the relevant context, and the implications, ensuring all these details support the monitoring question.\",\n    \"Assess utility by determining whether the description provides sufficient information to answer the monitoring question effectively, looking for actionable insights or explanations.\",\n    \"Integrate the evaluation by scoring the description from 1 to 10 based on clarity, coverage, and utility relative to the question; if below 8, suggest targeted improvements focused on missing or unclear elements in the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.6148047192393273"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess clarity: check if the description is understandable and directly relates to the question's terminology and intent.",
                            "Evaluate coverage by verifying if the description explains what is measured, the units involved, the relevant context, and the implications, ensuring all these details support the monitoring question.",
                            "Assess utility by determining whether the description provides sufficient information to answer the monitoring question effectively, looking for actionable insights or explanations.",
                            "Integrate the evaluation by scoring the description from 1 to 10 based on clarity, coverage, and utility relative to the question; if below 8, suggest targeted improvements focused on missing or unclear elements in the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"kms.amazonaws.com\\\", \\\"EventName\\\": \\\"Generate*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Generate* events related to a specific KMS key or are they generating new keys at an unusual rate?\", \"retrieval_context\": [\" - This pattern captures KMS creation events. It identifies when KMS keys are created, including key configurations, key material generation, and key policy settings.\", \" - This pattern captures KMS decryption events. It identifies when data is decrypted using KMS keys, useful for monitoring data access patterns and security compliance.\", \" - This pattern captures KMS key generation events. It identifies when cryptographic keys are generated, including data keys, random values, and key material.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4024127689726099,
                        "reason": "The Retrieval Context identifies that Generate* events capture KMS key generation and creation activities, which partially addresses whether these events relate to specific KMS keys or new key generation. However, it lacks clarity on whether the events are tied to specific keys or the rate of generation, and does not provide measurement units, context on unusual rates, or implications. The description is somewhat clear but incomplete, limiting the user's ability to confidently answer the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question.\",\n    \"Evaluate the clarity of the Retrieval Context ensuring it is concise and easily understandable without ambiguity.\",\n    \"Assess the coverage of the Retrieval Context by confirming it details what is measured, the measurement units, relevant context, and implications related to the question.\",\n    \"Determine the utility by judging if the Retrieval Context provides sufficient information for the user to confidently answer the Input question; also verify alignment between both.\"\n] \n \nRubric:\nNone \n \nScore: 0.4024127689726099"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question.",
                            "Evaluate the clarity of the Retrieval Context ensuring it is concise and easily understandable without ambiguity.",
                            "Assess the coverage of the Retrieval Context by confirming it details what is measured, the measurement units, relevant context, and implications related to the question.",
                            "Determine the utility by judging if the Retrieval Context provides sufficient information for the user to confidently answer the Input question; also verify alignment between both."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"kms.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CreateKey events indicating a new KMS key was created?\", \"retrieval_context\": [\" - This pattern captures KMS creation events. It identifies when KMS keys are created, including key configurations, key material generation, and key policy settings.\", \" - This pattern captures KMS deletion events. It identifies when KMS keys are removed, which can indicate cleanup operations or potential data access loss incidents.\", \" - This pattern captures KMS key generation events. It identifies when cryptographic keys are generated, including data keys, random values, and key material.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.815024639890374,
                        "reason": "The description directly addresses the monitoring question by focusing on CreateKey events related to new KMS key creation, as supported by the Retrieval Context. It is clear and concise, explaining what is measured (KMS creation events) and providing relevant context about key configurations and material generation. However, it could improve coverage by explicitly mentioning units or metrics and more clearly stating the implications of detecting such events for monitoring purposes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Retrieval Context and Input to ensure the description directly addresses the monitoring question.\",\n    \"Evaluate the Clarity of the Input description by checking if it is concise and easily understandable within the Retrieval Context.\",\n    \"Assess Coverage by verifying the description explains what is measured, the units used, contextual background, and the implications relevant to the question.\",\n    \"Determine Utility by judging whether the description enables a user to effectively answer the monitoring question, based on the information provided in both Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.815024639890374"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Retrieval Context and Input to ensure the description directly addresses the monitoring question.",
                            "Evaluate the Clarity of the Input description by checking if it is concise and easily understandable within the Retrieval Context.",
                            "Assess Coverage by verifying the description explains what is measured, the units used, contextual background, and the implications relevant to the question.",
                            "Determine Utility by judging whether the description enables a user to effectively answer the monitoring question, based on the information provided in both Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"logs.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events indicating a deliberate cleanup operation or an unexpected removal of log groups and streams?\", \"retrieval_context\": [\" - This pattern captures CloudWatch Logs deletion events. It identifies when log groups and log streams are removed, which can indicate cleanup operations or potential log loss incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures Secrets Manager deletion events. It identifies when secrets are removed, which can indicate cleanup operations or potential credential loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7,
                        "reason": "The description clearly relates to the Input question by identifying deletion events for log groups and streams and linking them to cleanup or potential loss incidents, addressing relevance and clarity. However, it lacks specific details on measurement units, frequency, or contextual thresholds, limiting its utility for fully determining whether deletions are deliberate or unexpected. Including more explicit guidance on interpreting these events or additional context would improve coverage and help users answer the question more effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description is clearly understandable and directly related to the question being asked, ensuring clarity and relevance.\",\n    \"Evaluate whether the description covers all necessary aspects including what is measured, units, context, and implications, and verify that this coverage aligns well with the specifics of the Input question.\",\n    \"Determine if the description is sufficiently detailed and structured to help a user effectively answer the Input question, assessing the overall utility in relation to the question's intent and scope.\",\n    \"If any of the above evaluations score below the threshold, identify specific gaps between the Input and Retrieval Context and provide targeted suggestions to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.7"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description is clearly understandable and directly related to the question being asked, ensuring clarity and relevance.",
                            "Evaluate whether the description covers all necessary aspects including what is measured, units, context, and implications, and verify that this coverage aligns well with the specifics of the Input question.",
                            "Determine if the description is sufficiently detailed and structured to help a user effectively answer the Input question, assessing the overall utility in relation to the question's intent and scope.",
                            "If any of the above evaluations score below the threshold, identify specific gaps between the Input and Retrieval Context and provide targeted suggestions to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"kms.amazonaws.com\\\", \\\"EventName\\\": \\\"Decrypt*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Decrypt events indicating unauthorized or excessive data access?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - This pattern captures KMS decryption events. It identifies when data is decrypted using KMS keys, useful for monitoring data access patterns and security compliance.\", \" - This pattern captures KMS encryption events. It identifies when data is encrypted using KMS keys, useful for monitoring data protection activities and compliance requirements.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31480471942727456,
                        "reason": "The Retrieval Context partially addresses the Input by describing KMS decryption events relevant to data access monitoring, but it lacks specific information about detecting spikes or unauthorized/excessive access. The description is somewhat clear but does not explain units, how spikes are identified, or implications of such spikes, limiting its utility in fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus and intent.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if the metric description is easily understandable and concise without ambiguity.\",\n    \"Assess the coverage of the Retrieval Context by verifying if it explains what is measured, the units used, relevant context, and potential implications, ensuring completeness in relation to the Input question.\",\n    \"Determine the utility by judging whether the Retrieval Context provides sufficient information to effectively answer the Input question, and if not, identify specific areas lacking detail.\"\n] \n \nRubric:\nNone \n \nScore: 0.31480471942727456"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus and intent.",
                            "Evaluate the clarity of the Retrieval Context by checking if the metric description is easily understandable and concise without ambiguity.",
                            "Assess the coverage of the Retrieval Context by verifying if it explains what is measured, the units used, relevant context, and potential implications, ensuring completeness in relation to the Input question.",
                            "Determine the utility by judging whether the Retrieval Context provides sufficient information to effectively answer the Input question, and if not, identify specific areas lacking detail."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"logs.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in the number of GetLogEvents requests that could indicate a performance issue with log retrieval?\", \"retrieval_context\": [\" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - This pattern captures CloudWatch Logs get operations. It identifies when log events are retrieved from log streams, useful for monitoring log access patterns and troubleshooting activities.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6924141814233309,
                        "reason": "The description is generally clear and understandable, focusing on spikes in GetLogEvents requests as a potential performance issue. The retrieval context provides relevant information about CloudWatch Logs get operations, including their purpose and monitoring value. However, the description lacks explicit mention of units or specific metrics measured, and the implications for performance issues are only implied rather than detailed. This limits full coverage and utility, as users may need additional information to confidently answer the monitoring question. Including units, clearer measurement details, and explicit implications would improve the response.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description against the retrieval context to assess clarity: check if the description is concise, understandable, and free of ambiguity when considered alongside the context.\",\n    \"Evaluate coverage by verifying that the description together with the retrieval context explains what is measured, includes units, provides relevant context, and discusses implications necessary to answer the monitoring question.\",\n    \"Assess utility by determining if the combined information from the input and retrieval context enables a user to confidently answer the monitoring question without additional information.\",\n    \"If any of the above criteria are not satisfactorily met (overall score below 8), recommend specific improvements targeting missing clarity, incomplete coverage, or lack of actionable help relevant to the provided question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6924141814233309"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description against the retrieval context to assess clarity: check if the description is concise, understandable, and free of ambiguity when considered alongside the context.",
                            "Evaluate coverage by verifying that the description together with the retrieval context explains what is measured, includes units, provides relevant context, and discusses implications necessary to answer the monitoring question.",
                            "Assess utility by determining if the combined information from the input and retrieval context enables a user to confidently answer the monitoring question without additional information.",
                            "If any of the above criteria are not satisfactorily met (overall score below 8), recommend specific improvements targeting missing clarity, incomplete coverage, or lack of actionable help relevant to the provided question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"secretsmanager.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in GetSecretsManager operations that could indicate unauthorized access to sensitive data?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - This pattern captures Secrets Manager deletion events. It identifies when secrets are removed, which can indicate cleanup operations or potential credential loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.19046505402422648,
                        "reason": "The response addresses Secrets Manager deletion events but does not align with the input metric about spikes in GetSecretsManager operations. It lacks clarity and does not explain what is measured, units, or implications related to unauthorized access, thus providing minimal utility for the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description and retrieval context to ensure they both address the same metric and monitoring question.\",\n    \"Evaluate the description's clarity and determine if it is concise and easily understandable in relation to the specific context provided.\",\n    \"Assess coverage by verifying if the description explains what is measured, the units involved, relevant context, and the implications in relation to the retrieval context.\",\n    \"Determine the utility by checking if the description effectively supports answering the monitoring question given the retrieval context and input.\"\n] \n \nRubric:\nNone \n \nScore: 0.19046505402422648"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description and retrieval context to ensure they both address the same metric and monitoring question.",
                            "Evaluate the description's clarity and determine if it is concise and easily understandable in relation to the specific context provided.",
                            "Assess coverage by verifying if the description explains what is measured, the units involved, relevant context, and the implications in relation to the retrieval context.",
                            "Determine the utility by checking if the description effectively supports answering the monitoring question given the retrieval context and input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"secretsmanager.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Secrets Manager update events related to a specific secret or role that could be causing an availability issue?\", \"retrieval_context\": [\" - This pattern captures Secrets Manager get operations. It identifies when secrets are retrieved, useful for monitoring secret access patterns and application authentication activities.\", \" - This pattern captures Secrets Manager put operations. It identifies when secrets are stored or updated, including secret value changes and configuration modifications.\", \" - This pattern captures Secrets Manager update events. It identifies when secrets are modified, including secret value updates, rotation configurations, and access policy changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.612776402095889,
                        "reason": "The input question is clear and concise, focusing on whether Secrets Manager update events relate to a specific secret or role causing availability issues, which aligns with the retrieval context about update, put, and get operations. However, the input lacks explicit mention of units, detailed context, or implications, limiting coverage and utility for fully answering the monitoring question. The description partially aligns with the retrieval context but could be improved by specifying how these events impact availability or identifying particular secrets or roles.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input description for clarity by checking if it is understandable and concise relative to the Retrieval Context.\",\n    \"Assess the Coverage by verifying if the description explains what is measured, including units, context, and implications as relevant to the question posed in the Retrieval Context.\",\n    \"Determine Utility by confirming if the description provides enough information to help answer the monitoring question in the Retrieval Context.\",\n    \"Compare Input and Retrieval Context to ensure the description aligns with the specific question and monitoring scenario, improving relevance and accuracy.\"\n] \n \nRubric:\nNone \n \nScore: 0.612776402095889"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input description for clarity by checking if it is understandable and concise relative to the Retrieval Context.",
                            "Assess the Coverage by verifying if the description explains what is measured, including units, context, and implications as relevant to the question posed in the Retrieval Context.",
                            "Determine Utility by confirming if the description provides enough information to help answer the monitoring question in the Retrieval Context.",
                            "Compare Input and Retrieval Context to ensure the description aligns with the specific question and monitoring scenario, improving relevance and accuracy."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ecs.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Create* events indicating a sudden increase in ECS resource creation?\", \"retrieval_context\": [\" - This pattern captures EC2 resource creation events. It identifies when EC2 resources are created, including security groups, network interfaces, and other EC2 components.\", \" - This pattern captures ECS creation events. It identifies when ECS resources are created, including clusters, services, task definitions, and capacity providers.\", \" - This pattern captures EKS creation events. It identifies when EKS clusters are created, including cluster configurations, node groups, and add-on installations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5737524452036413,
                        "reason": "The retrieval context identifies ECS creation events and the types of resources involved, which aligns with the input question about Create* events for ECS resource creation. However, it lacks details on detecting spikes or sudden increases, units of measurement, or implications, limiting clarity and utility for answering the question fully.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate Clarity by assessing if the description is understandable and concise in the context of the provided question.\",\n    \"Assess Coverage by checking if the description explains what is measured, the units used, relevant context, and implications that relate to the question.\",\n    \"Determine Utility by verifying if the description enables a user to effectively answer the input question using the provided context.\"\n] \n \nRubric:\nNone \n \nScore: 0.5737524452036413"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate Clarity by assessing if the description is understandable and concise in the context of the provided question.",
                            "Assess Coverage by checking if the description explains what is measured, the units used, relevant context, and implications that relate to the question.",
                            "Determine Utility by verifying if the description enables a user to effectively answer the input question using the provided context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"eks.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the EKS update events causing a spike in UnHealthyHostCount?\", \"retrieval_context\": [\" - This pattern captures EKS creation events. It identifies when EKS clusters are created, including cluster configurations, node groups, and add-on installations.\", \" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\", \" - This pattern captures EKS update events. It identifies when EKS clusters are modified, including version upgrades, node group updates, and add-on modifications.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2,
                        "reason": "The retrieval context explains patterns capturing EKS update, deletion, and creation events but does not describe UnHealthyHostCount or its measurement, units, or implications. It lacks information linking EKS update events to spikes in UnHealthyHostCount, limiting its utility to confidently answer the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains what is measured relevant to the question.\",\n    \"Evaluate if the description covers essential aspects such as units, measurement context, and implications that directly support answering the Input question.\",\n    \"Determine the utility of the description by verifying whether it provides sufficient information to confidently respond to the Input question without ambiguity.\",\n    \"If the description does not adequately relate to or support the Input question, identify specific missing elements or clarity issues that limit its effectiveness.\"\n] \n \nRubric:\nNone \n \nScore: 0.2"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains what is measured relevant to the question.",
                            "Evaluate if the description covers essential aspects such as units, measurement context, and implications that directly support answering the Input question.",
                            "Determine the utility of the description by verifying whether it provides sufficient information to confidently respond to the Input question without ambiguity.",
                            "If the description does not adequately relate to or support the Input question, identify specific missing elements or clarity issues that limit its effectiveness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"eks.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CreateCluster events that indicate a new EKS cluster is being created?\", \"retrieval_context\": [\" - This pattern captures EKS creation events. It identifies when EKS clusters are created, including cluster configurations, node groups, and add-on installations.\", \" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\", \" - This pattern captures EKS update events. It identifies when EKS clusters are modified, including version upgrades, node group updates, and add-on modifications.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7953350323351981,
                        "reason": "The description clearly addresses the question by identifying the pattern that captures EKS creation events, specifying that it detects when clusters are created along with configurations, node groups, and add-ons. It is concise and understandable, providing relevant context about what is measured. However, it lacks explicit mention of units or detailed implications, and does not explicitly state how to use this information to confirm new cluster creation events, which slightly limits its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the question being asked.\",\n    \"Evaluate the clarity of the description by assessing whether it is understandable and concise in relation to the question\u2019s focus.\",\n    \"Assess coverage by verifying if the description explains what is measured, including units, relevant context, and implications that are necessary to answer the question.\",\n    \"Determine utility by confirming that the description provides actionable information enabling the user to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.7953350323351981"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly addresses the question being asked.",
                            "Evaluate the clarity of the description by assessing whether it is understandable and concise in relation to the question\u2019s focus.",
                            "Assess coverage by verifying if the description explains what is measured, including units, relevant context, and implications that are necessary to answer the question.",
                            "Determine utility by confirming that the description provides actionable information enabling the user to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"eks.amazonaws.com\\\", \\\"EventName\\\": \\\"Describe*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in DescribeClusterOperations that correlate with errors in the EKS API?\", \"retrieval_context\": [\" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\", \" - This pattern captures EKS describe operations. It identifies when EKS cluster information is queried, including cluster status, node group details, and add-on configurations.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.29381394445659226,
                        "reason": "The Retrieval Context partially addresses the Input by describing 'DescribeClusterOperations' as capturing EKS describe operations, which relates to querying cluster information. However, it does not mention spikes, correlation with errors, or provide metrics, units, or implications needed to answer the question about spikes correlating with EKS API errors. The context is clear but lacks coverage and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable and concise relative to the terminology and scope set by the Input.\",\n    \"Assess Coverage by verifying the Retrieval Context explains what is measured, includes units, provides context, and outlines implications that relate to or clarify the Input question.\",\n    \"Judge Utility by determining if the Retrieval Context enables the user to confidently answer the Input question using the information provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.29381394445659226"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable and concise relative to the terminology and scope set by the Input.",
                            "Assess Coverage by verifying the Retrieval Context explains what is measured, includes units, provides context, and outlines implications that relate to or clarify the Input question.",
                            "Judge Utility by determining if the Retrieval Context enables the user to confidently answer the Input question using the information provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"eks.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a specific EKS cluster that was previously in use?\", \"retrieval_context\": [\" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\", \" - This pattern captures ElastiCache cluster deletion events. It identifies when ElastiCache clusters are removed, which can indicate cleanup operations or potential cache loss incidents.\", \" - This pattern captures KMS deletion events. It identifies when KMS keys are removed, which can indicate cleanup operations or potential data access loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6679178699175392,
                        "reason": "The description directly addresses deletion events related to EKS clusters, confirming that the Delete* events capture EKS cluster removals. It clearly explains what is measured (EKS deletion events) and the implications (cleanup or disruption). However, it does not explicitly confirm if these events are tied to a specific previously used EKS cluster, limiting full coverage and actionable insight for the user's question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to assess if the description directly addresses the user's question.\",\n    \"Evaluate the clarity of the description by checking its readability and conciseness in explaining the metric relevant to the question.\",\n    \"Assess coverage by verifying whether the description includes what is measured, the units used, relevant context, and implications tied to the question.\",\n    \"Judge utility by determining if the description provides actionable insights that can effectively answer the input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6679178699175392"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to assess if the description directly addresses the user's question.",
                            "Evaluate the clarity of the description by checking its readability and conciseness in explaining the metric relevant to the question.",
                            "Assess coverage by verifying whether the description includes what is measured, the units used, relevant context, and implications tied to the question.",
                            "Judge utility by determining if the description provides actionable insights that can effectively answer the input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"guardduty.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the GuardDuty threat detection events indicate a spike in unusual activity or security incidents?\", \"retrieval_context\": [\" - Detective security investigation events. Threat analysis, security incident investigation, and behavioral analytics. Security forensics and threat intelligence correlation.\", \" - GuardDuty threat detection events. Security alerts, threat intelligence, compromised credentials, unusual activity detection, and security incident response. Critical for security monitoring and threat hunting.\", \" - Security Hub findings and compliance events. Security posture assessment, compliance monitoring, security control evaluation, and risk management. Centralized security governance and audit reporting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.36224593382055115,
                        "reason": "The retrieval context mentions GuardDuty threat detection events and related concepts like unusual activity detection and security incident response, which aligns with the input question about spikes in unusual activity or security incidents. However, the description lacks specific details on what is measured, units, or how to identify a spike, limiting clarity and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to determine if the description directly addresses the question's intent and requirements.\",\n    \"Evaluate the Input and Retrieval Context together for clarity by checking if the metric description uses understandable language and concise explanations that relate clearly to the question.\",\n    \"Assess coverage by verifying if the Retrieval Context explains what is measured, including units, relevant context, and potential implications that can satisfy the input question's inquiry.\",\n    \"Determine utility by confirming whether the metric description provides sufficient details from the Retrieval Context that enable the user to confidently answer the question posed in the Input.\"\n] \n \nRubric:\nNone \n \nScore: 0.36224593382055115"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to determine if the description directly addresses the question's intent and requirements.",
                            "Evaluate the Input and Retrieval Context together for clarity by checking if the metric description uses understandable language and concise explanations that relate clearly to the question.",
                            "Assess coverage by verifying if the Retrieval Context explains what is measured, including units, relevant context, and potential implications that can satisfy the input question's inquiry.",
                            "Determine utility by confirming whether the metric description provides sufficient details from the Retrieval Context that enable the user to confidently answer the question posed in the Input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ssm.amazonaws.com\\\", \\\"EventName\\\": \\\"Send*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Send* events indicating a potential issue with instance management?\", \"retrieval_context\": [\" - This pattern captures EC2 instance termination events. It identifies when EC2 instances are stopped or terminated, which can indicate cost optimization or security incidents.\", \" - This pattern captures SQS message send events. It identifies when messages are sent to SQS queues, useful for monitoring message flow and application activity.\", \" - This pattern captures Systems Manager send events. It identifies when commands are sent to instances, including command executions, automation runs, and session manager connections.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5859740773758498,
                        "reason": "The description identifies different types of Send events, including Systems Manager send events and SQS message send events, which relate to the question about Send* event spikes. However, it lacks explicit information on how to detect spikes or interpret them as potential instance management issues. The operational context is partially covered, but the implications regarding instance management problems are not clearly connected, limiting confidence in answering the question fully.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to assess if the description clearly and concisely explains the metric relevant to the question.\",\n    \"Evaluate if the description covers all necessary aspects: what is measured, units, operational context, and the implications that relate specifically to the question asked.\",\n    \"Determine the utility of the description by checking if it provides sufficient information to confidently answer the question posed.\",\n    \"If any of these elements are lacking, assign a score below 8 and suggest specific improvements targeting the missing coverage or clarity gaps relative to the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.5859740773758498"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to assess if the description clearly and concisely explains the metric relevant to the question.",
                            "Evaluate if the description covers all necessary aspects: what is measured, units, operational context, and the implications that relate specifically to the question asked.",
                            "Determine the utility of the description by checking if it provides sufficient information to confidently answer the question posed.",
                            "If any of these elements are lacking, assign a score below 8 and suggest specific improvements targeting the missing coverage or clarity gaps relative to the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"securityhub.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the Security Hub findings indicate a spike in unauthorized access or data exposure?\", \"retrieval_context\": [\" - Detective security investigation events. Threat analysis, security incident investigation, and behavioral analytics. Security forensics and threat intelligence correlation.\", \" - GuardDuty threat detection events. Security alerts, threat intelligence, compromised credentials, unusual activity detection, and security incident response. Critical for security monitoring and threat hunting.\", \" - Security Hub findings and compliance events. Security posture assessment, compliance monitoring, security control evaluation, and risk management. Centralized security governance and audit reporting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3880747525440612,
                        "reason": "The retrieval context mentions Security Hub findings and related security monitoring activities but lacks specific details about what constitutes a spike in unauthorized access or data exposure, the units or metrics measured, and how to interpret the findings. The description is somewhat ambiguous and does not provide actionable insights directly addressing the monitoring question about spikes in unauthorized access or data exposure. To improve, the description should explicitly define the types of findings indicating unauthorized access or data exposure, include relevant metrics or thresholds, and clarify how to detect spikes or trends from the Security Hub data.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description is clearly written and free from ambiguity (Clarity).\",\n    \"Evaluate if the description sufficiently covers essential details such as what is measured, units, relevant context, and possible implications relevant to the question (Coverage).\",\n    \"Assess whether the description provides actionable insights or information that directly helps answer the monitoring question (Utility).\",\n    \"If any evaluation score is below 8, generate a concrete suggestion that specifically addresses missing clarity, coverage, or utility aspects relative to the input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3880747525440612"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description is clearly written and free from ambiguity (Clarity).",
                            "Evaluate if the description sufficiently covers essential details such as what is measured, units, relevant context, and possible implications relevant to the question (Coverage).",
                            "Assess whether the description provides actionable insights or information that directly helps answer the monitoring question (Utility).",
                            "If any evaluation score is below 8, generate a concrete suggestion that specifically addresses missing clarity, coverage, or utility aspects relative to the input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ecs.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events indicating a service disruption incident due to removal of ECS resources?\", \"retrieval_context\": [\" - This pattern captures ECS deletion events. It identifies when ECS resources are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7851952807606726,
                        "reason": "The description clearly relates to the question by identifying ECS deletion events and their potential to indicate service disruption incidents, addressing the monitoring question directly. It is concise and understandable without unnecessary information. However, it lacks explicit mention of measurement units or detailed contextual implications within the cloud environment, slightly limiting coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly relates to and can address the specific question asked.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise, with no ambiguous or unnecessary information that could confuse the user.\",\n    \"Assess Coverage by verifying the description includes what is measured, the units used, contextual details relevant to the cloud environment, and any implications of the metric values.\",\n    \"Judge Utility by determining if the description provides sufficient information to directly help a user answer the monitoring question, indicating practical relevance and usefulness.\"\n] \n \nRubric:\nNone \n \nScore: 0.7851952807606726"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly relates to and can address the specific question asked.",
                            "Evaluate Clarity by checking if the description is understandable and concise, with no ambiguous or unnecessary information that could confuse the user.",
                            "Assess Coverage by verifying the description includes what is measured, the units used, contextual details relevant to the cloud environment, and any implications of the metric values.",
                            "Judge Utility by determining if the description provides sufficient information to directly help a user answer the monitoring question, indicating practical relevance and usefulness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ssm.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Create* events for SSM resources, indicating potential configuration changes?\", \"retrieval_context\": [\" - This pattern captures Systems Manager creation events. It identifies when SSM resources are created, including parameters, documents, and maintenance windows.\", \" - This pattern captures Systems Manager deletion events. It identifies when SSM resources are removed, which can indicate cleanup operations or potential configuration loss incidents.\", \" - This pattern captures Systems Manager update events. It identifies when SSM resources are modified, including parameter updates, document modifications, and maintenance window changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5940457253711573,
                        "reason": "The Retrieval Context clearly describes the capture of creation, update, and deletion events for SSM resources, which aligns with the Input's focus on Create* events. However, it lacks explicit mention of detecting spikes or temporal patterns, which is central to the Input's intent of identifying potential configuration changes. The context provides relevant event types and scope but does not offer measurement details, units, or guidance on interpreting spikes, limiting its ability to fully answer the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's intent and scope.\",\n    \"Evaluate the Input for clarity and specificity, then verify if the Retrieval Context covers the necessary measurement details, units, and implications to sufficiently inform the user's inquiry.\",\n    \"Assess how well the Retrieval Context helps interpret or answer the Input by examining if it provides relevant context and actionable insights.\",\n    \"Integrate coverage and clarity by ensuring the description is concise but comprehensive enough to make the Input understandable and answerable; identify gaps where the Input's needs are unmet.\"\n] \n \nRubric:\nNone \n \nScore: 0.5940457253711573"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's intent and scope.",
                            "Evaluate the Input for clarity and specificity, then verify if the Retrieval Context covers the necessary measurement details, units, and implications to sufficiently inform the user's inquiry.",
                            "Assess how well the Retrieval Context helps interpret or answer the Input by examining if it provides relevant context and actionable insights.",
                            "Integrate coverage and clarity by ensuring the description is concise but comprehensive enough to make the Input understandable and answerable; identify gaps where the Input's needs are unmet."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ecs.amazonaws.com\\\", \\\"EventName\\\": \\\"Run*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in RunTaskCount indicating a potential issue with ECS task execution?\", \"retrieval_context\": [\" - This pattern captures ECS run events. It identifies when ECS tasks are started, including task launches, service deployments, and scheduled task executions.\", \" - This pattern captures ECS stop events. It identifies when ECS tasks are stopped, which can indicate task completion, scaling operations, or service updates.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3148047201790633,
                        "reason": "The retrieval context partially addresses the input by describing ECS run and stop events related to task execution, which is relevant to RunTaskCount spikes. However, it lacks explicit mention of the metric RunTaskCount itself, measurement units, or how spikes are identified or interpreted. The description is somewhat clear but does not provide sufficient detail or direct linkage to detecting potential issues from spikes, limiting its utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (the monitoring question) with the Retrieval Context (the metric description) to ensure the description directly addresses the question's intent and scope.\",\n    \"Evaluate Clarity by determining if the metric description is concise and easily understandable, focusing on whether terminology and phrasing support comprehension in relation to the question.\",\n    \"Assess Coverage by verifying if the description includes key details like what is measured, the measurement units, the context of usage, and implications, ensuring these elements align with the needs highlighted by the input question.\",\n    \"Judge Utility by considering whether the description provides sufficient information for a user to confidently answer the monitoring question, reflecting on how well the description serves the input's informational demand.\"\n] \n \nRubric:\nNone \n \nScore: 0.3148047201790633"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (the monitoring question) with the Retrieval Context (the metric description) to ensure the description directly addresses the question's intent and scope.",
                            "Evaluate Clarity by determining if the metric description is concise and easily understandable, focusing on whether terminology and phrasing support comprehension in relation to the question.",
                            "Assess Coverage by verifying if the description includes key details like what is measured, the measurement units, the context of usage, and implications, ensuring these elements align with the needs highlighted by the input question.",
                            "Judge Utility by considering whether the description provides sufficient information for a user to confidently answer the monitoring question, reflecting on how well the description serves the input's informational demand."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ecs.amazonaws.com\\\", \\\"EventName\\\": \\\"Stop*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Stop* events in ECS indicating a scaling operation or task completion, rather than an error condition?\", \"retrieval_context\": [\" - This pattern captures ECS stop events. It identifies when ECS tasks are stopped, which can indicate task completion, scaling operations, or service updates.\", \" - This pattern captures ELB deregistration events. It identifies when targets are removed from load balancers, which can indicate scaling operations or maintenance activities.\", \" - This pattern captures RDS stop events. It identifies when RDS instances are stopped, which can indicate cost optimization or maintenance operations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8924141814233308,
                        "reason": "The Input question is clear and the Retrieval Context directly addresses ECS stop events, explaining that these events indicate task completion, scaling operations, or service updates, which aligns well with the query. The context is concise and provides relevant implications, supporting the user's intent effectively. Minor improvement could be made by explicitly stating that these events are not error conditions to fully close the loop.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate Clarity by checking if the Input (monitoring question) and Retrieval Context (metric description) are both understandable and concise, ensuring that the description clearly relates to the question.\",\n    \"Assess Coverage by verifying that the Retrieval Context fully explains what is measured, including units, context, and implications, and that these details directly support the Input query.\",\n    \"Determine Utility by confirming that the Retrieval Context provides sufficient information to effectively answer the Input question, showing a clear connection between the metric description and the user's intent.\",\n    \"If the combined evaluation of Input and Retrieval Context results in a score below 8, identify specific gaps such as missing context, unclear terminology, or lack of actionable information, and suggest targeted improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.8924141814233308"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate Clarity by checking if the Input (monitoring question) and Retrieval Context (metric description) are both understandable and concise, ensuring that the description clearly relates to the question.",
                            "Assess Coverage by verifying that the Retrieval Context fully explains what is measured, including units, context, and implications, and that these details directly support the Input query.",
                            "Determine Utility by confirming that the Retrieval Context provides sufficient information to effectively answer the Input question, showing a clear connection between the metric description and the user's intent.",
                            "If the combined evaluation of Input and Retrieval Context results in a score below 8, identify specific gaps such as missing context, unclear terminology, or lack of actionable information, and suggest targeted improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ssm.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events related to a specific SSM resource that was previously in use?\", \"retrieval_context\": [\" - IAM resource deletion events. Identity cleanup, security incident response, access revocation, and compliance maintenance. Critical for security audit trails and access control monitoring.\", \" - This pattern captures Secrets Manager deletion events. It identifies when secrets are removed, which can indicate cleanup operations or potential credential loss incidents.\", \" - This pattern captures Systems Manager deletion events. It identifies when SSM resources are removed, which can indicate cleanup operations or potential configuration loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5701799796262443,
                        "reason": "The description partially aligns with the monitoring question by identifying deletion events related to SSM resources, which addresses the query about Delete* events and SSM. However, it lacks explicit mention of linking these events to specific previously used SSM resources, limiting clarity and coverage. The explanation is somewhat clear and jargon-free but could better clarify implications and units of measurement. Overall, it provides a basic understanding but insufficient detail for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the query and aligns with its intent.\",\n    \"Evaluate clarity by checking if the description is concise, jargon-free, and easy to understand given the question's complexity.\",\n    \"Assess coverage by verifying the description explains what is measured, the units involved, relevant context, and implications in relation to the monitoring question.\",\n    \"Determine utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; suggest improvements if the description falls short.\"\n] \n \nRubric:\nNone \n \nScore: 0.5701799796262443"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description directly addresses the query and aligns with its intent.",
                            "Evaluate clarity by checking if the description is concise, jargon-free, and easy to understand given the question's complexity.",
                            "Assess coverage by verifying the description explains what is measured, the units involved, relevant context, and implications in relation to the monitoring question.",
                            "Determine utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; suggest improvements if the description falls short."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"eks.amazonaws.com\\\", \\\"EventName\\\": \\\"List*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the EKS list operations causing a spike in API calls to eks.amazonaws.com?\", \"retrieval_context\": [\" - EKS list operations. Cluster enumeration, node group discovery, add-on inventory, and resource cataloging. Infrastructure discovery, capacity planning, and operational visibility.\", \" - This pattern captures EKS creation events. It identifies when EKS clusters are created, including cluster configurations, node groups, and add-on installations.\", \" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3182425528251267,
                        "reason": "The Retrieval Context mentions EKS list operations and related activities but does not explain how these operations impact API call volume or provide metrics on spikes. It lacks measurement details, units, or operational implications directly tied to API call spikes, limiting its utility in answering whether list operations cause a spike in API calls. The description is somewhat relevant but too vague and incomplete for the specific Input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input (monitoring question) and Retrieval Context (metric description) together to ensure the description directly addresses the question in an understandable and concise manner.\",\n    \"Assess the coverage of the Retrieval Context by verifying it explains the metric\u2019s measurement, units, operational context, and potential implications relevant to the Input.\",\n    \"Determine the utility by checking if the Retrieval Context enables a user to effectively answer the Input question based on the information provided.\",\n    \"Compare the Input and Retrieval Context for alignment\u2014verify that the description is neither too vague nor too detailed for the question, suggesting improvements if the score is below 8.\"\n] \n \nRubric:\nNone \n \nScore: 0.3182425528251267"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input (monitoring question) and Retrieval Context (metric description) together to ensure the description directly addresses the question in an understandable and concise manner.",
                            "Assess the coverage of the Retrieval Context by verifying it explains the metric\u2019s measurement, units, operational context, and potential implications relevant to the Input.",
                            "Determine the utility by checking if the Retrieval Context enables a user to effectively answer the Input question based on the information provided.",
                            "Compare the Input and Retrieval Context for alignment\u2014verify that the description is neither too vague nor too detailed for the question, suggesting improvements if the score is below 8."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ecs.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any ECS update events that indicate a change to the container instance type or capacity?\", \"retrieval_context\": [\" - CloudFormation stack update events. Infrastructure modifications, resource changes, and configuration updates. Change management and infrastructure evolution tracking.\", \" - This pattern captures ECS update events. It identifies when ECS resources are modified, including service updates, task definition revisions, and cluster configuration changes.\", \" - This pattern captures EKS update events. It identifies when EKS clusters are modified, including version upgrades, node group updates, and add-on modifications.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.33208212878370535,
                        "reason": "The Retrieval Context partially addresses the Input by mentioning ECS update events and related modifications, but it lacks clarity and specificity about changes to container instance type or capacity. It does not explain what exactly is measured, omit units or detailed implications, and does not explicitly link ECS update events to instance type or capacity changes, limiting its utility for fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) for clarity: ensure the description uses understandable language that directly relates to the question.\",\n    \"Evaluate coverage by checking if the Retrieval Context explains what is measured, includes units, provides relevant context, and details implications necessary to address the Input.\",\n    \"Assess utility by determining if the Retrieval Context enables a user to answer the Input question accurately and fully, highlighting any gaps between the two.\",\n    \"If gaps exist between Input and Retrieval Context in clarity, coverage, or utility, identify specific aspects for improvement focused on making the description more aligned and informative relative to the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.33208212878370535"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) for clarity: ensure the description uses understandable language that directly relates to the question.",
                            "Evaluate coverage by checking if the Retrieval Context explains what is measured, includes units, provides relevant context, and details implications necessary to address the Input.",
                            "Assess utility by determining if the Retrieval Context enables a user to answer the Input question accurately and fully, highlighting any gaps between the two.",
                            "If gaps exist between Input and Retrieval Context in clarity, coverage, or utility, identify specific aspects for improvement focused on making the description more aligned and informative relative to the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"secretsmanager.amazonaws.com\\\", \\\"EventName\\\": \\\"Put*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Put* events in Secrets Manager that could indicate a misconfigured secret?\", \"retrieval_context\": [\" - This pattern captures Secrets Manager deletion events. It identifies when secrets are removed, which can indicate cleanup operations or potential credential loss incidents.\", \" - This pattern captures Secrets Manager put operations. It identifies when secrets are stored or updated, including secret value changes and configuration modifications.\", \" - This pattern captures Secrets Manager update events. It identifies when secrets are modified, including secret value updates, rotation configurations, and access policy changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8089778722332731,
                        "reason": "The description clearly addresses the presence of Put* events in Secrets Manager by focusing on put operations that include storing or updating secrets, which aligns well with the monitoring question. It is concise and understandable, covering what is measured (put operations), the context (Secrets Manager), and implications (potential misconfiguration). However, it could improve by explicitly linking these events to misconfiguration risks to fully satisfy the question's focus.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses what the question is asking.\",\n    \"Evaluate the description's clarity and conciseness to determine if it is easily understandable in the context of the question.\",\n    \"Check that the description covers all necessary aspects\u2014what is measured, units, context, and implications\u2014and verify this aligns with the retrieval context's focus.\",\n    \"Assess the utility of the description in helping a user answer the monitoring question, ensuring that both input and retrieval context relate effectively to each other.\"\n] \n \nRubric:\nNone \n \nScore: 0.8089778722332731"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses what the question is asking.",
                            "Evaluate the description's clarity and conciseness to determine if it is easily understandable in the context of the question.",
                            "Check that the description covers all necessary aspects\u2014what is measured, units, context, and implications\u2014and verify this aligns with the retrieval context's focus.",
                            "Assess the utility of the description in helping a user answer the monitoring question, ensuring that both input and retrieval context relate effectively to each other."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ssm.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Update events from ssm.amazonaws.com that could indicate a configuration change affecting system availability?\", \"retrieval_context\": [\" - API Gateway update events. API modifications, configuration changes, and deployment updates. API versioning, performance tuning, and security policy updates.\", \" - This pattern captures Systems Manager deletion events. It identifies when SSM resources are removed, which can indicate cleanup operations or potential configuration loss incidents.\", \" - This pattern captures Systems Manager update events. It identifies when SSM resources are modified, including parameter updates, document modifications, and maintenance window changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6437823499114202,
                        "reason": "The retrieval context partially addresses the question by describing Systems Manager update events and the types of modifications captured, which aligns with the focus on update events from ssm.amazonaws.com. However, it lacks explicit mention of system availability impact or how these events relate to configuration changes affecting availability. The description is clear and concise but does not fully cover measurement context or implications, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question\u2019s focus.\",\n    \"Evaluate the clarity of the description to ensure it is straightforward and concise, facilitating user understanding in the context of the question.\",\n    \"Assess the coverage by verifying if the description details what is measured, the units used, the measurement context, and the implications relevant to the monitoring question.\",\n    \"Determine the utility of the description by checking if it provides sufficient information to confidently answer the question; if not, identify specific missing elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6437823499114202"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question\u2019s focus.",
                            "Evaluate the clarity of the description to ensure it is straightforward and concise, facilitating user understanding in the context of the question.",
                            "Assess the coverage by verifying if the description details what is measured, the units used, the measurement context, and the implications relevant to the monitoring question.",
                            "Determine the utility of the description by checking if it provides sufficient information to confidently answer the question; if not, identify specific missing elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ssm.amazonaws.com\\\", \\\"EventName\\\": \\\"Get*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Get* operations on SSM resources causing an unusual spike in traffic?\", \"retrieval_context\": [\" - S3 get operations. Object downloads, data retrieval, access pattern analysis, and usage monitoring. Data access auditing, performance optimization, and cost analysis.\", \" - This pattern captures Systems Manager get operations. It identifies when SSM resources are queried, including parameter retrievals, document downloads, and instance information requests.\", \" - This pattern captures Systems Manager update events. It identifies when SSM resources are modified, including parameter updates, document modifications, and maintenance window changes.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6015843748099394,
                        "reason": "The Retrieval Context partially addresses the question by describing that the pattern captures Systems Manager get operations, including parameter retrievals and instance information requests, which relates to the Get* operations on SSM resources. However, it lacks explicit information about traffic spikes or how to detect unusual traffic patterns, limiting its utility. The language is clear and concise, but the context does not provide measurable indicators, units, or implications needed to fully answer whether there is an unusual spike in traffic.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) to the Retrieval Context (description) to check if the description clearly addresses the question asked.\",\n    \"Evaluate the clarity of the Retrieval Context in terms of language simplicity and conciseness, ensuring it can be easily understood in relation to the Input.\",\n    \"Assess the coverage of the Retrieval Context to verify it includes what is measured, units, relevant context, and implications needed to answer the Input question.\",\n    \"Determine the utility by verifying if the Retrieval Context provides actionable information that directly supports resolving the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6015843748099394"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) to the Retrieval Context (description) to check if the description clearly addresses the question asked.",
                            "Evaluate the clarity of the Retrieval Context in terms of language simplicity and conciseness, ensuring it can be easily understood in relation to the Input.",
                            "Assess the coverage of the Retrieval Context to verify it includes what is measured, units, relevant context, and implications needed to answer the Input question.",
                            "Determine the utility by verifying if the Retrieval Context provides actionable information that directly supports resolving the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"config.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any configuration changes detected by AWS Config that could be causing resource drift or non-compliance?\", \"retrieval_context\": [\" - AWS Config compliance events. Configuration drift detection, compliance auditing, change tracking, and governance monitoring. Resource inventory management and regulatory compliance.\", \" - This pattern captures EC2 modification events. It identifies when EC2 resources are modified, including security group rules, instance attributes, and network configurations.\", \" - This pattern captures ELB modification events. It identifies when load balancers are modified, including configuration changes, target group updates, and listener modifications.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6019633292718871,
                        "reason": "The retrieval context partially addresses the question by mentioning AWS Config compliance events and configuration drift detection, which relate to detecting configuration changes causing drift or non-compliance. However, the description is somewhat fragmented and lacks explicit details on how these events are measured, the units, or the direct implications for resource drift or non-compliance. While it references relevant monitoring areas, it does not clearly explain how the metric helps answer the question, limiting clarity and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the metric description is easily understandable and concise enough to inform the user about the metric without ambiguity.\",\n    \"Assess Coverage by verifying the description includes what is measured, the units, the monitoring context, and the implications related to the question.\",\n    \"Determine Utility by confirming that the metric description provides sufficient information for the user to confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6019633292718871"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the metric description is easily understandable and concise enough to inform the user about the metric without ambiguity.",
                            "Assess Coverage by verifying the description includes what is measured, the units, the monitoring context, and the implications related to the question.",
                            "Determine Utility by confirming that the metric description provides sufficient information for the user to confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"detective.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do the number of security incidents reported by Detective exceed ten, indicating a potential security breach?\", \"retrieval_context\": [\" - Detective security investigation events. Threat analysis, security incident investigation, and behavioral analytics. Security forensics and threat intelligence correlation.\", \" - GuardDuty threat detection events. Security alerts, threat intelligence, compromised credentials, unusual activity detection, and security incident response. Critical for security monitoring and threat hunting.\", \" - Security Hub findings and compliance events. Security posture assessment, compliance monitoring, security control evaluation, and risk management. Centralized security governance and audit reporting.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31480472055495773,
                        "reason": "The retrieval context mentions 'Detective security investigation events' and related security activities but does not explicitly address the number of security incidents or provide any quantitative measure or threshold such as 'exceed ten.' The description is somewhat clear about the types of events but lacks specific coverage of the metric needed to answer the monitoring question, limiting its utility for confidently determining if incidents exceed ten.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly addresses the question\u2019s focus.\",\n    \"Evaluate Clarity by checking if the description is concise and easily understandable in relation to the question\u2019s terminology and intent.\",\n    \"Assess Coverage to ensure the description explains what is measured, the units used, relevant context, and implications directly related to the monitoring question.\",\n    \"Determine Utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question, highlighting gaps between input needs and the retrieval provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.31480472055495773"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description clearly addresses the question\u2019s focus.",
                            "Evaluate Clarity by checking if the description is concise and easily understandable in relation to the question\u2019s terminology and intent.",
                            "Assess Coverage to ensure the description explains what is measured, the units used, relevant context, and implications directly related to the monitoring question.",
                            "Determine Utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question, highlighting gaps between input needs and the retrieval provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"macie.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in Macie data discovery events indicating potential sensitive data exposure?\", \"retrieval_context\": [\" - GuardDuty threat detection events. Security alerts, threat intelligence, compromised credentials, unusual activity detection, and security incident response. Critical for security monitoring and threat hunting.\", \" - IAM policy attachment events. Permission assignment, access grant operations, and privilege escalation. Critical security monitoring for policy distribution and access control changes.\", \" - Macie data discovery events. Sensitive data identification, classification, data loss prevention, and privacy compliance. PII detection and data governance monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.42092920163852793,
                        "reason": "The input clearly asks about spikes in Macie data discovery events indicating potential sensitive data exposure, which is specific and actionable. The retrieval context mentions Macie data discovery events and related topics like sensitive data identification and PII detection, but it lacks explicit information about spikes, event frequency, or metrics that would indicate potential exposure. The context is broad and does not provide measurable or time-based data to directly answer the monitoring question, limiting its utility for detecting spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Assess the Input (monitoring question) for clarity and specificity to ensure it clearly defines what needs to be measured or understood.\",\n    \"Evaluate the Retrieval Context (metric description) for clarity and completeness, checking if it explicitly states what is measured, the units involved, relevant context, and possible implications.\",\n    \"Compare the Retrieval Context against the Input to determine if the description sufficiently addresses the monitoring question, ensuring it provides actionable information to answer the question.\",\n    \"Assign an overall quality score based on how well the Retrieval Context aligns with and supports the Input, and if below 8, suggest precise improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.42092920163852793"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Assess the Input (monitoring question) for clarity and specificity to ensure it clearly defines what needs to be measured or understood.",
                            "Evaluate the Retrieval Context (metric description) for clarity and completeness, checking if it explicitly states what is measured, the units involved, relevant context, and possible implications.",
                            "Compare the Retrieval Context against the Input to determine if the description sufficiently addresses the monitoring question, ensuring it provides actionable information to answer the question.",
                            "Assign an overall quality score based on how well the Retrieval Context aligns with and supports the Input, and if below 8, suggest precise improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"redshift.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Redshift cluster deletion events exceeding ten per minute, indicating a potential analytics infrastructure issue?\", \"retrieval_context\": [\" - Redshift cluster creation events. Data warehouse provisioning, cluster configuration, and analytics infrastructure setup. Business intelligence infrastructure deployment.\", \" - Redshift cluster deletion events. Data warehouse decommissioning, cost optimization, and infrastructure cleanup. Analytics infrastructure lifecycle management.\", \" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.32227001388253085,
                        "reason": "The Retrieval Context mentions Redshift cluster deletion events and related operational themes but does not specify the metric's units, measurement frequency, or thresholds such as 'exceeding ten per minute.' It lacks clarity and detail needed to directly answer whether deletion events surpass the specified rate or indicate infrastructure issues, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate if the Input (monitoring question) is clearly reflected in the Retrieval Context (metric description), ensuring the description addresses the question directly.\",\n    \"Assess the clarity of the Retrieval Context by verifying if it is succinct, understandable, and free of ambiguity.\",\n    \"Check the coverage within the Retrieval Context to ensure it explains what the metric measures, its units, relevant context, and potential implications.\",\n    \"Determine the utility by confirming whether the Retrieval Context provides sufficient information for the user to effectively answer the Input question; if not, identify gaps connecting the two.\"\n] \n \nRubric:\nNone \n \nScore: 0.32227001388253085"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate if the Input (monitoring question) is clearly reflected in the Retrieval Context (metric description), ensuring the description addresses the question directly.",
                            "Assess the clarity of the Retrieval Context by verifying if it is succinct, understandable, and free of ambiguity.",
                            "Check the coverage within the Retrieval Context to ensure it explains what the metric measures, its units, relevant context, and potential implications.",
                            "Determine the utility by confirming whether the Retrieval Context provides sufficient information for the user to effectively answer the Input question; if not, identify gaps connecting the two."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudformation.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Is the number of CreateStack events exceeding ten, indicating a potential infrastructure provisioning issue?\", \"retrieval_context\": [\" - CloudFormation stack creation events. Infrastructure provisioning, resource deployment, and environment setup. Infrastructure as Code monitoring and deployment tracking.\", \" - Redshift cluster creation events. Data warehouse provisioning, cluster configuration, and analytics infrastructure setup. Business intelligence infrastructure deployment.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4642721220881686,
                        "reason": "The retrieval context partially aligns with the input by mentioning CloudFormation stack creation events related to infrastructure provisioning, which is relevant to CreateStack events. However, it lacks specific details on the metric such as the count threshold (exceeding ten), units, or implications of exceeding this number. The description is somewhat clear but not concise or focused solely on the CreateStack event count, limiting its utility for accurately answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to verify if the description clearly aligns with the question's intent.\",\n    \"Evaluate Clarity by checking if the metric description is understandable, concise, and free of ambiguity relative to the question asked.\",\n    \"Assess Coverage by ensuring the description includes details on what is measured, units, relevant context, and implications needed to address the question.\",\n    \"Determine Utility by confirming if the description provides actionable information that a user can apply to answer the monitoring question accurately.\"\n] \n \nRubric:\nNone \n \nScore: 0.4642721220881686"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to verify if the description clearly aligns with the question's intent.",
                            "Evaluate Clarity by checking if the metric description is understandable, concise, and free of ambiguity relative to the question asked.",
                            "Assess Coverage by ensuring the description includes details on what is measured, units, relevant context, and implications needed to address the question.",
                            "Determine Utility by confirming if the description provides actionable information that a user can apply to answer the monitoring question accurately."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudformation.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the CloudFormation stack update events related to a recent change in HealthyHostCount?\", \"retrieval_context\": [\" - CloudFormation stack creation events. Infrastructure provisioning, resource deployment, and environment setup. Infrastructure as Code monitoring and deployment tracking.\", \" - CloudFormation stack update events. Infrastructure modifications, resource changes, and configuration updates. Change management and infrastructure evolution tracking.\", \" - Lambda function update events. Code deployments, configuration changes, environment variable modifications, memory scaling, timeout adjustments, and function versioning. Application lifecycle management and deployment tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.28807970670261973,
                        "reason": "The retrieval context describes CloudFormation stack update events in general terms but does not explicitly link these events to changes in the HealthyHostCount metric or explain how to determine such a relationship. It lacks details on what HealthyHostCount measures, its units, or how it relates to stack updates, limiting the utility for answering the monitoring question. To improve, the description should clarify the connection between stack update events and HealthyHostCount changes, including relevant metrics and implications.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description is clear and concise in addressing the question.\",\n    \"Evaluate if the description covers all necessary aspects (what is measured, units, context, and implications) relevant to the question asked.\",\n    \"Determine the utility of the description by judging if it enables a user to effectively answer the provided monitoring question based on the given information.\",\n    \"If any of the above aspects score below 8, identify specific gaps in clarity, coverage, or utility and provide targeted suggestions for improvement.\"\n] \n \nRubric:\nNone \n \nScore: 0.28807970670261973"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess whether the description is clear and concise in addressing the question.",
                            "Evaluate if the description covers all necessary aspects (what is measured, units, context, and implications) relevant to the question asked.",
                            "Determine the utility of the description by judging if it enables a user to effectively answer the provided monitoring question based on the given information.",
                            "If any of the above aspects score below 8, identify specific gaps in clarity, coverage, or utility and provide targeted suggestions for improvement."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"apigateway.amazonaws.com\\\", \\\"EventName\\\": \\\"Update*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Update events in the Apigateway that could be causing a disruption to our APIs?\", \"retrieval_context\": [\" - API Gateway creation events. API deployment, endpoint provisioning, and service integration. Application architecture monitoring and API lifecycle management.\", \" - API Gateway deletion events. API decommissioning, endpoint removal, and service cleanup. Application lifecycle management and resource optimization.\", \" - API Gateway update events. API modifications, configuration changes, and deployment updates. API versioning, performance tuning, and security policy updates.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8035466007186015,
                        "reason": "The response aligns well with the retrieval context by focusing on API Gateway update events, which directly relate to the user's question about potential disruptions caused by updates. The description is clear and concise, mentioning relevant aspects such as API modifications, configuration changes, and deployment updates. It covers what is measured (update events) and the context (API Gateway), enabling effective monitoring of disruptions. However, it could improve by explicitly detailing units or metrics used and more clearly linking implications to disruptions.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description with the retrieval context by assessing if both align clearly in terms of purpose and scope.\",\n    \"Evaluate the clarity of the metric description by ensuring it is concise and understandable within the context provided by the retrieval.\",\n    \"Check coverage by verifying that the description adequately details what is measured, units used, relevant context, and implications, as informed by the retrieval context.\",\n    \"Assess utility by determining whether the description, in conjunction with the retrieval context, enables the user to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.8035466007186015"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description with the retrieval context by assessing if both align clearly in terms of purpose and scope.",
                            "Evaluate the clarity of the metric description by ensuring it is concise and understandable within the context provided by the retrieval.",
                            "Check coverage by verifying that the description adequately details what is measured, units used, relevant context, and implications, as informed by the retrieval context.",
                            "Assess utility by determining whether the description, in conjunction with the retrieval context, enables the user to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"cloudformation.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are these CloudFormation stack deletion events related to a recent spike in UnHealthyHostCount?\", \"retrieval_context\": [\" - CloudFormation stack deletion events. Infrastructure cleanup, resource decommissioning, and environment teardown. Cost optimization and resource lifecycle management.\", \" - This pattern captures CloudFront distribution deletion events. It identifies when CloudFront distributions are removed, which can indicate cleanup operations or potential service disruption incidents.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.671745127901524,
                        "reason": "The description clearly relates to CloudFormation stack deletion events and their context of infrastructure cleanup, which supports investigating their relation to UnHealthyHostCount spikes. It is concise and understandable, mentioning relevant implications like resource decommissioning and potential service disruption. However, it lacks specific measurement details such as units or direct linkage to UnHealthyHostCount metrics, limiting full coverage and utility for conclusively answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context to assess if the description clearly relates to and supports answering the monitoring question.\",\n    \"Evaluate Clarity by checking if the metric description is concise and understandable within the context of the question and retrieval information.\",\n    \"Assess Coverage by verifying that the description details what is measured, including units, context, and implications relevant to the retrieval context.\",\n    \"Judge Utility by determining whether the description provides sufficient information to effectively answer the monitoring question based on the input and retrieval context alignment.\"\n] \n \nRubric:\nNone \n \nScore: 0.671745127901524"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context to assess if the description clearly relates to and supports answering the monitoring question.",
                            "Evaluate Clarity by checking if the metric description is concise and understandable within the context of the question and retrieval information.",
                            "Assess Coverage by verifying that the description details what is measured, including units, context, and implications relevant to the retrieval context.",
                            "Judge Utility by determining whether the description provides sufficient information to effectively answer the monitoring question based on the input and retrieval context alignment."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"ce.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in UnHealthyHostCount that correlate with drops in HealthyHostCount?\", \"retrieval_context\": [\" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - This pattern captures ElastiCache cluster deletion events. It identifies when ElastiCache clusters are removed, which can indicate cleanup operations or potential cache loss incidents.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.06224593312018546,
                        "reason": "The retrieval context does not address the specific monitoring question about spikes in UnHealthyHostCount correlating with drops in HealthyHostCount. The context only describes ElastiCache failover, Lambda invocation, and cluster deletion events, which are unrelated to the health counts of hosts. The description lacks clarity, coverage of the metric's units or implications, and utility for answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.\",\n    \"Evaluate Clarity by checking if the description is easy to understand and concise when explaining the metric in relation to the question.\",\n    \"Assess Coverage by verifying the description explains what is measured, including units, the monitoring context, and implications relevant to the question.\",\n    \"Judge Utility by determining if the description provides sufficient information to help a user confidently answer the monitoring question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.06224593312018546"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.",
                            "Evaluate Clarity by checking if the description is easy to understand and concise when explaining the metric in relation to the question.",
                            "Assess Coverage by verifying the description explains what is measured, including units, the monitoring context, and implications relevant to the question.",
                            "Judge Utility by determining if the description provides sufficient information to help a user confidently answer the monitoring question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"codedeploy.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Is the number of failed deployments exceeding five, indicating a potential issue with application rollout?\", \"retrieval_context\": [\" - CodeDeploy deployment events. Application deployment, release management, and deployment strategy execution. CI/CD pipeline monitoring and deployment automation tracking.\", \" - Lambda function update events. Code deployments, configuration changes, environment variable modifications, memory scaling, timeout adjustments, and function versioning. Application lifecycle management and deployment tracking.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2562176500885798,
                        "reason": "The retrieval context mentions CodeDeploy deployment events and deployment tracking, which is somewhat related to the input question about failed deployments. However, it does not directly address the number of failed deployments, thresholds like 'exceeding five,' or implications for application rollout issues. The description is somewhat clear but lacks specific metrics, units, or operational context needed to confidently answer the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description addresses the question directly and clearly.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable and concise, with no ambiguous or overly technical language relative to the Input.\",\n    \"Assess Coverage by verifying that the Retrieval Context includes information on what is measured, units, operational context, and potential implications relevant to the Input.\",\n    \"Determine Utility by confirming whether the Retrieval Context provides sufficient information for a user to confidently answer the Input question based on the description provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.2562176500885798"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description addresses the question directly and clearly.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable and concise, with no ambiguous or overly technical language relative to the Input.",
                            "Assess Coverage by verifying that the Retrieval Context includes information on what is measured, units, operational context, and potential implications relevant to the Input.",
                            "Determine Utility by confirming whether the Retrieval Context provides sufficient information for a user to confidently answer the Input question based on the description provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"apprunner.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any App Runner deployment events indicating a failed or cancelled deployment?\", \"retrieval_context\": [\" - App Runner deployment events. Containerized application deployment, service scaling, and application lifecycle management. Container orchestration and application hosting.\", \" - CodeDeploy deployment events. Application deployment, release management, and deployment strategy execution. CI/CD pipeline monitoring and deployment automation tracking.\", \" - CodePipeline execution events. CI/CD workflow execution, build processes, and deployment pipeline monitoring. Software delivery automation and release management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3119202928280223,
                        "reason": "The retrieval context mentions 'App Runner deployment events' but only in a general sense related to deployment, scaling, and lifecycle management without specifying whether it includes failed or cancelled deployment events. The description lacks clarity and detail on what specific events are tracked or how failures/cancellations are indicated, limiting its utility to definitively answer the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description directly addresses the question.\",\n    \"Evaluate the clarity of the Retrieval Context by ensuring it is concise and understandable without ambiguity.\",\n    \"Assess the coverage of the Retrieval Context by verifying it includes what is measured, the units, the monitoring context, and potential implications relevant to the Input question.\",\n    \"Determine the utility by judging whether the description empowers a user to definitively answer the monitoring question; if not, identify missing information linking the description to the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3119202928280223"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description directly addresses the question.",
                            "Evaluate the clarity of the Retrieval Context by ensuring it is concise and understandable without ambiguity.",
                            "Assess the coverage of the Retrieval Context by verifying it includes what is measured, the units, the monitoring context, and potential implications relevant to the Input question.",
                            "Determine the utility by judging whether the description empowers a user to definitively answer the monitoring question; if not, identify missing information linking the description to the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"kinesis.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any spikes in Kinesis PutRecord or PutRecords errors that could indicate a production issue?\", \"retrieval_context\": [\" - Kinesis streaming events. Real-time data processing, stream management, and data ingestion operations. Streaming analytics and event-driven architecture monitoring.\", \" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.10179862078894324,
                        "reason": "The retrieval context does not specifically address Kinesis PutRecord or PutRecords errors or spikes, instead focusing on ElastiCache failover events, general Kinesis streaming events, and Lambda function creation events. While it mentions Kinesis streaming events broadly, it lacks clarity, detail, and direct relevance to error spikes or production issues, providing minimal utility for answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.\",\n    \"Evaluate Clarity by checking if the description is straightforward and concise while directly relating to elements mentioned in the question.\",\n    \"Assess Coverage by verifying that the description includes what is measured, relevant units, contextual information, and possible implications that pertain to the question.\",\n    \"Determine Utility by confirming whether the description provides sufficient detail and insight to enable answering the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.10179862078894324"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.",
                            "Evaluate Clarity by checking if the description is straightforward and concise while directly relating to elements mentioned in the question.",
                            "Assess Coverage by verifying that the description includes what is measured, relevant units, contextual information, and possible implications that pertain to the question.",
                            "Determine Utility by confirming whether the description provides sufficient detail and insight to enable answering the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"states.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any failed or timed-out executions of the Step Functions workflows?\", \"retrieval_context\": [\" - CodePipeline execution events. CI/CD workflow execution, build processes, and deployment pipeline monitoring. Software delivery automation and release management.\", \" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - Step Functions workflow events. Serverless workflow execution, state machine operations, and business process automation. Workflow orchestration and process monitoring.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.23208212943308304,
                        "reason": "The Retrieval Context mentions Step Functions workflow events and related concepts but does not clearly address failed or timed-out executions specifically. It lacks explicit explanation of the metric being measured, units, or operational context related to failures or timeouts. Consequently, it provides limited actionable information to answer the Input question about execution failures or timeouts.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to assess clarity: ensure the description uses straightforward language that directly relates to the question without ambiguity.\",\n    \"Evaluate coverage by verifying if the Retrieval Context fully explains what metric is being measured, its units, operational context, and potential implications relevant to the Input question.\",\n    \"Assess utility by checking if the description provides enough actionable or interpretative information to effectively answer the Input question based on the Retrieval Context.\",\n    \"If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context, and suggest targeted improvements to enhance clarity, completeness, or practical relevance.\"\n] \n \nRubric:\nNone \n \nScore: 0.23208212943308304"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to assess clarity: ensure the description uses straightforward language that directly relates to the question without ambiguity.",
                            "Evaluate coverage by verifying if the Retrieval Context fully explains what metric is being measured, its units, operational context, and potential implications relevant to the Input question.",
                            "Assess utility by checking if the description provides enough actionable or interpretative information to effectively answer the Input question based on the Retrieval Context.",
                            "If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context, and suggest targeted improvements to enhance clarity, completeness, or practical relevance."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"glue.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any failed Glue ETL jobs that have exceeded their retry threshold?\", \"retrieval_context\": [\" - Glue ETL job events. Data processing, transformation workflows, and data pipeline execution. Data engineering monitoring and ETL job lifecycle management.\", \" - Redshift cluster deletion events. Data warehouse decommissioning, cost optimization, and infrastructure cleanup. Analytics infrastructure lifecycle management.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3101332807162901,
                        "reason": "The retrieval context partially addresses the question by mentioning Glue ETL job events related to data processing and ETL job lifecycle management, which is relevant. However, it lacks specific details about failed jobs, retry thresholds, or how to identify jobs exceeding retries. The description is somewhat clear but not fully comprehensive or actionable for determining failed Glue ETL jobs exceeding retry limits.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the specific monitoring question.\",\n    \"Evaluate Clarity by checking if the description is concise, free of jargon, and easy to understand in relation to the question.\",\n    \"Assess Coverage to confirm the description explains what is measured, the units involved, relevant context, and the potential impact, ensuring it fully supports answering the question.\",\n    \"Judge Utility by determining if the description provides enough actionable information to help a user effectively respond to the given monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3101332807162901"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and Retrieval Context (description) to ensure the description clearly addresses the specific monitoring question.",
                            "Evaluate Clarity by checking if the description is concise, free of jargon, and easy to understand in relation to the question.",
                            "Assess Coverage to confirm the description explains what is measured, the units involved, relevant context, and the potential impact, ensuring it fully supports answering the question.",
                            "Judge Utility by determining if the description provides enough actionable information to help a user effectively respond to the given monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"apigateway.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any CreateAPIGateway events indicating a potential deployment issue?\", \"retrieval_context\": [\" - API Gateway creation events. API deployment, endpoint provisioning, and service integration. Application architecture monitoring and API lifecycle management.\", \" - CloudFormation stack creation events. Infrastructure provisioning, resource deployment, and environment setup. Infrastructure as Code monitoring and deployment tracking.\", \" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6218678846531478,
                        "reason": "The description mentions API Gateway creation events relevant to deployment monitoring, aligning with the monitoring question about CreateAPIGateway events and potential deployment issues. However, it lacks explicit mention of specific event types, error indicators, or how these events signal deployment problems, limiting clarity and utility. Including details on what constitutes a deployment issue within these events would improve coverage and direct usefulness.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (metric description) and retrieval context to assess if the description clearly and concisely defines the metric with sufficient clarity.\",\n    \"Evaluate whether the description comprehensively covers what is measured, including units, contextual factors, and implications relevant to the monitoring question in the retrieval context.\",\n    \"Determine if the description\u2019s content is directly useful for answering the provided monitoring question, ensuring alignment between the input description and the retrieval context.\",\n    \"If deficiencies are found, suggest specific improvements that enhance clarity, coverage, or utility, especially focusing on gaps between the input description and retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6218678846531478"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (metric description) and retrieval context to assess if the description clearly and concisely defines the metric with sufficient clarity.",
                            "Evaluate whether the description comprehensively covers what is measured, including units, contextual factors, and implications relevant to the monitoring question in the retrieval context.",
                            "Determine if the description\u2019s content is directly useful for answering the provided monitoring question, ensuring alignment between the input description and the retrieval context.",
                            "If deficiencies are found, suggest specific improvements that enhance clarity, coverage, or utility, especially focusing on gaps between the input description and retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"apigateway.amazonaws.com\\\", \\\"EventName\\\": \\\"Delete*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Delete* events for APIs that are still receiving traffic?\", \"retrieval_context\": [\" - API Gateway deletion events. API decommissioning, endpoint removal, and service cleanup. Application lifecycle management and resource optimization.\", \" - Lambda function deletion events. Application cleanup, cost optimization, security incident response, and resource decommissioning. Critical for audit trails and compliance monitoring.\", \" - This pattern captures ELB deletion events. It identifies when load balancers are removed, which can indicate cleanup operations or potential service disruption incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4076904832780646,
                        "reason": "The Retrieval Context describes deletion events for APIs, Lambda functions, and ELBs, which relates to the Input question about Delete* events. However, it lacks specific information about whether these deletion events correspond to APIs still receiving traffic, missing key details on traffic status, measurement units, or consequences related to ongoing usage. The context is somewhat clear but does not provide sufficient coverage or utility to confidently answer the question about active traffic during deletions.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to verify that the description directly addresses the question asked.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it is concise and easy to understand in relation to the terminology and intent of the Input question.\",\n    \"Assess the coverage of the Retrieval Context by confirming it explains what is measured, including units, relevant context, and consequences, ensuring these elements align with the specifics of the Input.\",\n    \"Determine the utility by judging if the Retrieval Context provides sufficient information to confidently answer the Input question, highlighting any gaps or missing details.\"\n] \n \nRubric:\nNone \n \nScore: 0.4076904832780646"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to verify that the description directly addresses the question asked.",
                            "Evaluate the clarity of the Retrieval Context by checking if it is concise and easy to understand in relation to the terminology and intent of the Input question.",
                            "Assess the coverage of the Retrieval Context by confirming it explains what is measured, including units, relevant context, and consequences, ensuring these elements align with the specifics of the Input.",
                            "Determine the utility by judging if the Retrieval Context provides sufficient information to confidently answer the Input question, highlighting any gaps or missing details."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"redshift.amazonaws.com\\\", \\\"EventName\\\": \\\"Create*\\\"}\", \"hyperparameters\": null, \"input\": \"Are the Create* events for Redshift clusters exceeding ten per minute?\", \"retrieval_context\": [\" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\", \" - Redshift cluster creation events. Data warehouse provisioning, cluster configuration, and analytics infrastructure setup. Business intelligence infrastructure deployment.\", \" - Redshift cluster deletion events. Data warehouse decommissioning, cost optimization, and infrastructure cleanup. Analytics infrastructure lifecycle management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.40282946619423027,
                        "reason": "The retrieval context partially addresses the input by mentioning 'Redshift cluster creation events,' which aligns with 'Create* events for Redshift clusters.' However, it lacks specific information about the measurement units (events per minute) or thresholds like 'exceeding ten per minute.' The description is somewhat clear but does not explicitly explain what is measured or provide actionable insights to determine if the event rate exceeds the specified limit, limiting its utility in fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the specific query.\",\n    \"Evaluate Clarity by checking if the description is understandable, concise, and free of ambiguity in relation to the terminology used in the input question.\",\n    \"Assess Coverage by verifying if the description explains what is measured, including units, context, and implications that are relevant to answering the input question.\",\n    \"Judge Utility by determining if the description provides actionable or meaningful information that directly helps in answering the input monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.40282946619423027"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the specific query.",
                            "Evaluate Clarity by checking if the description is understandable, concise, and free of ambiguity in relation to the terminology used in the input question.",
                            "Assess Coverage by verifying if the description explains what is measured, including units, context, and implications that are relevant to answering the input question.",
                            "Judge Utility by determining if the description provides actionable or meaningful information that directly helps in answering the input monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"codepipeline.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any failed or cancelled CodePipeline executions that could indicate a problem with software delivery?\", \"retrieval_context\": [\" - CodeDeploy deployment events. Application deployment, release management, and deployment strategy execution. CI/CD pipeline monitoring and deployment automation tracking.\", \" - CodePipeline execution events. CI/CD workflow execution, build processes, and deployment pipeline monitoring. Software delivery automation and release management.\", \" - This pattern captures EC2 instance termination events. It identifies when EC2 instances are stopped or terminated, which can indicate cost optimization or security incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.30646279823984157,
                        "reason": "The retrieval context partially addresses the question by mentioning CodePipeline execution events and CI/CD workflow monitoring, which relate to software delivery. However, it does not explicitly mention failed or cancelled executions or how to identify them, limiting clarity and coverage. The description is somewhat vague and lacks specific measurement details or implications needed to effectively answer the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description addresses the question directly and fully.\",\n    \"Evaluate clarity by checking if the description is concise, uses understandable language, and avoids ambiguity when relating to the question.\",\n    \"Assess coverage by confirming the description includes what is being measured, measurement units, contextual information, and implications relevant to the question.\",\n    \"Determine utility by verifying if the description enables a user to effectively answer the monitoring question, indicating practical usefulness.\"\n] \n \nRubric:\nNone \n \nScore: 0.30646279823984157"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description addresses the question directly and fully.",
                            "Evaluate clarity by checking if the description is concise, uses understandable language, and avoids ambiguity when relating to the question.",
                            "Assess coverage by confirming the description includes what is being measured, measurement units, contextual information, and implications relevant to the question.",
                            "Determine utility by verifying if the description enables a user to effectively answer the monitoring question, indicating practical usefulness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"athena.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Athena query execution errors that exceed ten, indicating a potential issue with data analytics or business intelligence?\", \"retrieval_context\": [\" - Athena query execution events. Data analytics, SQL query processing, and data warehouse operations. Business intelligence monitoring and data exploration tracking.\", \" - Glue ETL job events. Data processing, transformation workflows, and data pipeline execution. Data engineering monitoring and ETL job lifecycle management.\", \" - Redshift cluster deletion events. Data warehouse decommissioning, cost optimization, and infrastructure cleanup. Analytics infrastructure lifecycle management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.26224593382055106,
                        "reason": "The retrieval context mentions Athena query execution events and related domains, aligning somewhat with the input question about Athena query execution errors. However, it lacks specific details on error counts, measurement units, thresholds (such as exceeding ten errors), or implications for data analytics or BI issues. The description is broad and does not clearly or concisely address the monitoring question's intent or provide sufficient information to determine if errors exceed a threshold, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent and terminology.\",\n    \"Evaluate Clarity by checking if the description is concise and easily understandable in relation to the complexity of the input question.\",\n    \"Assess Coverage by verifying whether the description includes what is measured, units, relevant context, and implications, ensuring these align with the information needed to answer the question.\",\n    \"Determine Utility by judging if the description provides sufficient and relevant information that directly helps a user respond to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.26224593382055106"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent and terminology.",
                            "Evaluate Clarity by checking if the description is concise and easily understandable in relation to the complexity of the input question.",
                            "Assess Coverage by verifying whether the description includes what is measured, units, relevant context, and implications, ensuring these align with the information needed to answer the question.",
                            "Determine Utility by judging if the description provides sufficient and relevant information that directly helps a user respond to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventSource\\\": \\\"elasticmapreduce.amazonaws.com\\\", \\\"EventName\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any EMR cluster events indicating a failure to launch or terminate clusters?\", \"retrieval_context\": [\" - EMR cluster events. Big data processing, Hadoop ecosystem operations, and data analytics workflows. Data processing pipeline monitoring and cluster lifecycle management.\", \" - This pattern captures EKS deletion events. It identifies when EKS clusters are removed, which can indicate cleanup operations or potential application disruption incidents.\", \" - This pattern captures ElastiCache cluster deletion events. It identifies when ElastiCache clusters are removed, which can indicate cleanup operations or potential cache loss incidents.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3777299866333616,
                        "reason": "The input question is clear and specific, asking about EMR cluster events related to failure to launch or terminate clusters. However, the retrieval context is incomplete and partially irrelevant: it mentions EMR cluster events only briefly without detailing failure events or their implications, and includes unrelated EKS and ElastiCache deletion events. This limits the context's usefulness in fully addressing the input question. To improve, the context should explicitly describe EMR failure events, their detection, and impact to better support the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input (monitoring question) to ensure it is specific and understandable, enabling precise interpretation of the required metric.\",\n    \"Assess the Retrieval Context (metric description) for completeness, verifying it explains what is measured, units, contextual background, and implications clearly and concisely.\",\n    \"Determine if the Retrieval Context aligns well with the Input by checking if the description sufficiently supports answering the monitoring question.\",\n    \"Provide a score based on combined clarity, coverage, and utility, and if below 8, suggest concrete improvements targeting gaps in explanation or relevance to the Input.\"\n] \n \nRubric:\nNone \n \nScore: 0.3777299866333616"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input (monitoring question) to ensure it is specific and understandable, enabling precise interpretation of the required metric.",
                            "Assess the Retrieval Context (metric description) for completeness, verifying it explains what is measured, units, contextual background, and implications clearly and concisely.",
                            "Determine if the Retrieval Context aligns well with the Input by checking if the description sufficiently supports answering the monitoring question.",
                            "Provide a score based on combined clarity, coverage, and utility, and if below 8, suggest concrete improvements targeting gaps in explanation or relevance to the Input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventName\\\": \\\"Tag*\\\"}\", \"hyperparameters\": null, \"input\": \"Are there any Tag* events that indicate a change to the cost allocation or resource organization of our production resources?\", \"retrieval_context\": [\" - Resource stop operations across all services. Cost optimization, maintenance operations, and service deactivation. Resource lifecycle management and cost control.\", \" - Resource tagging operations across all services. Cost allocation, resource organization, compliance tagging, and governance enforcement. Resource management and financial tracking.\", \" - This pattern captures CloudFront tagging events. It identifies when CloudFront distributions are tagged or untagged, useful for resource management and cost allocation.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.65,
                        "reason": "The Input question is clear and specific about seeking Tag events related to cost allocation or resource organization. The Retrieval Context mentions resource tagging operations, cost allocation, and resource organization, which aligns with the question. However, the context is somewhat fragmented and lacks explicit details on which specific Tag events indicate changes, and it does not clearly explain the implications or units of measurement. Thus, while it provides relevant information, it is insufficiently detailed to fully and directly answer the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to ensure it clearly states what information is sought.\",\n    \"Assess the Retrieval Context (metric description) for clarity by checking if it is understandable and concise in relation to the Input.\",\n    \"Check Coverage by verifying if the Retrieval Context includes what is measured, units, relevant context, and potential implications directly tied to the Input question.\",\n    \"Determine Utility by confirming whether the Retrieval Context provides sufficient information to directly and effectively answer the Input question, ensuring alignment between both.\"\n] \n \nRubric:\nNone \n \nScore: 0.65"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to ensure it clearly states what information is sought.",
                            "Assess the Retrieval Context (metric description) for clarity by checking if it is understandable and concise in relation to the Input.",
                            "Check Coverage by verifying if the Retrieval Context includes what is measured, units, relevant context, and potential implications directly tied to the Input question.",
                            "Determine Utility by confirming whether the Retrieval Context provides sufficient information to directly and effectively answer the Input question, ensuring alignment between both."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventName\\\": \\\"Start*\\\"}\", \"hyperparameters\": null, \"input\": \"Are Start* events triggering a spike in resource utilization?\", \"retrieval_context\": [\" - Resource start operations across all services. Service activation, cost optimization, and operational recovery. Resource lifecycle management and availability monitoring.\", \" - Resource stop operations across all services. Cost optimization, maintenance operations, and service deactivation. Resource lifecycle management and cost control.\", \" - This pattern captures RDS start events. It identifies when RDS instances are started, which can indicate cost optimization or service restoration.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3951858711850744,
                        "reason": "The retrieval context partially addresses the question by describing start events and their operational significance, but it lacks direct information on resource utilization spikes or metrics related to monitoring such spikes. The description is somewhat clear but does not specify what is measured, units, or how start events impact resource utilization, limiting its utility in answering the monitoring question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question's intent.\",\n    \"Evaluate Clarity by checking if the metric description is concise and easily understandable in relation to the question posed.\",\n    \"Assess Coverage by verifying that the description includes what is measured, the units, relevant context, and potential implications tied to the monitoring question.\",\n    \"Determine Utility by ensuring the description provides enough information to effectively answer the monitoring question; if it scores below 8, suggest specific enhancements focused on missing or unclear elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.3951858711850744"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question's intent.",
                            "Evaluate Clarity by checking if the metric description is concise and easily understandable in relation to the question posed.",
                            "Assess Coverage by verifying that the description includes what is measured, the units, relevant context, and potential implications tied to the monitoring question.",
                            "Determine Utility by ensuring the description provides enough information to effectively answer the monitoring question; if it scores below 8, suggest specific enhancements focused on missing or unclear elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventName\\\": \\\"Describe*\\\"}\", \"hyperparameters\": null, \"input\": \"Are Describe* events exceeding their normal rate, potentially indicating a configuration or resource issue?\", \"retrieval_context\": [\" - Resource description operations across all services. Configuration inspection, resource discovery, and operational visibility. Infrastructure monitoring and resource inventory management.\", \" - This pattern captures EKS describe operations. It identifies when EKS cluster information is queried, including cluster status, node group details, and add-on configurations.\", \" - This pattern captures ElastiCache failover events. It identifies when ElastiCache clusters experience failovers, which can indicate high availability events or potential issues.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2022977371039038,
                        "reason": "The Retrieval Context does not directly address whether Describe* events are exceeding their normal rate or indicate configuration/resource issues. While it mentions capturing EKS describe operations and resource description operations, it lacks information on event rates, normal baselines, or implications of increased Describe* events. The descriptions are somewhat clear but insufficiently detailed to answer the question about abnormal event rates or potential issues.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question asked.\",\n    \"Evaluate the clarity of the Retrieval Context: check if the description is understandable and concise relative to the complexity of the Input question.\",\n    \"Assess the coverage of the Retrieval Context in relation to the Input: verify if it explains what is measured, the units used, the relevant context, and possible implications that align with the Input question.\",\n    \"Determine the utility by judging whether the Retrieval Context provides sufficient information to effectively answer the Input question; if gaps are found, identify specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.2022977371039038"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question asked.",
                            "Evaluate the clarity of the Retrieval Context: check if the description is understandable and concise relative to the complexity of the Input question.",
                            "Assess the coverage of the Retrieval Context in relation to the Input: verify if it explains what is measured, the units used, the relevant context, and possible implications that align with the Input question.",
                            "Determine the utility by judging whether the Retrieval Context provides sufficient information to effectively answer the Input question; if gaps are found, identify specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventName\\\": \\\"Stop*\\\"}\", \"hyperparameters\": null, \"input\": \"Are Stop* events causing a spike in resource utilization, potentially impacting application performance?\", \"retrieval_context\": [\" - Lambda function invocation events. Application execution, performance monitoring, error tracking, cold start detection, and usage analytics. Critical for application health monitoring and cost optimization.\", \" - Resource stop operations across all services. Cost optimization, maintenance operations, and service deactivation. Resource lifecycle management and cost control.\", \" - This pattern captures RDS stop events. It identifies when RDS instances are stopped, which can indicate cost optimization or maintenance operations.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2939913345828574,
                        "reason": "The retrieval context mentions 'Resource stop operations' and 'RDS stop events' but does not explicitly link these stop events to resource utilization spikes or application performance impact. It covers what is measured and some context but lacks details on units, direct implications, or how stop events affect resource utilization or performance, limiting its utility in fully answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains the metric relevant to the question.\",\n    \"Evaluate whether the Retrieval Context covers key elements such as what is measured, units, context, and implications, ensuring it aligns well with the Input's informational needs.\",\n    \"Determine if the description provides sufficient utility by enabling a user to answer the Input question based on the information given in the Retrieval Context.\",\n    \"Synthesize clarity, coverage, and utility in relation to how well the Retrieval Context supports the Input question; if deficiencies exist, identify specific improvements targeted at making the connection clearer or more informative.\"\n] \n \nRubric:\nNone \n \nScore: 0.2939913345828574"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains the metric relevant to the question.",
                            "Evaluate whether the Retrieval Context covers key elements such as what is measured, units, context, and implications, ensuring it aligns well with the Input's informational needs.",
                            "Determine if the description provides sufficient utility by enabling a user to answer the Input question based on the information given in the Retrieval Context.",
                            "Synthesize clarity, coverage, and utility in relation to how well the Retrieval Context supports the Input question; if deficiencies exist, identify specific improvements targeted at making the connection clearer or more informative."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventName\\\": \\\"Scale*\\\"}\", \"hyperparameters\": null, \"input\": \"Are Scale* events triggering a surge in new instance creation, potentially overwhelming the system?\", \"retrieval_context\": [\" - Auto-scaling operations across all services. Capacity management, performance optimization, and cost control. Dynamic resource allocation and performance monitoring.\", \" - IAM user role policy group creation events. Critical security operation for access management, privilege escalation monitoring, and compliance auditing. High-priority alerts for new identity creation.\", \" - Lambda function creation events. Serverless compute deployment, function code uploads, runtime configuration, memory allocation, timeout settings, and environment variables. Critical for application deployment monitoring and cost tracking.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3536433346518894,
                        "reason": "The retrieval context includes descriptions related to auto-scaling operations and Lambda function creation events, which are somewhat relevant to the question about Scale* events and instance creation surges. However, the descriptions lack specific metrics, units, or direct linkage to new instance creation surges or system overwhelm, limiting clarity and coverage. Consequently, the utility is low as the user cannot effectively answer the monitoring question based on the provided context.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description is relevant and targeted to answering the question.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise when read in the context of the input question.\",\n    \"Assess Coverage by verifying if the description includes what is measured, units, relevant context, and potential implications to fully support the question.\",\n    \"Determine Utility by judging if the provided description enables the user to effectively answer the monitoring question, reflecting how well both inputs relate.\"\n] \n \nRubric:\nNone \n \nScore: 0.3536433346518894"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description is relevant and targeted to answering the question.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise when read in the context of the input question.",
                            "Assess Coverage by verifying if the description includes what is measured, units, relevant context, and potential implications to fully support the question.",
                            "Determine Utility by judging if the provided description enables the user to effectively answer the monitoring question, reflecting how well both inputs relate."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"EventName\\\": \\\"List*\\\"}\", \"hyperparameters\": null, \"input\": \"Are List* operations causing a spike in the number of resources being listed?\", \"retrieval_context\": [\" - EKS list operations. Cluster enumeration, node group discovery, add-on inventory, and resource cataloging. Infrastructure discovery, capacity planning, and operational visibility.\", \" - Resource listing operations across all services. Inventory management, resource enumeration, and operational discovery. Infrastructure visibility and capacity planning.\", \" - S3 list operations. Bucket enumeration, object discovery, directory browsing, and inventory management. Data exploration, security reconnaissance detection, and access pattern analysis.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.47865876940156704,
                        "reason": "The input clearly asks whether List* operations are causing a spike in resource listings, seeking insight on activity trends. The retrieval context provides descriptions of List operations across services, mentioning their purposes and contexts, but lacks explicit information on measurement, units, or how to detect spikes or trends. While the context is somewhat clear and relevant, it does not fully explain how to assess or quantify spikes, limiting its utility in directly answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) to ensure it clearly specifies what information or insight is sought from the metric description.\",\n    \"Assess the Retrieval Context (metric description) for clarity, verifying it is understandable and concise relative to the Input's requirements.\",\n    \"Check Coverage in the Retrieval Context by confirming it explains the metric's measurement, units, relevant context, and implications to fully address the Input.\",\n    \"Determine Utility by analyzing how well the Retrieval Context enables a user to answer the Input question effectively, considering completeness and relevance.\"\n] \n \nRubric:\nNone \n \nScore: 0.47865876940156704"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) to ensure it clearly specifies what information or insight is sought from the metric description.",
                            "Assess the Retrieval Context (metric description) for clarity, verifying it is understandable and concise relative to the Input's requirements.",
                            "Check Coverage in the Retrieval Context by confirming it explains the metric's measurement, units, relevant context, and implications to fully address the Input.",
                            "Determine Utility by analyzing how well the Retrieval Context enables a user to answer the Input question effectively, considering completeness and relevance."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        }
    }
}
