{
    "test_cases_lookup_map": {
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2\\\", \\\"MetricName\\\": \\\"BlendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does BlendedCost exceed $10,000, indicating an unexpected spike in EC2 compute costs?\", \"retrieval_context\": [\" - This pattern captures EC2 compute costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term, providing a more accurate monthly cost view.\", \" - This pattern captures EC2 compute costs using blended pricing. It identifies the blended rates for EC2 instances in consolidated billing accounts, providing a unified cost view across multiple accounts.\", \" - This pattern captures EC2 compute costs using unblended pricing. It identifies the actual costs charged for EC2 instances, including on-demand, reserved instance, and spot instance charges.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3,
                        "reason": "The retrieval context explains different EC2 cost pricing methods but does not directly address whether BlendedCost exceeds $10,000 or indicate what constitutes an unexpected spike. The language is clear but lacks specific measurement units, thresholds, or implications related to the monitoring question, limiting its utility for answering the question confidently.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent.\",\n    \"Evaluate the clarity of the metric description considering if it uses simple, unambiguous language that a user can quickly understand in relation to the question asked.\",\n    \"Check the coverage by confirming the description includes what is measured, units, relevant context, and implications, making it sufficient to answer the question.\",\n    \"Assess utility by determining if the description provides actionable information that helps the user confidently answer the monitoring question; if not, identify missing elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.3"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's intent.",
                            "Evaluate the clarity of the metric description considering if it uses simple, unambiguous language that a user can quickly understand in relation to the question asked.",
                            "Check the coverage by confirming the description includes what is measured, units, relevant context, and implications, making it sufficient to answer the question.",
                            "Assess utility by determining if the description provides actionable information that helps the user confidently answer the monitoring question; if not, identify missing elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3463900720423303,
                        "reason": "The Retrieval Context mentions unblended costs for specific services but does not clearly address the overall UnblendedCost metric or whether it can be used to determine if costs exceed $1000. It lacks explicit explanation of what is measured, units, or implications related to cost spikes, limiting its utility to confidently answer the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the specific question asked.\",\n    \"Evaluate the clarity of the Retrieval Context to ensure the metric description is understandable and concise in relation to the terminology and concepts used in the Input.\",\n    \"Assess coverage by verifying if the Retrieval Context explains what is measured, its units, context, and implications sufficiently to inform the Input question.\",\n    \"Determine the utility by checking if the description provides enough relevant information for a user to confidently answer the given monitoring question, considering both Input and Retrieval Context together.\"\n] \n \nRubric:\nNone \n \nScore: 0.3463900720423303"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the specific question asked.",
                            "Evaluate the clarity of the Retrieval Context to ensure the metric description is understandable and concise in relation to the terminology and concepts used in the Input.",
                            "Assess coverage by verifying if the Retrieval Context explains what is measured, its units, context, and implications sufficiently to inform the Input question.",
                            "Determine the utility by checking if the description provides enough relevant information for a user to confidently answer the given monitoring question, considering both Input and Retrieval Context together."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"RDS\\\", \\\"MetricName\\\": \\\"AmortizedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are there spikes in AmortizedCost exceeding $100, indicating potential issues with reserved instance utilization?\", \"retrieval_context\": [\" - This pattern captures EC2 compute costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term, providing a more accurate monthly cost view.\", \" - This pattern captures EC2 compute costs using unblended pricing. It identifies the actual costs charged for EC2 instances, including on-demand, reserved instance, and spot instance charges.\", \" - This pattern captures RDS database costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3787714752671885,
                        "reason": "The description explains what amortized cost is and its relation to reserved instances, addressing the cost measurement aspect. However, it does not directly mention spikes, thresholds like $100, or how to identify potential issues with reserved instance utilization, limiting its completeness and utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and completely.\",\n    \"Assess Clarity by evaluating if the description is concise, well-structured, and understandable in the context of the user\u2019s question.\",\n    \"Evaluate Coverage by verifying if the description explains what is measured, including units, relevant context, and implications that relate to the monitoring question.\",\n    \"Determine Utility by checking if the description provides sufficient information to allow the user to confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3787714752671885"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and completely.",
                            "Assess Clarity by evaluating if the description is concise, well-structured, and understandable in the context of the user\u2019s question.",
                            "Evaluate Coverage by verifying if the description explains what is measured, including units, relevant context, and implications that relate to the monitoring question.",
                            "Determine Utility by checking if the description provides sufficient information to allow the user to confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2-OTHER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\", \\\"UsageType\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5537036204462916,
                        "reason": "The description addresses the question by explaining that the pattern captures unblended costs for specific AWS services, which relates to measuring costs like UnblendedCost. However, it lacks explicit mention of the $1000 threshold or how to determine if costs exceed that amount, reducing clarity and utility for directly answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the question being asked.\",\n    \"Evaluate Clarity by checking if the metric description is concise and easily understandable when read alongside the monitoring question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, including units, relevant context, and implications necessary to answer the retrieval question.\",\n    \"Determine Utility by confirming if the description provides enough relevant detail to effectively help a user respond to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.5537036204462916"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context (monitoring question) to ensure the description clearly addresses the question being asked.",
                            "Evaluate Clarity by checking if the metric description is concise and easily understandable when read alongside the monitoring question.",
                            "Assess Coverage by verifying that the description explains what is measured, including units, relevant context, and implications necessary to answer the retrieval question.",
                            "Determine Utility by confirming if the description provides enough relevant detail to effectively help a user respond to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2\\\", \\\"MetricName\\\": \\\"AmortizedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does AmortizedCost exceed $1000, indicating potential overspending on EC2 Reserved Instances?\", \"retrieval_context\": [\" - This pattern captures EC2 compute costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term, providing a more accurate monthly cost view.\", \" - This pattern captures EC2 compute costs using unblended pricing. It identifies the actual costs charged for EC2 instances, including on-demand, reserved instance, and spot instance charges.\", \" - This pattern captures RDS database costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3437823499114202,
                        "reason": "The Retrieval Context explains amortized pricing for EC2 Reserved Instances but does not specify the metric 'AmortizedCost' or provide any units, thresholds, or direct indication of cost values. It lacks explicit information on how to determine if costs exceed $1000 or implications of overspending, limiting its utility to fully answer the Input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.\",\n    \"Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the Input's specific needs.\",\n    \"Check coverage by verifying if the Retrieval Context details what is measured, includes units, context, and implications relevant to the Input.\",\n    \"Assess utility by determining whether the Retrieval Context enables a user to effectively answer the Input question; if not, identify gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.3437823499114202"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.",
                            "Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise in relation to the Input's specific needs.",
                            "Check coverage by verifying if the Retrieval Context details what is measured, includes units, context, and implications relevant to the Input.",
                            "Assess utility by determining whether the Retrieval Context enables a user to effectively answer the Input question; if not, identify gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EBS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3536433348637746,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not directly address whether the UnblendedCost metric exceeds $1000 or explain how to interpret such a threshold. While the description clarifies what costs are captured and their unblended nature, it lacks clarity on units, measurement scope, and implications related to cost spikes, limiting its utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description addresses the question directly.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise, considering the terminology used in relation to the question's language.\",\n    \"Assess Coverage by confirming the description explains what is measured, including units, relevant context, and implications needed to answer the question.\",\n    \"Determine Utility by verifying whether the description provides sufficient information for a user to confidently answer the monitoring question; if not, identify specific missing elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.3536433348637746"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description addresses the question directly.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise, considering the terminology used in relation to the question's language.",
                            "Assess Coverage by confirming the description explains what is measured, including units, relevant context, and implications needed to answer the question.",
                            "Determine Utility by verifying whether the description provides sufficient information for a user to confidently answer the monitoring question; if not, identify specific missing elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"RDS\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this RDS UsageQuantity exceed 1000 units, indicating a potential resource bottleneck?\", \"retrieval_context\": [\" - This pattern captures RDS database costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term.\", \" - This pattern captures RDS usage quantities. It identifies the raw usage numbers for RDS instances, including instance hours and storage GB-months.\", \" - This pattern captures Redshift usage quantities. It identifies the raw usage numbers for Redshift clusters, including node hours and storage GB-months for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6499999999999999,
                        "reason": "The retrieval context addresses RDS usage quantities and specifies the types of units measured (instance hours and storage GB-months), which aligns with the question about usage quantity exceeding 1000 units. However, it lacks explicit explanation of what 'units' refer to in the question, does not clarify the implications of exceeding 1000 units, and provides limited contextual background on resource bottlenecks. The description is somewhat clear but could be more concise and detailed to fully support confident answering.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to assess if the description clearly addresses the question's focus.\",\n    \"Evaluate Clarity by determining whether the description is concise and understandable when read alongside the question.\",\n    \"Assess Coverage by checking if the description explains what is measured, the units, contextual background, and implications relative to the question.\",\n    \"Determine Utility by verifying if the description provides sufficient information for a user to confidently answer the question; if not, suggest specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6499999999999999"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to assess if the description clearly addresses the question's focus.",
                            "Evaluate Clarity by determining whether the description is concise and understandable when read alongside the question.",
                            "Assess Coverage by checking if the description explains what is measured, the units, contextual background, and implications relative to the question.",
                            "Determine Utility by verifying if the description provides sufficient information for a user to confidently answer the question; if not, suggest specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"LAMBDA\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in our Lambda service?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Lambda function costs using unblended pricing. It identifies the actual costs charged for Lambda function invocations, duration, and requests.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.37943322242888733,
                        "reason": "The retrieval context partially addresses the question by describing unblended costs for Lambda functions, which relates to the cost spike inquiry. However, it lacks clarity on whether the metric measures cost thresholds like exceeding $1000, does not specify units explicitly, and omits implications or guidance on interpreting cost spikes. The description is somewhat understandable but not fully sufficient or directly useful for answering the monitoring question about cost exceedance.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.\",\n    \"Evaluate Clarity by checking if the description is understandable, concise, and free of ambiguous terms relative to the question.\",\n    \"Assess Coverage to ensure the description includes what is measured, the units, relevant context, and implications necessary to answer the question.\",\n    \"Determine Utility by verifying if the description provides sufficient and relevant information that directly aids in answering the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.37943322242888733"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.",
                            "Evaluate Clarity by checking if the description is understandable, concise, and free of ambiguous terms relative to the question.",
                            "Assess Coverage to ensure the description includes what is measured, the units, relevant context, and implications necessary to answer the question.",
                            "Determine Utility by verifying if the description provides sufficient and relevant information that directly aids in answering the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"LAMBDA\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric's UsageQuantity exceed 10 million, indicating a potential Lambda function overload?\", \"retrieval_context\": [\" - This pattern captures CloudFront usage quantities. It identifies the raw usage numbers for CloudFront distributions, including data transfer GB and requests for capacity planning.\", \" - This pattern captures Lambda function costs using unblended pricing. It identifies the actual costs charged for Lambda function invocations, duration, and requests.\", \" - This pattern captures Lambda usage quantities. It identifies the raw usage numbers for Lambda functions, including invocation counts and duration for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.705885898429101,
                        "reason": "The description addresses Lambda usage quantities relevant to the question about UsageQuantity exceeding 10 million, indicating potential overload. It clearly states what is measured (raw usage numbers for Lambda functions) and the monitoring context (capacity planning). However, it lacks explicit mention of units or thresholds like '10 million' and does not directly link the metric to overload implications, limiting clarity and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) to the Retrieval Context (monitoring question) to assess if the description clearly addresses the question's intent.\",\n    \"Evaluate Clarity by checking if the description communicates the metric\u2019s purpose and data succinctly without ambiguity, ensuring it is easy to understand in the context of the question.\",\n    \"Assess Coverage by verifying the description includes key elements such as what is measured, units, monitoring context, and implications, specifically relevant to answering the question.\",\n    \"Judge Utility by determining whether the description provides sufficient and relevant information that a user could leverage to confidently respond to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.705885898429101"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) to the Retrieval Context (monitoring question) to assess if the description clearly addresses the question's intent.",
                            "Evaluate Clarity by checking if the description communicates the metric\u2019s purpose and data succinctly without ambiguity, ensuring it is easy to understand in the context of the question.",
                            "Assess Coverage by verifying the description includes key elements such as what is measured, units, monitoring context, and implications, specifically relevant to answering the question.",
                            "Judge Utility by determining whether the description provides sufficient and relevant information that a user could leverage to confidently respond to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"RDS\\\", \\\"MetricName\\\": \\\"BlendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does BlendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures EC2 compute costs using blended pricing. It identifies the blended rates for EC2 instances in consolidated billing accounts, providing a unified cost view across multiple accounts.\", \" - This pattern captures RDS database costs using blended pricing. It identifies the blended rates for RDS instances in consolidated billing accounts.\", \" - This pattern captures S3 storage costs using blended pricing. It identifies the blended rates for S3 storage in consolidated billing accounts.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.39758723076659497,
                        "reason": "The Input question is clear and concise, asking specifically if BlendedCost exceeds $100 to indicate a cost spike. However, the Retrieval Context, while describing blended pricing patterns for RDS, S3, and EC2 costs, does not provide any quantitative data, thresholds, or direct information about cost values or how to determine if costs exceed $100. This lack of specific cost metrics or guidance limits the ability to directly answer the Input question, reducing alignment and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the clarity of the Input description against the Retrieval Context to ensure both are easily understandable and concise.\",\n    \"Evaluate if the Retrieval Context adequately covers what is measured, including units, context, and implications, in alignment with the Input question.\",\n    \"Assess whether the Retrieval Context provides sufficient information to directly answer the Input question, ensuring utility is maintained.\",\n    \"If any aspect scores below 8, suggest specific improvements focused on enhancing alignment and completeness between Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.39758723076659497"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the clarity of the Input description against the Retrieval Context to ensure both are easily understandable and concise.",
                            "Evaluate if the Retrieval Context adequately covers what is measured, including units, context, and implications, in alignment with the Input question.",
                            "Assess whether the Retrieval Context provides sufficient information to directly answer the Input question, ensuring utility is maintained.",
                            "If any aspect scores below 8, suggest specific improvements focused on enhancing alignment and completeness between Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\", \\\"UsageType\\\": \\\"*\\\"}\", \"hyperparameters\": null, \"input\": \"Do spikes in UnblendedCost exceed 20% of the average monthly spend, indicating a potential cost anomaly?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Shield DDoS protection costs using unblended pricing. It identifies the actual costs charged for Shield Advanced protection and data transfer.\", \" - This pattern captures Single Sign-On costs using unblended pricing. It identifies the actual costs charged for SSO user management and application access.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2,
                        "reason": "The Retrieval Context is clear in describing unblended cost patterns for specific services but does not address the Input question about spikes in UnblendedCost exceeding 20% of average monthly spend or indicate how to detect cost anomalies. It lacks coverage of the metric's measurement, units, or implications related to cost anomalies, reducing its utility in answering the question. To improve, the context should explicitly explain how to measure cost spikes, define thresholds, and interpret anomalies.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate Clarity by checking if the Input (monitoring question) and Retrieval Context (metric description) are both understandable and concise, ensuring the description clearly communicates its purpose relative to the question.\",\n    \"Assess Coverage by verifying that the Retrieval Context fully explains what metric is measured, the units involved, relevant contextual information, and implications, ensuring these details are sufficient to address the Input question.\",\n    \"Determine Utility by examining whether the Retrieval Context provides actionable information that directly helps answer the Input question, highlighting how well the description supports resolving the query.\",\n    \"If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context that reduce effectiveness, and suggest targeted improvements to clarify, expand coverage, or increase practical usefulness.\"\n] \n \nRubric:\nNone \n \nScore: 0.2"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate Clarity by checking if the Input (monitoring question) and Retrieval Context (metric description) are both understandable and concise, ensuring the description clearly communicates its purpose relative to the question.",
                            "Assess Coverage by verifying that the Retrieval Context fully explains what metric is measured, the units involved, relevant contextual information, and implications, ensuring these details are sufficient to address the Input question.",
                            "Determine Utility by examining whether the Retrieval Context provides actionable information that directly helps answer the Input question, highlighting how well the description supports resolving the query.",
                            "If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context that reduce effectiveness, and suggest targeted improvements to clarify, expand coverage, or increase practical usefulness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"RDS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Do spikes in UnblendedCost exceed 20% of average monthly spend, indicating a potential cost anomaly?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Single Sign-On costs using unblended pricing. It identifies the actual costs charged for SSO user management and application access.\", \" - This pattern captures Snow Family costs using unblended pricing. It identifies the actual costs charged for Snow Family devices and data transfer services.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.1880797081106935,
                        "reason": "The retrieval context describes costs for specific services using unblended pricing but does not address spikes, percentage changes, or anomalies relative to average monthly spend as asked in the monitoring question. While the descriptions clarify what is measured and the pricing type, they lack clarity and coverage regarding the detection of cost anomalies or thresholds, limiting their utility for answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question\u2019s focus.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise when read alongside the monitoring question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, the units used, the context, and the implications relevant to the monitoring question.\",\n    \"Determine Utility by confirming if the description provides sufficient information to enable a user to confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.1880797081106935"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question\u2019s focus.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise when read alongside the monitoring question.",
                            "Assess Coverage by verifying that the description explains what is measured, the units used, the context, and the implications relevant to the monitoring question.",
                            "Determine Utility by confirming if the description provides sufficient information to enable a user to confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric indicate an unexpected spike in EC2 usage quantities?\", \"retrieval_context\": [\" - This pattern captures CloudFront usage quantities. It identifies the raw usage numbers for CloudFront distributions, including data transfer GB and requests for capacity planning.\", \" - This pattern captures EC2 compute costs using amortized pricing. It identifies costs with upfront fees (like Reserved Instances) spread over the term, providing a more accurate monthly cost view.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6247493456234815,
                        "reason": "The description addresses EC2 usage quantities by explaining it captures raw usage numbers including instance hours, which relates to the monitoring question about EC2 usage spikes. However, it lacks explicit mention of detecting unexpected spikes or how the metric signals such anomalies, limiting clarity and utility. The description is fairly clear and concise but does not fully cover implications for monitoring unexpected changes, reducing its effectiveness for the specific question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.\",\n    \"Evaluate Clarity by checking if the description is concise, easy to understand, and free of jargon or ambiguity relative to the question's focus.\",\n    \"Assess Coverage by verifying the description explains what metric is measured, including units, relevant context, and the implications for monitoring in relation to the question.\",\n    \"Determine Utility by judging if the description provides actionable information that supports answering the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.6247493456234815"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the specific question asked.",
                            "Evaluate Clarity by checking if the description is concise, easy to understand, and free of jargon or ambiguity relative to the question's focus.",
                            "Assess Coverage by verifying the description explains what metric is measured, including units, relevant context, and the implications for monitoring in relation to the question.",
                            "Determine Utility by judging if the description provides actionable information that supports answering the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EBS\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Is the UsageQuantity exceeding 50% of total provisioned capacity, indicating potential resource constraints?\", \"retrieval_context\": [\" - This pattern captures Athena usage quantities. It identifies the raw usage numbers for Athena queries and data scanned TB for capacity planning.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures Route 53 usage quantities. It identifies the raw usage numbers for Route 53 queries and health checks for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.23775406687981454,
                        "reason": "The description partially addresses the question by explaining that usage quantities are captured for capacity planning, but it lacks clarity and specificity regarding whether UsageQuantity is measured as a percentage of total provisioned capacity or how to determine if it exceeds 50%. The language is somewhat clear but does not directly relate to the monitoring question's focus on resource constraints or provide actionable thresholds or implications.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question asked.\",\n    \"Evaluate the description's clarity by assessing if the language is concise and easy to understand in relation to the terminology used in the question.\",\n    \"Assess the coverage by verifying that the description explains what is measured, the units, relevant context, and implications essential to answering the Input question.\",\n    \"Determine the utility by confirming whether the description provides enough actionable information to effectively answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.23775406687981454"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question asked.",
                            "Evaluate the description's clarity by assessing if the language is concise and easy to understand in relation to the terminology used in the question.",
                            "Assess the coverage by verifying that the description explains what is measured, the units, relevant context, and implications essential to answering the Input question.",
                            "Determine the utility by confirming whether the description provides enough actionable information to effectively answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"BlendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are S3 storage costs exceeding $100, indicating a potential cost anomaly?\", \"retrieval_context\": [\" - This pattern captures S3 Glacier costs using unblended pricing. It identifies the actual costs charged for Glacier storage, retrieval, and data transfer.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\", \" - This pattern captures detailed S3 costs broken down by usage type. It identifies specific charges for storage classes, data transfer, and API requests for cost optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5852730161427713,
                        "reason": "The retrieval context clearly describes the components of S3 storage costs using unblended pricing, which aligns with the monitoring question about S3 storage costs. However, it lacks explicit mention of cost thresholds like $100 or how to identify anomalies, limiting its utility for directly answering if costs are exceeding that amount or indicating anomalies. The language is clear and relevant, but coverage and utility are partial due to missing specific guidance on cost anomaly detection.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question.\",\n    \"Evaluate Clarity by checking if the metric description uses simple, understandable language relevant to the monitoring question.\",\n    \"Assess Coverage by verifying if the description explains what is measured, units, context, and potential implications that relate directly to the input question.\",\n    \"Determine Utility by judging if the description provides sufficient information to effectively answer the monitoring question; if not, identify specific gaps linking Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.5852730161427713"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question.",
                            "Evaluate Clarity by checking if the metric description uses simple, understandable language relevant to the monitoring question.",
                            "Assess Coverage by verifying if the description explains what is measured, units, context, and potential implications that relate directly to the input question.",
                            "Determine Utility by judging if the description provides sufficient information to effectively answer the monitoring question; if not, identify specific gaps linking Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this S3 UsageQuantity exceed 10% of total allocated capacity?\", \"retrieval_context\": [\" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures S3 usage quantities. It identifies the raw usage numbers for S3 storage, including GB-months and API requests for capacity planning.\", \" - This pattern captures SNS usage quantities. It identifies the raw usage numbers for SNS messages, including publish requests and deliveries for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2562176500885798,
                        "reason": "The retrieval context partially addresses the input by describing S3 usage quantities and their units (GB-months, API requests) relevant for capacity planning, which aligns with the input question about S3 UsageQuantity. However, it lacks clarity and detail on total allocated capacity or how to determine if usage exceeds 10%, limiting its utility for directly answering the question. The context is somewhat concise but does not fully cover implications or provide sufficient guidance for comparison against a 10% threshold.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description directly addresses the question.\",\n    \"Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise, aiding comprehension of the input question.\",\n    \"Assess coverage by verifying that the description includes what is measured, the units, context, and implications, as these relate directly to the metric the input question targets.\",\n    \"Determine the utility by judging if the description sufficiently enables a user to answer the input question, ensuring alignment between input and retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.2562176500885798"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description directly addresses the question.",
                            "Evaluate the clarity of the Retrieval Context to ensure it is understandable and concise, aiding comprehension of the input question.",
                            "Assess coverage by verifying that the description includes what is measured, the units, context, and implications, as these relate directly to the metric the input question targets.",
                            "Determine the utility by judging if the description sufficiently enables a user to answer the input question, ensuring alignment between input and retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"S3\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $1000, indicating a potential cost spike in our S3 service?\", \"retrieval_context\": [\" - This pattern captures S3 Glacier costs using unblended pricing. It identifies the actual costs charged for Glacier storage, retrieval, and data transfer.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\", \" - This pattern captures detailed S3 costs broken down by usage type. It identifies specific charges for storage classes, data transfer, and API requests for cost optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.410671391505271,
                        "reason": "The retrieval context explains what UnblendedCost captures for S3 services, including storage, data transfer, and API requests, which partially addresses the input question about cost spikes. However, it lacks explicit information on units (e.g., dollars), how to interpret cost thresholds like $1000, and the operational implications of exceeding that amount. This limits the user's ability to definitively determine if a cost spike is occurring based solely on the description.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly and concisely explains the metric in terms that directly address the question.\",\n    \"Evaluate whether the Retrieval Context covers all critical aspects: what is measured, units, operational context, and implications relevant to the Input's question.\",\n    \"Assess the utility of the description by determining if it enables a user to accurately answer the Input question without needing additional information.\",\n    \"If any gaps in clarity, coverage, or utility are found when comparing the Input and context, formulate a specific improvement suggestion targeting the missing element(s).\"\n] \n \nRubric:\nNone \n \nScore: 0.410671391505271"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly and concisely explains the metric in terms that directly address the question.",
                            "Evaluate whether the Retrieval Context covers all critical aspects: what is measured, units, operational context, and implications relevant to the Input's question.",
                            "Assess the utility of the description by determining if it enables a user to accurately answer the Input question without needing additional information.",
                            "If any gaps in clarity, coverage, or utility are found when comparing the Input and context, formulate a specific improvement suggestion targeting the missing element(s)."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ELB\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric's UsageQuantity exceed 90% of its average value, indicating potential resource constraints?\", \"retrieval_context\": [\" - This pattern captures CloudWatch usage quantities. It identifies the raw usage numbers for CloudWatch metrics, log ingestion, and custom metrics for capacity planning.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures Redshift usage quantities. It identifies the raw usage numbers for Redshift clusters, including node hours and storage GB-months for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2679178699175393,
                        "reason": "The description identifies that the metric captures raw usage quantities for EC2, Redshift, and CloudWatch, which relates to usage quantity in the input question. However, it lacks clarity and detail on what the metric specifically measures, its units, or how to interpret values relative to averages or thresholds like 90%. It does not explain implications for resource constraints or provide sufficient information for the user to confidently assess if usage exceeds 90% of average value.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to assess if the description clearly addresses the specific user question.\",\n    \"Evaluate Clarity by checking if the description is concise, understandable, and free of jargon, ensuring the user can easily interpret it in relation to the input question.\",\n    \"Assess Coverage by verifying whether the description explains what the metric measures, its units, operational context, and the implications relevant to the monitoring question.\",\n    \"Determine Utility by judging if the description provides sufficient information for the user to confidently answer the monitoring question; if not, identify gaps linking description to input.\"\n] \n \nRubric:\nNone \n \nScore: 0.2679178699175393"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to assess if the description clearly addresses the specific user question.",
                            "Evaluate Clarity by checking if the description is concise, understandable, and free of jargon, ensuring the user can easily interpret it in relation to the input question.",
                            "Assess Coverage by verifying whether the description explains what the metric measures, its units, operational context, and the implications relevant to the monitoring question.",
                            "Determine Utility by judging if the description provides sufficient information for the user to confidently answer the monitoring question; if not, identify gaps linking description to input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ELB\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.624226852983893,
                        "reason": "The description relates to unblended costs for specific AWS security services, which partially addresses the question about UnblendedCost exceeding $100. However, it lacks clarity on what exactly is measured (total cost or per service), units, and does not explicitly mention thresholds or implications of cost spikes. The description is concise but could be improved by explicitly linking cost values to potential spikes and providing clearer context on aggregation and monitoring.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (metric description) with the retrieval context (monitoring question) to assess if the description clearly relates to and can help answer the question.\",\n    \"Evaluate Clarity by checking if the description is concise and easy to understand without ambiguity or unnecessary complexity.\",\n    \"Assess Coverage by verifying whether the description explains what is measured, the units involved, relevant context, and potential implications.\",\n    \"Determine Utility by judging if the description provides actionable or insightful information that directly helps answer the monitoring question; if not, suggest specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.624226852983893"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (metric description) with the retrieval context (monitoring question) to assess if the description clearly relates to and can help answer the question.",
                            "Evaluate Clarity by checking if the description is concise and easy to understand without ambiguity or unnecessary complexity.",
                            "Assess Coverage by verifying whether the description explains what is measured, the units involved, relevant context, and potential implications.",
                            "Determine Utility by judging if the description provides actionable or insightful information that directly helps answer the monitoring question; if not, suggest specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CLOUDFRONT\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric's UsageQuantity exceed 10% of its average value, indicating a potential performance issue?\", \"retrieval_context\": [\" - This pattern captures DynamoDB usage quantities. It identifies the raw usage numbers for DynamoDB tables, including read/write capacity units and storage GB-months.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures Lambda usage quantities. It identifies the raw usage numbers for Lambda functions, including invocation counts and duration for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.20179862090409095,
                        "reason": "The Retrieval Context describes what usage quantities are captured for various services but does not explain how to determine if UsageQuantity exceeds 10% of its average value or indicate performance issues. It lacks units, measurement methods, or implications directly related to the Input's monitoring question, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly addresses the question without ambiguity.\",\n    \"Evaluate the clarity of the Retrieval Context in explaining the metric in terms that directly relate to the Input\u2019s monitoring question.\",\n    \"Assess coverage by checking if the Retrieval Context includes what is measured, units, relevant context, and implications in a way that supports answering the Input.\",\n    \"Determine utility by verifying if the Retrieval Context enables a user to confidently answer the Input question based on the information provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.20179862090409095"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly addresses the question without ambiguity.",
                            "Evaluate the clarity of the Retrieval Context in explaining the metric in terms that directly relate to the Input\u2019s monitoring question.",
                            "Assess coverage by checking if the Retrieval Context includes what is measured, units, relevant context, and implications in a way that supports answering the Input.",
                            "Determine utility by verifying if the Retrieval Context enables a user to confidently answer the Input question based on the information provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"DYNAMODB\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in our DynamoDB service?\", \"retrieval_context\": [\" - This pattern captures Backup service costs using unblended pricing. It identifies the actual costs charged for AWS Backup storage and data transfer.\", \" - This pattern captures DynamoDB costs using unblended pricing. It identifies the actual costs charged for DynamoDB tables, read/write capacity, and storage.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5451725843158924,
                        "reason": "The retrieval context directly addresses the monitoring question by describing unblended costs for DynamoDB, which aligns with the input's focus. The language is clear and concise, making it easy to understand. However, the description lacks specific measurement units, thresholds (such as the $1000 cost spike), and implications related to cost spikes, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) and the Retrieval Context (description) to ensure the description directly addresses the monitoring question.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if the description uses concise language that is easy to understand in relation to the Input.\",\n    \"Assess the coverage of the description by verifying if it includes what is measured, units, relevant context, and implications tied to the Input.\",\n    \"Determine the utility by judging if the description provides enough information for a user to confidently answer the Input question based on the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.5451725843158924"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) and the Retrieval Context (description) to ensure the description directly addresses the monitoring question.",
                            "Evaluate the clarity of the Retrieval Context by checking if the description uses concise language that is easy to understand in relation to the Input.",
                            "Assess the coverage of the description by verifying if it includes what is measured, units, relevant context, and implications tied to the Input.",
                            "Determine the utility by judging if the description provides enough information for a user to confidently answer the Input question based on the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EC2-OTHER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2962673115868584,
                        "reason": "The retrieval context mentions unblended costs for various services but does not specifically define or explain the 'UnblendedCost' metric or its units, nor does it address the threshold of $1000 or implications of exceeding it. While it is somewhat relevant by referencing unblended pricing, it lacks clarity, coverage, and utility needed to confidently answer whether the UnblendedCost exceeds $1000 indicating a cost spike.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and relevantly.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise, free of jargon, and easily understandable in relation to the question posed in the Input.\",\n    \"Assess Coverage by verifying if the Retrieval Context explains what is measured, includes units, provides contextual information, and implications needed to answer the Input question.\",\n    \"Determine Utility by judging whether the Retrieval Context equips a user with sufficient information to confidently answer the Input question; if gaps exist, note areas needing improvement.\"\n] \n \nRubric:\nNone \n \nScore: 0.2962673115868584"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and relevantly.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise, free of jargon, and easily understandable in relation to the question posed in the Input.",
                            "Assess Coverage by verifying if the Retrieval Context explains what is measured, includes units, provides contextual information, and implications needed to answer the Input question.",
                            "Determine Utility by judging whether the Retrieval Context equips a user with sufficient information to confidently answer the Input question; if gaps exist, note areas needing improvement."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"KINESIS\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric indicate that Kinesis usage quantities have exceeded their normal levels?\", \"retrieval_context\": [\" - This pattern captures Kinesis streaming costs using unblended pricing. It identifies the actual costs charged for Kinesis data streams and data processing.\", \" - This pattern captures Kinesis usage quantities. It identifies the raw usage numbers for Kinesis streams, including shard hours and data transfer for capacity planning.\", \" - This pattern captures Redshift usage quantities. It identifies the raw usage numbers for Redshift clusters, including node hours and storage GB-months for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6224377428257205,
                        "reason": "The retrieval context addresses Kinesis usage quantities by describing raw usage numbers such as shard hours and data transfer, which relates to usage levels. However, it does not explicitly state whether these quantities indicate if usage has exceeded normal levels, limiting direct utility. The context is clear and concise but lacks coverage on thresholds or comparisons to normal usage, reducing its ability to fully answer the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description clearly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise and understandable in relation to the terminology and intent given in the Input.\",\n    \"Assess Coverage by verifying that the Retrieval Context explains what is being measured, including units, context, and implications that align with the Input's requirements.\",\n    \"Determine Utility by considering if the Retrieval Context provides sufficient information to directly answer or support answering the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6224377428257205"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description clearly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise and understandable in relation to the terminology and intent given in the Input.",
                            "Assess Coverage by verifying that the Retrieval Context explains what is being measured, including units, context, and implications that align with the Input's requirements.",
                            "Determine Utility by considering if the Retrieval Context provides sufficient information to directly answer or support answering the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SNS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31687949515290303,
                        "reason": "The retrieval context explains that the patterns capture unblended costs for specific AWS security services but does not provide any information about the actual cost values, units, or thresholds such as $100. Therefore, it lacks sufficient detail to determine if the UnblendedCost exceeds $100 or indicates a cost spike, limiting its utility in confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) to understand what specific information or metric insights are required.\",\n    \"Assess the Retrieval Context (metric description) for clarity and conciseness in explaining what is measured, including units and relevant context.\",\n    \"Compare the Retrieval Context against the Input to determine if the description provides sufficient coverage and implications to directly address the monitoring question.\",\n    \"Judge the utility of the description by verifying if it enables a user to confidently answer the provided question; if not, identify precise improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.31687949515290303"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) to understand what specific information or metric insights are required.",
                            "Assess the Retrieval Context (metric description) for clarity and conciseness in explaining what is measured, including units and relevant context.",
                            "Compare the Retrieval Context against the Input to determine if the description provides sufficient coverage and implications to directly address the monitoring question.",
                            "Judge the utility of the description by verifying if it enables a user to confidently answer the provided question; if not, identify precise improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EKS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike in our EKS clusters?\", \"retrieval_context\": [\" - This pattern captures ECS container costs using unblended pricing. It identifies the actual costs charged for ECS tasks, services, and cluster management.\", \" - This pattern captures EKS Kubernetes costs using unblended pricing. It identifies the actual costs charged for EKS cluster management and control plane.\", \" - This pattern captures ElastiCache costs using unblended pricing. It identifies the actual costs charged for ElastiCache clusters, including Redis and Memcached instances.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.42569549472517804,
                        "reason": "The description partially aligns with the question by identifying EKS costs using unblended pricing, which relates to the cost spike inquiry. However, it lacks clarity on the specific metric of exceeding $1000, does not mention units explicitly, and omits implications or actionable guidance on detecting or responding to cost spikes. The context is somewhat fragmented and includes unrelated ECS and ElastiCache costs, reducing focus and utility for the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly aligns with the question's intent and scope.\",\n    \"Evaluate Clarity by determining if the description is understandable and concise, considering if any jargon or ambiguity affects comprehension relative to the question.\",\n    \"Assess Coverage by checking if the description includes what is measured, units, context, and implications, ensuring these elements sufficiently address the monitoring question.\",\n    \"Determine Utility by verifying if the description provides actionable information that directly helps answer the monitoring question; if not, identify gaps linking the description to the input.\"\n] \n \nRubric:\nNone \n \nScore: 0.42569549472517804"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly aligns with the question's intent and scope.",
                            "Evaluate Clarity by determining if the description is understandable and concise, considering if any jargon or ambiguity affects comprehension relative to the question.",
                            "Assess Coverage by checking if the description includes what is measured, units, context, and implications, ensuring these elements sufficiently address the monitoring question.",
                            "Determine Utility by verifying if the description provides actionable information that directly helps answer the monitoring question; if not, identify gaps linking the description to the input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SQS\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric indicate an unexpected spike in SQS message requests?\", \"retrieval_context\": [\" - This pattern captures SNS usage quantities. It identifies the raw usage numbers for SNS messages, including publish requests and deliveries for capacity planning.\", \" - This pattern captures SQS queue costs using unblended pricing. It identifies the actual costs charged for SQS message processing and API requests.\", \" - This pattern captures SQS usage quantities. It identifies the raw usage numbers for SQS messages, including request counts for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.44447095984860197,
                        "reason": "The Retrieval Context addresses SQS message requests by describing usage quantities and request counts, which relates to the Input's focus on SQS message spikes. However, it lacks explicit mention of detecting unexpected spikes or how to interpret changes in request counts. The context is somewhat clear but includes jargon like 'unblended pricing' without explanation. It covers what is measured and units (request counts) but misses implications or guidance on identifying anomalies, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description directly addresses the question\u2019s focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise, jargon-free, and easy to understand in relation to the Input\u2019s requirements.\",\n    \"Assess Coverage by ensuring the Retrieval Context includes what is measured, units, contextual background, and implications necessary to answer the Input.\",\n    \"Determine Utility by judging if the Retrieval Context provides sufficient and relevant information that enables a user to confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.44447095984860197"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description directly addresses the question\u2019s focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise, jargon-free, and easy to understand in relation to the Input\u2019s requirements.",
                            "Assess Coverage by ensuring the Retrieval Context includes what is measured, units, contextual background, and implications necessary to answer the Input.",
                            "Determine Utility by judging if the Retrieval Context provides sufficient and relevant information that enables a user to confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CLOUDFRONT\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2957854011307113,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not explicitly address whether the UnblendedCost exceeds $1000 or indicate how to detect a cost spike. The description is somewhat clear about what costs are measured and their context but lacks units, thresholds, or implications related to the monitoring question. It provides limited utility for answering the question directly and would benefit from including explicit cost thresholds and guidance on interpreting cost spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to verify if the description explicitly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description is concise and understandable without ambiguity in relation to the question asked.\",\n    \"Assess Coverage by confirming the description explains what is measured, units, relevant context, and implications necessary to respond to the question adequately.\",\n    \"Determine Utility by verifying if the description provides actionable or insightful information that directly helps answer the monitoring question; if not, identify gaps and suggest specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.2957854011307113"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to verify if the description explicitly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description is concise and understandable without ambiguity in relation to the question asked.",
                            "Assess Coverage by confirming the description explains what is measured, units, relevant context, and implications necessary to respond to the question adequately.",
                            "Determine Utility by verifying if the description provides actionable or insightful information that directly helps answer the monitoring question; if not, identify gaps and suggest specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"APIGATEWAY\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in the APIGATEWAY service?\", \"retrieval_context\": [\" - This pattern captures API Gateway costs using unblended pricing. It identifies the actual costs charged for API Gateway requests, data transfer, and cache usage.\", \" - This pattern captures KMS key management costs using unblended pricing. It identifies the actual costs charged for KMS key usage and API requests.\", \" - This pattern captures Storage Gateway costs using unblended pricing. It identifies the actual costs charged for Storage Gateway file, volume, and tape gateways.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3536433348637746,
                        "reason": "The retrieval context partially addresses the question by describing unblended costs related to API Gateway, but it lacks clarity on the specific metric 'UnblendedCost' exceeding $1000 and does not explain units, measurement details, or implications of a cost spike. The description is somewhat understandable but insufficient for confidently answering the monitoring question about cost thresholds and potential spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description clearly addresses the question's intent.\",\n    \"Evaluate the clarity of the metric description in relation to the question \u2014 check if it is understandable and concise enough for the user to grasp the metric's meaning.\",\n    \"Assess the coverage of the description, ensuring it explains what is measured, the units involved, relevant context, and implications that relate directly to the monitoring question.\",\n    \"Determine the utility by judging if the description provides sufficient information for the user to confidently answer the monitoring question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3536433348637746"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to verify if the description clearly addresses the question's intent.",
                            "Evaluate the clarity of the metric description in relation to the question \u2014 check if it is understandable and concise enough for the user to grasp the metric's meaning.",
                            "Assess the coverage of the description, ensuring it explains what is measured, the units involved, relevant context, and implications that relate directly to the monitoring question.",
                            "Determine the utility by judging if the description provides sufficient information for the user to confidently answer the monitoring question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"DYNAMODB\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 100 million, indicating a potential performance issue with DynamoDB?\", \"retrieval_context\": [\" - This pattern captures DynamoDB costs using unblended pricing. It identifies the actual costs charged for DynamoDB tables, read/write capacity, and storage.\", \" - This pattern captures DynamoDB usage quantities. It identifies the raw usage numbers for DynamoDB tables, including read/write capacity units and storage GB-months.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4576940857784123,
                        "reason": "The Retrieval Context partially aligns with the Input by describing DynamoDB usage quantities, including read/write capacity units and storage, which relates to UsageQuantity. However, it lacks specific information about the measurement units, thresholds (such as 100 million), or implications for performance issues, limiting clarity and utility in directly answering whether UsageQuantity exceeds 100 million and indicates a performance problem.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly aligns with and addresses the specific question.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it is concise, easily understandable, and free of ambiguity, particularly in relation to the terms used in the Input.\",\n    \"Assess coverage by verifying that the Retrieval Context fully explains what is measured, including units, context, and implications, and that these elements are relevant to answering the Input question.\",\n    \"Determine utility by confirming that the Retrieval Context provides sufficient information to accurately answer the Input question, ensuring the description and question are meaningfully connected.\"\n] \n \nRubric:\nNone \n \nScore: 0.4576940857784123"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly aligns with and addresses the specific question.",
                            "Evaluate the clarity of the Retrieval Context by checking if it is concise, easily understandable, and free of ambiguity, particularly in relation to the terms used in the Input.",
                            "Assess coverage by verifying that the Retrieval Context fully explains what is measured, including units, context, and implications, and that these elements are relevant to answering the Input question.",
                            "Determine utility by confirming that the Retrieval Context provides sufficient information to accurately answer the Input question, ensuring the description and question are meaningfully connected."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"REDSHIFT\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 90% of its average value over the past month?\", \"retrieval_context\": [\" - This pattern captures DynamoDB usage quantities. It identifies the raw usage numbers for DynamoDB tables, including read/write capacity units and storage GB-months.\", \" - This pattern captures RDS usage quantities. It identifies the raw usage numbers for RDS instances, including instance hours and storage GB-months.\", \" - This pattern captures S3 usage quantities. It identifies the raw usage numbers for S3 storage, including GB-months and API requests for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3037326888281171,
                        "reason": "The retrieval context lists usage quantities for various services with units but does not directly address whether UsageQuantity can be compared to 90% of its average over the past month. It lacks clarity on how to interpret or calculate averages or thresholds, limiting utility for answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (the monitoring question) with the Retrieval Context (the metric description) to ensure the description directly addresses the question\u2019s focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is easy to understand and concise, without irrelevant information that obscures the Input\u2019s intent.\",\n    \"Assess Coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the Input question.\",\n    \"Determine Utility by judging whether the Retrieval Context provides sufficient and relevant details to help a user confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3037326888281171"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (the monitoring question) with the Retrieval Context (the metric description) to ensure the description directly addresses the question\u2019s focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is easy to understand and concise, without irrelevant information that obscures the Input\u2019s intent.",
                            "Assess Coverage by verifying that the description explains what is measured, including units, context, and implications relevant to the Input question.",
                            "Determine Utility by judging whether the Retrieval Context provides sufficient and relevant details to help a user confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SQS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.25,
                        "reason": "The retrieval context mentions unblended costs for specific services but does not address the overall UnblendedCost metric or whether it exceeds $100, which is the core of the monitoring question. The descriptions are somewhat clear but limited to individual service costs without explaining units, measurement scope, or implications related to cost spikes, resulting in poor coverage and limited utility for answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description addresses the specific monitoring needs posed by the question.\",\n    \"Evaluate the Clarity of the Retrieval Context to ensure it is understandable and concise when related to the Input's terminology and concepts.\",\n    \"Assess Coverage by verifying the description explains what is measured, units, relevant context, and implications that directly support the Input's question.\",\n    \"Determine Utility by judging whether the description provides sufficient information to effectively answer the monitoring question, highlighting any gaps between the Input and Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.25"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description addresses the specific monitoring needs posed by the question.",
                            "Evaluate the Clarity of the Retrieval Context to ensure it is understandable and concise when related to the Input's terminology and concepts.",
                            "Assess Coverage by verifying the description explains what is measured, units, relevant context, and implications that directly support the Input's question.",
                            "Determine Utility by judging whether the description provides sufficient information to effectively answer the monitoring question, highlighting any gaps between the Input and Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ROUTE53\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3522311164187409,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not directly address whether the UnblendedCost metric exceeds $1000 or indicate how to detect a cost spike. The description is somewhat clear about what is measured (actual costs charged) but lacks units, thresholds, or implications related to cost spikes, limiting its utility for answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question posed.\",\n    \"Evaluate Clarity by determining if the metric description is concise and easily understandable within the context of the input question.\",\n    \"Assess Coverage by checking if the description includes what is measured, the units, context, and implications relevant to the monitoring question.\",\n    \"Judge Utility by verifying if the description provides enough information for a user to effectively answer the monitoring question; if not, identify specific gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.3522311164187409"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description directly addresses the question posed.",
                            "Evaluate Clarity by determining if the metric description is concise and easily understandable within the context of the input question.",
                            "Assess Coverage by checking if the description includes what is measured, the units, context, and implications relevant to the monitoring question.",
                            "Judge Utility by verifying if the description provides enough information for a user to effectively answer the monitoring question; if not, identify specific gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"APIGATEWAY\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does UsageQuantity exceed 100 million, indicating a potential capacity issue with the API Gateway?\", \"retrieval_context\": [\" - This pattern captures API Gateway costs using unblended pricing. It identifies the actual costs charged for API Gateway requests, data transfer, and cache usage.\", \" - This pattern captures API Gateway usage quantities. It identifies the raw usage numbers for API Gateway requests and data transfer for capacity planning.\", \" - This pattern captures S3 usage quantities. It identifies the raw usage numbers for S3 storage, including GB-months and API requests for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5777299866333616,
                        "reason": "The Retrieval Context partially addresses the Input by mentioning API Gateway usage quantities and their role in capacity planning, which relates to UsageQuantity. However, it lacks specific details on units (e.g., whether usage is measured in requests, data volume, or other metrics) and does not explicitly state thresholds like 100 million or clarify what constitutes a capacity issue. This limits clarity and utility in directly answering whether UsageQuantity exceeds 100 million and its implications.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) for clarity by checking if the description directly addresses and is easily understandable in relation to the question asked.\",\n    \"Evaluate the coverage by verifying that the Retrieval Context fully explains what is measured, includes units, relevant context, and implications necessary to answer the Input question.\",\n    \"Assess utility by determining if the Retrieval Context provides sufficient and relevant information that enables answering the Input question effectively.\",\n    \"If any assessment scores below 8, identify specific gaps between the Input and Retrieval Context and suggest targeted improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.5777299866333616"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) for clarity by checking if the description directly addresses and is easily understandable in relation to the question asked.",
                            "Evaluate the coverage by verifying that the Retrieval Context fully explains what is measured, includes units, relevant context, and implications necessary to answer the Input question.",
                            "Assess utility by determining if the Retrieval Context provides sufficient and relevant information that enables answering the Input question effectively.",
                            "If any assessment scores below 8, identify specific gaps between the Input and Retrieval Context and suggest targeted improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"REDSHIFT\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.37223805227103096,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not explicitly define 'UnblendedCost' as a whole or clarify if it aggregates these costs. It lacks measurement units and does not directly address the threshold of $1000 or implications of exceeding it, limiting clarity and utility for answering the monitoring question about cost spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly relates to and supports answering the question.\",\n    \"Evaluate Clarity of the description by checking if it is both understandable and concise in the context of the question.\",\n    \"Assess Coverage by verifying if the description includes what is measured, measurement units, relevant context, and implications related to the question.\",\n    \"Determine Utility by judging whether the description provides sufficient information to help a user effectively answer the monitoring question; if not, suggest targeted improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.37223805227103096"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly relates to and supports answering the question.",
                            "Evaluate Clarity of the description by checking if it is both understandable and concise in the context of the question.",
                            "Assess Coverage by verifying if the description includes what is measured, measurement units, relevant context, and implications related to the question.",
                            "Determine Utility by judging whether the description provides sufficient information to help a user effectively answer the monitoring question; if not, suggest targeted improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ELASTICACHE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.29381394499617597,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not explicitly address whether the UnblendedCost metric can be used to determine if costs exceed $1000 or indicate a cost spike. The description is somewhat clear but lacks direct explanation of the metric's units, threshold relevance, or implications related to the monitoring question, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question\u2019s specific focus.\",\n    \"Evaluate Clarity in the description by ensuring it is concise, free of jargon, and understandable in the context of the question\u2019s intent.\",\n    \"Assess Coverage by verifying that the description explains what the metric measures, its units, relevant context, and potential implications, specifically in relation to the monitoring question.\",\n    \"Determine Utility by judging whether the description enables a user to confidently answer the monitoring question, considering both Input and Retrieval Context together.\"\n] \n \nRubric:\nNone \n \nScore: 0.29381394499617597"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question\u2019s specific focus.",
                            "Evaluate Clarity in the description by ensuring it is concise, free of jargon, and understandable in the context of the question\u2019s intent.",
                            "Assess Coverage by verifying that the description explains what the metric measures, its units, relevant context, and potential implications, specifically in relation to the monitoring question.",
                            "Determine Utility by judging whether the description enables a user to confidently answer the monitoring question, considering both Input and Retrieval Context together."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ELASTICACHE\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 80% of total cluster capacity?\", \"retrieval_context\": [\" - This pattern captures CloudFront usage quantities. It identifies the raw usage numbers for CloudFront distributions, including data transfer GB and requests for capacity planning.\", \" - This pattern captures ElastiCache usage quantities. It identifies the raw usage numbers for ElastiCache clusters, including node hours for capacity planning.\", \" - This pattern captures Redshift usage quantities. It identifies the raw usage numbers for Redshift clusters, including node hours and storage GB-months for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3029312232447289,
                        "reason": "The retrieval context provides a general description of usage quantities for various services, including Redshift, ElastiCache, and CloudFront, mentioning raw usage numbers and units like node hours and GB-months. However, it does not explicitly define 'UsageQuantity' or explain how it relates to total cluster capacity or how to determine if it exceeds 80%. The context lacks clarity and direct relevance to the input question, limiting its utility for answering whether UsageQuantity exceeds a specific percentage of cluster capacity. To improve, the description should explicitly define UsageQuantity, specify its units, and explain how it compares to total cluster capacity.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains the metric in a way that directly relates to the question.\",\n    \"Evaluate the Coverage of the Retrieval Context by verifying if it includes what is measured, the units, relevant context, and the implications, ensuring it fully supports answering the Input question.\",\n    \"Assess the Utility by determining whether the description enables a user to effectively answer the Input question when considering the details provided in the Retrieval Context.\",\n    \"If any aspect (Clarity, Coverage, or Utility) is insufficient in linking the Retrieval Context to the Input question, provide a concrete suggestion to improve that connection.\"\n] \n \nRubric:\nNone \n \nScore: 0.3029312232447289"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly and concisely explains the metric in a way that directly relates to the question.",
                            "Evaluate the Coverage of the Retrieval Context by verifying if it includes what is measured, the units, relevant context, and the implications, ensuring it fully supports answering the Input question.",
                            "Assess the Utility by determining whether the description enables a user to effectively answer the Input question when considering the details provided in the Retrieval Context.",
                            "If any aspect (Clarity, Coverage, or Utility) is insufficient in linking the Retrieval Context to the Input question, provide a concrete suggestion to improve that connection."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SNS\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 10 million, indicating potential SNS message delivery issues?\", \"retrieval_context\": [\" - This pattern captures SNS notification costs using unblended pricing. It identifies the actual costs charged for SNS message publishing and delivery.\", \" - This pattern captures SNS usage quantities. It identifies the raw usage numbers for SNS messages, including publish requests and deliveries for capacity planning.\", \" - This pattern captures SQS usage quantities. It identifies the raw usage numbers for SQS messages, including request counts for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6503572536893273,
                        "reason": "The input directly relates to the monitoring question by focusing on UsageQuantity exceeding 10 million as an indicator of SNS message delivery issues, aligning with the Retrieval Context's focus on SNS usage quantities for capacity planning. The language is clear and concise, but the description lacks detail on units, exact measurement methods, and potential implications beyond delivery issues. While it enables a user to assess the question, it could be more comprehensive to fully cover all necessary aspects outlined in the Retrieval Context.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and Retrieval Context to ensure the description directly relates to the monitoring question and provides relevant information.\",\n    \"Evaluate if the Input provides clear, understandable, and concise language that aligns with the technical context outlined in the Retrieval Context.\",\n    \"Check that the description covers all necessary aspects including what is measured, units, context, and potential implications as per the Retrieval Context.\",\n    \"Assess the utility of the Input by determining if it enables a user to confidently answer the given question based on the information and context provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.6503572536893273"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and Retrieval Context to ensure the description directly relates to the monitoring question and provides relevant information.",
                            "Evaluate if the Input provides clear, understandable, and concise language that aligns with the technical context outlined in the Retrieval Context.",
                            "Check that the description covers all necessary aspects including what is measured, units, context, and potential implications as per the Retrieval Context.",
                            "Assess the utility of the Input by determining if it enables a user to confidently answer the given question based on the information and context provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"KINESIS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in our Kinesis service?\", \"retrieval_context\": [\" - This pattern captures EKS Kubernetes costs using unblended pricing. It identifies the actual costs charged for EKS cluster management and control plane.\", \" - This pattern captures KMS key management costs using unblended pricing. It identifies the actual costs charged for KMS key usage and API requests.\", \" - This pattern captures Kinesis streaming costs using unblended pricing. It identifies the actual costs charged for Kinesis data streams and data processing.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7562176500885798,
                        "reason": "The description clearly defines the metric as UnblendedCost exceeding $1000, indicating a potential cost spike specifically for the Kinesis service, which aligns well with the retrieval context that details unblended pricing for Kinesis streaming costs. It provides sufficient detail to understand the implication of a cost spike and is concise and unambiguous. However, it could improve by explicitly stating the units (e.g., USD) and clarifying the time frame over which the cost is measured to fully support accurate monitoring.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Input) against the Retrieval Context to check if it clearly defines what is being measured, including units and relevant context.\",\n    \"Assess whether the description provides sufficient detail and coverage to understand the implications of the metric in relation to the monitoring question posed.\",\n    \"Evaluate the clarity and conciseness of the description to ensure it is easily understandable and free of ambiguity when used to answer the monitoring question.\",\n    \"Determine the utility of the description by verifying if the information directly supports a user in accurately addressing the monitoring question based on the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.7562176500885798"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Input) against the Retrieval Context to check if it clearly defines what is being measured, including units and relevant context.",
                            "Assess whether the description provides sufficient detail and coverage to understand the implications of the metric in relation to the monitoring question posed.",
                            "Evaluate the clarity and conciseness of the description to ensure it is easily understandable and free of ambiguity when used to answer the monitoring question.",
                            "Determine the utility of the description by verifying if the information directly supports a user in accurately addressing the monitoring question based on the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ECS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3222700133666384,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not directly address the monitoring question about whether the UnblendedCost exceeds $1000 or indicate a potential cost spike. The description is clear and concise but lacks explicit measurement units, threshold information, or guidance on interpreting cost spikes, limiting its utility for answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate the clarity of the description in terms of language simplicity and conciseness and verify if it complements the question without ambiguity.\",\n    \"Assess the coverage of the description by checking if it includes what is being measured, units, relevant context, and any implications that relate to the monitoring question.\",\n    \"Determine the utility by judging whether the description sufficiently equips a user to answer the monitoring question based on the provided context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3222700133666384"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate the clarity of the description in terms of language simplicity and conciseness and verify if it complements the question without ambiguity.",
                            "Assess the coverage of the description by checking if it includes what is being measured, units, relevant context, and any implications that relate to the monitoring question.",
                            "Determine the utility by judging whether the description sufficiently equips a user to answer the monitoring question based on the provided context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CLOUDWATCH\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3964886330125148,
                        "reason": "The description clarifies that unblended costs for specific AWS security services are captured using unblended pricing, which relates to the question about UnblendedCost. However, it lacks explicit information about the measurement units, the threshold of $1000, or how to interpret a cost spike, limiting clarity and utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.\",\n    \"Evaluate the Clarity of the description to confirm it is understandable and concise enough to relate directly to the question.\",\n    \"Check Coverage within the description by verifying if it explains what is measured, the units used, the monitoring context, and implications relevant to the question.\",\n    \"Assess the Utility by determining if the description provides enough information to confidently answer the monitoring question; if not, identify specific gaps.\"\n] \n \nRubric:\nNone \n \nScore: 0.3964886330125148"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.",
                            "Evaluate the Clarity of the description to confirm it is understandable and concise enough to relate directly to the question.",
                            "Check Coverage within the description by verifying if it explains what is measured, the units used, the monitoring context, and implications relevant to the question.",
                            "Assess the Utility by determining if the description provides enough information to confidently answer the monitoring question; if not, identify specific gaps."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SES\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are spikes in UnblendedCost exceeding $1000 indicative of an unexpected increase in SES email usage?\", \"retrieval_context\": [\" - This pattern captures SES email costs using unblended pricing. It identifies the actual costs charged for SES email sending, including data transfer and API requests.\", \" - This pattern captures Single Sign-On costs using unblended pricing. It identifies the actual costs charged for SSO user management and application access.\", \" - This pattern captures WorkMail email costs using unblended pricing. It identifies the actual costs charged for WorkMail user accounts and data transfer.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31480471999111614,
                        "reason": "The Retrieval Context identifies that the UnblendedCost pattern captures SES email costs using unblended pricing, which is relevant to the question about spikes in UnblendedCost for SES email usage. However, it lacks clarity and detail on what constitutes a spike, the measurement units beyond cost, and whether exceeding $1000 is unexpected or typical. The description does not provide actionable insight or implications to determine if such spikes indicate unexpected increases, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question posed, ensuring relevance and alignment.\",\n    \"Evaluate the clarity of the Retrieval Context by assessing if the description is understandable and concise in relation to the terminology used in the Input.\",\n    \"Assess Coverage by verifying whether the Retrieval Context explains the measurement details, units, context, and implications necessary to answer the Input question.\",\n    \"Judge Utility by determining if the Retrieval Context provides actionable information that directly helps answer the Input question; if not, identify aspects lacking in the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.31480471999111614"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question posed, ensuring relevance and alignment.",
                            "Evaluate the clarity of the Retrieval Context by assessing if the description is understandable and concise in relation to the terminology used in the Input.",
                            "Assess Coverage by verifying whether the Retrieval Context explains the measurement details, units, context, and implications necessary to answer the Input question.",
                            "Judge Utility by determining if the Retrieval Context provides actionable information that directly helps answer the Input question; if not, identify aspects lacking in the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ROUTE53\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric's UsageQuantity exceed its normal baseline, indicating potential capacity issues?\", \"retrieval_context\": [\" - This pattern captures CloudFront usage quantities. It identifies the raw usage numbers for CloudFront distributions, including data transfer GB and requests for capacity planning.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures SQS usage quantities. It identifies the raw usage numbers for SQS messages, including request counts for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3964533991458114,
                        "reason": "The input question is clear and specific, asking if UsageQuantity exceeds its normal baseline to indicate capacity issues. The retrieval context provides descriptions of usage quantities for CloudFront, SQS, and EC2, including units like data transfer GB, requests, and instance hours, and mentions capacity planning. However, it lacks explicit explanation of what the 'normal baseline' is, how to determine if usage exceeds it, and the direct implications for capacity issues. This limits its utility in fully answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to determine if it is well-defined and focused.\",\n    \"Assess the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise while matching the terminology and scope of the Input.\",\n    \"Compare the Retrieval Context against the Input to verify coverage: confirm that the description explains what is measured, units, context, and implications relevant to answering the question.\",\n    \"Judge the Utility by determining if the information provided in the Retrieval Context directly supports answering the Input question effectively; if gaps exist, suggest specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.3964533991458114"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to determine if it is well-defined and focused.",
                            "Assess the Retrieval Context (metric description) for clarity, ensuring it is understandable and concise while matching the terminology and scope of the Input.",
                            "Compare the Retrieval Context against the Input to verify coverage: confirm that the description explains what is measured, units, context, and implications relevant to answering the question.",
                            "Judge the Utility by determining if the information provided in the Retrieval Context directly supports answering the Input question effectively; if gaps exist, suggest specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SES\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 10 million, indicating potential SES resource exhaustion?\", \"retrieval_context\": [\" - This pattern captures EFS usage quantities. It identifies the raw usage numbers for EFS storage GB-months for capacity planning.\", \" - This pattern captures SES usage quantities. It identifies the raw usage numbers for SES emails sent and data transfer for capacity planning.\", \" - This pattern captures SQS usage quantities. It identifies the raw usage numbers for SQS messages, including request counts for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.42243774333830786,
                        "reason": "The input question is clear and specific, focusing on SES UsageQuantity exceeding 10 million to indicate resource exhaustion. However, the retrieval context is only partially relevant; it mentions SES usage quantities but does not specify units, thresholds, or how to interpret the raw numbers in relation to the 10 million limit. Additionally, the context includes unrelated patterns for SQS and EFS, which reduces focus. To improve, the context should explicitly define SES UsageQuantity units, clarify the significance of the 10 million threshold, and exclude unrelated service metrics.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to ensure it provides a clear goal that the description must address.\",\n    \"Assess the Retrieval Context (metric description) for clarity, coverage, and relevance, checking if it thoroughly explains what is measured, units, context, and implications.\",\n    \"Compare the Input and Retrieval Context to verify if the description directly supports answering the monitoring question with sufficient detail and practicality.\",\n    \"Assign a score based on combined clarity, coverage, and utility; if the score is less than 8, suggest improvements that enhance explanation completeness or alignment with the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.42243774333830786"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to ensure it provides a clear goal that the description must address.",
                            "Assess the Retrieval Context (metric description) for clarity, coverage, and relevance, checking if it thoroughly explains what is measured, units, context, and implications.",
                            "Compare the Input and Retrieval Context to verify if the description directly supports answering the monitoring question with sufficient detail and practicality.",
                            "Assign a score based on combined clarity, coverage, and utility; if the score is less than 8, suggest improvements that enhance explanation completeness or alignment with the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"VPC\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in VPC networking?\", \"retrieval_context\": [\" - This pattern captures EC2 other costs using unblended pricing. It identifies costs for EC2-related services like data transfer, NAT gateway, Elastic IPs, and other EC2 charges.\", \" - This pattern captures Snowcone edge computing costs using unblended pricing. It identifies the actual costs charged for Snowcone device usage and data transfer.\", \" - This pattern captures VPC networking costs using unblended pricing. It identifies the actual costs charged for VPC NAT gateways, endpoints, and data transfer.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6123694727764735,
                        "reason": "The retrieval context partially addresses the input question by describing that the pattern captures VPC networking costs using unblended pricing, including NAT gateways, endpoints, and data transfer, which is relevant to monitoring cost spikes. However, it lacks explicit mention of the $1000 threshold or how to interpret exceeding that amount, limiting clarity and utility. The description is somewhat clear and concise but does not fully explain the measurement units or implications, reducing coverage and practical insight for answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description specifically addresses the question asked, providing relevant information.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise, making it easy for a user to link it to the monitoring question.\",\n    \"Assess Coverage to confirm the description explains what is measured, units, context, and implications clearly in relation to the input question.\",\n    \"Determine Utility by verifying whether the description provides sufficient insight and detail enabling a user to effectively answer the monitoring question based on the given metric.\"\n] \n \nRubric:\nNone \n \nScore: 0.6123694727764735"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description specifically addresses the question asked, providing relevant information.",
                            "Evaluate Clarity by checking if the description is understandable and concise, making it easy for a user to link it to the monitoring question.",
                            "Assess Coverage to confirm the description explains what is measured, units, context, and implications clearly in relation to the input question.",
                            "Determine Utility by verifying whether the description provides sufficient insight and detail enabling a user to effectively answer the monitoring question based on the given metric."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CLOUDWATCH\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does this metric's UsageQuantity exceed 90% of its average value, indicating potential resource constraints?\", \"retrieval_context\": [\" - This pattern captures CloudWatch usage quantities. It identifies the raw usage numbers for CloudWatch metrics, log ingestion, and custom metrics for capacity planning.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures Redshift usage quantities. It identifies the raw usage numbers for Redshift clusters, including node hours and storage GB-months for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6130291506345243,
                        "reason": "The description is somewhat clear in identifying that UsageQuantity relates to raw usage numbers for various AWS services, but it lacks explicit units and does not clearly define what 'UsageQuantity' specifically measures. Coverage is partial as it mentions usage quantities and their relevance to capacity planning but does not explicitly connect exceeding 90% of average usage to potential resource constraints. Utility is limited because the description does not fully support answering whether exceeding 90% of average usage indicates constraints, nor does it clarify the implications or thresholds. To improve, the description should explicitly define UsageQuantity units, clarify the significance of the 90% threshold, and directly link usage patterns to resource constraints.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (metric description) and retrieval context to assess clarity, ensuring the description is understandable and concise with no ambiguity.\",\n    \"Evaluate coverage by verifying if the description explains what is being measured, includes units, relevant context, and potential implications clearly tied to the monitoring question.\",\n    \"Assess utility by determining if the description sufficiently supports answering the provided monitoring question based on the given context.\",\n    \"If the description scores below 8 on any criterion, identify specific gaps in clarity, coverage, or utility relative to the input and retrieval context and suggest concrete improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.6130291506345243"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (metric description) and retrieval context to assess clarity, ensuring the description is understandable and concise with no ambiguity.",
                            "Evaluate coverage by verifying if the description explains what is being measured, includes units, relevant context, and potential implications clearly tied to the monitoring question.",
                            "Assess utility by determining if the description sufficiently supports answering the provided monitoring question based on the given context.",
                            "If the description scores below 8 on any criterion, identify specific gaps in clarity, coverage, or utility relative to the input and retrieval context and suggest concrete improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ATHENA\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.31361778220576286,
                        "reason": "The description addresses costs related to specific AWS services using unblended pricing but does not explicitly mention the UnblendedCost metric or its units, nor does it clarify if or how costs exceeding $100 are identified. The retrieval context lacks direct linkage to the monitoring question about cost spikes, limiting clarity and utility for answering the inquiry.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (question) to determine if the description addresses the specific monitoring need or inquiry presented.\",\n    \"Assess the Retrieval Context (description) for clarity, ensuring it is understandable and concise relative to the question asked.\",\n    \"Check coverage by verifying that the description explains what metric is measured, including units, relevant context, and implications, in a way that aligns with the input query.\",\n    \"Judge the utility by determining if the description provides sufficient information to answer the monitoring question effectively, considering both clarity and coverage.\"\n] \n \nRubric:\nNone \n \nScore: 0.31361778220576286"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (question) to determine if the description addresses the specific monitoring need or inquiry presented.",
                            "Assess the Retrieval Context (description) for clarity, ensuring it is understandable and concise relative to the question asked.",
                            "Check coverage by verifying that the description explains what metric is measured, including units, relevant context, and implications, in a way that aligns with the input query.",
                            "Judge the utility by determining if the description provides sufficient information to answer the monitoring question effectively, considering both clarity and coverage."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EFS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.375973500019518,
                        "reason": "The description clarifies that the costs are unblended and related to specific security services, which partially addresses the question about UnblendedCost. However, it lacks explicit mention of the cost exceeding $100 or how to interpret a potential cost spike, limiting its utility and coverage in fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses what the question asks.\",\n    \"Evaluate Clarity by checking if the description is concise and easy to understand relative to the complexity of the Input question.\",\n    \"Assess Coverage to ensure the description explains what is measured, including units, the metric's context, and its implications in relation to the monitoring question.\",\n    \"Determine Utility by verifying whether the description provides enough relevant information to answer the Input question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.375973500019518"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses what the question asks.",
                            "Evaluate Clarity by checking if the description is concise and easy to understand relative to the complexity of the Input question.",
                            "Assess Coverage to ensure the description explains what is measured, including units, the metric's context, and its implications in relation to the monitoring question.",
                            "Determine Utility by verifying whether the description provides enough relevant information to answer the Input question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"EFS\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 80% of total allocated capacity, indicating potential storage constraints?\", \"retrieval_context\": [\" - This pattern captures EFS usage quantities. It identifies the raw usage numbers for EFS storage GB-months for capacity planning.\", \" - This pattern captures Glacier usage quantities. It identifies the raw usage numbers for Glacier storage GB-months and retrieval requests for capacity planning.\", \" - This pattern captures Redshift usage quantities. It identifies the raw usage numbers for Redshift clusters, including node hours and storage GB-months for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4043740263534589,
                        "reason": "The retrieval context relates to usage quantities for various storage services, which is relevant to the question about UsageQuantity and capacity. However, it lacks explicit explanation of what UsageQuantity specifically measures, the units in a unified manner, and does not address the 80% threshold or implications for storage constraints, limiting clarity, coverage, and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly relates to and supports answering the question.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise and easily understandable in relation to the terminology and concepts used in the Input.\",\n    \"Assess Coverage by verifying that the Retrieval Context clearly explains what is measured, the units involved, the context of measurement, and any implications relevant to the Input question.\",\n    \"Determine Utility by confirming that the Retrieval Context provides sufficient information and relevance to enable a user to confidently answer the Input question based on the description.\"\n] \n \nRubric:\nNone \n \nScore: 0.4043740263534589"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly relates to and supports answering the question.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise and easily understandable in relation to the terminology and concepts used in the Input.",
                            "Assess Coverage by verifying that the Retrieval Context clearly explains what is measured, the units involved, the context of measurement, and any implications relevant to the Input question.",
                            "Determine Utility by confirming that the Retrieval Context provides sufficient information and relevance to enable a user to confidently answer the Input question based on the description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"COGNITO\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in Cognito services?\", \"retrieval_context\": [\" - This pattern captures Cognito identity costs using unblended pricing. It identifies the actual costs charged for Cognito user pools, identity pools, and data transfer.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\", \" - This pattern captures IoT Device costs using unblended pricing. It identifies the actual costs charged for IoT Device management and connectivity.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4277149188055252,
                        "reason": "The retrieval context partially addresses the question by describing unblended costs for Cognito identity services, which aligns with the question's focus on UnblendedCost for Cognito. However, it lacks clarity and completeness as it does not specify units, thresholds, or implications related to cost spikes exceeding $1000. The description is somewhat fragmented and includes unrelated IoT costs, reducing clarity and utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question\u2019s intent.\",\n    \"Evaluate Clarity by checking if the description is concise and easily understandable in relation to the question\u2019s terminology.\",\n    \"Assess Coverage by verifying that the description explains what is measured, units, relevant context, and implications needed to answer the question.\",\n    \"Determine Utility by judging if the description provides sufficient information for a user to confidently respond to the monitoring question based on the given metric description.\"\n] \n \nRubric:\nNone \n \nScore: 0.4277149188055252"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question\u2019s intent.",
                            "Evaluate Clarity by checking if the description is concise and easily understandable in relation to the question\u2019s terminology.",
                            "Assess Coverage by verifying that the description explains what is measured, units, relevant context, and implications needed to answer the question.",
                            "Determine Utility by judging if the description provides sufficient information for a user to confidently respond to the monitoring question based on the given metric description."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CODECOMMIT\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5976712641531224,
                        "reason": "The Input is clear and concise in asking whether UnblendedCost exceeds $100, indicating a potential cost spike, which aligns with the monitoring question. However, it lacks specific context about which service's cost is being measured (IoT Device Defender, GuardDuty, or Inspector) as detailed in the Retrieval Context. Including the service name and clarifying the implications of exceeding $100 would improve coverage and utility for answering the monitoring question more precisely.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the clarity of the Input description against the Retrieval Context to ensure the description is understandable and concise in relation to the monitoring question.\",\n    \"Evaluate the coverage of the Input by verifying if the description includes what is measured, units, context, and implications as relevant to the Retrieval Context.\",\n    \"Assess the utility by determining if the Input description sufficiently informs or helps answer the specific monitoring question posed by the Retrieval Context.\",\n    \"If the Input lacks clarity, coverage, or utility compared to the Retrieval Context, provide a specific suggestion to improve alignment and completeness.\"\n] \n \nRubric:\nNone \n \nScore: 0.5976712641531224"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the clarity of the Input description against the Retrieval Context to ensure the description is understandable and concise in relation to the monitoring question.",
                            "Evaluate the coverage of the Input by verifying if the description includes what is measured, units, context, and implications as relevant to the Retrieval Context.",
                            "Assess the utility by determining if the Input description sufficiently informs or helps answer the specific monitoring question posed by the Retrieval Context.",
                            "If the Input lacks clarity, coverage, or utility compared to the Retrieval Context, provide a specific suggestion to improve alignment and completeness."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"GLACIER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in our S3 Glacier service?\", \"retrieval_context\": [\" - This pattern captures S3 Glacier costs using unblended pricing. It identifies the actual costs charged for Glacier storage, retrieval, and data transfer.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\", \" - This pattern captures Snowmobile data transfer costs using unblended pricing. It identifies the actual costs charged for Snowmobile data transfer services.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4510290043652678,
                        "reason": "The input clearly asks if UnblendedCost exceeds $1000 for S3 Glacier, indicating a cost spike. The retrieval context partially describes unblended pricing for S3 Glacier costs but lacks explicit mention of cost thresholds, units (e.g., dollars), or how to determine if costs exceed a specific amount. It also includes unrelated patterns (Snowmobile, general S3 storage) that may cause confusion. To improve, the context should explicitly define the metric's units, clarify how to measure cost spikes, and focus solely on S3 Glacier unblended costs.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the Input (monitoring question) for clarity and specificity to determine if it clearly defines what information is sought.\",\n    \"Assess the Retrieval Context (metric description) for clarity and completeness, ensuring it defines the metric function, units, and context.\",\n    \"Compare the Retrieval Context against the Input to check if the description provides sufficient information to answer the question effectively.\",\n    \"Provide feedback on gaps or ambiguities where the description fails to cover aspects requested by the Input, and suggest improvements accordingly.\"\n] \n \nRubric:\nNone \n \nScore: 0.4510290043652678"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the Input (monitoring question) for clarity and specificity to determine if it clearly defines what information is sought.",
                            "Assess the Retrieval Context (metric description) for clarity and completeness, ensuring it defines the metric function, units, and context.",
                            "Compare the Retrieval Context against the Input to check if the description provides sufficient information to answer the question effectively.",
                            "Provide feedback on gaps or ambiguities where the description fails to cover aspects requested by the Input, and suggest improvements accordingly."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ATHENA\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 100, indicating potential resource constraints?\", \"retrieval_context\": [\" - This pattern captures Athena usage quantities. It identifies the raw usage numbers for Athena queries and data scanned TB for capacity planning.\", \" - This pattern captures EC2 usage quantities. It identifies the raw usage numbers for EC2 instances, including instance hours, which is useful for capacity planning and optimization.\", \" - This pattern captures Kinesis usage quantities. It identifies the raw usage numbers for Kinesis streams, including shard hours and data transfer for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5232586686563336,
                        "reason": "The description explains that UsageQuantity captures raw usage numbers for EC2, Athena, and Kinesis, including relevant units like instance hours and data scanned, which partially addresses what is measured and the context. However, it lacks clarity on the specific metric 'UsageQuantity' exceeding 100 and its direct implications for resource constraints, limiting the user's ability to confidently answer the monitoring question. Including explicit thresholds, units for the 100 value, and clearer implications would improve coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly and concisely explains the metric relevant to the question.\",\n    \"Evaluate whether the description provides adequate coverage by detailing what is measured, units, context, and implications to address the monitoring question fully.\",\n    \"Assess the utility by determining if the description enables a user to confidently answer the monitoring question based on the information presented.\",\n    \"If any gaps in clarity, coverage, or utility are found in the description relative to the question, suggest specific improvements to enhance understanding and relevance.\"\n] \n \nRubric:\nNone \n \nScore: 0.5232586686563336"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to check if the description clearly and concisely explains the metric relevant to the question.",
                            "Evaluate whether the description provides adequate coverage by detailing what is measured, units, context, and implications to address the monitoring question fully.",
                            "Assess the utility by determining if the description enables a user to confidently answer the monitoring question based on the information presented.",
                            "If any gaps in clarity, coverage, or utility are found in the description relative to the question, suggest specific improvements to enhance understanding and relevance."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"WORKDOCS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are spikes in UnblendedCost exceeding $1000, indicating a potential issue with WorkDocs storage or data transfer?\", \"retrieval_context\": [\" - This pattern captures WorkDocs storage costs using unblended pricing. It identifies the actual costs charged for WorkDocs storage and data transfer.\", \" - This pattern captures WorkMail email costs using unblended pricing. It identifies the actual costs charged for WorkMail user accounts and data transfer.\", \" - This pattern captures WorkSpaces desktop costs using unblended pricing. It identifies the actual costs charged for WorkSpaces instances and data transfer.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6324822659377596,
                        "reason": "The retrieval context partially addresses the question by describing that the pattern captures WorkDocs storage costs using unblended pricing and identifies actual costs for storage and data transfer, which relates to the question about spikes exceeding $1000. However, it lacks explicit mention of the measurement units, threshold values, or implications of cost spikes, limiting clarity and utility for directly answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.\",\n    \"Evaluate clarity by checking if the description is concise and understandable within the context of the question.\",\n    \"Assess coverage by verifying the metric description includes what is measured, units, relevant context, and implications in relation to the question.\",\n    \"Determine utility by judging whether the description provides sufficient information to answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.6324822659377596"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question.",
                            "Evaluate clarity by checking if the description is concise and understandable within the context of the question.",
                            "Assess coverage by verifying the metric description includes what is measured, units, relevant context, and implications in relation to the question.",
                            "Determine utility by judging whether the description provides sufficient information to answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"WORKMAIL\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are spikes in UnblendedCost exceeding $1000, indicating a potential issue with WorkMail usage?\", \"retrieval_context\": [\" - This pattern captures SES email costs using unblended pricing. It identifies the actual costs charged for SES email sending, including data transfer and API requests.\", \" - This pattern captures WorkDocs storage costs using unblended pricing. It identifies the actual costs charged for WorkDocs storage and data transfer.\", \" - This pattern captures WorkMail email costs using unblended pricing. It identifies the actual costs charged for WorkMail user accounts and data transfer.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": true,
                        "score": 0.8182425517138989,
                        "reason": "The description clearly addresses the monitoring question by focusing on spikes in UnblendedCost related to WorkMail usage, which aligns with the retrieval context about WorkMail email costs. It is concise and understandable, specifying a threshold of $1000 to indicate potential issues. However, it could improve coverage by explicitly stating the units (e.g., dollars) and elaborating on the implications of exceeding the threshold beyond indicating a 'potential issue.'",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input metric description against the Retrieval Context to assess if the description clearly addresses the monitoring question provided.\",\n    \"Evaluate the Input for clarity: check if the description is understandable and concise when viewed alongside the context provided in the Retrieval Context.\",\n    \"Assess coverage by verifying whether the description explains what is being measured, includes units, provides relevant context, and mentions implications directly related to the retrieval context.\",\n    \"Determine utility by judging if the description effectively helps answer the monitoring question, considering both the Input and the supporting details from the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.8182425517138989"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input metric description against the Retrieval Context to assess if the description clearly addresses the monitoring question provided.",
                            "Evaluate the Input for clarity: check if the description is understandable and concise when viewed alongside the context provided in the Retrieval Context.",
                            "Assess coverage by verifying whether the description explains what is being measured, includes units, provides relevant context, and mentions implications directly related to the retrieval context.",
                            "Determine utility by judging if the description effectively helps answer the monitoring question, considering both the Input and the supporting details from the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"GLACIER\\\", \\\"MetricName\\\": \\\"UsageQuantity\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UsageQuantity exceed 100,000, indicating a potential storage or retrieval issue?\", \"retrieval_context\": [\" - This pattern captures DynamoDB usage quantities. It identifies the raw usage numbers for DynamoDB tables, including read/write capacity units and storage GB-months.\", \" - This pattern captures Glacier usage quantities. It identifies the raw usage numbers for Glacier storage GB-months and retrieval requests for capacity planning.\", \" - This pattern captures S3 usage quantities. It identifies the raw usage numbers for S3 storage, including GB-months and API requests for capacity planning.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5631963073476671,
                        "reason": "The retrieval context is relevant to the input question about UsageQuantity, as it describes usage quantities for Glacier, DynamoDB, and S3, which aligns with the question's focus on usage metrics. The description is fairly clear and concise, explaining that raw usage numbers are captured for storage and requests. However, it lacks explicit mention of units (e.g., GB-months) in a way that directly ties to the threshold of 100,000, and does not clarify implications of exceeding that value or how to interpret the metric in relation to potential storage or retrieval issues. Thus, coverage and utility are moderate but incomplete for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) to the Retrieval Context (metric description) to ensure relevance and consistency between the question and the information provided.\",\n    \"Evaluate Clarity by checking if the metric description is understandable and concise when addressing the specific aspects raised in the Input.\",\n    \"Assess Coverage by verifying that the description explains what is measured, including units, context, and implications directly related to the monitoring question.\",\n    \"Judge Utility by determining if the description provides sufficient information for a user to accurately answer the monitoring question based on the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.5631963073476671"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) to the Retrieval Context (metric description) to ensure relevance and consistency between the question and the information provided.",
                            "Evaluate Clarity by checking if the metric description is understandable and concise when addressing the specific aspects raised in the Input.",
                            "Assess Coverage by verifying that the description explains what is measured, including units, context, and implications directly related to the monitoring question.",
                            "Judge Utility by determining if the description provides sufficient information for a user to accurately answer the monitoring question based on the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"DIRECTCONNECT\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this spike in UnblendedCost exceed $10,000, indicating a potential cost overrun?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3,
                        "reason": "The retrieval context is somewhat relevant as it describes unblended cost patterns for specific AWS security services, which relates to the input question about unblended cost spikes. However, it lacks clarity and coverage because it does not specify units, thresholds, or how to interpret a spike exceeding $10,000. The description does not provide actionable information to determine if the cost overrun condition is met.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description is directly relevant and aligned with the question asked.\",\n    \"Evaluate Clarity by checking if the description uses concise language that is easy to understand in relation to the terminology and needs expressed in the input question.\",\n    \"Assess Coverage by verifying that the description details what is measured, including units, operational context, and implications necessary to answer the input question fully.\",\n    \"Determine Utility by considering whether the description provides sufficient and actionable information to help a user effectively respond to or analyze the input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description is directly relevant and aligned with the question asked.",
                            "Evaluate Clarity by checking if the description uses concise language that is easy to understand in relation to the terminology and needs expressed in the input question.",
                            "Assess Coverage by verifying that the description details what is measured, including units, operational context, and implications necessary to answer the input question fully.",
                            "Determine Utility by considering whether the description provides sufficient and actionable information to help a user effectively respond to or analyze the input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CODEPIPELINE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $100, indicating a potential cost spike in CodePipeline operations?\", \"retrieval_context\": [\" - This pattern captures CloudFront CDN costs using unblended pricing. It identifies the actual costs charged for CloudFront distributions, data transfer, and requests.\", \" - This pattern captures CodeCommit repository costs using unblended pricing. It identifies the actual costs charged for CodeCommit storage and data transfer.\", \" - This pattern captures CodePipeline CI/CD costs using unblended pricing. It identifies the actual costs charged for CodePipeline pipeline executions and data transfer.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7583399299034999,
                        "reason": "The description clearly links UnblendedCost to CodePipeline operations and specifies the threshold of $100, aligning well with the retrieval context that details unblended pricing for CodePipeline costs. It covers the measurement specifics and units adequately, enabling users to identify potential cost spikes. However, it could improve by explicitly stating the time frame or aggregation period for the cost measurement to enhance completeness and practical utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description against the retrieval context to assess if the description is clear and concise, ensuring mutual understanding without redundancy or ambiguity.\",\n    \"Evaluate whether the metric description fully covers measurement specifics, units, and contextual implications as informed by the retrieval context, verifying completeness relative to the monitoring question.\",\n    \"Determine if the combined information from the input and retrieval context sufficiently enables a user to answer the monitoring question effectively, emphasizing practical utility.\",\n    \"Score the description based on clarity, coverage, and utility in relation to the retrieval context, and provide specific improvement suggestions if the score is below 8.\"\n] \n \nRubric:\nNone \n \nScore: 0.7583399299034999"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description against the retrieval context to assess if the description is clear and concise, ensuring mutual understanding without redundancy or ambiguity.",
                            "Evaluate whether the metric description fully covers measurement specifics, units, and contextual implications as informed by the retrieval context, verifying completeness relative to the monitoring question.",
                            "Determine if the combined information from the input and retrieval context sufficiently enables a user to answer the monitoring question effectively, emphasizing practical utility.",
                            "Score the description based on clarity, coverage, and utility in relation to the retrieval context, and provide specific improvement suggestions if the score is below 8."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"WORKSPACES\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.29898831055985053,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not directly address whether the UnblendedCost exceeds $1000 or indicate how to detect a cost spike. The description lacks clarity and coverage regarding measurement units, thresholds, or implications related to the monitoring question, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate Clarity by checking if the description is concise and understandable, using the question as a guide for relevance.\",\n    \"Assess Coverage by verifying the description includes what is measured, units, context, and implications aligned with the needs of the question.\",\n    \"Judge Utility by determining if the description enables a user to confidently answer the monitoring question based on the information provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.29898831055985053"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate Clarity by checking if the description is concise and understandable, using the question as a guide for relevance.",
                            "Assess Coverage by verifying the description includes what is measured, units, context, and implications aligned with the needs of the question.",
                            "Judge Utility by determining if the description enables a user to confidently answer the monitoring question based on the information provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"QUICKSIGHT\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5416690053747584,
                        "reason": "The description clearly explains that the metric measures unblended costs for specific AWS security services, including IoT Device Defender, Inspector, and GuardDuty, which aligns with the question about UnblendedCost. However, it lacks explicit mention of units (e.g., dollars) and does not directly address the threshold of $1000 or the implication of a cost spike, limiting its utility for answering the specific monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate Clarity by checking if the description is clear, concise, and easily understandable in relation to the question asked.\",\n    \"Assess Coverage to see if the description adequately explains what metric is measured, the units, the context of measurement, and the implications relevant to the retrieval context.\",\n    \"Determine Utility by verifying whether the description provides sufficient information to help a user answer the specific monitoring question based on the retrieval context.\",\n    \"Compare Input (the question) and Retrieval Context (description) to ensure the description directly addresses the question's focus and provides actionable insight.\"\n] \n \nRubric:\nNone \n \nScore: 0.5416690053747584"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate Clarity by checking if the description is clear, concise, and easily understandable in relation to the question asked.",
                            "Assess Coverage to see if the description adequately explains what metric is measured, the units, the context of measurement, and the implications relevant to the retrieval context.",
                            "Determine Utility by verifying whether the description provides sufficient information to help a user answer the specific monitoring question based on the retrieval context.",
                            "Compare Input (the question) and Retrieval Context (description) to ensure the description directly addresses the question's focus and provides actionable insight."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IAM\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost anomaly?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2562176500885798,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not directly address whether UnblendedCost exceeds $1000 or indicate how to detect a cost anomaly. The description is somewhat clear about what is measured (unblended pricing costs for certain services) but lacks units, thresholds, or implications related to cost anomalies. Therefore, it provides limited utility for confidently answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description directly addresses the question.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise, ensuring it matches the terminology and intent of the input question.\",\n    \"Assess Coverage by verifying whether the description explains what is measured, the units used, the context of the metric, and its implications relevant to the input question.\",\n    \"Measure Utility by determining if the description provides sufficient information to help a user confidently answer the input question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.2562176500885798"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to assess if the description directly addresses the question.",
                            "Evaluate Clarity by checking if the description is understandable and concise, ensuring it matches the terminology and intent of the input question.",
                            "Assess Coverage by verifying whether the description explains what is measured, the units used, the context of the metric, and its implications relevant to the input question.",
                            "Measure Utility by determining if the description provides sufficient information to help a user confidently answer the input question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SECRETSMANAGER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $100,000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.29241418147556225,
                        "reason": "The retrieval context mentions that the costs are captured using unblended pricing for specific services, which partially relates to the Input's focus on UnblendedCost. However, it does not clarify what UnblendedCost measures overall, lacks units or thresholds like the $100,000 mentioned, and does not address how to determine if costs exceed that amount or indicate a spike. The context is somewhat clear but incomplete and insufficient to directly answer the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly addresses the question.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise, simple, and understandable without ambiguity relative to the Input.\",\n    \"Assess Coverage by verifying if the Retrieval Context explains what is measured, including units, operational context, and implications relevant to the Input question.\",\n    \"Judge Utility by determining if the Retrieval Context provides sufficient information to directly or indirectly answer the Input question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.29241418147556225"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description clearly addresses the question.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise, simple, and understandable without ambiguity relative to the Input.",
                            "Assess Coverage by verifying if the Retrieval Context explains what is measured, including units, operational context, and implications relevant to the Input question.",
                            "Judge Utility by determining if the Retrieval Context provides sufficient information to directly or indirectly answer the Input question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"KMS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike in KMS key management?\", \"retrieval_context\": [\" - This pattern captures Certificate Manager costs using unblended pricing. It identifies the actual costs charged for SSL/TLS certificates and renewals.\", \" - This pattern captures EKS Kubernetes costs using unblended pricing. It identifies the actual costs charged for EKS cluster management and control plane.\", \" - This pattern captures KMS key management costs using unblended pricing. It identifies the actual costs charged for KMS key usage and API requests.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6374400079414216,
                        "reason": "The description partially clarifies that UnblendedCost relates to actual costs charged for KMS key usage and API requests, aligning with the retrieval context. However, it lacks explicit mention of units (dollars) and does not fully explain the operational context or implications of exceeding $1000, limiting its utility in assessing a cost spike. Adding details on units and the significance of the threshold would improve clarity and coverage.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input description against the retrieval context to assess clarity, ensuring the description language is precise and avoids ambiguity relative to the question.\",\n    \"Evaluate coverage by verifying that the description fully addresses what the metric measures, including units, operational context, and potential implications, as relevant to the retrieval context.\",\n    \"Assess utility by determining if the description, informed by the retrieval context, effectively aids in answering the monitoring question, highlighting any missing information that impairs usability.\",\n    \"If the description scores below 8, identify specific gaps or unclear points in the description compared to the retrieval context and suggest targeted improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.6374400079414216"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input description against the retrieval context to assess clarity, ensuring the description language is precise and avoids ambiguity relative to the question.",
                            "Evaluate coverage by verifying that the description fully addresses what the metric measures, including units, operational context, and potential implications, as relevant to the retrieval context.",
                            "Assess utility by determining if the description, informed by the retrieval context, effectively aids in answering the monitoring question, highlighting any missing information that impairs usability.",
                            "If the description scores below 8, identify specific gaps or unclear points in the description compared to the retrieval context and suggest targeted improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CLOUDTRAIL\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.32227001439842334,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not directly address whether UnblendedCost exceeds $1000 or indicate how to detect a cost spike. The description is somewhat clear about what costs are measured but lacks explicit units, threshold context, or implications related to the monitoring question, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and clearly.\",\n    \"Evaluate Clarity by checking if the metric description is concise, unambiguous, and easy to understand relative to the terminology used in the Input.\",\n    \"Assess Coverage by verifying if the description explains what is measured, the units, context, and implications in a way that aligns with the monitoring question.\",\n    \"Judge Utility by determining if the description provides sufficient information to confidently answer the monitoring question; if not, identify specific missing elements.\"\n] \n \nRubric:\nNone \n \nScore: 0.32227001439842334"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly and clearly.",
                            "Evaluate Clarity by checking if the metric description is concise, unambiguous, and easy to understand relative to the terminology used in the Input.",
                            "Assess Coverage by verifying if the description explains what is measured, the units, context, and implications in a way that aligns with the monitoring question.",
                            "Judge Utility by determining if the description provides sufficient information to confidently answer the monitoring question; if not, identify specific missing elements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"WAF\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in WAF security expenses?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures WAF security costs using unblended pricing. It identifies the actual costs charged for WAF web ACLs, rules, and requests.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5345467135746296,
                        "reason": "The description clearly addresses WAF security costs using unblended pricing, aligning with the question's focus on UnblendedCost for WAF. The language is understandable and concise, directly relating to the question's terminology. However, it lacks explicit mention of cost thresholds (e.g., $1000), measurement units, or implications of cost spikes, limiting its utility for confidently answering whether costs exceed the specified amount.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input ('Question') with the Retrieval Context ('Description') to verify if the description clearly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description uses understandable, concise language that directly relates to the question's terminology.\",\n    \"Assess Coverage by confirming the description includes what is measured, units, context, and implications relevant to answering the question.\",\n    \"Judge Utility by determining if the description provides sufficient information to confidently answer the question; if not, identify missing or unclear details.\"\n] \n \nRubric:\nNone \n \nScore: 0.5345467135746296"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input ('Question') with the Retrieval Context ('Description') to verify if the description clearly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description uses understandable, concise language that directly relates to the question's terminology.",
                            "Assess Coverage by confirming the description includes what is measured, units, context, and implications relevant to answering the question.",
                            "Judge Utility by determining if the description provides sufficient information to confidently answer the question; if not, identify missing or unclear details."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"GUARDDUTY\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3077496164602671,
                        "reason": "The retrieval context mentions unblended costs for specific services but does not clearly define what 'UnblendedCost' means overall or provide units or thresholds like $1000. It lacks direct explanation of how to determine if costs exceed $1000 or implications of a cost spike, limiting its utility in answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.\",\n    \"Evaluate the clarity of the Retrieval Context in relation to the Input by checking if the description is concise and understandable without extraneous jargon.\",\n    \"Assess the coverage of the Retrieval Context ensuring it explains what is measured, includes units and relevant context, and provides implications that directly relate to the Input question.\",\n    \"Determine the utility by judging if the description enables a user to effectively answer the Input question using the information in the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3077496164602671"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the question's intent.",
                            "Evaluate the clarity of the Retrieval Context in relation to the Input by checking if the description is concise and understandable without extraneous jargon.",
                            "Assess the coverage of the Retrieval Context ensuring it explains what is measured, includes units and relevant context, and provides implications that directly relate to the Input question.",
                            "Determine the utility by judging if the description enables a user to effectively answer the Input question using the information in the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CERTIFICATEMANAGER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are UnblendedCost spikes exceeding $100,000, indicating a potential cost overrun in Certificate Manager?\", \"retrieval_context\": [\" - This pattern captures Certificate Manager costs using unblended pricing. It identifies the actual costs charged for SSL/TLS certificates and renewals.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures Systems Manager costs using unblended pricing. It identifies the actual costs charged for Systems Manager parameters, automation, and session manager.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3903626444520758,
                        "reason": "The retrieval context partially addresses the input by describing Certificate Manager costs using unblended pricing, which aligns with the question about UnblendedCost spikes. However, it lacks specific information about cost spikes exceeding $100,000, measurement units, or implications for cost overruns, limiting clarity and utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the description is understandable and concise given the specific terminology in the input question.\",\n    \"Assess Coverage by verifying that the description includes what is measured, units, relevant context, and implications necessary to answer the input question.\",\n    \"Determine Utility by judging whether the description enables a user to confidently respond to the input question based on the provided context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3903626444520758"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the description is understandable and concise given the specific terminology in the input question.",
                            "Assess Coverage by verifying that the description includes what is measured, units, relevant context, and implications necessary to answer the input question.",
                            "Determine Utility by judging whether the description enables a user to confidently respond to the input question based on the provided context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"MACIE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $100,000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5462631258473172,
                        "reason": "The input is clear and concise in asking whether UnblendedCost exceeds $100,000, indicating a cost spike, which relates to monitoring costs. However, it lacks specificity about which service's costs are being measured, while the retrieval context focuses on costs for IoT Device Defender, Inspector, and GuardDuty separately. The input does not clarify units beyond the dollar amount or explicitly connect to the individual services, limiting its alignment and utility in fully addressing the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate the clarity of the Input (metric description) by checking if it is understandable and concise in relation to the Retrieval Context (monitoring question).\",\n    \"Assess the coverage in the Input by verifying if the description explains what is measured, including units, context, and implications relevant to the Retrieval Context.\",\n    \"Determine the utility of the Input by judging whether the description helps directly answer the Retrieval Context's monitoring question.\",\n    \"Compare Input and Retrieval Context closely to ensure the description aligns well with the question\u2019s focus; if misaligned or incomplete, identify specific areas for improvement.\"\n] \n \nRubric:\nNone \n \nScore: 0.5462631258473172"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate the clarity of the Input (metric description) by checking if it is understandable and concise in relation to the Retrieval Context (monitoring question).",
                            "Assess the coverage in the Input by verifying if the description explains what is measured, including units, context, and implications relevant to the Retrieval Context.",
                            "Determine the utility of the Input by judging whether the description helps directly answer the Retrieval Context's monitoring question.",
                            "Compare Input and Retrieval Context closely to ensure the description aligns well with the question\u2019s focus; if misaligned or incomplete, identify specific areas for improvement."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"INSPECTOR\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3811570465996056,
                        "reason": "The description partially addresses the question by explaining that the patterns capture unblended costs for specific security services, which relates to cost monitoring. However, it lacks clarity and completeness regarding the specific metric 'UnblendedCost' exceeding $100, does not specify units or thresholds, and does not explicitly connect cost spikes to implications. Thus, it provides limited utility for fully answering the retrieval question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (metric description) with the retrieval context (related monitoring question) to ensure the description directly addresses the question asked.\",\n    \"Evaluate the clarity of the input by checking if the description is concise and easily understandable in the context of the retrieval question.\",\n    \"Assess coverage by verifying that the description explains what is measured, units, context, and implications relevant to the retrieval question.\",\n    \"Determine the utility by confirming if the description enables a user to answer the retrieval question effectively and completely.\"\n] \n \nRubric:\nNone \n \nScore: 0.3811570465996056"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (metric description) with the retrieval context (related monitoring question) to ensure the description directly addresses the question asked.",
                            "Evaluate the clarity of the input by checking if the description is concise and easily understandable in the context of the retrieval question.",
                            "Assess coverage by verifying that the description explains what is measured, units, context, and implications relevant to the retrieval question.",
                            "Determine the utility by confirming if the description enables a user to answer the retrieval question effectively and completely."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SHIELD\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $100,000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.26224593312018546,
                        "reason": "The Retrieval Context relates to unblended costs for specific AWS security services, which partially aligns with the Input question about UnblendedCost exceeding $100,000. However, it lacks clarity and conciseness due to repetitive phrasing and technical terms without defining 'unblended pricing' or explaining measurement units. It also fails to address the threshold or implications of exceeding $100,000, limiting its utility in answering the question about a potential cost spike.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly relates to and addresses the question.\",\n    \"Evaluate the clarity of the Retrieval Context by checking if it is concise and understandable, avoiding technical jargon that might obscure meaning.\",\n    \"Assess the coverage by verifying that the Retrieval Context explains what is measured, the measurement units, relevant context, and any implications for interpretation.\",\n    \"Determine the utility by judging whether the Retrieval Context provides enough information to appropriately answer the Input question; identify gaps where the description fails to support the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.26224593312018546"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly relates to and addresses the question.",
                            "Evaluate the clarity of the Retrieval Context by checking if it is concise and understandable, avoiding technical jargon that might obscure meaning.",
                            "Assess the coverage by verifying that the Retrieval Context explains what is measured, the measurement units, relevant context, and any implications for interpretation.",
                            "Determine the utility by judging whether the Retrieval Context provides enough information to appropriately answer the Input question; identify gaps where the description fails to support the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CONFIG\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.5294464258692615,
                        "reason": "The retrieval context aligns with the input by explaining that unblended costs represent actual charges for specific AWS services, reinforcing the term 'UnblendedCost.' The language is clear and straightforward without jargon. However, the retrieval context does not specify units or explicitly mention the $1000 threshold or the implication of a cost spike, limiting full coverage and utility for answering the monitoring question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description with the retrieval context to ensure terms and explanations align and reinforce each other.\",\n    \"Evaluate the clarity by verifying that both input and retrieval context use straightforward language without ambiguity or excessive jargon.\",\n    \"Assess coverage by confirming that together the input and retrieval context fully describe what is measured, including units, relevant context, and implications.\",\n    \"Judge utility by determining if the combined information from input and retrieval context sufficiently equips the user to answer the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.5294464258692615"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description with the retrieval context to ensure terms and explanations align and reinforce each other.",
                            "Evaluate the clarity by verifying that both input and retrieval context use straightforward language without ambiguity or excessive jargon.",
                            "Assess coverage by confirming that together the input and retrieval context fully describe what is measured, including units, relevant context, and implications.",
                            "Judge utility by determining if the combined information from input and retrieval context sufficiently equips the user to answer the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"OPSWORKS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3022810926568987,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not directly address whether UnblendedCost exceeds $100 or indicate how to detect a cost spike. The description is somewhat clear about what is measured (actual costs charged) but lacks coverage of units, thresholds, or implications related to the input question. Consequently, it provides limited utility for confidently answering the question about cost spikes.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by determining if the description is concise and understandable in relation to the terminology and concepts presented in the Input.\",\n    \"Assess Coverage by verifying that the description explains what is measured, the units, context, and implications relevant to the question in the Input.\",\n    \"Judge Utility by considering whether the description provides sufficient information to enable a user to confidently answer the Input question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3022810926568987"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description directly addresses the question's focus.",
                            "Evaluate Clarity by determining if the description is concise and understandable in relation to the terminology and concepts presented in the Input.",
                            "Assess Coverage by verifying that the description explains what is measured, the units, context, and implications relevant to the question in the Input.",
                            "Judge Utility by considering whether the description provides sufficient information to enable a user to confidently answer the Input question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"CLOUDHSM\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3377540675801802,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not explicitly address whether the UnblendedCost metric can be used to detect if costs exceed $1000 or indicate a cost spike. The description is somewhat clear about what is measured (actual costs for certain services) but lacks information on units, thresholds, or implications related to cost spikes, limiting its utility in answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.\",\n    \"Evaluate the clarity of the description to ensure it is concise and easily understandable, facilitating straightforward interpretation in relation to the query.\",\n    \"Assess the coverage of the description by verifying that it explains what is measured, the units used, relevant context, and potential implications, ensuring completeness relative to the question.\",\n    \"Determine the utility by confirming whether the description provides sufficient information to effectively answer the monitoring question, highlighting any gaps between the description and the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3377540675801802"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the question's focus.",
                            "Evaluate the clarity of the description to ensure it is concise and easily understandable, facilitating straightforward interpretation in relation to the query.",
                            "Assess the coverage of the description by verifying that it explains what is measured, the units used, relevant context, and potential implications, ensuring completeness relative to the question.",
                            "Determine the utility by confirming whether the description provides sufficient information to effectively answer the monitoring question, highlighting any gaps between the description and the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ORGANIZATIONS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are spikes in UnblendedCost exceeding $1000 indicative of an issue with Organizations management?\", \"retrieval_context\": [\" - This pattern captures Organizations management costs using unblended pricing. It identifies the actual costs charged for Organizations account management and policies.\", \" - This pattern captures Single Sign-On costs using unblended pricing. It identifies the actual costs charged for SSO user management and application access.\", \" - This pattern captures Systems Manager costs using unblended pricing. It identifies the actual costs charged for Systems Manager parameters, automation, and session manager.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4088593704010868,
                        "reason": "The retrieval context partially addresses the input by describing Organizations management costs using unblended pricing, which relates to the question about cost spikes. However, it lacks clarity and detail on what constitutes a spike, the significance of exceeding $1000, or whether such spikes indicate an issue. The context is somewhat fragmented and does not explicitly connect cost spikes to potential problems, limiting its utility for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is concise and easily understandable, avoiding ambiguity that could confuse interpreting the Input.\",\n    \"Assess Coverage by verifying the description includes what is measured, the units, relevant context, and potential implications, ensuring it complements the Input question.\",\n    \"Determine Utility by judging if the description provides enough detail and relevance to help a user confidently answer the Input question using the Retrieval Context.\"\n] \n \nRubric:\nNone \n \nScore: 0.4088593704010868"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is concise and easily understandable, avoiding ambiguity that could confuse interpreting the Input.",
                            "Assess Coverage by verifying the description includes what is measured, the units, relevant context, and potential implications, ensuring it complements the Input question.",
                            "Determine Utility by judging if the description provides enough detail and relevance to help a user confidently answer the Input question using the Retrieval Context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SYSTEMSMANAGER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.32227001491431584,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not directly address whether UnblendedCost exceeds $1000 or indicate how to detect a cost spike. The description is somewhat clear about what is measured (actual costs for certain services) but lacks units, thresholds, or implications related to the input question. Therefore, it provides limited utility for answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question posed.\",\n    \"Evaluate the clarity of the metric description to verify if it is understandable and concise in relation to the input question.\",\n    \"Assess coverage by checking if the description explains what is measured, includes units, provides relevant context, and states implications that help answer the input question.\",\n    \"Determine the utility by confirming whether the description enables the user to effectively answer the monitoring question based on the given metric information.\"\n] \n \nRubric:\nNone \n \nScore: 0.32227001491431584"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description directly addresses the question posed.",
                            "Evaluate the clarity of the metric description to verify if it is understandable and concise in relation to the input question.",
                            "Assess coverage by checking if the description explains what is measured, includes units, provides relevant context, and states implications that help answer the input question.",
                            "Determine the utility by confirming whether the description enables the user to effectively answer the monitoring question based on the given metric information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"TRANSFER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in Transfer Family services?\", \"retrieval_context\": [\" - This pattern captures Snow Family costs using unblended pricing. It identifies the actual costs charged for Snow Family devices and data transfer services.\", \" - This pattern captures Transfer Family costs using unblended pricing. It identifies the actual costs charged for SFTP, FTPS, and FTP over AWS file transfer services.\", \" - This pattern captures WorkDocs storage costs using unblended pricing. It identifies the actual costs charged for WorkDocs storage and data transfer.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.47917726428197416,
                        "reason": "The retrieval context partially addresses the monitoring question by identifying that the pattern captures Transfer Family costs using unblended pricing, which relates to the cost spike inquiry. However, it lacks clarity and completeness as it does not specify units, thresholds (e.g., $1000), or implications of exceeding the cost, limiting its utility in fully answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the specific monitoring question.\",\n    \"Evaluate the description\u2019s Clarity by verifying that the language is understandable and concise, avoiding ambiguity in relation to the question.\",\n    \"Assess Coverage by ensuring the description explains what is measured, including units, context, and implications relevant to the input question.\",\n    \"Judge Utility by determining if the description provides sufficient information and insight to accurately answer the monitoring question; if not, identify gaps for improvement.\"\n] \n \nRubric:\nNone \n \nScore: 0.47917726428197416"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description clearly addresses the specific monitoring question.",
                            "Evaluate the description\u2019s Clarity by verifying that the language is understandable and concise, avoiding ambiguity in relation to the question.",
                            "Assess Coverage by ensuring the description explains what is measured, including units, context, and implications relevant to the input question.",
                            "Judge Utility by determining if the description provides sufficient information and insight to accurately answer the monitoring question; if not, identify gaps for improvement."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SINGLESIGNON\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $100,000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.21480472055495775,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not directly address the UnblendedCost metric or whether it exceeds $100,000. The description lacks clarity on the metric's units, operational context, or implications related to cost spikes, providing limited actionable insight for the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (monitoring question) with the retrieval context (metric description) to assess if the description is directly relevant and addresses the question posed.\",\n    \"Evaluate the clarity of the description in relation to the complexity and terminology of the input question, ensuring the description uses understandable and concise language appropriate for the user's needs.\",\n    \"Check coverage by confirming that the description explains what metric is measured, its units, operational context, and implications, and verify these elements align with the aspects needed to answer the question.\",\n    \"Assess utility by determining if the description provides actionable insight that can effectively help a user answer the monitoring question, ensuring coherence between input and retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.21480472055495775"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (monitoring question) with the retrieval context (metric description) to assess if the description is directly relevant and addresses the question posed.",
                            "Evaluate the clarity of the description in relation to the complexity and terminology of the input question, ensuring the description uses understandable and concise language appropriate for the user's needs.",
                            "Check coverage by confirming that the description explains what metric is measured, its units, operational context, and implications, and verify these elements align with the aspects needed to answer the question.",
                            "Assess utility by determining if the description provides actionable insight that can effectively help a user answer the monitoring question, ensuring coherence between input and retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"DIRECTORY\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this spike in UnblendedCost exceed $1000, indicating a potential cost overrun?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.30179862092054066,
                        "reason": "The Retrieval Context describes costs related to specific security services using unblended pricing but does not address the spike in UnblendedCost or whether it exceeds $1000, which is the core of the Input question. The description lacks clarity on the metric's measurement units, thresholds, or implications for cost overruns, limiting its utility to answer the question effectively. To improve, the description should explicitly define what constitutes a spike, include cost thresholds like $1000, and clarify how to interpret these costs in relation to potential overruns.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) for Clarity: ensure the metric description uses clear and concise language that directly relates to the question's intent.\",\n    \"Evaluate Coverage by checking if the Retrieval Context fully explains what is measured, including units, relevant context, and implications, in relation to what the Input question asks.\",\n    \"Assess Utility by determining whether the Retrieval Context provides sufficient information to effectively answer the Input question.\",\n    \"If any of the criteria score below 8, provide a specific suggestion to improve the description, ensuring the Input and Retrieval Context alignment is enhanced.\"\n] \n \nRubric:\nNone \n \nScore: 0.30179862092054066"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) for Clarity: ensure the metric description uses clear and concise language that directly relates to the question's intent.",
                            "Evaluate Coverage by checking if the Retrieval Context fully explains what is measured, including units, relevant context, and implications, in relation to what the Input question asks.",
                            "Assess Utility by determining whether the Retrieval Context provides sufficient information to effectively answer the Input question.",
                            "If any of the criteria score below 8, provide a specific suggestion to improve the description, ensuring the Input and Retrieval Context alignment is enhanced."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"STORAGEGATEWAY\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike in Storage Gateway services?\", \"retrieval_context\": [\" - This pattern captures Backup service costs using unblended pricing. It identifies the actual costs charged for AWS Backup storage and data transfer.\", \" - This pattern captures S3 storage costs using unblended pricing. It identifies the actual costs charged for S3 storage, data transfer, and API requests.\", \" - This pattern captures Storage Gateway costs using unblended pricing. It identifies the actual costs charged for Storage Gateway file, volume, and tape gateways.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.7027111382880419,
                        "reason": "The description clearly states the metric being measured (UnblendedCost exceeding $1000) and its relation to a potential cost spike in Storage Gateway services, aligning with the monitoring question. However, it lacks detailed measurement units beyond the dollar amount, and does not explicitly connect how the unblended cost is derived or aggregated from the specific Storage Gateway components mentioned in the retrieval context. Including these details would improve coverage and utility by better linking the metric to the context and clarifying implications.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (metric description) and the Retrieval Context to assess if the description clearly and concisely explains what is being measured in relation to the monitoring question.\",\n    \"Evaluate coverage by checking if the description includes measurement details, units, relevant context, and the implications necessary to understand how it relates to the question.\",\n    \"Determine utility by judging whether the description directly supports answering the monitoring question from the Retrieval Context, ensuring relevance and applicability.\",\n    \"If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context, and suggest targeted improvements to enhance clarity, coverage, or utility.\"\n] \n \nRubric:\nNone \n \nScore: 0.7027111382880419"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (metric description) and the Retrieval Context to assess if the description clearly and concisely explains what is being measured in relation to the monitoring question.",
                            "Evaluate coverage by checking if the description includes measurement details, units, relevant context, and the implications necessary to understand how it relates to the question.",
                            "Determine utility by judging whether the description directly supports answering the monitoring question from the Retrieval Context, ensuring relevance and applicability.",
                            "If any aspect scores below 8, identify specific gaps between the Input and Retrieval Context, and suggest targeted improvements to enhance clarity, coverage, or utility."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"BACKUP\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike in the Backup service?\", \"retrieval_context\": [\" - This pattern captures Backup service costs using unblended pricing. It identifies the actual costs charged for AWS Backup storage and data transfer.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\", \" - This pattern captures RDS database costs using unblended pricing. It identifies the actual costs charged for RDS instances, storage, and backup storage.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.4734140342064229,
                        "reason": "The retrieval context partially addresses the monitoring question by describing that the pattern captures Backup service costs using unblended pricing, which relates to the UnblendedCost metric. However, it lacks explicit information about the metric's units, the threshold of $1000, or how to interpret a cost spike, limiting clarity and practical utility. The context does not directly explain how to determine if the cost exceeds $1000 or the implications of such an event, reducing its usefulness for confidently answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description is clearly understandable and concise, ensuring the explanation directly relates to the question.\",\n    \"Evaluate whether the Retrieval Context thoroughly covers what the metric measures, its units, the contextual background, and potential implications to fully support understanding of the Input question.\",\n    \"Determine the utility by judging if the description enables a user to confidently answer the monitoring question based on the information provided, indicating strong relevance and practicality.\",\n    \"If the assessment score is below 8, identify specific gaps or ambiguities in the description that hinder clarity, coverage, or utility, and suggest targeted improvements aligned with the Input question's needs.\"\n] \n \nRubric:\nNone \n \nScore: 0.4734140342064229"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description is clearly understandable and concise, ensuring the explanation directly relates to the question.",
                            "Evaluate whether the Retrieval Context thoroughly covers what the metric measures, its units, the contextual background, and potential implications to fully support understanding of the Input question.",
                            "Determine the utility by judging if the description enables a user to confidently answer the monitoring question based on the information provided, indicating strong relevance and practicality.",
                            "If the assessment score is below 8, identify specific gaps or ambiguities in the description that hinder clarity, coverage, or utility, and suggest targeted improvements aligned with the Input question's needs."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"APPLICATIONMIGRATIONSERVICE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in the Application Migration Service?\", \"retrieval_context\": [\" - This pattern captures Application Migration Service costs using unblended pricing. It identifies the actual costs charged for application migration and replication.\", \" - This pattern captures Migration Hub costs using unblended pricing. It identifies the actual costs charged for Migration Hub application discovery and migration tracking.\", \" - This pattern captures Single Sign-On costs using unblended pricing. It identifies the actual costs charged for SSO user management and application access.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.35861779085218487,
                        "reason": "The retrieval context partially addresses the input question by identifying that it captures Application Migration Service costs using unblended pricing, which aligns with the question's focus. However, it lacks clarity and conciseness regarding what exactly is measured (e.g., no mention of units or thresholds like $1000), and does not explain implications or how to interpret a cost spike. This limits the utility for confidently answering whether costs exceed $1000 or indicate a spike.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question's focus and terminology.\",\n    \"Evaluate the clarity of the description by checking if the language is straightforward and concise, avoiding ambiguity in relation to the input question.\",\n    \"Assess coverage by verifying that the description explains what is measured, includes units, context, and potential implications relevant to the monitoring question.\",\n    \"Determine utility by judging if the description enables a user to confidently answer the input question, ensuring the description provides actionable or insightful information.\"\n] \n \nRubric:\nNone \n \nScore: 0.35861779085218487"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description clearly addresses the question's focus and terminology.",
                            "Evaluate the clarity of the description by checking if the language is straightforward and concise, avoiding ambiguity in relation to the input question.",
                            "Assess coverage by verifying that the description explains what is measured, includes units, context, and potential implications relevant to the monitoring question.",
                            "Determine utility by judging if the description enables a user to confidently answer the input question, ensuring the description provides actionable or insightful information."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"MIGRATIONHUB\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are UnblendedCosts exceeding budget, indicating a potential cost overrun?\", \"retrieval_context\": [\" - This pattern captures DynamoDB costs using unblended pricing. It identifies the actual costs charged for DynamoDB tables, read/write capacity, and storage.\", \" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.24378234991142017,
                        "reason": "The retrieval context describes unblended costs for specific AWS services but does not address whether these costs exceed a budget or indicate cost overruns, which is the core of the monitoring question. The description lacks clarity on measurement units, operational context related to budgeting, and thresholds for cost overruns, limiting its utility for answering the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input (monitoring question) and retrieval context (metric description) for alignment to ensure the description is relevant and appropriately targeted to answer the question.\",\n    \"Evaluate the description\u2019s clarity by checking if it explains the metric in straightforward terms without ambiguity, making sure the language is concise and accessible.\",\n    \"Assess coverage by confirming that the description includes what is measured, units of measurement, operational context, and any implications or thresholds related to the metric.\",\n    \"Determine utility by verifying whether the description provides sufficient information for a user to confidently answer the given monitoring question, identifying any gaps or missing details.\"\n] \n \nRubric:\nNone \n \nScore: 0.24378234991142017"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input (monitoring question) and retrieval context (metric description) for alignment to ensure the description is relevant and appropriately targeted to answer the question.",
                            "Evaluate the description\u2019s clarity by checking if it explains the metric in straightforward terms without ambiguity, making sure the language is concise and accessible.",
                            "Assess coverage by confirming that the description includes what is measured, units of measurement, operational context, and any implications or thresholds related to the metric.",
                            "Determine utility by verifying whether the description provides sufficient information for a user to confidently answer the given monitoring question, identifying any gaps or missing details."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SNOWBALL\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6221414164459619,
                        "reason": "The description clearly explains that the metric measures unblended costs for specific AWS security services, including IoT Device Defender, Inspector, and GuardDuty, which aligns with the input's focus on UnblendedCost. However, it lacks explicit mention of units (dollars) and does not directly address the threshold of $1000 or the implication of a cost spike, limiting its utility in fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Assess clarity by checking if the description is straightforward and concise enough for a user with basic telemetry knowledge to understand.\",\n    \"Evaluate coverage by verifying that the description includes what is measured, the units involved, the context in which the metric operates, and its implications.\",\n    \"Determine utility by judging whether the description provides sufficient information to answer the given monitoring question accurately.\",\n    \"Compare the description's content (retrieval context) directly against the question (input) to ensure relevance and completeness in addressing the query.\"\n] \n \nRubric:\nNone \n \nScore: 0.6221414164459619"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Assess clarity by checking if the description is straightforward and concise enough for a user with basic telemetry knowledge to understand.",
                            "Evaluate coverage by verifying that the description includes what is measured, the units involved, the context in which the metric operates, and its implications.",
                            "Determine utility by judging whether the description provides sufficient information to answer the given monitoring question accurately.",
                            "Compare the description's content (retrieval context) directly against the question (input) to ensure relevance and completeness in addressing the query."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SNOWFAMILY\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $100,000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3003077159324917,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not directly address the overall UnblendedCost metric or whether it exceeds $100,000. It lacks clarity on the metric's units, overall cost aggregation, or implications of a cost spike, limiting its utility in answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description is relevant and adequately addresses the question.\",\n    \"Evaluate Clarity by checking if the description is easily understandable and concise with respect to the terminology used in the question.\",\n    \"Assess Coverage by verifying if the description explains what metric is measured, units, the context of measurement, and any implications directly related to the question.\",\n    \"Judge Utility by determining whether the description provides enough actionable information to help answer the monitoring question; if not, identify specific missing details.\"\n] \n \nRubric:\nNone \n \nScore: 0.3003077159324917"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to ensure the description is relevant and adequately addresses the question.",
                            "Evaluate Clarity by checking if the description is easily understandable and concise with respect to the terminology used in the question.",
                            "Assess Coverage by verifying if the description explains what metric is measured, units, the context of measurement, and any implications directly related to the question.",
                            "Judge Utility by determining whether the description provides enough actionable information to help answer the monitoring question; if not, identify specific missing details."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SNOWCONE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Are UnblendedCosts exceeding budget, indicating potential cost overrun or unexpected usage?\", \"retrieval_context\": [\" - This pattern captures API Gateway costs using unblended pricing. It identifies the actual costs charged for API Gateway requests, data transfer, and cache usage.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures Snowcone edge computing costs using unblended pricing. It identifies the actual costs charged for Snowcone device usage and data transfer.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.29225038369388,
                        "reason": "The retrieval context describes unblended costs for specific services but does not explicitly address whether these costs exceed budget or indicate cost overruns, which is the core of the input question. While the description is clear and concise about what is measured (actual costs for certain services), it lacks coverage of budget comparison, units, or implications related to cost overruns. Therefore, it provides limited utility for answering the monitoring question about exceeding budget.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description is clearly relevant and addresses the question.\",\n    \"Assess Clarity by verifying if the description is straightforward, concise, and free of ambiguity in relation to the input question.\",\n    \"Evaluate Coverage by checking if the description includes what is measured, units, context, and implications that connect directly to the monitoring question.\",\n    \"Determine Utility by confirming if the description provides sufficient information for a user to effectively answer the given question based on the retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.29225038369388"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description is clearly relevant and addresses the question.",
                            "Assess Clarity by verifying if the description is straightforward, concise, and free of ambiguity in relation to the input question.",
                            "Evaluate Coverage by checking if the description includes what is measured, units, context, and implications that connect directly to the monitoring question.",
                            "Determine Utility by confirming if the description provides sufficient information for a user to effectively answer the given question based on the retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"SNOWMOBILE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3975667517081293,
                        "reason": "The description partially addresses the question by explaining that the pattern captures unblended costs for specific AWS security services, which relates to cost measurement. However, it lacks clarity and directness about what 'UnblendedCost' specifically measures, the units involved, and does not mention the $1000 threshold or implications of a cost spike. The explanation is somewhat fragmented and does not provide enough context for confidently answering whether costs exceed the specified amount.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question.\",\n    \"Evaluate Clarity by checking if the description is easy to understand and concise without unnecessary jargon.\",\n    \"Assess Coverage by verifying if the description explains what is measured, the units involved, the monitoring context, and the implications of the metric.\",\n    \"Determine Utility by judging if the description provides enough relevant information to help the user confidently answer the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.3975667517081293"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description directly addresses the question.",
                            "Evaluate Clarity by checking if the description is easy to understand and concise without unnecessary jargon.",
                            "Assess Coverage by verifying if the description explains what is measured, the units involved, the monitoring context, and the implications of the metric.",
                            "Determine Utility by judging if the description provides enough relevant information to help the user confidently answer the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTANALYTICS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in IoT Analytics?\", \"retrieval_context\": [\" - This pattern captures IoT Analytics costs using unblended pricing. It identifies the actual costs charged for IoT Analytics data processing and storage.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\", \" - This pattern captures IoT Events costs using unblended pricing. It identifies the actual costs charged for IoT Events detection and response to IoT data.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2924141823112637,
                        "reason": "The description partially addresses the question by explaining that it captures IoT Analytics costs using unblended pricing, which relates to the cost metric in question. However, it lacks clarity and completeness as it does not specify the measurement units, thresholds (e.g., $1000), or implications of exceeding the cost. The context is somewhat fragmented and does not directly link to identifying a cost spike, limiting its utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the metric description (Retrieval Context) against the monitoring question (Input) to determine if the description directly addresses the question.\",\n    \"Evaluate Clarity by assessing whether the description is concise and easy to understand without unnecessary jargon or ambiguity.\",\n    \"Assess Coverage to ensure the description explains what is being measured, includes units, provides sufficient context, and notes implications relevant to answering the question.\",\n    \"Judge Utility by considering whether the description equips the user with enough information to confidently answer the monitoring question, checking the alignment and completeness relative to the Input.\"\n] \n \nRubric:\nNone \n \nScore: 0.2924141823112637"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the metric description (Retrieval Context) against the monitoring question (Input) to determine if the description directly addresses the question.",
                            "Evaluate Clarity by assessing whether the description is concise and easy to understand without unnecessary jargon or ambiguity.",
                            "Assess Coverage to ensure the description explains what is being measured, includes units, provides sufficient context, and notes implications relevant to answering the question.",
                            "Judge Utility by considering whether the description equips the user with enough information to confidently answer the monitoring question, checking the alignment and completeness relative to the Input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"ROBOMAKER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in RoboMaker services?\", \"retrieval_context\": [\" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\", \" - This pattern captures RoboMaker robotics costs using unblended pricing. It identifies the actual costs charged for RoboMaker simulation and robot applications.\", \" - This pattern captures Snowmobile data transfer costs using unblended pricing. It identifies the actual costs charged for Snowmobile data transfer services.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.41218644175960745,
                        "reason": "The Retrieval Context mentions that it captures RoboMaker robotics costs using unblended pricing, which aligns with the Input's focus on UnblendedCost for RoboMaker services. However, it lacks clarity on whether the cost exceeds $1000 or indicates a cost spike, and does not specify units or thresholds. The context is somewhat clear but incomplete, providing limited actionable insight to directly answer the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.\",\n    \"Evaluate Clarity by checking if the Retrieval Context is understandable and concise in relation to the terminology and concepts posed by the Input.\",\n    \"Assess Coverage by verifying that the Retrieval Context fully explains what is measured, including units and relevant context, so it supports answering the Input question.\",\n    \"Determine Utility by confirming that the Retrieval Context provides actionable or informative details that directly help answer or clarify the Input monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.41218644175960745"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description clearly addresses the question's focus.",
                            "Evaluate Clarity by checking if the Retrieval Context is understandable and concise in relation to the terminology and concepts posed by the Input.",
                            "Assess Coverage by verifying that the Retrieval Context fully explains what is measured, including units and relevant context, so it supports answering the Input question.",
                            "Determine Utility by confirming that the Retrieval Context provides actionable or informative details that directly help answer or clarify the Input monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"GROUNDSTATION\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3377540668798146,
                        "reason": "The description addresses unblended costs related to specific AWS security services but does not directly explain what 'UnblendedCost' means or how it relates to exceeding $1000. It lacks clarity on the measurement units, the exact metric being monitored, and the implications of a cost spike. While it provides some context about the services involved, it does not sufficiently enable a user to confidently answer whether the cost exceeds the threshold or understand the significance of such an event.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly.\",\n    \"Evaluate the description's Clarity by checking if it is concise and understandable within the scope of the monitoring question.\",\n    \"Assess Coverage by verifying that the description explains what is measured, the units used, relevant context, and possible implications related to the question.\",\n    \"Determine Utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; if not, identify gaps and suggest improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.3377540668798146"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to ensure the description addresses the question directly.",
                            "Evaluate the description's Clarity by checking if it is concise and understandable within the scope of the monitoring question.",
                            "Assess Coverage by verifying that the description explains what is measured, the units used, relevant context, and possible implications related to the question.",
                            "Determine Utility by judging if the description provides sufficient information for a user to confidently answer the monitoring question; if not, identify gaps and suggest improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOT\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in IoT Core services?\", \"retrieval_context\": [\" - This pattern captures IoT Core costs using unblended pricing. It identifies the actual costs charged for IoT device messaging, rules engine, and device management.\", \" - This pattern captures IoT Device costs using unblended pricing. It identifies the actual costs charged for IoT Device management and connectivity.\", \" - This pattern captures IoT Events costs using unblended pricing. It identifies the actual costs charged for IoT Events detection and response to IoT data.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6539118208525713,
                        "reason": "The description clearly addresses the monitoring question by focusing on IoT Core costs using unblended pricing, which aligns with the question about UnblendedCost exceeding $1000. It concisely references key elements such as IoT device messaging, rules engine, and device management. However, it lacks explicit measurement details like the $1000 threshold, units, or direct implications of a cost spike, limiting its utility for actionable insights.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly addresses the question being asked.\",\n    \"Evaluate Clarity by checking if the description is concise and easy to understand while directly referencing the key elements of the input question.\",\n    \"Assess Coverage by verifying the presence of measurement details, units, relevant context, and potential implications within the description in relation to the monitoring question.\",\n    \"Determine Utility by judging whether the description provides actionable information that directly supports answering the input question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.6539118208525713"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to assess if the description clearly addresses the question being asked.",
                            "Evaluate Clarity by checking if the description is concise and easy to understand while directly referencing the key elements of the input question.",
                            "Assess Coverage by verifying the presence of measurement details, units, relevant context, and potential implications within the description in relation to the monitoring question.",
                            "Determine Utility by judging whether the description provides actionable information that directly supports answering the input question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"DATASYNC\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $100,000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.29919765642997154,
                        "reason": "The retrieval context mentions unblended pricing costs for specific services but does not directly address the UnblendedCost exceeding $100,000 or indicate how to detect a cost spike. The description is somewhat clear about what costs are captured but lacks details on measurement units, thresholds, or implications needed to confidently answer the question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to ensure the description directly addresses the question.\",\n    \"Assess Clarity by verifying if the description is understandable, concise, and free of ambiguity relative to the question's requirements.\",\n    \"Evaluate Coverage by checking if the description details what is measured, units, relevant context, and implications that support answering the question.\",\n    \"Determine Utility by confirming whether the description provides sufficient information for a user to confidently answer the monitoring question based on the Input.\"\n] \n \nRubric:\nNone \n \nScore: 0.29919765642997154"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to ensure the description directly addresses the question.",
                            "Assess Clarity by verifying if the description is understandable, concise, and free of ambiguity relative to the question's requirements.",
                            "Evaluate Coverage by checking if the description details what is measured, units, relevant context, and implications that support answering the question.",
                            "Determine Utility by confirming whether the description provides sufficient information for a user to confidently answer the monitoring question based on the Input."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTGREENGUARD\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6544159861817644,
                        "reason": "The retrieval context directly addresses unblended costs for specific AWS security services, which relates to the question about UnblendedCost exceeding $1000. The description is clear and concise, explaining that it captures actual costs charged using unblended pricing. However, it lacks explicit mention of units (e.g., dollars) and does not specify the time frame or how the cost spike is determined, limiting full coverage and utility for confidently answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to ensure the description directly addresses the monitoring question.\",\n    \"Evaluate the description for clarity by checking if it is understandable and concise without ambiguity or unnecessary complexity.\",\n    \"Assess coverage by verifying if the description details what is measured, including units, context, and relevant implications to fully inform the question.\",\n    \"Determine utility by judging whether the description provides sufficient information to confidently answer the monitoring question based on the given context.\"\n] \n \nRubric:\nNone \n \nScore: 0.6544159861817644"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to ensure the description directly addresses the monitoring question.",
                            "Evaluate the description for clarity by checking if it is understandable and concise without ambiguity or unnecessary complexity.",
                            "Assess coverage by verifying if the description details what is measured, including units, context, and relevant implications to fully inform the question.",
                            "Determine utility by judging whether the description provides sufficient information to confidently answer the monitoring question based on the given context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTWIRELESS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in IoT Wireless services?\", \"retrieval_context\": [\" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\", \" - This pattern captures IoT Device costs using unblended pricing. It identifies the actual costs charged for IoT Device management and connectivity.\", \" - This pattern captures IoT Wireless costs using unblended pricing. It identifies the actual costs charged for IoT Wireless LoRaWAN network and device management.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6160119325878105,
                        "reason": "The description addresses IoT Wireless costs using unblended pricing, which aligns with the question about UnblendedCost for IoT Wireless services. However, it lacks explicit mention of cost thresholds or how to detect if costs exceed $1000, limiting clarity and utility for identifying a cost spike. The description explains what is measured and the context but does not specify units or implications related to the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (question) with the Retrieval Context (description) to check if the description directly addresses the monitoring question.\",\n    \"Evaluate the clarity of the description by assessing if it is understandable and concise when considered alongside the specific question asked.\",\n    \"Assess coverage by verifying that the description explains what is measured, its units, the context of measurement, and any implications relevant to the question.\",\n    \"Determine utility by judging whether the description provides sufficient information to confidently answer the monitoring question; if not, identify gaps relative to the question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6160119325878105"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (question) with the Retrieval Context (description) to check if the description directly addresses the monitoring question.",
                            "Evaluate the clarity of the description by assessing if it is understandable and concise when considered alongside the specific question asked.",
                            "Assess coverage by verifying that the description explains what is measured, its units, the context of measurement, and any implications relevant to the question.",
                            "Determine utility by judging whether the description provides sufficient information to confidently answer the monitoring question; if not, identify gaps relative to the question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTBUTTON\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this UnblendedCost exceed $100, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.26791787056691696,
                        "reason": "The retrieval context describes unblended costs for multiple services but does not specifically address whether the UnblendedCost metric can be compared to a $100 threshold or indicate a cost spike. It is somewhat clear in explaining what costs are captured but lacks details on units, thresholds, or implications relevant to detecting cost spikes, limiting its utility for confidently answering the input question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description specifically addresses the question\u2019s topic and intent.\",\n    \"Evaluate the Clarity of the Retrieval Context by checking if it is concise and understandable without ambiguity, ensuring the user can easily interpret the metric.\",\n    \"Check Coverage by verifying that the Retrieval Context fully explains what the metric measures, including units, operational context, and any implications relevant to the Input question.\",\n    \"Assess Utility by determining if the information in the Retrieval Context sufficiently empowers the user to confidently answer the Input question or make informed decisions based on it.\"\n] \n \nRubric:\nNone \n \nScore: 0.26791787056691696"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description specifically addresses the question\u2019s topic and intent.",
                            "Evaluate the Clarity of the Retrieval Context by checking if it is concise and understandable without ambiguity, ensuring the user can easily interpret the metric.",
                            "Check Coverage by verifying that the Retrieval Context fully explains what the metric measures, including units, operational context, and any implications relevant to the Input question.",
                            "Assess Utility by determining if the information in the Retrieval Context sufficiently empowers the user to confidently answer the Input question or make informed decisions based on it."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTTWINMAKER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3763701637060652,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not explicitly address whether UnblendedCost exceeding $1000 indicates a cost spike. The description clarifies what is measured (actual unblended costs) and the context (security monitoring services), but it lacks information on measurement units, thresholds, or implications related to cost spikes, limiting its utility in fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the user's query.\",\n    \"Evaluate the clarity of the description by checking if the metric's purpose and terminology are understandable and concise within the retrieval context.\",\n    \"Check coverage by verifying if the description explains what is measured, the measurement units, relevant context, and potential implications related to the input question.\",\n    \"Assess the utility by determining whether the description provides sufficient information to effectively answer the monitoring question, ensuring alignment between input and context.\"\n] \n \nRubric:\nNone \n \nScore: 0.3763701637060652"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to assess if the description clearly addresses the user's query.",
                            "Evaluate the clarity of the description by checking if the metric's purpose and terminology are understandable and concise within the retrieval context.",
                            "Check coverage by verifying if the description explains what is measured, the measurement units, relevant context, and potential implications related to the input question.",
                            "Assess the utility by determining whether the description provides sufficient information to effectively answer the monitoring question, ensuring alignment between input and context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTDEVICE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does this spike in UnblendedCost exceed $1000, indicating a potential cost overrun?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6244692252490882,
                        "reason": "The description clearly states the metric (UnblendedCost spike exceeding $1000) and the monitoring question about potential cost overruns, aligning with the retrieval context's focus on unblended pricing for specific security services. However, it lacks explicit mention of which service's cost is being monitored (Inspector, GuardDuty, or IoT Device Defender), reducing clarity and coverage. Including the specific service and units consistently would improve utility and alignment with the retrieval context.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input metric description and retrieval context for clarity: ensure the description is understandable and concise while aligning with the terminology and scope of the retrieval context.\",\n    \"Evaluate coverage by verifying that the description sufficiently explains what is measured, including units, relevant context, and implications, relative to the retrieval context requirements.\",\n    \"Assess utility by determining whether the description enables answering the provided monitoring question effectively, checking for completeness and relevance in both input and retrieval context.\",\n    \"Identify gaps between the input and retrieval context that reduce clarity, coverage, or utility, and suggest improvements to better align description details to the monitoring question.\"\n] \n \nRubric:\nNone \n \nScore: 0.6244692252490882"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input metric description and retrieval context for clarity: ensure the description is understandable and concise while aligning with the terminology and scope of the retrieval context.",
                            "Evaluate coverage by verifying that the description sufficiently explains what is measured, including units, relevant context, and implications, relative to the retrieval context requirements.",
                            "Assess utility by determining whether the description enables answering the provided monitoring question effectively, checking for completeness and relevance in both input and retrieval context.",
                            "Identify gaps between the input and retrieval context that reduce clarity, coverage, or utility, and suggest improvements to better align description details to the monitoring question."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTSITEWISE\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3689422718248764,
                        "reason": "The retrieval context uses terminology related to unblended costs for specific AWS security services, which partially aligns with the input question about UnblendedCost exceeding $1000. However, it lacks clarity and direct explanation about what UnblendedCost measures overall, the units, or how to interpret a cost spike. The descriptions focus on specific services rather than the general cost metric, limiting utility in answering the question about a potential cost spike. Including a clear definition of UnblendedCost, its units, and threshold implications would improve coverage and utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare Input (monitoring question) and Retrieval Context (metric description) for alignment in terminology and scope to ensure relevance.\",\n    \"Evaluate clarity by checking if the metric description is understandable and concise in relation to the question's intent.\",\n    \"Assess coverage by verifying if the description explains what is measured, units, context, and implications that help address the question.\",\n    \"Judge utility by determining if the description provides sufficient information to directly answer the monitoring question; if not, suggest specific improvements.\"\n] \n \nRubric:\nNone \n \nScore: 0.3689422718248764"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare Input (monitoring question) and Retrieval Context (metric description) for alignment in terminology and scope to ensure relevance.",
                            "Evaluate clarity by checking if the metric description is understandable and concise in relation to the question's intent.",
                            "Assess coverage by verifying if the description explains what is measured, units, context, and implications that help address the question.",
                            "Judge utility by determining if the description provides sufficient information to directly answer the monitoring question; if not, suggest specific improvements."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTDEVICEMANAGEMENT\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike in IoT Device Management?\", \"retrieval_context\": [\" - This pattern captures IoT Core costs using unblended pricing. It identifies the actual costs charged for IoT device messaging, rules engine, and device management.\", \" - This pattern captures IoT Device Management costs using unblended pricing. It identifies the actual costs charged for IoT Device Management fleet operations and jobs.\", \" - This pattern captures IoT Device costs using unblended pricing. It identifies the actual costs charged for IoT Device management and connectivity.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.6898667183792657,
                        "reason": "The input is clear and concise, specifying the metric (UnblendedCost) and the threshold ($1000) indicating a potential cost spike in IoT Device Management. It aligns well with the retrieval context, which details unblended costs related to IoT Device Management and related services. However, the input lacks explicit units or further context about the time frame or scope of the cost measurement, which limits coverage and utility for fully answering the monitoring question.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Evaluate Clarity by assessing if the Input (metric description) is presented in a clear, concise, and understandable manner, ensuring it is easy to comprehend without ambiguity.\",\n    \"Evaluate Coverage by checking if the Input provides sufficient details about what is measured, including units, relevant context, and implications related to the Retrieval Context (monitoring question).\",\n    \"Evaluate Utility by determining if the Input effectively supports answering the Retrieval Context, ensuring the description helps in directly addressing the monitoring question.\",\n    \"Compare Input and Retrieval Context to verify alignment: ensure the description covers the aspects required by the question and that the question can be satisfactorily answered using the description provided.\"\n] \n \nRubric:\nNone \n \nScore: 0.6898667183792657"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Evaluate Clarity by assessing if the Input (metric description) is presented in a clear, concise, and understandable manner, ensuring it is easy to comprehend without ambiguity.",
                            "Evaluate Coverage by checking if the Input provides sufficient details about what is measured, including units, relevant context, and implications related to the Retrieval Context (monitoring question).",
                            "Evaluate Utility by determining if the Input effectively supports answering the Retrieval Context, ensuring the description helps in directly addressing the monitoring question.",
                            "Compare Input and Retrieval Context to verify alignment: ensure the description covers the aspects required by the question and that the question can be satisfactorily answered using the description provided."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOT1CLICK\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $100,000, indicating a potential cost spike in IoT 1-Click services?\", \"retrieval_context\": [\" - This pattern captures IoT 1-Click costs using unblended pricing. It identifies the actual costs charged for IoT 1-Click device management and simple applications.\", \" - This pattern captures IoT Button costs using unblended pricing. It identifies the actual costs charged for IoT Button device usage and simple applications.\", \" - This pattern captures IoT Device costs using unblended pricing. It identifies the actual costs charged for IoT Device management and connectivity.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.38028955384753627,
                        "reason": "The retrieval context relates to the input question by describing costs for IoT 1-Click and related services using unblended pricing, which aligns with the question about UnblendedCost. However, the context lacks clarity and conciseness as it repeats similar phrases without explicitly explaining what is measured, the units, or the threshold of $100,000. It does not address the implication of a cost spike or provide sufficient detail to determine if costs exceed the specified amount, limiting its utility for answering the question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the input question with the retrieval context description to assess if the description clearly relates to and addresses the question.\",\n    \"Evaluate the clarity and conciseness of the retrieval context to ensure it is easily understandable in relation to the input question.\",\n    \"Check coverage by verifying whether the description explains what is measured, the units, context, and implications relevant to the input question.\",\n    \"Assess utility by determining if the description can effectively help a user answer the provided question, based on the alignment and content of both input and retrieval context.\"\n] \n \nRubric:\nNone \n \nScore: 0.38028955384753627"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the input question with the retrieval context description to assess if the description clearly relates to and addresses the question.",
                            "Evaluate the clarity and conciseness of the retrieval context to ensure it is easily understandable in relation to the input question.",
                            "Check coverage by verifying whether the description explains what is measured, the units, context, and implications relevant to the input question.",
                            "Assess utility by determining if the description can effectively help a user answer the provided question, based on the alignment and content of both input and retrieval context."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTTHINGSGRAPH\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.3407474781816817,
                        "reason": "The retrieval context describes unblended costs for specific AWS security services but does not directly address the monitoring question about whether UnblendedCost exceeds $1000 or indicate a potential cost spike. The description lacks clarity on measurement units, threshold context, and implications for cost spikes, limiting its utility for answering the question effectively.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description addresses the exact question posed.\",\n    \"Evaluate Clarity by checking if the description is concise and comprehensible in relation to the technical concepts implied by the question.\",\n    \"Assess Coverage by verifying that the description includes what is measured, units, context, and implications relevant to the question.\",\n    \"Determine Utility by confirming the description provides actionable information that enables answering the monitoring question effectively.\"\n] \n \nRubric:\nNone \n \nScore: 0.3407474781816817"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and Retrieval Context (metric description) to ensure the description addresses the exact question posed.",
                            "Evaluate Clarity by checking if the description is concise and comprehensible in relation to the technical concepts implied by the question.",
                            "Assess Coverage by verifying that the description includes what is measured, units, context, and implications relevant to the question.",
                            "Determine Utility by confirming the description provides actionable information that enables answering the monitoring question effectively."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTEVENTS\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does UnblendedCost exceed $100,000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures GuardDuty threat detection costs using unblended pricing. It identifies the actual costs charged for GuardDuty monitoring and findings.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.2,
                        "reason": "The Retrieval Context describes costs related to specific security services using unblended pricing but does not directly address the question about whether UnblendedCost exceeds $100,000 or indicate how to detect a cost spike. The description is somewhat clear about what costs are captured but lacks details on measurement units, thresholds, or implications relevant to the input question, limiting its utility in answering the monitoring query.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description directly addresses the question posed.\",\n    \"Evaluate the clarity of the Retrieval Context by determining if the metric description is written in a clear and concise manner that is easily understood relative to the question.\",\n    \"Assess the coverage of the Retrieval Context by verifying it includes details on what the metric measures, units, contextual relevance, and implications in relation to the Input question.\",\n    \"Judge the utility of the Retrieval Context by confirming if the information provided helps the user effectively answer the Input question, and note suggestions if it falls short.\"\n] \n \nRubric:\nNone \n \nScore: 0.2"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) with the Retrieval Context (metric description) to check if the description directly addresses the question posed.",
                            "Evaluate the clarity of the Retrieval Context by determining if the metric description is written in a clear and concise manner that is easily understood relative to the question.",
                            "Assess the coverage of the Retrieval Context by verifying it includes details on what the metric measures, units, contextual relevance, and implications in relation to the Input question.",
                            "Judge the utility of the Retrieval Context by confirming if the information provided helps the user effectively answer the Input question, and note suggestions if it falls short."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        },
        "{\"actual_output\": \"\", \"context\": null, \"expected_output\": \"{\\\"Service\\\": \\\"IOTDEVICEDEFENDER\\\", \\\"MetricName\\\": \\\"UnblendedCost\\\"}\", \"hyperparameters\": null, \"input\": \"Does the UnblendedCost exceed $1000, indicating a potential cost spike?\", \"retrieval_context\": [\" - This pattern captures CloudWatch monitoring costs using unblended pricing. It identifies the actual costs charged for CloudWatch metrics, logs, and alarms.\", \" - This pattern captures Inspector security assessment costs using unblended pricing. It identifies the actual costs charged for Inspector assessments and findings.\", \" - This pattern captures IoT Device Defender costs using unblended pricing. It identifies the actual costs charged for IoT Device Defender security monitoring and audits.\"]}": {
            "cached_metrics_data": [
                {
                    "metric_data": {
                        "name": "Metric Description Quality [GEval]",
                        "threshold": 0.8,
                        "success": false,
                        "score": 0.30364579313774354,
                        "reason": "The retrieval context mentions unblended costs for specific AWS services but does not clearly define 'UnblendedCost' as a whole or explain what it measures, its units, or how to interpret values exceeding $1000. The description is somewhat clear about capturing actual costs but lacks direct relevance to the input question about cost spikes and threshold evaluation, limiting its utility.",
                        "strictMode": false,
                        "evaluationModel": "gpt-4.1-mini",
                        "evaluationCost": 0,
                        "verboseLogs": "Criteria:\n\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n \n \nEvaluation Steps:\n[\n    \"Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question.\",\n    \"Evaluate the clarity of the description ensuring it is understandable, concise, and free of ambiguity in relation to the terminology used in the input question.\",\n    \"Assess the coverage of the description to confirm it explains what is measured, including units, context, and implications, all relevant to the input question.\",\n    \"Determine utility by verifying if the description provides sufficient information to effectively answer the input question; if not, suggest improvements targeting missing details or clarity.\"\n] \n \nRubric:\nNone \n \nScore: 0.30364579313774354"
                    },
                    "metric_configuration": {
                        "threshold": 0.8,
                        "evaluation_model": "gpt-4.1-mini",
                        "strict_mode": false,
                        "criteria": "\nYou are an expert in cloud telemetry and documentation.\nGiven a metric description and a related monitoring question, evaluate the quality of the description on a scale from 1 (poor) to 10 (excellent) based on:\n- Clarity: Is the description understandable and concise?\n- Coverage: Does it explain what is measured, units, context, and implications?\n- Utility: Can this description help a user answer the provided question?\nIf the score is less than 8, provide a concrete suggestion for improvement.\n\nIMPORTANT RULES:\n- Respond ONLY with a JSON object.\n- Do NOT include any text before or after the JSON.\n- If unsure, output a JSON with {\"score\":1,\"reason\":\"Invalid description\",\"suggestion\":\"Review input\"}.\n\nIMPORTANT: Your entire response MUST be valid JSON, and nothing else. Do not include code blocks, explanations, or any extra text. Only output this object:\n{\n  \"score\": <integer from 1 to 10>,\n  \"reason\": \"<brief rationale>\",\n  \"suggestion\": \"<concrete suggestion, or null if score >= 8>\"\n}\n\nDescription:\n{context}\nQuestion:\n{input}\n",
                        "include_reason": false,
                        "evaluation_steps": [
                            "Compare the Input (monitoring question) and the Retrieval Context (metric description) to check if the description clearly addresses the question.",
                            "Evaluate the clarity of the description ensuring it is understandable, concise, and free of ambiguity in relation to the terminology used in the input question.",
                            "Assess the coverage of the description to confirm it explains what is measured, including units, context, and implications, all relevant to the input question.",
                            "Determine utility by verifying if the description provides sufficient information to effectively answer the input question; if not, suggest improvements targeting missing details or clarity."
                        ],
                        "evaluation_params": [
                            "input",
                            "retrieval_context"
                        ]
                    }
                }
            ]
        }
    }
}
