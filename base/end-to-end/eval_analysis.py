"""
Analysis Quality Evaluator for End-to-End Evaluation Pipeline

This module evaluates the quality of monitoring analysis output using DeepEval
framework with comprehensive rubrics. It assesses analysis accuracy, relevance,
completeness, and actionability across multiple evaluation modes.

Key Features:
    - Multi-mode evaluation (executive_summary, key_findings, full analysis)
    - Data accuracy assessment with no-data detection
    - Relevance scoring for monitoring questions
    - Completeness evaluation with actionability requirements
    - DeepEval integration with GPT models

Evaluation Criteria:
    1. Data Accuracy (0-4): Time range fidelity, metric semantics, no fabricated data
    2. Relevance to Question (0-3): Direct answers, clear conclusions
    3. Completeness (0-3): Coverage of requested aspects, actionable recommendations
    4. Bonus (0-1): Multi-source correlation when appropriate

Dependencies:
    - deepeval: LLM evaluation framework
    - json: JSON parsing and generation
    - argparse: Command-line argument parsing

Usage:
    python3 eval_analysis.py --input test_results.json --actual-mode executive_summary

Author: Autoptic Team
"""

import json
import argparse
from typing import Any, Dict, List, Optional

from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval import evaluate
from deepeval.models import GPTModel


# ---------------------------------------------------------------------------
# Analysis Evaluation Rubric - Comprehensive monitoring analysis quality assessment
# ---------------------------------------------------------------------------
ANALYSIS_RUBRIC = """
ROLE: You are a senior cloud telemetry analyst. Judge ACTUAL_OUTPUT (the analysis) for INPUT (the monitoring prompt).

EVALUATION ORDER (follow in sequence):
1) NO-DATA CHECK (hard rule)
   - If ACTUAL_OUTPUT indicates insufficient data or empty analysis
     (e.g., "no time series", "no aggregates", "no datapoints", "empty result", "no data available"),
     OR the text is effectively a placeholder,
     THEN return exactly:
       {"score": 1, "reason": "no data", "suggestion": "Verify data exists for the stated time range, dimensions, and source; fix query/filters; re-run."}

2) DATA ACCURACY (0-4 points)
   - Time range fidelity; metric semantics (rate vs count); correct namespaces/sources.
   - No fabricated numbers or unsupported claims.

3) RELEVANCE TO QUESTION (0-3 points)
   - Directly answers INPUT; focuses on the asked signals/scope; clear conclusion tied to the prompt.

4) COMPLETENESS (0-3 points)
   - Covers requested aspects (anomalies/breakdowns/trends/alerts).
   - At least one actionable recommendation tied to findings.
   - States assumptions/limitations when relevant.

5) BONUS (0-1; cap total at 10)
   - If multi-source correlation is implied and done well, +1 (do not exceed 10).

SCORING:
- Start at 0; add sections 2-4; add bonus in 5; cap at 10.
- Strong analyses (>=8) are accurate, on-point, complete, and actionable.
- Generic boilerplate not grounded in INPUT should score <=3.

OUTPUT FORMAT (STRICT):
Return only one JSON object:
{"score": <integer 1..10>, "reason": "<brief>", "suggestion": "<specific improvement, or null if score >= 8>"}
If unsure or invalid, return:
{"score": 1, "reason": "Invalid response", "suggestion": "Review input"}
"""


# ---------------------------------------------------------------------------
# JSON loading
# ---------------------------------------------------------------------------
def load_run_file(file_path: str) -> Dict[str, Any]:
    """
    Load structured test results from JSON file.
    
    Loads the test results file generated by consume_logs.py containing
    parsed WebSocket interactions and final analysis results.

    Args:
        file_path (str): Path to the structured test results JSON file
        
    Returns:
        Dict[str, Any]: Complete test run data including test cases and analysis results
        
    Expected Structure:
        {
            "run_info": {...},
            "test_cases": [{
                "name": "...",
                "final_result": {
                    "executive_summary": "...",
                    "key_findings": "...",
                    "recommendations": "..."
                }
            }],
            "summary": {...}
        }
        
    Raises:
        FileNotFoundError: If the input file doesn't exist
        json.JSONDecodeError: If the file contains invalid JSON
    """
    with open(file_path, "r", encoding="utf-8") as file_handle:
        run_payload: Dict[str, Any] = json.load(file_handle)

    return run_payload


# ---------------------------------------------------------------------------
# Build ACTUAL_OUTPUT for the analysis judge
# ---------------------------------------------------------------------------
def build_analysis_actual_output(final_result: Dict[str, Any], mode: str) -> str:
    """
    Extract analysis text for evaluation based on specified mode.
    
    Processes the final_result object from test case execution to extract
    the specific analysis content for quality evaluation. Supports multiple
    evaluation modes for focused assessment of different analysis aspects.

    Args:
        final_result (Dict[str, Any]): Analysis results from successful test case execution
        mode (str): Evaluation mode - "executive_summary", "key_findings", or "full"
        
    Returns:
        str: Analysis text to be evaluated, or placeholder if content missing
        
    Evaluation Modes:
        - executive_summary: Evaluates only the executive summary section
        - key_findings: Evaluates only the key findings section  
        - full: Evaluates concatenated analysis (all sections combined)
        
    Full Mode Section Order:
        1. executive_summary: High-level overview
        2. key_findings: Primary insights
        3. trends_anomalies: Pattern analysis
        4. recommendations: Actionable suggestions
        5. data_quality: Data assessment
        6. alerts: Alert conditions
        7. context: Background information
        8. key_metrics: Important metrics
        
    Fallback Behavior:
        Returns descriptive placeholder text if requested content is missing
        or if final_result structure is invalid.
    """
    # Validate input structure
    if not isinstance(final_result, dict):
        return "(no final_result)"

    # Handle single-section evaluation modes
    if mode == "executive_summary":
        executive_summary: Optional[str] = final_result.get("executive_summary")
        if isinstance(executive_summary, str) and len(executive_summary) > 0:
            return executive_summary
        return "(no executive_summary)"

    if mode == "key_findings":
        key_findings: Optional[str] = final_result.get("key_findings")
        if isinstance(key_findings, str) and len(key_findings) > 0:
            return key_findings
        return "(no key_findings)"

    # Handle full analysis mode - concatenate all available sections
    concatenated_parts: List[str] = []

    # Define logical order for analysis sections
    ordered_keys: List[str] = [
        "executive_summary",  # High-level overview first
        "key_findings",       # Primary insights
        "trends_anomalies",   # Pattern analysis
        "recommendations",    # Actionable suggestions
        "data_quality",       # Data assessment
        "alerts",             # Alert conditions
        "context",            # Background information
        "key_metrics",        # Important metrics
    ]

    # Build concatenated analysis from available sections
    for field_name in ordered_keys:
        field_value: Any = final_result.get(field_name)
        if isinstance(field_value, str) and len(field_value) > 0:
            labeled_segment: str = f"{field_name}: {field_value}"
            concatenated_parts.append(labeled_segment)

    # Return concatenated analysis or placeholder if nothing available
    if len(concatenated_parts) == 0:
        return "(no final_result)"

    full_text: str = "\n\n".join(concatenated_parts)
    return full_text


# ---------------------------------------------------------------------------
# Optional retrieval context
# ---------------------------------------------------------------------------
def build_retrieval_context(final_result: Dict[str, Any]) -> List[str]:
    """
    Build retrieval context from analysis results for contextual evaluation.
    
    Extracts specific analysis sections that provide context for evaluation.
    While not required for the current GEval rubric, this context can be
    useful for other evaluation metrics that require background information.

    Args:
        final_result (Dict[str, Any]): Analysis results from test case execution
        
    Returns:
        List[str]: List of context strings from analysis sections
        
    Context Sections:
        - key_metrics: Important metric values and thresholds
        - recommendations: Actionable suggestions for improvement
        - context: Background information and assumptions
        - alerts: Alert conditions and thresholds
        
    Note:
        This function is reserved for future evaluation metrics that may
        require contextual information beyond the main analysis text.
    """
    retrieval_context: List[str] = []

    if not isinstance(final_result, dict):
        return retrieval_context

    # Select fields that provide evaluation context
    candidate_keys: List[str] = ["key_metrics", "recommendations", "context", "alerts"]

    # Extract non-empty string values as context
    for field_name in candidate_keys:
        field_value: Any = final_result.get(field_name)
        if isinstance(field_value, str) and len(field_value) > 0:
            retrieval_context.append(field_value)

    return retrieval_context


# ---------------------------------------------------------------------------
# Test case construction
# ---------------------------------------------------------------------------
def build_deepeval_test_cases(
    test_cases_json: List[Dict[str, Any]],
    actual_mode: str,
) -> List[LLMTestCase]:
    """
    Convert test case results into DeepEval LLMTestCase objects for analysis evaluation.
    
    Transforms structured test results into the format required by DeepEval
    framework for analysis quality assessment. Each test case becomes an evaluation
    where the INPUT is the original monitoring question and ACTUAL_OUTPUT is the
    generated analysis (in the specified mode).
    
    Args:
        test_cases_json (List[Dict[str, Any]]): List of test case results from structured JSON
        actual_mode (str): Analysis evaluation mode ("executive_summary", "key_findings", "full")
        
    Returns:
        List[LLMTestCase]: DeepEval test case objects ready for analysis evaluation
        
    DeepEval Structure:
        - INPUT: Original natural language monitoring question
        - ACTUAL_OUTPUT: Generated analysis text (mode-specific)
        - EXPECTED_OUTPUT: None (unsupervised evaluation based on rubric)
        - RETRIEVAL_CONTEXT: Supporting analysis sections for context
        - METADATA: Session info, timing, execution status
        
    Tags Applied:
        - "analysis": Indicates this is analysis quality evaluation
        - {actual_mode}: Specific evaluation mode for filtering
        
    Metadata Preserved:
        - session_id: Unique identifier for tracing
        - when: Time range parameter from original request
        - status: Test execution status (success/error)
        - duration: Execution time in seconds
    """
    deepeval_test_cases: List[LLMTestCase] = []

    for test_case_json in test_cases_json:
        # Extract original monitoring question for evaluation INPUT
        request_object: Dict[str, Any] = test_case_json.get("request") or {}

        monitoring_prompt: str = ""
        prompt_raw: Any = request_object.get("prompt")
        if isinstance(prompt_raw, str):
            monitoring_prompt = prompt_raw

        # Get analysis results for evaluation
        final_result: Dict[str, Any] = test_case_json.get("final_result") or {}

        # Extract analysis text based on evaluation mode
        actual_output_text: str = build_analysis_actual_output(
            final_result=final_result,
            mode=actual_mode,
        )

        # Build context for potential future evaluation metrics
        retrieval_context: List[str] = build_retrieval_context(
            final_result=final_result
        )

        # Get test case identification
        test_case_name: Optional[str] = test_case_json.get("name")

        # Apply tags for filtering and categorization
        tags: List[str] = []
        tags.append("analysis")     # Analysis quality evaluation
        tags.append(actual_mode)    # Specific evaluation mode

        # Preserve metadata for analysis and debugging
        additional_metadata: Dict[str, Any] = {}
        additional_metadata["session_id"] = test_case_json.get("session_id")
        additional_metadata["when"] = request_object.get("when")
        additional_metadata["status"] = test_case_json.get("status")
        additional_metadata["duration"] = test_case_json.get("duration")

        # Create DeepEval test case for analysis evaluation
        deepeval_case: LLMTestCase = LLMTestCase(
            input=monitoring_prompt,        # Natural language monitoring question
            actual_output=actual_output_text, # Analysis text to evaluate
            expected_output=None,            # Unsupervised evaluation
            retrieval_context=retrieval_context if len(retrieval_context) > 0 else None,
            name=test_case_name,
            tags=tags,
            additional_metadata=additional_metadata,
        )

        deepeval_test_cases.append(deepeval_case)

    return deepeval_test_cases


# ---------------------------------------------------------------------------
# GEval metric factory
# ---------------------------------------------------------------------------
def create_analysis_quality_metric(model_name: str) -> GEval:
    """
    Create configured GEval metric for analysis quality assessment.
    
    Sets up the DeepEval GEval metric with the comprehensive analysis rubric
    and specified LLM model. Configures evaluation parameters and thresholds
    for consistent analysis quality assessment.
    
    Args:
        model_name (str): GPT model identifier (e.g., "gpt-4.1-mini", "gpt-3.5-turbo")
        
    Returns:
        GEval: Configured evaluation metric ready for analysis assessment
        
    Configuration:
        - Rubric: Multi-criteria analysis quality assessment (data accuracy, relevance, completeness)
        - Model: GPT-based with temperature=0 for consistent scoring
        - Threshold: 0.8 (8/10) for pass/fail determination
        - Verbose: Enabled for detailed feedback and reasoning
        
    Evaluation Parameters:
        - INPUT: Original natural language monitoring question
        - ACTUAL_OUTPUT: Generated analysis text to assess
        
    Note:
        The rubric includes hard no-data validation and evaluates 3 key criteria
        plus bonus points, totaling up to 10 points for comprehensive assessment.
    """
    # Configure GPT model with deterministic temperature
    base_model: GPTModel = GPTModel(model=model_name, temperature=0)

    # Create GEval metric with analysis-specific rubric
    analysis_quality_metric: GEval = GEval(
        name="Analysis Quality",
        criteria=ANALYSIS_RUBRIC,  # Comprehensive analysis quality rubric
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        model=base_model,
        threshold=0.8,             # 8/10 score threshold for pass/fail
        verbose_mode=True,         # Enable detailed feedback
    )

    return analysis_quality_metric


# ---------------------------------------------------------------------------
# Entrypoint
# ---------------------------------------------------------------------------
def main() -> None:
    """
    Main entry point for analysis quality evaluation.
    
    Orchestrates the complete analysis evaluation process:
    1. Load structured test results from JSON file
    2. Extract analysis data based on evaluation mode
    3. Build DeepEval test cases for assessment
    4. Configure evaluation metric with specified model
    5. Execute comprehensive analysis quality assessment
    6. Display results with scores and feedback
    
    Command-line Arguments:
        --input: Path to structured test results JSON file (required)
        --actual-mode: Analysis section to evaluate (executive_summary, key_findings, full)
        --model: GPT model for evaluation (default: gpt-4.1-mini)
        
    Exit Conditions:
        - SystemExit if no test cases found in input
        - Normal completion with evaluation results displayed
        
    Output:
        - Individual test case scores and feedback
        - Overall evaluation summary
        - Pass/fail status based on threshold
        - Mode-specific analysis quality assessment
    """
    # Configure command-line argument parsing
    argument_parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Evaluate monitoring analysis quality using DeepEval framework"
    )
    argument_parser.add_argument(
        "--input",
        required=True,
        help="Path to structured test results JSON file containing test_cases",
    )
    argument_parser.add_argument(
        "--actual_mode",
        choices=["executive_summary", "key_findings", "full"],
        default="executive_summary",
        help="Analysis section to evaluate as ACTUAL_OUTPUT",
    )
    argument_parser.add_argument(
        "--model",
        default="gpt-4.1-mini",
        help="GPT model for evaluation (default: gpt-4.1-mini)",
    )

    args = argument_parser.parse_args()

    # Load test results from structured JSON file
    run_payload: Dict[str, Any] = load_run_file(args.input)

    # Extract test cases array from results
    test_cases_json: List[Dict[str, Any]] = run_payload.get("test_cases") or []
    if len(test_cases_json) == 0:
        raise SystemExit("No test_cases found in the input JSON.")

    # Convert to DeepEval format with specified evaluation mode
    deepeval_test_cases: List[LLMTestCase] = build_deepeval_test_cases(
        test_cases_json=test_cases_json,
        actual_mode=args.actual_mode,
    )

    # Create configured analysis quality metric
    analysis_quality_metric: GEval = create_analysis_quality_metric(args.model)

    # Execute analysis quality evaluation
    evaluate(
        test_cases=deepeval_test_cases,
        metrics=[analysis_quality_metric],
    )


if __name__ == "__main__":
    # Execute main function when run as script
    main()
