"""
PQL Quality Evaluator for End-to-End Evaluation Pipeline

This module evaluates the quality of generated PQL (Query Language) using DeepEval
framework with comprehensive rubrics. It assesses PQL structure, correctness,
and semantic quality using GPT-based evaluation models.

Key Features:
    - Automated PQL detection and validation
    - Multi-criteria evaluation (time range, metrics, dimensions, structure, focus)
    - DeepEval integration with GPT models
    - Detailed feedback and improvement suggestions
    - Configurable evaluation models and thresholds

Evaluation Criteria:
    1. Time Range Fidelity (0-2): Correct timeframe interpretation
    2. Metric & Source Correctness (0-3): Appropriate namespace/metric selection  
    3. Dimensions & Grouping (0-2): Proper dimension usage and filtering
    4. Structure & Syntax Quality (0-2): PQL structural coherence
    5. Focus & Parsimony (0-1): Minimal yet sufficient PQL generation

Dependencies:
    - deepeval: LLM evaluation framework
    - json: JSON parsing and generation
    - argparse: Command-line argument parsing

Usage:
    python3 eval_pql.py --input test_results.json --model gpt-4.1-mini

Author: Autoptic Team
"""

import json
import argparse
from typing import Any, Dict, List, Optional

from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval import evaluate
from deepeval.models import GPTModel


# ---------------------------------------------------------------------------
# PQL Evaluation Rubric - Comprehensive quality assessment criteria
# ---------------------------------------------------------------------------
PQL_RUBRIC = """
ROLE: You are a senior reviewer for Autoptic's PQL DSL. Judge ACTUAL_OUTPUT (the generated PQL) for INPUT (the user prompt).

HARD VALIDITY CHECK:
- If ACTUAL_OUTPUT is empty OR does not look like PQL (e.g., missing core primitives such as where(…), when(…), what(…), request(…)),
  return exactly:
  {"score": 1, "reason": "invalid pql", "suggestion": "Include where(), when(), what(), and request() with correct syntax."}

SCORING (0-10):
Score by adding the following criteria. Cap the total at 10.

1) TIME RANGE FIDELITY (0-2)
   - Correctly interprets and applies the timeframe implied by INPUT (e.g., "last 24h" → when(24h)).
   - 2 = exact and explicit; 1 = present but ambiguous/mismatched; 0 = missing or wrong.

2) METRIC & SOURCE CORRECTNESS (0-3)
   - Chooses the right namespace(s)/metric(s) for the question (e.g., CloudWatch API Gateway 4XXError vs CloudFront 4xxErrorRate).
   - Avoids unrelated metrics unless clearly justified by the question.
   - 3 = all correct; 2 = mostly correct with minor drift; 1 = partially correct; 0 = wrong source/metric.

3) DIMENSIONS & GROUPING (0-2)
   - Uses appropriate dimensions/filters to answer the question (e.g., ApiName/Stage for API Gateway; DistributionId/Region for CloudFront; LoadBalancer/TargetGroup for ALB).
   - Wildcards (*) are acceptable if the prompt is broad; include grouping-ready dimensions when the prompt implies breakdowns.
   - 2 = appropriate and sufficient; 1 = present but incomplete/excessive; 0 = missing/misused.

4) STRUCTURE & SYNTAX QUALITY (0-2)
   - PQL is structurally coherent: where()->when()->what()->request()->as($var); indexes $where[i]/$what[j]/$when[k] are consistent; variables are unique/referenced correctly.
   - Charts/aggregations (e.g., average(), chart()) are consistent with the metric semantics (rate vs count).
   - 2 = clean and executable; 1 = minor issues; 0 = clearly broken.

5) FOCUS & PARSIMONY (0-1)
   - The PQL is minimal-yet-sufficient to answer INPUT (no unnecessary metrics/noise).
   - 1 = focused; 0 = unfocused or bloated.

OUTPUT FORMAT (STRICT):
Return ONLY one JSON object:
{"score": <integer 1..10>, "reason": "<brief rationale>", "suggestion": "<specific fix, or null if score >= 8>"}
If unsure, return:
{"score": 1, "reason": "Invalid response", "suggestion": "Review input"}
"""


# ---------------------------------------------------------------------------
# JSON loading
# ---------------------------------------------------------------------------
def load_run_file(file_path: str) -> Dict[str, Any]:
    """
    Load structured test results from JSON file.
    
    Loads the test results file generated by consume_logs.py containing
    parsed WebSocket interactions and analysis results.

    Args:
        file_path (str): Path to the structured test results JSON file
        
    Returns:
        Dict[str, Any]: Complete test run data including test cases and metadata
        
    Expected Structure:
        {
            "run_info": {...},
            "test_cases": [{"name": ..., "responses": [...], "final_result": {...}}, ...],
            "summary": {...}
        }
        
    Raises:
        FileNotFoundError: If the input file doesn't exist
        json.JSONDecodeError: If the file contains invalid JSON
    """
    with open(file_path, "r", encoding="utf-8") as file_handle:
        run_payload: Dict[str, Any] = json.load(file_handle)

    return run_payload


# ---------------------------------------------------------------------------
# PQL detection (heuristic)
# ---------------------------------------------------------------------------
def is_probable_pql(pql_text: str) -> bool:
    """
    Heuristic validation to determine if text represents valid Autoptic PQL.
    
    Performs structural validation by checking for:
    1. Core PQL primitives: where(), .when(), .what(), .request()
    2. Selector-like key/value patterns commonly used in metric queries
    
    This validation ensures we only evaluate strings that are likely to be
    genuine PQL queries rather than error messages or incomplete responses.

    Args:
        pql_text (str): Text string to validate as PQL
        
    Returns:
        bool: True if text appears to be valid PQL, False otherwise
        
    Validation Requirements:
        - Must contain all core PQL primitives in proper syntax
        - Must contain at least one metric selector pattern
        - Must be non-empty string type
        
    Common Selector Patterns:
        - MetricName='...'  (primary metric identifier)
        - Namespace='...'   (AWS CloudWatch namespace)
        - Region='...'      (geographic region)
        - ApiName='...'     (API Gateway specific)
        - LoadBalancer='...' (ELB specific)
    """
    # Type and emptiness validation
    if not isinstance(pql_text, str):
        return False

    if len(pql_text.strip()) == 0:
        return False

    # Normalize text for pattern matching (remove spaces, lowercase)
    compact_text: str = pql_text.replace(" ", "").lower()

    # Check for all required core PQL primitives
    core_tokens: List[str] = ["where(", ".when(", ".what(", ".request("]
    core_primitives_present: bool = True

    for token in core_tokens:
        if token not in compact_text:
            core_primitives_present = False
            break

    if not core_primitives_present:
        return False

    # Check for metric selector patterns (key='value' format)
    selector_hints: List[str] = [
        "metricname='",     # Primary metric identifier
        "namespace='",      # AWS CloudWatch namespace
        "distributionid='", # CloudFront distribution
        "targetgroup='",    # ALB target group
        "loadbalancer='",   # ELB load balancer
        "hostedzone='",     # Route53 hosted zone
        "region='",         # Geographic region
        "apiname='",        # API Gateway API name
        "stage='",          # API Gateway stage
    ]

    # Require at least one selector pattern for valid PQL
    has_selector_like_kv: bool = False
    for hint in selector_hints:
        if hint in compact_text:
            has_selector_like_kv = True
            break

    if not has_selector_like_kv:
        return False

    return True


# ---------------------------------------------------------------------------
# PQL extraction helpers
# ---------------------------------------------------------------------------
def find_pql_in_step_list(
    step_list: List[Dict[str, Any]],
    required_message_substring: str,
) -> Optional[str]:
    """
    Search WebSocket response steps for PQL data in completion frames.
    
    Searches through WebSocket response frames (in reverse chronological order)
    to find PQL data associated with successful workflow completion. This function
    looks for frames with specific completion messages and validates that the
    associated data field contains valid PQL.
    
    Args:
        step_list (List[Dict[str, Any]]): List of WebSocket response frames
        required_message_substring (str): Message substring indicating completion
                                         (typically "Workflow generated successfully")
        
    Returns:
        Optional[str]: Valid PQL string if found, None if no valid PQL located
        
    Search Strategy:
        1. Iterate frames in reverse order (latest first for most recent PQL)
        2. Check message/status fields for completion indicators
        3. Validate data field contains non-empty string
        4. Confirm data passes PQL structure validation
        
    Frame Structure Expected:
        {
            "message": "Workflow generated successfully",
            "status": "success", 
            "data": "where(...).when(...).what(...).request()...",
            "ts": "timestamp"
        }
    """
    if step_list is None:
        return None

    # Search from newest to oldest frames (prefer latest PQL generation)
    index: int = len(step_list) - 1
    while index >= 0:
        step: Dict[str, Any] = step_list[index]

        # Extract message content from either message or status field
        message_field_raw: Any = step.get("message")
        status_field_raw: Any = step.get("status")
        data_field_raw: Any = step.get("data")

        message_or_status: str = ""
        if isinstance(message_field_raw, str):
            message_or_status = message_field_raw
        elif isinstance(status_field_raw, str):
            message_or_status = status_field_raw

        # Check if this frame indicates successful completion
        if isinstance(message_or_status, str):
            if required_message_substring in message_or_status:
                # Validate data field contains valid PQL
                if isinstance(data_field_raw, str):
                    if len(data_field_raw.strip()) > 0:
                        if is_probable_pql(data_field_raw):
                            return data_field_raw

        index = index - 1

    return None


def extract_pql_from_case(test_case_json: Dict[str, Any]) -> str:
    """
    Extract generated PQL from a single test case's WebSocket responses.
    
    Processes a test case's response frames to locate and extract the generated
    PQL query. Searches for completion frames indicating successful PQL generation
    and validates the extracted content.
    
    Args:
        test_case_json (Dict[str, Any]): Single test case with responses from WebSocket execution
        
    Returns:
        str: Extracted PQL query string, or "(no PQL found)" if extraction fails
        
    Expected Test Case Structure:
        {
            "name": "Test Case Name",
            "responses": [
                {"message": "progress", "data": "..."},
                {"message": "Workflow generated successfully", "data": "PQL_HERE"}
            ],
            "status": "success"
        }
        
    Fallback Behavior:
        - Returns placeholder text if no valid PQL found
        - Handles missing or empty responses gracefully
        - Validates PQL structure before returning
    """
    # Get responses array from test case (WebSocket frames)
    responses: List[Dict[str, Any]] = test_case_json.get("responses") or []

    # Look for successful completion message in responses
    required_substring: str = "Workflow generated successfully"

    pql_from_responses: Optional[str] = find_pql_in_step_list(
        step_list=responses,
        required_message_substring=required_substring,
    )

    if pql_from_responses is not None:
        return pql_from_responses

    # Return clear indicator if no valid PQL found
    return "(no PQL found)"


# ---------------------------------------------------------------------------
# Test case construction
# ---------------------------------------------------------------------------
def build_deepeval_test_cases(
    test_cases_json: List[Dict[str, Any]]
) -> List[LLMTestCase]:
    """
    Convert test case results into DeepEval LLMTestCase objects for evaluation.
    
    Transforms structured test results into the format required by DeepEval
    framework for PQL quality assessment. Each test case becomes an evaluation
    where the INPUT is the original user prompt and ACTUAL_OUTPUT is the generated PQL.
    
    Args:
        test_cases_json (List[Dict[str, Any]]): List of test case results from structured JSON
        
    Returns:
        List[LLMTestCase]: DeepEval test case objects ready for evaluation
        
    DeepEval Structure:
        - INPUT: Original natural language monitoring question
        - ACTUAL_OUTPUT: Generated PQL query to be evaluated
        - EXPECTED_OUTPUT: None (unsupervised evaluation based on rubric)
        - METADATA: Session info, timing, execution status
        
    Metadata Preserved:
        - session_id: Unique identifier for tracing
        - when: Time range parameter from request
        - status: Test execution status (success/error)
        - duration: Execution time in seconds
    """
    deepeval_test_cases: List[LLMTestCase] = []

    for test_case_json in test_cases_json:
        # Extract original request data (prompt and parameters)
        request_object: Dict[str, Any] = test_case_json.get("request") or {}

        # Get the natural language prompt for evaluation INPUT
        test_case_prompt: str = ""
        if isinstance(request_object.get("prompt"), str):
            test_case_prompt = request_object["prompt"]

        # Extract generated PQL for evaluation ACTUAL_OUTPUT
        raw_pql: str = extract_pql_from_case(test_case_json)

        # Get test case identification
        test_case_name: Optional[str] = test_case_json.get("name")

        # Tag for filtering evaluations
        tags: List[str] = []
        tags.append("pql")

        # Preserve metadata for analysis and debugging
        additional_metadata: Dict[str, Any] = {}
        additional_metadata["session_id"] = test_case_json.get("session_id")
        additional_metadata["when"] = request_object.get("when")
        additional_metadata["status"] = test_case_json.get("status")
        additional_metadata["duration"] = test_case_json.get("duration")

        # Create DeepEval test case for this PQL evaluation
        deepeval_case: LLMTestCase = LLMTestCase(
            input=test_case_prompt,      # Natural language question
            actual_output=raw_pql,       # Generated PQL to evaluate
            expected_output=None,        # Unsupervised evaluation
            name=test_case_name,
            tags=tags,
            additional_metadata=additional_metadata,
        )

        deepeval_test_cases.append(deepeval_case)

    return deepeval_test_cases


# ---------------------------------------------------------------------------
# GEval metric factory
# ---------------------------------------------------------------------------
def create_pql_quality_metric(model_name: str) -> GEval:
    """
    Create configured GEval metric for PQL quality assessment.
    
    Sets up the DeepEval GEval metric with the comprehensive PQL rubric
    and specified LLM model. Configures evaluation parameters and thresholds
    for consistent PQL quality assessment.
    
    Args:
        model_name (str): GPT model identifier (e.g., "gpt-4.1-mini", "gpt-3.5-turbo")
        
    Returns:
        GEval: Configured evaluation metric ready for test case assessment
        
    Configuration:
        - Rubric: Multi-criteria PQL quality assessment (time, metrics, structure, etc.)
        - Model: GPT-based with temperature=0 for consistent scoring
        - Threshold: 0.8 (8/10) for pass/fail determination
        - Verbose: Enabled for detailed feedback and reasoning
        
    Evaluation Parameters:
        - INPUT: Original natural language monitoring question
        - ACTUAL_OUTPUT: Generated PQL query to assess
        
    Note:
        The rubric evaluates 5 key criteria with specific point allocations
        totaling up to 10 points for comprehensive quality assessment.
    """
    # Configure GPT model with deterministic temperature
    base_model: GPTModel = GPTModel(model=model_name, temperature=0)

    # Create GEval metric with PQL-specific rubric
    pql_quality_metric: GEval = GEval(
        name="PQL Quality",
        criteria=PQL_RUBRIC,  # Comprehensive multi-criteria rubric
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        model=base_model,
        threshold=0.8,        # 8/10 score threshold for pass/fail
        verbose_mode=True,    # Enable detailed feedback
    )

    return pql_quality_metric


# ---------------------------------------------------------------------------
# Entrypoint
# ---------------------------------------------------------------------------
def main() -> None:
    """
    Main entry point for PQL quality evaluation.
    
    Orchestrates the complete PQL evaluation process:
    1. Load structured test results from JSON file
    2. Extract PQL data and build DeepEval test cases
    3. Configure evaluation metric with specified model
    4. Execute comprehensive PQL quality assessment
    5. Display results with scores and feedback
    
    Command-line Arguments:
        --input: Path to structured test results JSON file (required)
        --model: GPT model for evaluation (default: gpt-4.1-mini)
        
    Exit Conditions:
        - SystemExit if no test cases found in input
        - Normal completion with evaluation results displayed
        
    Output:
        - Individual test case scores and feedback
        - Overall evaluation summary
        - Pass/fail status based on threshold
    """
    # Configure command-line argument parsing
    argument_parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Evaluate PQL quality using DeepEval framework"
    )
    argument_parser.add_argument(
        "--input",
        required=True,
        help="Path to structured test results JSON file containing test_cases",
    )
    argument_parser.add_argument(
        "--model",
        default="gpt-4.1-mini",
        help="GPT model for evaluation (default: gpt-4.1-mini)",
    )

    args = argument_parser.parse_args()

    # Load test results from structured JSON file
    run_payload: Dict[str, Any] = load_run_file(args.input)

    # Extract test cases array from results
    test_cases_json: List[Dict[str, Any]] = run_payload.get("test_cases") or []
    if len(test_cases_json) == 0:
        raise SystemExit("No test_cases found in the input JSON.")

    # Convert to DeepEval format for evaluation
    deepeval_test_cases: List[LLMTestCase] = build_deepeval_test_cases(test_cases_json)

    # Create configured PQL quality metric
    pql_quality_metric: GEval = create_pql_quality_metric(args.model)

    # Execute PQL quality evaluation
    evaluate(
        test_cases=deepeval_test_cases,
        metrics=[pql_quality_metric],
    )


if __name__ == "__main__":
    # Execute main function when run as script
    main()
