[
  {
    "query": {
      "MetricName": "node_arp_entries",
      "device": "*"
    },
    "description": "The node_arp_entries metric measures the number of ARP (Address Resolution Protocol) entries stored on a node by device. ARP is a protocol used to resolve IP addresses to MAC (Media Access Control) addresses at the data link layer. This metric can be useful in monitoring network connectivity and identifying potential issues with device communication. For example, if the count of ARP entries for a specific device increases rapidly or remains high over time, it may indicate a problem with the device's network connection or a configuration issue. Conversely, a sudden drop in ARP entries could suggest a network partition or a device going offline. This metric can be used to create alerts and notifications when unusual patterns are detected, enabling proactive troubleshooting and minimizing downtime.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_boot_time_seconds"
    },
    "description": "The node_boot_time_seconds metric measures the time it takes for a Prometheus node to boot up from a cold start, represented in Unix time seconds. This metric can be used to monitor and optimize the startup process of nodes in a cluster, potentially indicating issues with configuration, dependencies, or resource constraints. It may also serve as an input for alerting rules that trigger when a node takes excessively long to boot, which could impact overall system availability and performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_context_switches_total"
    },
    "description": "The node_context_switches_total metric measures the total number of context switches that have occurred on a given node. A context switch is an event where the CPU switches from executing one process to another. This can be due to various reasons such as time slicing, I/O operations, or other system events. High context switch rates may indicate performance issues, resource contention, or inefficient scheduling algorithms. Monitoring this metric can help identify potential bottlenecks and inform decisions on resource allocation, process optimization, or even hardware upgrades. It is recommended to use this metric in conjunction with other node-level metrics such as CPU usage, memory utilization, and disk I/O to gain a comprehensive understanding of system performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_cooling_device_cur_state",
      "name": "*",
      "type": "*"
    },
    "description": "The node_cooling_device_cur_state metric measures the current throttle state of the cooling device on a given node. This metric indicates whether the cooling device is currently operating at its maximum capacity or if it has been throttled back due to various reasons such as overheating, power constraints, or system load. The possible values for this metric are likely '0' (throttled) and '1' (not throttled), but this may vary depending on the specific implementation. This metric can be used in monitoring and alerting to detect potential issues with cooling devices, such as overheating, which could lead to hardware damage or data corruption. It can also help operators identify nodes that are experiencing power constraints or system load issues, allowing them to take corrective action to prevent throttling of the cooling device.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_cooling_device_max_state",
      "name": "*",
      "type": "*"
    },
    "description": "The node_cooling_device_max_state metric measures the maximum throttle state of the cooling device on a given node. This value indicates the highest level at which the cooling system has been throttled to prevent overheating. It is essential for monitoring and maintaining optimal temperatures within the node, as excessive heat can lead to hardware failure or data corruption. Potential implications include: \n\n- Identifying nodes with high cooling demands, indicating potential issues with power supply, ambient temperature, or cooling system efficiency.\n- Triggering alerts when the maximum throttle state exceeds a certain threshold, ensuring prompt action is taken to prevent overheating and potential damage.\n- Correlating this metric with other node metrics (e.g., CPU usage, memory consumption) to diagnose performance bottlenecks related to heat generation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_cpu_guest_seconds_total",
      "cpu": "*",
      "mode": "*"
    },
    "description": "This metric measures the total number of seconds that the CPUs spent in guest mode (i.e., running virtual machines or containers) for each mode. It provides insight into the CPU utilization and performance impact of running VMs or containers on the node. Potential implications include identifying nodes with high guest CPU usage, which may indicate overutilization or inefficient resource allocation. This metric can be used to monitor and alert on excessive guest CPU usage, ensuring optimal performance and resource efficiency in the cluster.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_cpu_seconds_total",
      "cpu": "*",
      "mode": "*"
    },
    "description": "The **node_cpu_seconds_total** metric in Prometheus tracks the cumulative number of seconds each CPU core spends in various modes (e.g., user, system, idle, iowait, nice) on a node. It is a counter that increases over time, allowing you to calculate CPU usage percentages by comparing the rate of change across modes. This metric helps SREs understand CPU utilization patterns, detect abnormal CPU behavior, and identify resource contention or performance bottlenecks.\n\n**Alerting guidance:**  \n- Trigger an alert if the rate of CPU time spent in the **idle** mode drops below a defined threshold (e.g., less than 10% idle over 5 minutes), indicating high CPU saturation.  \n- Alternatively, alert if the **iowait** mode time is consistently high (e.g., above 20% over 5 minutes), which may signal disk I/O bottlenecks affecting CPU availability.\n\n**Impact of values:**  \n- **High user or system mode values** indicate heavy CPU usage, which may be expected during peak workloads but could also signal runaway processes or inefficient code.  \n- **Low idle time** suggests the CPU is heavily utilized, potentially leading to increased latency or degraded application performance.  \n- **High iowait time** points to CPU waiting on I/O operations, often a sign of storage or network bottlenecks.\n\n**Example usage:**  \nTo monitor CPU utilization, calculate the percentage of non-idle CPU time over a 5-minute window:\n\n```\n100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)\n```\n\nAn alert rule example to detect high CPU saturation:\n\n```yaml\nalert: HighCpuUsage\nexpr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 90\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"High CPU usage on {{ $labels.instance }}\"\n  description: \"CPU idle time is below 10% for more than 5 minutes, indicating high CPU load.\"\n```\n\nThis metric is foundational for CPU monitoring and should be combined with other system metrics for comprehensive performance analysis.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_discard_time_seconds_total",
      "device": "*"
    },
    "description": "This metric measures the total time spent by all discards on a node's disk. Discard operations are typically used to free up space on a device without actually deleting the data, which can be useful for maintaining performance and avoiding potential issues with data integrity. The metric is likely collected from the Linux kernel's block layer and represents the cumulative sum of discard times across all devices on the node. It may be used in monitoring or alerting to detect potential issues with disk performance, such as high discard rates indicating a need for more storage capacity or optimization of discard operations.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_discarded_sectors_total",
      "device": "*"
    },
    "description": "The node_disk_discarded_sectors_total metric measures the total number of sectors that have been successfully discarded by a node's disk storage system. This can occur due to various reasons such as bad blocks, read errors, or other issues that prevent data from being written to or read from specific sectors on the disk. The metric is incremented each time a sector is discarded and is not reset even if the underlying issue causing the discard is resolved. This means that the total count of discarded sectors will continue to grow over time unless actively managed by the system administrator. In monitoring and alerting, this metric can be used to detect potential issues with disk health, identify nodes with high rates of sector discards, or trigger notifications when a threshold of discarded sectors is exceeded. However, without additional context or information about the underlying storage system, it's difficult to determine what constitutes 'successful' discard or whether the count includes retries or other edge cases.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_discards_completed_total",
      "device": "*"
    },
    "description": "The node_disk_discards_completed_total metric measures the total number of discards completed successfully on a node's disk. A discard is an operation where data is written to a block device but then immediately discarded, often used for testing or debugging purposes. This metric can be useful in monitoring and alerting scenarios where high discard rates may indicate issues with storage performance, data corruption, or other underlying problems. It could also be used to track changes in discard behavior over time, helping operators identify potential issues before they impact system availability.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_discards_merged_total",
      "device": "*"
    },
    "description": "The node_disk_discards_merged_total metric measures the total number of discards merged by the Linux kernel on a given node. A discard is an operation where the operating system informs the storage device that it can reclaim space previously allocated to a file or block device. This metric is incremented each time a discard request is made, but the actual discard may not be executed immediately due to various reasons such as I/O scheduling or device limitations. The total count of merged discards provides insight into the number of times the operating system has requested storage devices to reclaim space, which can indicate potential issues with disk usage, fragmentation, or storage performance. This metric can be used in monitoring and alerting to detect abnormal discard activity, which may signal underlying problems such as slow disk I/O, storage capacity issues, or misconfigured storage settings.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_filesystem_info",
      "device": "*",
      "type": "*",
      "usage": "*",
      "uuid": "*",
      "version": "*"
    },
    "description": "The node_disk_filesystem_info metric provides information about the disk filesystems on a given node. It measures various attributes of each filesystem, including its name, mount point, type (e.g., ext4, xfs), usage statistics such as used and available space, inodes, and more. This metric is useful for monitoring disk storage utilization, detecting potential issues with file system health, and ensuring adequate free space on critical nodes. It can be utilized to trigger alerts when filesystems approach or exceed their capacity thresholds, enabling proactive maintenance and preventing data loss due to disk full conditions.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_flush_requests_time_seconds_total",
      "device": "*"
    },
    "description": "This metric measures the cumulative time, in seconds, spent processing disk flush requests on a specific device. It records the total duration of all flush operations initiated to write data from memory to disk, ensuring data integrity. The value increases with each flush request and reflects the overall disk flush workload over time. Monitoring this metric helps identify disk performance issues or resource bottlenecks affecting storage reliability.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_flush_requests_total",
      "device": "*"
    },
    "description": "The **node_disk_flush_requests_total** metric tracks the cumulative number of successful disk flush requests completed by the operating system on a specific device. A disk flush request forces buffered data to be written from memory to the physical disk, ensuring data integrity and durability. Monitoring this metric helps SREs understand disk write activity and identify potential storage performance issues.\n\n**Purpose:**  \nUse this metric to monitor disk write flush frequency per device, which can indicate how often the system is committing data to disk. Sudden spikes or sustained high rates may signal heavy disk I/O load or inefficient buffering, while unusually low values could suggest stalled or failing flush operations.\n\n**Alert Thresholds:**  \n- Trigger an alert if the rate of flush requests per device exceeds a defined threshold (e.g., > 100 flushes per second sustained over 5 minutes), indicating potential disk saturation or excessive write pressure.  \n- Alert if the flush request rate drops to near zero unexpectedly, which may indicate disk write failures or stalled flush operations.\n\n**Impact:**  \n- **High values:** May lead to increased disk latency, reduced application performance, or early signs of disk wear. High flush rates often correlate with heavy write workloads or inefficient caching.  \n- **Low values:** Could indicate that data is not being flushed properly, risking data loss on crashes or power failures.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-device rate of `node_disk_flush_requests_total` using the Prometheus query:  \n  `rate(node_disk_flush_requests_total{device=~\".+\"}[5m])`  \n  This shows the average flush requests per second over the last 5 minutes, helping visualize disk write activity trends.\n\n- **Alert Rule Example:**  \n  ```yaml\n  alert: HighDiskFlushRequestRate\n  expr: rate(node_disk_flush_requests_total{device=~\".+\"}[5m]) > 100\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High disk flush request rate on {{ $labels.device }}\"\n    description: \"Disk flush requests on device {{ $labels.device }} have exceeded 100 per second for more than 5 minutes, indicating potential disk write pressure.\"\n  ```",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_info",
      "device": "*",
      "major": "*",
      "minor": "*",
      "model": "*",
      "path": "*",
      "revision": "*",
      "serial": "*",
      "wwn": "*"
    },
    "description": "The **node_disk_info** metric provides detailed static information about each disk device on a node, sourced from the `/sys/block/<block_device>` directory. It includes attributes such as device name (`device`), major and minor device numbers (`major`, `minor`), hardware model (`model`), device path (`path`), firmware revision (`revision`), serial number (`serial`), and World Wide Name (`wwn`). This metric itself does not represent dynamic usage or health data but serves as a foundational reference to identify and correlate disk devices in other metrics that track disk performance, errors, or capacity.\n\n**Purpose in Prometheus:**  \nUse **node_disk_info** to uniquely identify and label disk devices across your monitoring data. This enables precise filtering, aggregation, and correlation with dynamic metrics like disk I/O, error counts, or usage percentages.\n\n**Thresholds and Alerts:**  \nSince **node_disk_info** is static metadata, it does not have thresholds or alert conditions directly associated with it. Instead, it supports alerting by providing consistent device identifiers for metrics that do have thresholds (e.g., disk error rates or utilization).\n\n**Impact of Values:**  \nThe values in **node_disk_info** help ensure accurate device mapping and tracking. Missing or incorrect metadata can lead to misinterpretation of disk health or performance metrics, potentially causing missed alerts or false positives.\n\n**Example Usage:**  \nIn a dashboard, use **node_disk_info** labels to display disk-specific metrics clearly:\n\n```promql\nnode_disk_io_time_seconds_total{device=~\".*\"} \n```\n\ncombined with labels from **node_disk_info** to show device model or serial number.\n\nFor alerting, correlate with error metrics:\n\n```yaml\nalert: DiskErrorRateHigh\nexpr: rate(node_disk_errors_total{device=~\".*\"}[5m]) > 0.01\nlabels:\n  severity: warning\nannotations:\n  summary: \"High disk error rate on device {{ $labels.device }} (model: {{ $labels.model }})\"\n  description: \"Disk {{ $labels.device }} has a high error rate, indicating potential hardware issues.\"\n```\n\nThis approach leverages **node_disk_info** to provide context in alerts and dashboards, improving observability and troubleshooting.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_io_now",
      "device": "*"
    },
    "description": "The node_disk_io_now metric measures the number of I/O operations currently in progress on a node's disk storage. This includes both read and write operations, such as file system accesses, database queries, or other disk-intensive tasks. The value represents the instantaneous count of ongoing I/O operations, providing insight into the current workload and potential bottlenecks. High values may indicate resource contention, slow performance, or even hardware issues. This metric can be used to monitor node utilization, detect anomalies in disk activity, and trigger alerts for potential problems before they impact application availability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_io_time_seconds_total",
      "device": "*"
    },
    "description": "This metric measures the total time spent on disk I/O operations across all disks attached to a node. It represents the cumulative sum of seconds spent doing reads and writes to disk storage devices. This metric can be used to identify potential issues with disk performance, such as slow disk speeds or high queue depths, which may impact application performance or cause data corruption. In monitoring or alerting, this metric could trigger alerts when it exceeds a certain threshold, indicating that the node's disk I/O is becoming a bottleneck. Additionally, this metric can be used to correlate with other metrics, such as CPU usage or memory consumption, to understand the root cause of performance issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_io_time_weighted_seconds_total",
      "device": "*"
    },
    "description": "This metric measures the weighted total time spent doing I/O operations on a node's disk. It represents the average time spent per second performing read and write operations to the disk, taking into account the number of I/Os performed during that period. The metric is useful for identifying potential issues with disk performance, such as slow disk speeds or high I/O wait times, which can impact application performance and responsiveness. It may be used in monitoring and alerting to detect anomalies in disk I/O performance, trigger proactive maintenance, or optimize resource allocation.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_read_bytes_total",
      "device": "*"
    },
    "description": "The node_disk_read_bytes_total metric measures the total number of bytes read successfully from disk storage on a given node. This metric is useful for monitoring and alerting purposes to detect potential issues related to disk I/O performance or capacity. It can be used to identify bottlenecks, optimize resource allocation, and ensure data integrity. For example, if this metric consistently shows high values, it may indicate that the node's disk storage is being overwhelmed, leading to slow application performance or even crashes. Conversely, low values might suggest underutilized resources, which could be optimized for better efficiency.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_read_time_seconds_total",
      "device": "*"
    },
    "description": "The node_disk_read_time_seconds_total metric measures the total amount of time spent by all disk read operations on a given node in seconds. This includes the cumulative sum of time taken for each individual read operation to complete. It provides insight into the overall performance and efficiency of disk I/O operations, helping operators identify potential bottlenecks or issues with storage systems. Potential implications include: (1) detecting slow disk reads that may impact application performance, (2) identifying nodes with high disk utilization, and (3) correlating with other metrics to diagnose root causes of performance degradation. This metric can be used in monitoring and alerting to trigger notifications when the total read time exceeds a certain threshold or shows an unusual trend.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_reads_completed_total",
      "device": "*"
    },
    "description": "The node_disk_reads_completed_total metric measures the total number of disk read operations that have completed successfully on a given node. This metric is incremented each time a read operation completes without errors. It provides insight into the I/O performance and load on the node's storage system, helping to identify potential bottlenecks or issues with disk access times. In monitoring or alerting, this metric can be used to detect sudden spikes in disk reads, which may indicate a problem with the underlying storage infrastructure, such as a failing disk or inadequate capacity. It can also be used to track long-term trends and optimize storage configuration for better performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_reads_merged_total",
      "device": "*"
    },
    "description": "The node_disk_reads_merged_total metric measures the total number of read requests that have been merged by the Linux kernel's I/O scheduler on a given node. This metric is an indicator of the efficiency of the disk I/O operations and can be used to identify potential bottlenecks or performance issues related to disk reads. A high value may indicate that the system is experiencing disk contention, while a low value could suggest that the disk I/O operations are being efficiently merged by the kernel. This metric can be used in monitoring and alerting to detect anomalies in disk read performance, such as sudden spikes or drops in the number of merged reads.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_write_time_seconds_total",
      "device": "*"
    },
    "description": "This metric, **node_disk_write_time_seconds_total{device=\"*\"}**, tracks the cumulative time in seconds that all write operations have spent on a specific disk device on a node. It helps SREs monitor disk write latency and identify potential storage performance issues. \n\n**Purpose:**  \nIt reflects how long write operations take to complete on each disk device, enabling detection of slow or overloaded disks that could degrade application performance or cause system bottlenecks.\n\n**Thresholds for Alerting:**  \nA common alert threshold is when the rate of increase in write time per second (i.e., the derivative of this cumulative metric) exceeds a device-specific baseline, for example:  \n- Alert if the write time per second exceeds 0.5 seconds per second (50% of time spent writing), sustained over 5 minutes.  \n- Thresholds should be tuned based on normal workload patterns and device capabilities.\n\n**Impact of Values:**  \n- **High values or rapid increases** indicate that write operations are taking longer, possibly due to disk saturation, hardware degradation, or misconfiguration, which can lead to increased I/O wait times and application slowdowns.  \n- **Low or stable values** suggest healthy disk write performance with minimal latency.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-second rate of `node_disk_write_time_seconds_total` (using `rate(node_disk_write_time_seconds_total[5m])`) per device to visualize disk write latency trends over time.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by (device) (rate(node_disk_write_time_seconds_total[5m])) > 0.5\n  ```  \n  This triggers an alert if any device spends more than 0.5 seconds per second writing to disk over 5 minutes, indicating potential disk write bottlenecks requiring investigation.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_writes_completed_total",
      "device": "*"
    },
    "description": "The **node_disk_writes_completed_total** metric tracks the cumulative number of successful write operations completed on each disk device of a node. It reflects all types of disk write I/O, including file system writes, database commits, and other storage updates. Monitoring this metric helps SREs understand disk write activity and identify potential performance bottlenecks or hardware issues.\n\n**Purpose:**  \n- Measure disk write throughput per device over time.  \n- Detect abnormal spikes or drops in write operations that may indicate workload changes, disk saturation, or failures.  \n- Correlate with latency and error metrics to diagnose storage-related incidents.\n\n**Threshold guidance for alerts:**  \n- Alert if the write rate (calculated as the increase in node_disk_writes_completed_total over a short interval) drops below a baseline threshold, e.g., less than 10 writes/sec for a device that normally handles hundreds per second, which may indicate stalled writes or hardware issues.  \n- Alert if the write rate exceeds a high threshold sustained over time, e.g., above 1000 writes/sec for more than 5 minutes, potentially signaling disk saturation or excessive load that could degrade performance.\n\n**Impact of values:**  \n- **High values / sustained high write rates:** May cause increased disk latency, higher I/O wait times, and potential degradation of application performance. Could indicate heavy workload or inefficient disk usage.  \n- **Low or zero values:** Could indicate idle disks, but if unexpected, may signal stalled writes, disk failures, or connectivity issues.\n\n**Example usage:**  \n- **Dashboard:** Plot the per-device write rate using the Prometheus query:  \n  `rate(node_disk_writes_completed_total{device!=\"loop\"}[5m])`  \n  This shows the average writes per second over the last 5 minutes, helping visualize trends and spot anomalies.\n\n- **Alert rule example:**  \n  ```yaml\n  alert: DiskWriteStall\n  expr: rate(node_disk_writes_completed_total{device!=\"loop\"}[5m]) < 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Low disk write rate detected on {{ $labels.device }}\"\n    description: \"Disk writes on device {{ $labels.device }} have dropped below 10 writes/sec for over 5 minutes, indicating possible disk stall or failure.\"\n  ```",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_writes_merged_total",
      "device": "*"
    },
    "description": "The node_disk_writes_merged_total metric measures the total number of write requests merged by the Linux kernel's block layer before being written to disk. This value can indicate the efficiency of the storage system and potential bottlenecks in I/O operations. A high rate of writes merged may suggest that the storage system is experiencing a high volume of write traffic, which could lead to performance issues or even data corruption if not properly managed. This metric can be used in monitoring to detect anomalies in disk usage patterns, alerting teams to potential storage-related issues before they impact application performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_disk_written_bytes_total",
      "device": "*"
    },
    "description": "The **node_disk_written_bytes_total** metric records the cumulative number of bytes successfully written to a specific disk device since the system started. It is measured in bytes and labeled by the device name (e.g., sda, nvme0n1). This metric helps monitor disk write activity and identify potential performance bottlenecks or hardware issues. Use it alongside related metrics like node_disk_read_bytes_total for a complete view of disk I/O.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_dmi_info",
      "bios_date": "*",
      "bios_release": "*",
      "bios_vendor": "*",
      "bios_version": "*",
      "board_asset_tag": "*",
      "board_name": "*",
      "board_vendor": "*",
      "board_version": "*",
      "chassis_asset_tag": "*",
      "chassis_vendor": "*",
      "chassis_version": "*",
      "product_family": "*",
      "product_name": "*",
      "product_sku": "*",
      "product_version": "*",
      "system_vendor": "*"
    },
    "description": "The **node_dmi_info** metric in Prometheus exposes detailed hardware and system information extracted from the system's Desktop Management Interface (DMI). It provides static labels such as BIOS vendor, version, release date, motherboard details, chassis info, and product identifiers. This metric is primarily used for inventory, asset tracking, and correlating hardware characteristics with system behavior rather than for numeric threshold-based alerting.\n\n**Purpose:**  \n- Enables SREs to identify and verify hardware configurations across nodes in a cluster.  \n- Helps detect hardware mismatches, outdated BIOS versions, or unauthorized hardware changes that could impact system stability or security.  \n- Supports forensic analysis by correlating hardware attributes with incidents or performance anomalies.\n\n**Thresholds & Alerts:**  \n- Since the metric is informational and label-based (not numeric), it does not have traditional thresholds or high/low values to alert on.  \n- Alerts should be based on changes or discrepancies in labels over time, for example:  \n  - BIOS version differs from an approved baseline.  \n  - Unexpected changes in board or chassis asset tags indicating possible hardware replacement or tampering.  \n- Example alert condition: Trigger if the BIOS version label changes unexpectedly or if the system vendor label does not match the expected value for a given environment.\n\n**Impact of Changes:**  \n- A change in BIOS version or vendor may indicate a firmware update or rollback, which could affect system stability or security posture.  \n- Mismatched or unknown board or chassis information might signal hardware replacement or misconfiguration, potentially leading to unsupported hardware issues or warranty concerns.\n\n**Example Usage:**  \n- Dashboard: Display node_dmi_info labels per host to quickly verify hardware inventory and BIOS versions across the fleet.  \n- Alert Rule Example (PromQL):  \n  ```yaml\n  alert: UnexpectedBIOSVersion\n  expr: node_dmi_info{bios_version!~\"^1\\.2\\..*\"} == 1\n  for: 10m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Node {{ $labels.instance }} has an unexpected BIOS version {{ $labels.bios_version }}\"\n    description: \"The BIOS version on node {{ $labels.instance }} does not match the approved baseline.\"\n  ```\n\nThis approach helps SREs maintain hardware consistency, detect unauthorized changes, and ensure system reliability by monitoring static system metadata exposed via node_dmi_info.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_entropy_available_bits"
    },
    "description": "The node_entropy_available_bits metric measures the number of available bits of entropy in a Prometheus node. Entropy refers to the measure of uncertainty or randomness in a system. In this context, it represents the amount of unpredictable data that can be used for cryptographic purposes, such as generating keys or nonces. This metric is useful for monitoring and ensuring the security posture of nodes within a cluster. High entropy values indicate a sufficient level of unpredictability, while low values may suggest issues with the system's randomness generation capabilities. Potential implications include alerting on low entropy values to prevent potential security vulnerabilities, such as predictable keys or nonces. This metric can also be used in conjunction with other metrics to identify potential causes of low entropy, such as hardware or software issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_entropy_pool_size_bits"
    },
    "description": "The node_entropy_pool_size_bits metric measures the size of the entropy pool in bits for a given node. The entropy pool is a buffer used by the kernel to store random numbers generated from various sources such as user input, network packets, and other system events. This pool is used to seed the random number generator (RNG) which is essential for generating cryptographically secure pseudo-random numbers. A larger entropy pool size indicates that the node has more randomness available for use by applications and services, which can be beneficial for tasks such as key generation, encryption, and secure communication protocols. In monitoring or alerting, this metric could be used to detect potential issues with system security or RNG functionality. For example, a consistently low entropy pool size might indicate a problem with the system's ability to generate randomness, potentially leading to compromised security. Conversely, an excessively large entropy pool size could indicate inefficient use of system resources.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_exporter_build_info",
      "branch": "*",
      "goarch": "*",
      "goos": "*",
      "goversion": "*",
      "revision": "*",
      "tags": "*",
      "version": "*"
    },
    "description": "The **node_exporter_build_info** metric provides static information about the Node Exporter\u2019s build environment at compile time. It includes details such as the version, Git revision, branch, Go language version, operating system (GoOS), architecture (GoArch), and build tags. The metric\u2019s value is always 1, serving as a constant indicator rather than a measured quantity. This data helps identify the exact build of Node Exporter running, useful for debugging and tracking version changes.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filefd_allocated"
    },
    "description": "The node_filefd_allocated metric measures the number of file descriptors allocated by a Node.js process. File descriptors are used to represent open files or connections in an operating system. This metric can be used to monitor and troubleshoot issues related to file descriptor usage, such as running out of available file descriptors, which can lead to errors like 'Too many open files' or 'EMFILE'. It may also indicate potential resource leaks or inefficient use of resources by the application. In monitoring or alerting, this metric could be used to set thresholds for maximum allowed allocated file descriptors, triggering alerts when the threshold is exceeded. Additionally, it can be used in conjunction with other metrics, such as node_filefd_limit, to monitor and manage file descriptor usage more effectively.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filefd_maximum"
    },
    "description": "The **node_filefd_maximum** metric represents the highest number of file descriptors that the node has had open simultaneously since the system started. In Prometheus, this metric helps SREs monitor the peak file descriptor usage, which reflects the system\u2019s capacity to handle concurrent file and network operations. \n\n**Purpose:**  \nTrack the maximum concurrent file descriptor usage to identify trends toward resource exhaustion or leaks that could impact system stability.\n\n**Alert Threshold:**  \nSet alerts when **node_filefd_maximum** approaches a critical percentage (e.g., 80-90%) of the system\u2019s configured file descriptor limit (`ulimit -n`). For example, if the system limit is 100,000, trigger an alert when **node_filefd_maximum > 80,000** sustained over 5 minutes.\n\n**Impact of Values:**  \n- **High values near the system limit:** Indicate potential risk of file descriptor exhaustion, which can cause new connections or file operations to fail, leading to service degradation or outages.  \n- **Low or stable values:** Suggest healthy resource usage with sufficient headroom.  \n- **Rapid increases:** May signal leaks or spikes in workload requiring investigation.\n\n**Example Usage:**  \n- **Dashboard:** Plot **node_filefd_maximum** alongside the system file descriptor limit to visualize headroom and trends over time.  \n- **Alert Rule (PromQL):**  \n  ```\n  node_filefd_maximum > (0.8 * node_filefd_limit)\n  ```\n  where `node_filefd_limit` is a metric representing the system\u2019s max file descriptors. Alert if this condition persists for 5 minutes to catch sustained high usage.\n\nBy monitoring **node_filefd_maximum** with these guidelines, SREs can proactively detect and mitigate file descriptor exhaustion risks before they impact service availability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filesystem_avail_bytes",
      "device": "*",
      "device_error": "*",
      "fstype": "*",
      "mountpoint": "*"
    },
    "description": "The node_filesystem_avail_bytes metric measures the amount of available disk space on a filesystem that is accessible to non-root users in bytes. This value represents the total capacity of the filesystem minus any reserved space for root and other system processes. It can be used to monitor the available storage capacity on a node, which is crucial for ensuring that there is sufficient space for logs, temporary files, and other data generated by applications running on the node. High values may indicate underutilization or inefficient use of disk resources, while low values could signal potential issues with storage capacity, such as running out of space due to high log volume or unexpected file growth.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filesystem_device_error",
      "device": "*",
      "device_error": "*",
      "fstype": "*",
      "mountpoint": "*"
    },
    "description": "The node_filesystem_device_error metric measures whether an error occurred while retrieving statistics for a specific device on the node. This metric indicates potential issues with file system access or data retrieval from the device. It can be used to monitor and alert on errors that may impact data integrity, storage availability, or overall system performance. Possible implications include: (1) Data corruption or loss due to failed disk operations; (2) Storage capacity issues or misconfiguration; (3) File system inconsistencies or crashes. This metric is particularly useful for identifying and troubleshooting file system-related problems on the node.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filesystem_files",
      "device": "*",
      "device_error": "*",
      "fstype": "*",
      "mountpoint": "*"
    },
    "description": "The **node_filesystem_files** metric reports the total number of inodes (files and directories) available on a filesystem as collected by the Prometheus node exporter. It helps SREs monitor filesystem usage beyond just disk space, focusing on the count of files which can be a limiting factor on many filesystems.\n\n**Purpose:**  \nTrack inode consumption to detect when a filesystem is nearing its maximum file count capacity, which can cause failures in file creation even if disk space remains available.\n\n**Alert Thresholds:**  \nTrigger alerts when inode usage exceeds a critical threshold, commonly set at 85-90% of total available inodes, e.g., when `(node_filesystem_files - node_filesystem_files_free) / node_filesystem_files > 0.85`. This indicates the filesystem is close to running out of inodes.\n\n**Impact of Values:**  \n- **High values (near total inode limit):** Risk of application errors due to inability to create new files, potential service disruptions, and degraded system performance.  \n- **Low values:** Normal operation; sufficient capacity for file creation.\n\n**Example Usage:**  \n- **Dashboard:** Display inode usage percentage per mountpoint to visualize inode consumption trends over time.  \n- **Alert Rule:**  \n```yaml\nalert: FilesystemInodeUsageHigh\nexpr: (node_filesystem_files - node_filesystem_files_free) / node_filesystem_files > 0.85\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High inode usage on {{ $labels.mountpoint }}\"\n  description: \"Inode usage is above 85% on filesystem mounted at {{ $labels.mountpoint }} (device: {{ $labels.device }}). This may prevent new files from being created.\"\n```\nMonitoring this metric proactively helps prevent inode exhaustion issues that can cause application failures despite available disk space.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filesystem_files_free",
      "device": "*",
      "device_error": "*",
      "fstype": "*",
      "mountpoint": "*"
    },
    "description": "The node_filesystem_files_free metric measures the total number of free file nodes in a filesystem on a given node. This metric is collected by Prometheus from the node and represents the available space for new files within that filesystem. It can be used to monitor the disk space utilization and detect potential issues such as running out of space, which may impact system performance or cause errors. In monitoring or alerting, this metric can be used to set thresholds for free file nodes, triggering alerts when the available space falls below a certain threshold. This allows operations teams to proactively address storage capacity issues before they become critical.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filesystem_free_bytes",
      "device": "*",
      "device_error": "*",
      "fstype": "*",
      "mountpoint": "*"
    },
    "description": "The **node_filesystem_free_bytes** metric reports the amount of free disk space, in bytes, available on each filesystem mounted on a Prometheus-monitored node. It helps SREs monitor storage health by indicating how much space remains before a filesystem becomes full. \n\n**Purpose:** Track available disk space per device, filesystem type, and mount point to prevent storage exhaustion that can cause application failures, degraded performance, or data loss.\n\n**Alert Thresholds:** Common practice is to alert when free space falls below a critical threshold, such as 10% of total filesystem capacity or an absolute value (e.g., less than 5 GB free). Thresholds should be tailored based on filesystem size and application requirements.\n\n**Impact of Values:**  \n- **Low values:** Indicate risk of disk full errors, which can lead to service outages, inability to write logs or data, and degraded system stability. Immediate remediation is required.  \n- **High values:** Generally healthy, but sudden drops may signal abnormal data growth or leaks.\n\n**Example Alert Rule:**  \n```yaml\nalert: LowDiskSpace\nexpr: (node_filesystem_free_bytes{fstype!~\"tmpfs|overlay\"} / node_filesystem_size_bytes{fstype!~\"tmpfs|overlay\"}) < 0.10\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Disk space low on {{ $labels.instance }} ({{ $labels.mountpoint }})\"\n  description: \"Free disk space is below 10% on {{ $labels.device }} mounted at {{ $labels.mountpoint }}.\"\n```\n\n**Example Dashboard Usage:**  \nVisualize **node_filesystem_free_bytes** alongside **node_filesystem_size_bytes** to show free vs total space per mount point, enabling quick identification of disks nearing capacity and trending storage usage over time.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filesystem_readonly",
      "device": "*",
      "device_error": "*",
      "fstype": "*",
      "mountpoint": "*"
    },
    "description": "The node_filesystem_readonly metric measures whether a filesystem on the monitored node is in read-only mode. This status indicates that the filesystem has been locked for reading or writing, potentially due to issues such as disk full conditions, quota exceeded errors, or explicit lock commands. The metric can be used to detect and alert on potential storage capacity issues, identify misconfigured systems, or notify administrators of intentional data protection measures.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_filesystem_size_bytes",
      "device": "*",
      "device_error": "*",
      "fstype": "*",
      "mountpoint": "*"
    },
    "description": "The **node_filesystem_size_bytes** metric reports the total size, in bytes, of a specific filesystem on a node, identified by labels such as device, device_error, fstype, and mountpoint. In Prometheus, this metric helps SREs understand the maximum storage capacity available on each mounted filesystem, enabling capacity planning and resource management.\n\n**Purpose:**  \n- Track the total disk size per filesystem to monitor storage allocation.  \n- Combine with metrics like `node_filesystem_free_bytes` or `node_filesystem_avail_bytes` to calculate used space and utilization percentages.  \n- Identify filesystems nearing full capacity to prevent service disruptions.\n\n**Alert Threshold Guidance:**  \n- Alert when available space falls below a critical threshold, e.g., less than 10% of total size (`node_filesystem_avail_bytes / node_filesystem_size_bytes < 0.1`).  \n- High values of `node_filesystem_size_bytes` alone are normal (indicating large disks), but sudden decreases may indicate unmounted or failed filesystems.  \n- Low or zero values may indicate missing mounts or device errors.\n\n**Impact:**  \n- High total size with low available space risks application failures due to insufficient disk space.  \n- Unexpected drops in total size can signal hardware issues or misconfigurations.\n\n**Example Usage:**  \n- **Dashboard:** Display total size alongside used and available space per mountpoint to visualize disk utilization trends.  \n- **Alert Rule Example:**  \n```yaml\nalert: FilesystemRunningOutOfSpace\nexpr: (node_filesystem_avail_bytes{fstype!~\"tmpfs|overlay\"} / node_filesystem_size_bytes{fstype!~\"tmpfs|overlay\"}) < 0.1\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is running low on disk space\"\n  description: \"Available disk space is below 10% of total size for more than 5 minutes.\"\n```\nThis alert helps proactively manage disk capacity and avoid outages caused by full filesystems.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_forks_total"
    },
    "description": "The node_forks_total metric measures the total number of forks that have occurred on a given node in an etcd cluster. A fork is essentially a divergent copy of the etcd database, which can happen due to various reasons such as network partitions or concurrent updates. This metric provides insight into the health and stability of the etcd cluster by indicating potential issues with data consistency and replication. High values of this metric may indicate problems with node communication, configuration errors, or other operational issues that need to be addressed promptly. It can be used in monitoring and alerting to detect anomalies, trigger investigations, and implement corrective actions to prevent data loss or corruption.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_intr_total"
    },
    "description": "The node_intr_total metric measures the total number of interrupts serviced by a node in a system. An interrupt is an asynchronous event that requires immediate attention from the CPU, such as a hardware request for I/O or a timer expiration. This metric can be used to monitor and troubleshoot issues related to interrupt handling, which can impact system performance and responsiveness. High values may indicate excessive interrupt activity, potentially caused by hardware issues, driver problems, or resource contention. Conversely, low values could suggest that the node is not receiving interrupts as expected, possibly due to configuration errors or faulty hardware. This metric can be used in conjunction with other metrics, such as CPU usage and memory consumption, to identify potential bottlenecks and optimize system performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_load1"
    },
    "description": "The node_load1 metric measures the average number of processes in the run queue over a 1-minute interval on the monitored node. This value represents the system's workload and can indicate potential bottlenecks or resource constraints. A high load average may suggest that the system is under heavy CPU usage, which could lead to performance degradation or even crashes. In monitoring or alerting, this metric can be used to detect sudden spikes in load, identify trends over time, or trigger notifications when thresholds are exceeded. It's essential to consider other related metrics, such as node_cpu_usage and node_memory_usage, for a more comprehensive understanding of system resource utilization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_load15"
    },
    "description": "The node_load15 metric measures the average system load over a 15-minute window on the monitored node. This value represents the number of processes competing for CPU resources during that time period. A higher load average indicates increased competition for CPU resources, which can lead to performance degradation and potential bottlenecks in system operations. Potential implications or usage in monitoring or alerting include: triggering alerts when the load average exceeds a certain threshold (e.g., > 5) to prevent system overload; using this metric as a key performance indicator (KPI) for CPU utilization; and correlating with other metrics, such as node_cpu_usage, to identify potential bottlenecks in resource allocation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_load5"
    },
    "description": "The node_load5 metric measures the average number of processes in the run queue over a 5-minute window on the monitored node. This value represents the system's ability to handle incoming workload and can indicate potential bottlenecks or resource constraints. High values may suggest that the system is overloaded, leading to decreased performance, increased latency, or even crashes. This metric can be used to monitor system utilization, detect anomalies, and trigger alerts for proactive maintenance or scaling.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Active_anon_bytes"
    },
    "description": "The **node_memory_Active_anon_bytes** metric in Prometheus represents the amount of memory actively used by the system for anonymous pages\u2014memory not backed by files on disk, such as process heap, stack, and data segments. This metric helps SREs understand how much memory is consumed by running processes that cannot be reclaimed easily, indicating real workload memory usage.\n\n**Purpose:**  \nMonitor active anonymous memory usage to detect memory pressure caused by applications and processes. It is critical for identifying memory-intensive workloads that may lead to resource exhaustion.\n\n**Alert Threshold Guidance:**  \nSet alerts when **node_memory_Active_anon_bytes** approaches a high percentage of total available memory (e.g., >80% of node_memory_MemTotal_bytes) sustained over a period (e.g., 5 minutes). This threshold helps catch scenarios where anonymous memory usage is dangerously high, risking system stability.\n\n**Impact of Values:**  \n- **High values:** Indicate heavy memory usage by applications, which can lead to swapping, degraded performance, or out-of-memory (OOM) conditions if unchecked.  \n- **Low values:** Suggest underutilization of memory or that workloads are not memory-intensive, which may be acceptable but should be correlated with overall system performance.\n\n**Example Alert Rule:**  \n```yaml\nalert: HighActiveAnonMemoryUsage\nexpr: (node_memory_Active_anon_bytes / node_memory_MemTotal_bytes) > 0.8\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High active anonymous memory usage on {{ $labels.instance }}\"\n  description: \"Active anonymous memory usage is above 80% for more than 5 minutes, indicating potential memory pressure.\"\n```\n\n**Example Dashboard Usage:**  \nVisualize **node_memory_Active_anon_bytes** alongside **node_memory_MemTotal_bytes** and **node_memory_Active_file_bytes** to get a comprehensive view of memory consumption patterns, helping to distinguish between anonymous and file-backed memory usage and to identify trends or sudden spikes in process memory demand.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Active_bytes"
    },
    "description": "The **node_memory_Active_bytes** metric measures the amount of memory in bytes that is actively used by the Linux system\u2019s processes and kernel. This includes memory recently accessed and not eligible for reclaim but excludes cached or swapped-out memory. High values indicate significant memory usage, which may signal memory pressure or potential performance issues. This metric helps monitor active memory consumption and detect abnormal increases that could suggest memory leaks.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Active_file_bytes"
    },
    "description": "The **node_memory_Active_file_bytes** metric in Prometheus represents the amount of memory, in bytes, actively used by file-backed pages that are currently mapped and in use by the system. This includes memory used for file I/O operations such as reading and writing files, excluding other memory uses like caches or buffers. Monitoring this metric helps SREs understand how much memory is dedicated to active file data, which is critical for diagnosing memory pressure related to file operations.\n\n**Purpose:**  \nTrack the memory actively used by file-backed pages to detect abnormal file I/O memory usage patterns that could impact system performance or stability.\n\n**Thresholds and Alerts:**  \n- **Warning threshold:** When `node_memory_Active_file_bytes` exceeds 70-80% of total system memory, it may indicate excessive file-backed memory usage that could reduce available memory for applications.  \n- **Critical threshold:** Above 90% of total memory, this signals a high risk of memory exhaustion, potentially causing system slowdowns or crashes.\n\n**Impact:**  \n- **High values:** Suggest heavy file I/O activity or memory leaks in file-backed pages, which can lead to memory pressure, swapping, or OOM (Out Of Memory) events.  \n- **Low values:** Typically normal, indicating file-backed memory is not consuming excessive resources.\n\n**Example Usage:**  \n- **Dashboard:** Display `node_memory_Active_file_bytes` alongside total memory and other memory metrics to visualize file-backed memory trends over time.  \n- **Alert Rule (PromQL):**  \n  ```promql\n  node_memory_Active_file_bytes / node_memory_MemTotal_bytes > 0.8\n  ```  \n  This alert fires when active file memory exceeds 80% of total memory, prompting investigation into file I/O patterns or memory leaks.\n\nBy monitoring **node_memory_Active_file_bytes** with these guidelines, SREs can proactively detect and mitigate memory issues related to file-backed memory usage.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_AnonHugePages_bytes"
    },
    "description": "The node_memory_AnonHugePages_bytes metric measures the amount of memory allocated for anonymous huge pages on a Linux node. Anonymous huge pages are a type of memory allocation used by the kernel to provide large contiguous blocks of memory for applications that require it, such as databases and virtual machines. This metric can be used to monitor the usage of these resources and detect potential issues with memory allocation or fragmentation. High values may indicate inefficient use of memory or resource contention between applications. It can also be used in conjunction with other metrics, such as node_memory_MemTotal_bytes, to calculate memory utilization and identify trends over time.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_AnonPages_bytes"
    },
    "description": "The **node_memory_AnonPages_bytes** metric in Prometheus tracks the total amount of anonymous memory (in bytes) currently used by a Linux node. Anonymous memory consists of pages not backed by any file or disk storage, typically allocated for process heap, stack, and other runtime data. This metric helps SREs monitor the memory footprint of applications and the system\u2019s overall memory usage beyond file-backed caches.\n\n**Purpose:**  \nUse this metric to identify excessive anonymous memory consumption that could indicate memory leaks, inefficient memory usage, or potential pressure on system memory resources.\n\n**Alert Thresholds:**  \nA common alert threshold is when anonymous memory usage exceeds a significant percentage of total system memory (e.g., 80%). For example:  \n```\n(node_memory_AnonPages_bytes / node_memory_MemTotal_bytes) > 0.8\n```\nThis condition suggests the node is using a large portion of its memory for anonymous pages, which may lead to out-of-memory (OOM) situations.\n\n**Impact of Values:**  \n- **High values:** May indicate memory leaks, runaway processes, or applications with large in-memory data structures. Sustained high anonymous memory usage can cause system instability or OOM kills.  \n- **Low values:** Generally normal, indicating that anonymous memory usage is within expected limits.\n\n**Example Usage:**  \n- **Dashboard:** Plot `node_memory_AnonPages_bytes` alongside `node_memory_MemTotal_bytes` and `node_memory_Cached_bytes` to visualize memory distribution and identify trends in anonymous memory growth.  \n- **Alert Rule Example:**  \n```yaml\nalert: HighAnonymousMemoryUsage\nexpr: (node_memory_AnonPages_bytes / node_memory_MemTotal_bytes) > 0.8\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High anonymous memory usage on {{ $labels.instance }}\"\n  description: \"Anonymous memory usage is above 80% of total memory for more than 5 minutes, which may lead to system instability.\"\n```\nThis alert helps proactively detect and mitigate memory pressure caused by excessive anonymous memory consumption.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Bounce_bytes"
    },
    "description": "The node_memory_Bounce_bytes metric measures the number of bytes that have been bounced (i.e., written and then immediately read back) from memory on a given node. This can be an indicator of memory contention or thrashing, where the system is constantly moving data in and out of memory due to high disk I/O or other factors. High bounce rates may lead to performance degradation, increased latency, and potential crashes. Monitoring this metric can help identify issues related to memory efficiency, cache utilization, and overall system stability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Buffers_bytes"
    },
    "description": "The node_memory_Buffers_bytes metric measures the amount of memory allocated for buffers in bytes on a Prometheus node export. Buffers are a type of memory allocation used by the Linux kernel to store data temporarily while it is being processed. This can include network packets, disk I/O, and other system operations. High values or sudden spikes in this metric may indicate issues with system performance, such as slow network traffic, high disk usage, or inefficient memory management. It could also be a sign of resource-intensive processes consuming excessive buffer space. Monitoring this metric can help identify potential bottlenecks and inform decisions on optimizing system configuration, upgrading hardware, or adjusting process priorities.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Cached_bytes"
    },
    "description": "The node_memory_Cached_bytes metric measures the amount of memory that is currently cached on a Node in bytes. This includes data that has been recently accessed and is likely to be needed again soon. The cache is used by the operating system to improve performance by reducing the number of times data needs to be retrieved from slower storage devices. High values for this metric may indicate efficient use of memory, while low values could suggest issues with memory allocation or inefficient caching. This metric can be used in monitoring and alerting to detect potential memory bottlenecks or issues that may impact system performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_CommitLimit_bytes"
    },
    "description": "The **node_memory_CommitLimit_bytes** metric measures the maximum amount of virtual memory (in bytes) that the Linux kernel can allocate to all processes combined. This limit includes both anonymous memory and memory mapped to files. It represents the total commit charge limit, beyond which the system may trigger out-of-memory (OOM) conditions. Monitoring this metric helps prevent memory overcommitment and system instability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Committed_AS_bytes"
    },
    "description": "The node_memory_Committed_AS_bytes metric measures the amount of memory that is committed to a process or application on the system, in bytes. This value represents the portion of virtual memory that has been allocated for use by a process and is guaranteed to be available when needed. It does not include memory that may be swapped out to disk. High values of this metric can indicate memory pressure, where the system is running low on physical RAM and may start using swap space, leading to performance degradation. This metric can be used in monitoring and alerting to detect potential issues with memory usage, such as high memory consumption by specific processes or applications, or overall system memory saturation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_DirectMap1G_bytes"
    },
    "description": "The node_memory_DirectMap1G_bytes metric measures the amount of memory that is directly mapped by the kernel for 1 GB pages. This type of memory mapping allows for efficient access to large contiguous blocks of memory, which can be beneficial for certain workloads or applications. In monitoring and alerting, this metric can be used to detect potential issues with memory allocation or fragmentation, particularly if the DirectMap1G_bytes value is consistently low or decreasing over time. This could indicate a need for additional memory resources or optimization of memory usage patterns. However, without further context or information about the specific system configuration and workload, it's difficult to provide more detailed insights into the implications of this metric.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_DirectMap2M_bytes"
    },
    "description": "The node_memory_DirectMap2M_bytes metric measures the amount of memory (in bytes) that is directly mapped by the kernel for pages of size 2MB. This type of memory mapping is used to optimize performance-critical code paths and reduce page table overhead. High values may indicate efficient use of large-page support, while low values could suggest issues with kernel configuration or hardware limitations. Potential implications include monitoring for optimal system performance, identifying potential bottlenecks in memory usage, or alerting on anomalies that may require further investigation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_DirectMap4k_bytes"
    },
    "description": "This metric measures the amount of memory that is directly mapped to a specific page size (4KB) on the node. Direct mapping refers to the process where physical memory pages are directly mapped to virtual memory addresses without any additional translation or lookup required. This can improve performance by reducing the overhead associated with memory access. The value represents the total number of bytes allocated for direct mapping at this page size. High values may indicate efficient use of memory, while low values could suggest potential issues with memory allocation or fragmentation. Monitoring this metric can help identify trends in memory usage and optimize system configuration to improve performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Dirty_bytes"
    },
    "description": "The node_memory_Dirty_bytes metric measures the amount of memory that has been modified since it was last written to disk, but not yet flushed to disk. This value represents the number of bytes in the dirty page cache, which is a buffer used by the operating system to improve performance by reducing the need for disk I/O operations. High values may indicate issues with disk performance or memory usage, and could be used as an indicator for potential problems such as slow disk write times or excessive memory allocation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_FileHugePages_bytes"
    },
    "description": "The **node_memory_FileHugePages_bytes** metric measures the total amount of memory, in bytes, allocated to huge pages used by the file system on a Linux node. Huge pages are large, contiguous memory blocks that improve performance by reducing page table overhead. This metric helps monitor the usage and availability of huge pages, which is critical for applications relying on large memory pages. Tracking this value aids in identifying memory pressure or configuration issues related to huge page allocation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_FilePmdMapped_bytes"
    },
    "description": "The node_memory_FilePmdMapped_bytes metric measures the amount of memory mapped by the File PMD (Page Map Directory) in bytes on a Linux node. The File PMD is responsible for managing file-backed mappings, which are used to map files into memory. This metric can be used to monitor and troubleshoot issues related to memory usage, such as high memory consumption or memory leaks. It may also indicate potential problems with file system performance or disk I/O. In monitoring or alerting, this metric could be used to trigger alerts when the mapped memory exceeds a certain threshold, indicating potential memory-related issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_HardwareCorrupted_bytes"
    },
    "description": "The node_memory_HardwareCorrupted_bytes metric measures the total number of bytes that are considered corrupted or unreliable by the system's hardware. This value is typically reported by the Linux kernel and represents memory that has been marked as faulty due to various reasons such as hardware errors, thermal throttling, or other issues. High values for this metric may indicate a problem with the system's hardware, potentially leading to data corruption, crashes, or other reliability issues. Monitoring this metric can help identify potential hardware problems early on, allowing for proactive maintenance and reducing downtime.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_HugePages_Free"
    },
    "description": "The **node_memory_HugePages_Free** metric in Prometheus reports the current number of free Huge Pages available on a Linux node. Huge Pages are large memory pages (typically 2MB or larger) that reduce overhead in memory management and improve performance for memory-intensive applications such as databases or virtualization.\n\n**Purpose:**  \nThis metric helps SREs monitor the availability of Huge Pages to ensure that applications relying on them have sufficient memory resources. Tracking this metric can prevent performance degradation caused by insufficient Huge Pages or wasted memory due to over-provisioning.\n\n**Alert Thresholds:**  \n- **Low threshold:** Trigger an alert if `node_memory_HugePages_Free` falls below a critical value (e.g., less than 10% of total Huge Pages) for a sustained period (e.g., 5 minutes). This indicates the system is running low on Huge Pages, which may cause application slowdowns or failures.  \n- **High threshold:** While less common, consistently high free Huge Pages (e.g., >90% free) may indicate over-allocation, leading to inefficient memory usage.\n\n**Impact of Values:**  \n- **Low values:** Risk of Huge Page exhaustion, causing applications to fallback to regular pages, increasing TLB misses and reducing performance. May also lead to application crashes if Huge Pages are mandatory.  \n- **High values:** Potential underutilization of reserved Huge Pages, wasting memory that could be used elsewhere.\n\n**Example Alert Rule:**  \n```yaml\nalert: LowHugePagesFree\nexpr: (node_memory_HugePages_Free / node_memory_HugePages_Total) < 0.1\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Low Huge Pages Free on {{ $labels.instance }}\"\n  description: \"Free Huge Pages are below 10% for more than 5 minutes. This may impact performance of memory-intensive applications.\"\n```\n\n**Dashboard Usage:**  \nVisualize `node_memory_HugePages_Free` alongside `node_memory_HugePages_Total` and `node_memory_HugePages_Rsvd` to track Huge Page allocation and usage trends. Use percentage free to quickly assess if Huge Pages are becoming scarce or underutilized. Combine with other memory metrics like `node_memory_MemFree` and `node_memory_SwapFree` for a holistic view of system memory health.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_HugePages_Rsvd"
    },
    "description": "The **node_memory_HugePages_Rsvd** metric measures the number of Huge Pages reserved on a Linux node but not yet allocated to any process. It is reported as a count of pages, where each page is typically 2MB or larger depending on system configuration. This reserved memory is set aside for future use by applications requiring Huge Pages but is not currently in use. Monitoring this metric helps identify potential memory allocation inefficiencies or resource constraints related to Huge Pages.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_HugePages_Surp"
    },
    "description": "The **node_memory_HugePages_Surp** metric measures the number of surplus huge pages on a Linux node. It represents the count of huge pages that have been allocated but are currently unused. Huge pages are large memory blocks, typically 2MB or 1GB in size, used to optimize performance for certain applications. This metric helps identify inefficient memory allocation by showing excess huge pages that may lead to wasted resources.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_HugePages_Total"
    },
    "description": "The node_memory_HugePages_Total metric measures the total number of huge pages allocated on a Linux node. Huge pages are large memory blocks that can be used to improve performance by reducing the overhead of traditional page tables. This metric is useful for monitoring and optimizing memory usage, particularly in environments with high memory requirements or where memory-intensive workloads are running. Potential implications include identifying nodes with insufficient huge page allocation, which could lead to performance issues or even crashes. In monitoring or alerting, this metric can be used to trigger notifications when the total number of huge pages falls below a certain threshold, indicating potential memory bottlenecks.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Hugepagesize_bytes"
    },
    "description": "This metric reports the size in bytes of a single huge page configured on the node. Huge pages are large memory pages (commonly 2MB or 1GB) used to reduce overhead in virtual memory management and improve performance for memory-intensive applications. In Prometheus, **node_memory_Hugepagesize_bytes** helps SREs verify that huge pages are configured correctly and consistently across nodes.\n\n**Purpose:**  \n- Confirm the huge page size configured on each node matches application and system requirements.  \n- Detect configuration drift or misconfiguration that could impact memory allocation efficiency.\n\n**Thresholds and Alerts:**  \n- Alert if the huge page size differs from the expected value (e.g., not 2MB or 1GB as per your environment).  \n- Alert if huge pages are expected but the size is reported as zero or missing, indicating huge pages are not enabled.\n\n**Impact of Values:**  \n- **High or expected value:** Indicates huge pages are configured and available, enabling optimized memory usage and reduced TLB misses.  \n- **Zero or unexpected value:** Suggests huge pages are disabled or misconfigured, potentially causing increased memory fragmentation, higher latency, and degraded application performance.\n\n**Example Usage:**  \n- **Dashboard:** Display the huge page size per node alongside huge page usage metrics to ensure consistency and capacity planning.  \n- **Alert Rule Example:**  \n```yaml\nalert: HugePageSizeMismatch  \nexpr: node_memory_Hugepagesize_bytes != 2097152  # expecting 2MB huge pages  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"Huge page size mismatch on {{ $labels.instance }}\"  \n  description: \"Expected huge page size is 2MB, but node reports {{ $value }} bytes. Verify huge page configuration.\"  \n```\n\nMonitoring **node_memory_Hugepagesize_bytes** ensures huge pages are properly configured, helping maintain optimal memory performance and preventing issues related to memory allocation inefficiencies.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Hugetlb_bytes"
    },
    "description": "The node_memory_Hugetlb_bytes metric measures the total amount of memory allocated to HugeTLB pages on a given node. HugeTLB is a mechanism that allows for large page allocations, which can be beneficial for certain workloads such as databases or in-memory caching applications. This metric provides insight into how much memory is being utilized by HugeTLB and can help identify potential issues related to memory allocation or fragmentation. Potential implications of high values include memory exhaustion, performance degradation due to excessive page table lookups, or resource competition with other processes. In monitoring or alerting, this metric could be used to detect unusual memory usage patterns, trigger notifications for potential memory-related issues, or inform capacity planning decisions.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Inactive_anon_bytes"
    },
    "description": "The node_memory_Inactive_anon_bytes metric measures the amount of inactive anonymous memory in bytes on a Node Exporter host. Anonymous memory refers to memory allocated by the kernel for processes but not directly associated with any specific process. Inactive anonymous memory is a subset of this, representing memory that has been allocated but is currently not being used or referenced by any process. This metric can be useful in monitoring and troubleshooting memory-related issues on the system, such as identifying potential memory leaks or detecting when a system is running low on available memory. It may also be used to set thresholds for alerting when inactive anonymous memory exceeds a certain threshold, indicating potential performance degradation or resource exhaustion.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Inactive_bytes"
    },
    "description": "The node_memory_Inactive_bytes metric measures the amount of inactive memory on a node in bytes. Inactive memory refers to the portion of RAM that is not currently being used by running processes but has been allocated for future use. This can include memory that has been reserved for system calls, file buffers, or other purposes. High levels of inactive memory may indicate inefficient memory usage patterns, while low levels could suggest insufficient memory allocation. This metric can be used to monitor and alert on potential memory bottlenecks, optimize resource utilization, and troubleshoot performance issues related to memory allocation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Inactive_file_bytes"
    },
    "description": "The node_memory_Inactive_file_bytes metric measures the amount of memory on a Node that is allocated to files but not currently in use. This includes memory used by file system metadata, such as inode tables and buffer caches. High values for this metric may indicate inefficient file system usage or potential issues with disk I/O performance. It can be used to monitor and alert on situations where the inactive file cache is consuming a large portion of available memory, potentially leading to performance degradation or even crashes. This metric can also be useful in identifying nodes that require more memory or better storage configurations.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_KReclaimable_bytes"
    },
    "description": "The node_memory_KReclaimable_bytes metric measures the amount of memory that is reclaimable by the kernel in bytes. This includes memory that has been allocated to processes but is no longer needed and can be freed up for other uses. It's a subset of the total memory available on the system, excluding memory that is currently being used by running processes or is reserved for system purposes. High values may indicate inefficient memory usage patterns, while low values could suggest sufficient memory availability. This metric can be used to monitor memory utilization and identify potential issues with kernel memory management.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_KernelStack_bytes"
    },
    "description": "The node_memory_KernelStack_bytes metric measures the amount of memory used by the kernel stack in bytes. This includes the memory allocated for kernel threads and other kernel-related activities. It is a critical component of system performance as it directly affects the system's ability to handle concurrent tasks and interrupts. High values may indicate inefficient use of resources, potential deadlocks, or even kernel-level issues that require immediate attention. This metric can be used in monitoring and alerting to detect anomalies in kernel stack memory usage, enabling proactive measures to prevent system crashes or performance degradation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Mapped_bytes"
    },
    "description": "The **node_memory_Mapped_bytes** metric measures the total amount of memory, in bytes, that the Linux kernel has mapped into the process address space. This includes memory used for memory-mapped files, shared libraries, and other kernel mappings. It reflects how much virtual memory is actively mapped to physical memory pages. Monitoring this metric helps identify unusual memory usage related to mapped files or libraries.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_MemAvailable_bytes"
    },
    "description": "The **node_memory_MemAvailable_bytes** metric measures the estimated amount of memory available for use on a node, expressed in bytes. This value reflects memory that can be allocated to applications without causing the system to swap or run out of memory. It excludes memory reserved for the kernel and system processes. Monitoring this metric helps ensure sufficient free memory is available for applications and can trigger alerts when memory availability is low.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_MemFree_bytes"
    },
    "description": "The node_memory_MemFree_bytes metric measures the amount of free memory available on a node in bytes. This value represents the total amount of system RAM that is not currently allocated to any process or kernel component. It can be used as an indicator of available memory for new processes, and low values may indicate memory pressure or potential issues with resource allocation. In monitoring or alerting, this metric can be used to detect situations where free memory falls below a certain threshold, potentially leading to performance degradation or even system crashes.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_MemTotal_bytes"
    },
    "description": "**node_memory_MemTotal_bytes** reports the total physical memory (RAM) installed on a node, measured in bytes. In Prometheus, this metric serves as a baseline to understand the node\u2019s memory capacity and to calculate memory usage percentages when combined with other memory metrics (e.g., free or available memory). Since this value is static for a given node, it does not itself fluctuate, but it is essential for setting meaningful thresholds on memory consumption.\n\n**Alerting guidance:** While **node_memory_MemTotal_bytes** alone does not trigger alerts, it is used to define thresholds for memory usage alerts. For example, an alert should fire if the node\u2019s available memory falls below 20% of **node_memory_MemTotal_bytes**, indicating less than 20% free memory remaining, which risks application performance degradation or crashes.\n\n**Impact:**  \n- A high **node_memory_MemTotal_bytes** value indicates a node with more RAM capacity, capable of handling larger workloads or more applications.  \n- A low value indicates limited memory resources, which may require workload balancing or scaling decisions.\n\n**Example usage:**  \nTo monitor memory pressure, combine this metric with **node_memory_MemAvailable_bytes** in an alert rule:\n\n```yaml\nalert: LowNodeMemoryAvailable\nexpr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) < 0.20\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Node memory available is below 20%\"\n  description: \"Node {{ $labels.instance }} has less than 20% memory available for over 5 minutes.\"\n```\n\nIn dashboards, use **node_memory_MemTotal_bytes** as a reference to calculate and visualize memory usage percentages, enabling quick comparison of memory pressure across nodes.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Mlocked_bytes"
    },
    "description": "The node_memory_Mlocked_bytes metric measures the amount of memory locked by a process on the node. This value represents the total number of bytes that are currently locked and cannot be swapped out to disk. Memory locking is typically used by applications that require low-latency access to data, such as databases or real-time systems. High values for this metric may indicate resource-intensive processes consuming excessive memory resources, potentially leading to performance issues or even crashes. This metric can be used in monitoring and alerting to detect potential memory bottlenecks, identify resource-hungry applications, or optimize system configuration for better performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_NFS_Unstable_bytes"
    },
    "description": "The **node_memory_NFS_Unstable_bytes** metric in Prometheus tracks the total amount of memory (in bytes) allocated to NFS unstable pages on a node. These unstable pages represent data that has been written to memory but not yet committed to stable storage, making them vulnerable to loss if the system crashes or experiences memory pressure.\n\n**Purpose:**  \nThis metric helps SREs monitor the memory footprint of NFS write operations that are pending flush to disk. Elevated values can indicate heavy NFS write activity or potential delays in committing data to storage, which may lead to increased latency or data integrity risks.\n\n**Thresholds and Alerting:**  \n- **Warning threshold:** When unstable memory usage exceeds 100 MB (1.0e8 bytes), it suggests growing memory pressure related to NFS writes.  \n- **Critical threshold:** Usage above 500 MB (5.0e8 bytes) indicates a high risk of memory exhaustion or delayed disk writes, warranting immediate investigation.\n\n**Impact of Values:**  \n- **Low values:** Normal operation with efficient flushing of NFS data to disk.  \n- **High values:** Potential bottlenecks in disk I/O or network congestion causing unstable pages to accumulate, which can degrade performance and increase the risk of OOM events or data loss.\n\n**Example Usage:**  \n- **Dashboard:** Plot `node_memory_NFS_Unstable_bytes` over time alongside disk I/O and network throughput metrics to correlate spikes in unstable memory with system activity.  \n- **Alert Rule (Prometheus):**  \n  ```yaml\n  alert: HighNFSUnstableMemory\n  expr: node_memory_NFS_Unstable_bytes > 5.0e8\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"High NFS unstable memory usage on {{ $labels.instance }}\"\n    description: \"NFS unstable memory usage has exceeded 500 MB for more than 5 minutes, indicating potential disk or network issues.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_PageTables_bytes"
    },
    "description": "The node_memory_PageTables_bytes metric measures the total amount of memory used by page tables in bytes on a given node. Page tables are data structures that manage virtual-to-physical address translations for processes running on the system. High values or sudden spikes in this metric may indicate excessive memory usage, inefficient memory allocation, or potential issues with process creation and management. This metric can be used to monitor and alert on memory-related performance issues, such as slow process startup times, high memory consumption, or system crashes due to out-of-memory conditions.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Percpu_bytes"
    },
    "description": "The **node_memory_Percpu_bytes** metric measures the amount of memory, in bytes, allocated to per-CPU kernel data structures on a node. It reflects the memory used individually by each CPU for kernel operations. This metric helps monitor how memory is distributed across CPUs and can identify imbalances or excessive per-CPU memory usage. It is useful for detecting potential performance issues related to per-CPU memory consumption.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_SReclaimable_bytes"
    },
    "description": "The node_memory_SReclaimable_bytes metric measures the amount of memory that can be reclaimed by the kernel without involving the user space. This includes memory that has been allocated to processes but is no longer in use and can be freed up for other purposes. It represents a subset of the memory that is considered reclaimable, specifically the 'slab' memory which is used for caching and other system services. High values or increasing trends may indicate inefficient memory usage, potential memory leaks, or issues with kernel-level caching mechanisms.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_SUnreclaim_bytes"
    },
    "description": "The **node_memory_SUnreclaim_bytes** metric in Prometheus represents the amount of kernel memory that cannot be reclaimed or freed by the system\u2019s memory reclamation processes. This memory is typically used by kernel modules or drivers that have allocated memory which remains locked and unavailable for reuse until the system is rebooted or the module is unloaded.\n\n**Purpose:**  \nThis metric helps SREs monitor the portion of memory that is effectively \"lost\" to the system because it cannot be reclaimed, which can contribute to overall memory pressure. Tracking this metric is crucial for identifying kernel memory leaks or misbehaving drivers that cause memory to be permanently reserved.\n\n**Thresholds and Alerting:**  \n- A typical alert threshold might be when **node_memory_SUnreclaim_bytes** exceeds 10-15% of total system memory, indicating abnormal kernel memory usage.  \n- For example, if total memory is 16GB, an alert could trigger if **node_memory_SUnreclaim_bytes** exceeds 1.5GB.  \n- Thresholds should be adjusted based on baseline observations of your environment.\n\n**Impact of Values:**  \n- **High values:** Indicate that a significant amount of memory is locked and unavailable for general use, which can reduce the memory available for applications and lead to increased memory pressure, swapping, or even out-of-memory (OOM) conditions. Persistent high values may point to kernel memory leaks or faulty drivers.  \n- **Low values:** Indicate normal kernel memory usage with most memory available for reclaiming and allocation.\n\n**Example Usage:**  \n- **Dashboard:** Display **node_memory_SUnreclaim_bytes** alongside total memory and other memory metrics to visualize kernel memory pressure trends over time.  \n- **Alert Rule (PromQL):**  \n  ```promql\n  node_memory_SUnreclaim_bytes / node_memory_MemTotal_bytes > 0.15\n  ```  \n  This alert fires when unreclaimable kernel memory exceeds 15% of total memory, signaling potential kernel memory leaks or issues requiring investigation.\n\nBy monitoring **node_memory_SUnreclaim_bytes**, SREs can proactively detect and troubleshoot kernel memory issues before they impact system stability or application performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_SecPageTables_bytes"
    },
    "description": "The node_memory_SecPageTables_bytes metric measures the amount of memory used by secure page tables in bytes on a given node. Secure page tables are a type of kernel data structure that maps virtual addresses to physical frame numbers for secure processes. This metric can be used to monitor and troubleshoot issues related to memory usage, particularly in environments where secure processes are running. Potential implications include identifying memory bottlenecks, detecting anomalies in secure process behavior, or triggering alerts when secure page table memory usage exceeds a certain threshold.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_ShmemHugePages_bytes"
    },
    "description": "The **node_memory_ShmemHugePages_bytes** metric reports the total bytes of Huge Pages allocated to Shared Memory (Shmem) on a Linux node. In Prometheus, this metric helps SREs monitor the usage of large, contiguous memory blocks shared between processes, which are critical for performance-sensitive applications relying on efficient inter-process communication.\n\n**Purpose:** Track the consumption of Huge Pages in Shmem to detect abnormal memory usage patterns that could lead to resource contention or degraded system performance.\n\n**Alert Threshold:** Trigger an alert if the value consistently exceeds 80% of the node\u2019s configured Huge Pages for Shmem, indicating potential memory pressure or misconfiguration. For example, if the node has 1 GB of Huge Pages reserved for Shmem, alert when usage surpasses 800 MB over a 5-minute window.\n\n**Impact of Values:**\n- **High values:** May indicate heavy use of shared Huge Pages, risking memory exhaustion, increased latency, or allocation failures.\n- **Low or zero values:** Could suggest underutilization or misconfiguration, possibly leading to inefficient memory usage or application performance issues.\n\n**Example Alert Rule:**\n```yaml\nalert: HighShmemHugePagesUsage\nexpr: node_memory_ShmemHugePages_bytes / node_memory_HugePages_Total_bytes > 0.8\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High Shared Memory Huge Pages usage on {{ $labels.instance }}\"\n  description: \"Shared Memory Huge Pages usage is above 80% for more than 5 minutes.\"\n```\n\n**Example Dashboard Usage:** Display this metric alongside total Huge Pages and application memory usage to correlate shared memory consumption with application behavior and identify potential bottlenecks.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_ShmemPmdMapped_bytes"
    },
    "description": "The **node_memory_ShmemPmdMapped_bytes** metric measures the total number of bytes mapped by the kernel's shared memory (SHM) mechanism using process-private mappings. It represents the size of shared memory segments mapped into a process's virtual address space, expressed in bytes. This metric includes memory that may not be currently resident in physical RAM. Monitoring this value helps assess the usage and efficiency of shared memory mappings in the system.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Shmem_bytes"
    },
    "description": "The **node_memory_Shmem_bytes** metric measures the total amount of shared memory currently used on a Linux system, expressed in bytes. Shared memory is a segment of RAM that multiple processes can access simultaneously for efficient data sharing. This metric helps track the usage of shared memory resources and can indicate potential issues like memory leaks or resource contention. Monitoring this value aids in detecting abnormal shared memory consumption that may affect system performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Slab_bytes"
    },
    "description": "The **node_memory_Slab_bytes** metric measures the total amount of memory, in bytes, used by slab caches on a Linux system. Slab caches are kernel-managed memory pools that store frequently used data structures to improve performance. Monitoring this metric helps track kernel memory usage and detect abnormal increases that may indicate memory leaks or fragmentation. It is useful for identifying potential system performance issues related to kernel memory allocation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_SwapCached_bytes"
    },
    "description": "The **node_memory_SwapCached_bytes** metric in Prometheus represents the amount of memory that was previously swapped out to disk but is currently cached in swap space, allowing faster retrieval without accessing the slower swap device again. This metric helps SREs understand how much swapped memory is being actively reused, reflecting the efficiency of memory management on a Linux node.\n\n**Purpose:**  \n- Monitor the usage of swap cache to assess memory pressure and swapping behavior.  \n- Identify if the system is frequently swapping memory pages back and forth, which can degrade performance.\n\n**Impact of Values:**  \n- **High values:** Indicate significant memory pages are cached in swap, suggesting the system is under memory pressure and swapping is active. This can lead to increased I/O latency and degraded application performance. Persistent high swap cache usage may warrant investigation into memory allocation or workload optimization.  \n- **Low or zero values:** Typically normal on systems with sufficient RAM and low swapping activity. However, a sudden drop from high values might indicate a system reboot or swap cache being cleared.\n\n**Alert Threshold Guidance:**  \n- Trigger an alert if **node_memory_SwapCached_bytes** exceeds a threshold (e.g., > 100MB) for a sustained period (e.g., 5 minutes), indicating excessive swapping and potential memory pressure.  \n- Combine with other metrics like **node_memory_MemAvailable_bytes** and **node_memory_SwapFree_bytes** to confirm memory exhaustion or swap usage issues.\n\n**Example Alert Rule:**  \n```yaml\nalert: HighSwapCacheUsage\nexpr: node_memory_SwapCached_bytes > 1e8  # 100MB\n  and node_memory_SwapFree_bytes < 5e7    # less than 50MB swap free\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High swap cache usage on {{ $labels.instance }}\"\n  description: \"Swap cache usage is above 100MB for more than 5 minutes, indicating potential memory pressure and swapping.\"\n```\n\n**Dashboard Usage:**  \n- Visualize **node_memory_SwapCached_bytes** alongside **node_memory_SwapFree_bytes** and **node_memory_MemAvailable_bytes** to correlate swap cache usage with overall memory availability.  \n- Use time-series graphs to detect trends or spikes in swap cache, helping diagnose memory bottlenecks or inefficient memory usage patterns.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_SwapFree_bytes"
    },
    "description": "The node_memory_SwapFree_bytes metric measures the amount of free swap memory available on a node in bytes. Swap memory is a type of virtual memory that is stored on disk and used to supplement physical RAM when the system runs low on memory. This metric can be used to monitor the available swap space, which can indicate potential issues with memory utilization or disk storage capacity. High values may indicate sufficient swap space, while low values could suggest insufficient swap space, potentially leading to performance degradation or even system crashes. This metric is useful for monitoring and alerting purposes, such as detecting when a node's swap space is running low, allowing for proactive measures like increasing swap space allocation or optimizing memory usage.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_SwapTotal_bytes"
    },
    "description": "The **node_memory_SwapTotal_bytes** metric measures the total size of swap space configured on a Linux node, expressed in bytes. Swap space is disk storage reserved to extend physical memory by temporarily holding inactive pages from RAM. This metric indicates the maximum amount of swap available for use by the system. Monitoring it helps assess system memory configuration and capacity for virtual memory usage.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Unevictable_bytes"
    },
    "description": "The node_memory_Unevictable_bytes metric measures the amount of memory on a Node that is reserved for system purposes and cannot be reclaimed by the kernel's memory reclamation mechanisms. This includes memory allocated to hardware components such as device drivers, firmware, and other low-level system resources. High values may indicate issues with system resource utilization or configuration problems. Potential implications include alerting on high memory usage, investigating system resource bottlenecks, or optimizing system configurations to reclaim unused memory.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_VmallocChunk_bytes"
    },
    "description": "The node_memory_VmallocChunk_bytes metric measures the amount of memory allocated by the Linux kernel's vmalloc mechanism in bytes. The vmalloc mechanism is used to allocate large blocks of memory that are not contiguous in physical memory. This can be a critical resource for systems with limited physical memory, as it allows them to use virtual memory to access larger amounts of data. High values or increasing trends in this metric may indicate issues with memory fragmentation, kernel performance, or system resource utilization. It could also be used to detect potential memory leaks or misconfigurations that lead to excessive vmalloc usage.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_VmallocTotal_bytes"
    },
    "description": "The node_memory_VmallocTotal_bytes metric measures the total amount of virtual memory that can be allocated by the Linux kernel on a given node. This value represents the maximum size of virtual memory available for use by processes running on the system. It is expressed in bytes and includes both anonymous memory (e.g., malloc() allocations) and file-backed memory (e.g., mmap() of files). High values may indicate that the system has sufficient virtual memory to handle large workloads, while low values could suggest a potential issue with available virtual address space. This metric can be used in monitoring or alerting scenarios to detect issues related to virtual memory exhaustion or insufficient allocation capacity.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_VmallocUsed_bytes"
    },
    "description": "The node_memory_VmallocUsed_bytes metric measures the amount of memory used by the Virtual Memory (VM) allocator on a Linux system. This includes memory allocated for various kernel data structures and other internal purposes. It does not account for user-space memory usage. High values may indicate inefficient memory allocation or potential issues with kernel performance. This metric can be used to monitor and alert on excessive VM memory usage, which could lead to system crashes or stability issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_WritebackTmp_bytes"
    },
    "description": "The node_memory_WritebackTmp_bytes metric measures the amount of temporary writeback memory used by the Linux kernel on a given node. This value represents the amount of memory allocated for writeback purposes, which is typically used to cache dirty pages that need to be written back to disk. High values may indicate issues with disk I/O performance or storage capacity, potentially leading to system slowdowns or crashes. Monitoring this metric can help identify potential bottlenecks in the system's storage infrastructure and inform decisions about resource allocation or optimization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Writeback_bytes"
    },
    "description": "The node_memory_Writeback_bytes metric measures the amount of memory that has been written back to disk from the page cache in bytes. This value indicates the amount of data that has been evicted from the page cache and written back to storage, which can be a sign of system resource constraints or inefficient memory usage. Potential implications for monitoring include: \n\n* High values may indicate that the system is experiencing memory pressure, leading to decreased performance.\n* Sustained high values could suggest issues with disk I/O or storage capacity.\n* This metric can also be used in conjunction with other metrics (e.g., node_memory_MemFree_bytes) to monitor overall system memory usage and identify potential bottlenecks.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Zswap_bytes"
    },
    "description": "The node_memory_Zswap_bytes metric measures the total amount of memory used by Zswap compression in bytes on a given node. Zswap is a Linux kernel feature that compresses swap pages to reduce memory usage and improve performance. This metric can be used to monitor the effectiveness of Zswap in managing memory resources, identify potential bottlenecks or issues with Zswap configuration, and inform decisions about scaling or optimizing system resources. It may also serve as an indicator for potential memory-related problems or resource exhaustion on the node.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_memory_Zswapped_bytes"
    },
    "description": "The node_memory_Zswapped_bytes metric measures the total amount of bytes used by the Z-Swap (compressed swap) mechanism on a Linux node. This metric is part of the memory information collected by Prometheus from the node exporter. The Z-Swap mechanism is a feature in some Linux kernels that allows for compressed swapping, which can help reduce disk usage and improve performance. High values or increasing trends in this metric may indicate issues with system resource utilization, such as insufficient swap space or inefficient use of available resources. This metric can be used to monitor the health of the node's memory management and trigger alerts when thresholds are exceeded or anomalies occur.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Icmp6_InErrors"
    },
    "description": "The node_netstat_Icmp6_InErrors metric measures the number of ICMPv6 error messages received by a node on its incoming network interface. This includes errors such as 'Destination Unreachable', 'Packet Too Big', and 'Parameter Problem'. In monitoring or alerting, this metric can be used to detect potential issues with network connectivity, routing, or packet transmission. High values may indicate problems with neighboring nodes, misconfigured firewalls, or other network-related issues that require investigation.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Icmp6_InMsgs"
    },
    "description": "The node_netstat_Icmp6_InMsgs metric measures the number of incoming ICMPv6 messages received by a node. This includes error reports, echo requests, and other types of ICMPv6 packets that are directed towards the node. In monitoring or alerting contexts, this metric can be used to detect potential issues with network connectivity, packet loss, or firewall configurations. For example, an unusual spike in incoming ICMPv6 messages could indicate a denial-of-service (DoS) attack or a misconfigured firewall rule. Conversely, a steady decline in incoming ICMPv6 messages might suggest a problem with the node's network interface or routing configuration.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Icmp6_OutMsgs"
    },
    "description": "The node_netstat_Icmp6_OutMsgs metric measures the number of outgoing ICMPv6 messages sent by a node. This includes error messages, such as Destination Unreachable and Packet Too Big, as well as informational messages like Echo Reply. It does not include incoming ICMPv6 messages. This metric can be used to monitor network connectivity issues or identify potential problems with routing or firewall configurations. For example, an increase in outgoing ICMPv6 error messages may indicate a problem with a neighboring node's routing table or a misconfigured firewall rule. In monitoring and alerting, this metric could trigger alerts for unusual spikes in outgoing ICMPv6 traffic, helping operations teams to quickly identify and resolve network connectivity issues.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Icmp_InErrors"
    },
    "description": "The node_netstat_Icmp_InErrors metric measures the number of ICMP (Internet Control Message Protocol) error messages received by a node. This includes errors such as destination unreachable, time exceeded, and parameter problem. It is a statistic collected from the netstat command on the node, which provides information about network socket statistics. In monitoring or alerting, this metric can be used to detect potential issues with network connectivity or configuration, such as misconfigured firewalls or routing problems. High values of IcmpInErrors may indicate that packets are being sent to unreachable destinations, causing errors and potentially impacting application performance.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Icmp_InMsgs"
    },
    "description": "The node_netstat_Icmp_InMsgs metric measures the total number of incoming ICMP (Internet Control Message Protocol) messages received by a node. ICMP is used for error-reporting and diagnostic functions in the Internet layer of the TCP/IP protocol suite. This metric can be useful in monitoring network connectivity, packet loss, or other issues related to ICMP traffic. Potential implications include identifying potential security threats, detecting network congestion, or troubleshooting connectivity problems between nodes.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Icmp_OutMsgs"
    },
    "description": "The node_netstat_Icmp_OutMsgs metric measures the number of outgoing ICMP (Internet Control Message Protocol) messages sent by a node. This includes error-reporting and informational messages such as echo replies, destination unreachable notifications, and time exceeded notifications. The metric can be used to monitor network connectivity issues, detect anomalies in ICMP traffic patterns, or identify potential security threats. It may also indicate problems with the node's routing configuration or neighboring nodes' responsiveness.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Ip6_InOctets"
    },
    "description": "The node_netstat_Ip6_InOctets metric measures the total number of incoming IPv6 octets (8-bit bytes) received by the system's network interfaces. This metric is collected from the 'Ip6InOctets' field in the netstat output, which provides a snapshot of the system's network statistics. In monitoring and observability, this metric can be used to detect potential issues with IPv6 connectivity or data transfer rates. For example, if the rate of incoming IPv6 octets is consistently high or increasing rapidly, it may indicate a denial-of-service (DoS) attack or an issue with the system's network configuration. Conversely, a sudden drop in incoming IPv6 octets could suggest a problem with the underlying network infrastructure or a misconfiguration of the system's network interfaces.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Ip6_OutOctets"
    },
    "description": "The **node_netstat_Ip6_OutOctets** metric in Prometheus tracks the cumulative number of octets (8-bit bytes) transmitted over IPv6 network interfaces on a node. It reflects the volume of outbound IPv6 traffic and helps SREs monitor network utilization and detect anomalies in IPv6 data transmission.\n\n**Purpose:**  \nUse this metric to understand IPv6 network load, identify unusual spikes or drops in outbound traffic, and ensure IPv6 connectivity is performing as expected.\n\n**Threshold guidance:**  \nSet alert thresholds based on typical traffic patterns for your environment. For example, trigger a warning if outbound IPv6 traffic exceeds 80% of the network interface\u2019s capacity over a sustained period (e.g., 5 minutes), which may indicate congestion or potential DDoS activity. Conversely, a sudden drop to near zero could signal network outages or misconfigurations.\n\n**Impact of values:**  \n- **High values:** May indicate heavy IPv6 usage, potential network saturation, or abnormal traffic such as data exfiltration or attacks.  \n- **Low or zero values:** Could suggest network interface issues, IPv6 routing problems, or that IPv6 traffic is not being generated as expected.\n\n**Example usage:**  \n- **Dashboard:** Plot the rate of increase (`rate(node_netstat_Ip6_OutOctets[5m])`) to visualize outbound IPv6 traffic throughput over time.  \n- **Alert rule example:**  \n```yaml\nalert: HighIPv6OutboundTraffic  \nexpr: rate(node_netstat_Ip6_OutOctets[5m]) > 80000000  # e.g., 80 Mbps threshold  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"High IPv6 outbound traffic on {{ $labels.instance }}\"  \n  description: \"Outbound IPv6 traffic has exceeded 80 Mbps for more than 5 minutes, which may indicate network congestion or abnormal activity.\"  \n```\n\nThis actionable description enables SREs to effectively monitor, alert, and respond to IPv6 network traffic conditions using the **node_netstat_Ip6_OutOctets** metric.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_IpExt_InOctets"
    },
    "description": "The node_netstat_IpExt_InOctets metric measures the total number of incoming IP packets (in octets) at the network interface level, as reported by the IpExtInOctets statistic in the Linux kernel's netstat output. This metric provides insight into the amount of incoming network traffic received by the system, which can be useful for monitoring and troubleshooting purposes.\n\nPotential implications or usage in monitoring or alerting include:\n- Identifying potential network bottlenecks or congestion issues.\n- Detecting anomalies in incoming traffic patterns that may indicate a security threat or misconfiguration.\n- Correlating with other metrics to understand the overall system performance and resource utilization.\n\nNote: This metric is specific to Linux systems and may not be applicable to other operating systems.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_IpExt_OutOctets"
    },
    "description": "The node_netstat_IpExt_OutOctets metric measures the total number of octets (8-bit bytes) sent out by the IP extension agent on a node. This metric is collected from the 'IpExtOutOctets' field in the netstat output, which provides information about network interface statistics. The metric can be used to monitor and track the amount of outgoing network traffic generated by the system, helping to identify potential issues with network connectivity or data transfer rates. It may also be useful for capacity planning and forecasting future network requirements.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Ip_Forwarding"
    },
    "description": "The node_netstat_Ip_Forwarding metric measures whether IP forwarding is enabled on a given node. This setting determines whether the node can act as a router for other devices on the network, allowing them to forward packets between different networks. In monitoring and observability contexts, this metric can be used to ensure that nodes are configured correctly for their intended role in the network infrastructure. Potential implications include identifying misconfigured nodes, detecting potential security vulnerabilities, or verifying compliance with network architecture standards.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_ListenDrops"
    },
    "description": "The node_netstat_TcpExt_ListenDrops metric measures the number of TCP listen drops on a node. This statistic is part of the TcpExt group in the netstat output and represents the number of times a connection was refused because the listener was already in use, resulting in a dropped connection attempt. In monitoring or alerting, this metric can be used to detect potential issues with TCP listening sockets, such as resource exhaustion or configuration errors. High values may indicate that the system is unable to handle incoming connections, leading to service disruptions or performance degradation.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_ListenOverflows"
    },
    "description": "The node_netstat_TcpExt_ListenOverflows metric measures the number of TCP listen overflows on a node. This occurs when the system's listening queue for incoming connections is full and cannot accept any more connections, resulting in dropped packets. High values may indicate network congestion, misconfigured systems, or resource constraints. Potential implications include increased latency, packet loss, or even service unavailability. Monitoring this metric can help identify potential issues with TCP connection handling and inform capacity planning or tuning efforts to prevent overflows.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_SyncookiesFailed"
    },
    "description": "The node_netstat_TcpExt_SyncookiesFailed metric measures the number of TCP SYN packets that were rejected due to a lack of available socket space on the system. This can occur when the system is under heavy network load and the TCP stack is unable to allocate new sockets quickly enough. High values for this metric may indicate issues with system resource utilization, such as high memory or CPU usage, which are impacting the ability of the system to handle incoming network connections. Potential implications include increased latency, dropped connections, or even complete denial-of-service (DoS) scenarios if left unchecked. This metric can be used in monitoring and alerting to detect potential issues with system resource utilization and prevent service degradation or outages.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_SyncookiesRecv"
    },
    "description": "This metric measures the number of TCP SYN cookies received by a node, as reported by the TcpExtSyncookiesRecv statistic in the netstat command. It indicates the number of incoming TCP connections that were validated using SYN cookies, which are used to prevent SYN flooding attacks. A high value may indicate a potential security issue or a misconfigured system. This metric can be used to monitor and alert on potential security threats, as well as to troubleshoot issues related to TCP connection establishment.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_SyncookiesSent"
    },
    "description": "The node_netstat_TcpExt_SyncookiesSent metric measures the number of SYN cookies sent by the TCP stack on a given node. A SYN cookie is a mechanism used to prevent SYN flooding attacks, where an attacker sends a large number of SYN packets to a server without completing the three-way handshake. When a SYN packet is received, the server responds with a SYN cookie, which contains information about the connection and a random value that must be echoed back by the client in the ACK packet. If the client fails to send the ACK packet or sends an incorrect value, the server will not establish the connection. This metric can be used to monitor for potential SYN flooding attacks or issues with TCP connections on the node. It may also indicate problems with the network stack or configuration.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_TCPOFOQueue"
    },
    "description": "The node_netstat_TcpExt_TCPOFOQueue metric measures the number of TCP sockets in the 'partial close' state, where a connection is being closed but the remote end has not yet acknowledged the closure. This can indicate issues with network connectivity or server resource constraints. Potential implications for monitoring and alerting include: detecting network congestion, identifying server overload, or pinpointing specific connections causing performance degradation.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_TCPSynRetrans"
    },
    "description": "The node_netstat_TcpExt_TCPSynRetrans metric measures the number of TCP SYN retransmissions on a node. This statistic is part of the TcpExt group in the netstat output and represents the total number of times that TCP has retransmitted a SYN segment to initiate a connection. High values may indicate network congestion, packet loss, or issues with the remote server's ability to respond to SYN packets. This metric can be used to detect potential issues with network connectivity, troubleshoot slow connections, or identify bottlenecks in the system. It is recommended to monitor this metric alongside other network-related metrics, such as node_netstat_TcpExt_TCPSynSent and node_netstat_TcpExt_TCPSynRcvd, for a more comprehensive understanding of TCP connection establishment and retransmission behavior.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_TcpExt_TCPTimeouts"
    },
    "description": "The node_netstat_TcpExt_TCPTimeouts metric measures the total number of TCP connections that have timed out on a given node. This can be due to various reasons such as network connectivity issues, server overload, or configuration problems. In monitoring and alerting, this metric can be used to identify potential issues with TCP connection establishment and termination, which may impact application performance and user experience. It can also serve as an indicator for underlying infrastructure problems that need attention from the operations team.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_ActiveOpens"
    },
    "description": "The **node_netstat_Tcp_ActiveOpens** metric tracks the cumulative count of TCP connections that have successfully completed the three-way handshake and transitioned into the established state on a given node. In Prometheus, this metric helps SREs monitor the rate at which new outbound or inbound TCP connections are being actively opened, providing insight into network connection activity and application behavior.\n\n**Purpose:**  \n- To observe connection establishment trends and detect abnormal spikes or drops that may indicate network issues, application misbehavior, or resource constraints.  \n- To identify potential problems such as connection storms, firewall or load balancer misconfigurations, or network congestion affecting connection setup.\n\n**Threshold Guidance:**  \n- A sudden, sustained increase in the rate of active opens (e.g., a 2x or greater spike compared to baseline) may warrant an alert, as it could indicate a connection flood or DoS attack.  \n- Conversely, a sharp drop or near-zero rate over a period when traffic is expected may signal network failures or application issues preventing new connections.  \n- Thresholds should be tailored to typical traffic patterns; for example, alert if the rate of active opens exceeds 1000 connections per minute for more than 5 minutes, or falls below 10 connections per minute during peak hours.\n\n**Impact of Values:**  \n- **High values:** May lead to resource exhaustion (e.g., ephemeral ports, file descriptors), increased CPU/network load, or degraded application performance.  \n- **Low values:** Could indicate connectivity problems, service downtime, or client-side issues reducing connection attempts.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-minute rate of `node_netstat_Tcp_ActiveOpens` (using `rate(node_netstat_Tcp_ActiveOpens[1m])`) alongside other network metrics to visualize connection trends and correlate with application load.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: HighTcpActiveOpensRate\n  expr: rate(node_netstat_Tcp_ActiveOpens[5m]) > 1000\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of TCP active opens detected on {{ $labels.instance }}\"\n    description: \"The rate of TCP active opens has exceeded 1000 connections per minute for over 5 minutes, indicating potential connection storms or network issues.\"\n  ```",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_CurrEstab"
    },
    "description": "The node_netstat_Tcp_CurrEstab metric measures the current number of established TCP connections on a given node. This statistic is collected from the system's netstat command and provides insight into the network activity and resource utilization of the node. It can be used to monitor the overall network health, detect potential issues such as connection bottlenecks or resource exhaustion, and inform capacity planning decisions. Potential implications include alerting on high connection counts that may indicate a denial-of-service (DoS) attack or monitoring for unusual spikes in established connections that could signal a security breach.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_InErrs"
    },
    "description": "The **node_netstat_Tcp_InErrs** metric tracks the total number of TCP input errors on a node, as reported by the netstat command. In Prometheus, this metric helps SREs monitor the health of the node\u2019s TCP networking stack by indicating issues such as malformed packets, checksum errors, or protocol violations that prevent successful TCP data reception.\n\n**Purpose:**  \nUse this metric to detect and troubleshoot network-level problems affecting TCP connections, which can lead to degraded application performance or connectivity failures.\n\n**Alert Threshold:**  \nAn alert should be triggered if the rate of TCP input errors exceeds a low baseline, for example:  \n`rate(node_netstat_Tcp_InErrs[5m]) > 0.1` errors per second sustained over 5 minutes. This threshold may be adjusted based on normal traffic patterns and node roles.\n\n**Impact of Values:**  \n- **Low or zero values:** Indicate healthy TCP input processing with no detected errors.  \n- **Sustained high values:** Suggest persistent network issues such as corrupted packets, faulty NICs, or driver problems, potentially causing packet loss, retransmissions, or connection timeouts that degrade service reliability.\n\n**Example Usage:**  \n- **Dashboard:** Plot `rate(node_netstat_Tcp_InErrs[5m])` over time alongside other network metrics to correlate spikes with application issues.  \n- **Alert Rule:**  \n```yaml\nalert: HighTcpInputErrors  \nexpr: rate(node_netstat_Tcp_InErrs[5m]) > 0.1  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"High TCP input errors detected on {{ $labels.instance }}\"  \n  description: \"TCP input errors have exceeded 0.1 errors/sec for more than 5 minutes, indicating potential network stack issues.\"  \n```\n\nMonitoring this metric proactively helps maintain network reliability and prevents cascading failures in distributed systems.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_InSegs"
    },
    "description": "The **node_netstat_Tcp_InSegs** metric in Prometheus counts the total number of incoming TCP segments received by the system's network stack since the last restart. It reflects the volume of TCP traffic arriving at the node and is useful for monitoring network health and diagnosing connectivity issues.\n\n**Purpose:**  \nThis metric helps SREs track incoming TCP traffic load and detect anomalies such as unusually high traffic that may indicate SYN flood attacks or network scanning, as well as drops in traffic that could signal network outages or interface failures.\n\n**Thresholds and Alerting:**  \n- A sustained **high rate** of incoming TCP segments (e.g., a sudden spike exceeding normal baseline by 2-3x) may indicate a potential SYN flood or DDoS attack and should trigger a warning alert.  \n- A **sharp drop** or near-zero incoming segments over a period (e.g., 5 minutes) could indicate network interface failure or upstream connectivity loss and should trigger a critical alert.\n\n**Impact of Values:**  \n- **High values:** May cause increased CPU/network stack load, degraded application performance, or indicate security threats.  \n- **Low or zero values:** Could mean network connectivity issues, misconfigurations, or that the node is isolated from traffic.\n\n**Example Usage:**  \n- **Alert rule:**  \n  ```yaml\n  - alert: HighIncomingTcpSegments\n    expr: rate(node_netstat_Tcp_InSegs[5m]) > 10000\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"High incoming TCP segments rate detected\"\n      description: \"The node is receiving an unusually high rate of TCP segments (>10,000 per 5 minutes), which may indicate a SYN flood or network anomaly.\"\n  ```\n- **Dashboard:**  \n  Plot the `rate(node_netstat_Tcp_InSegs[1m])` over time to visualize incoming TCP traffic trends and correlate spikes with application or network events.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_OutRsts"
    },
    "description": "The node_netstat_Tcp_OutRsts metric measures the number of TCP resets received by a node over a specified time period. This statistic is collected from the netstat command and represents the total count of TCP resets (RST) received on all network interfaces of the node. A high value for this metric may indicate issues with network connectivity, firewall misconfigurations, or problems with remote servers. It can be used in monitoring to detect potential network problems and trigger alerts when the rate of RSTs exceeds a certain threshold.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_OutSegs"
    },
    "description": "The **node_netstat_Tcp_OutSegs** metric in Prometheus tracks the total number of TCP segments sent out by a node over time, reflecting the volume of outbound TCP traffic handled by the system\u2019s network stack. It helps SREs monitor network activity and identify issues affecting outgoing TCP connections, such as network congestion, firewall misconfigurations, or application-level stalls.\n\n**Purpose:**  \nUse this metric to observe trends and detect anomalies in outbound TCP traffic. A sudden spike may indicate abnormal network usage or potential flooding, while a sharp drop could signal network outages, blocked connections, or application failures.\n\n**Alert Thresholds:**  \n- **High threshold:** Alert if the rate of outgoing TCP segments exceeds a baseline by 2-3x over a sustained period (e.g., `rate(node_netstat_Tcp_OutSegs[5m]) > 3 * avg_over_time(rate(node_netstat_Tcp_OutSegs[5m])[1h])`), which may indicate network flooding or misbehaving applications.  \n- **Low threshold:** Alert if the rate drops below a minimal expected baseline (e.g., `rate(node_netstat_Tcp_OutSegs[5m]) < 0.1 * avg_over_time(rate(node_netstat_Tcp_OutSegs[5m])[1h])`), potentially signaling network outages or application stalls.\n\n**Impact of Values:**  \n- **High values:** Could lead to network congestion, increased latency, or resource exhaustion on the node.  \n- **Low values:** May indicate loss of connectivity, firewall blocking outbound traffic, or application failures preventing TCP communication.\n\n**Example Usage:**  \n- **Dashboard:** Plot `rate(node_netstat_Tcp_OutSegs[5m])` over time alongside other network metrics to correlate outbound TCP traffic with application behavior and network health.  \n- **Alert Rule Example:**  \n```yaml\nalert: HighTcpOutSegs\nexpr: rate(node_netstat_Tcp_OutSegs[5m]) > 3 * avg_over_time(rate(node_netstat_Tcp_OutSegs[5m])[1h])\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High outbound TCP segments on {{ $labels.instance }}\"\n  description: \"The rate of outgoing TCP segments is unusually high, indicating potential network flooding or application issues.\"\n```\n\nBy monitoring this metric with appropriate thresholds and context, SREs can proactively detect and respond to network-related problems affecting application performance and reliability.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_PassiveOpens"
    },
    "description": "The node_netstat_Tcp_PassiveOpens metric measures the number of passive TCP connection attempts initiated by a node in the system. This statistic is collected from the netstat command and represents the total count of incoming TCP connections that were accepted by the system. It does not include active connections established by the local host. In monitoring or alerting, this metric can be used to detect potential security threats or issues with network connectivity. For example, a sudden spike in passive opens might indicate a brute-force attack on the node's services. Conversely, a steady increase over time could suggest a legitimate growth in service usage. It is essential to consider other related metrics, such as node_netstat_Tcp_ActiveOpens and node_netstat_Tcp_Established, for a comprehensive understanding of TCP connection activity.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Tcp_RetransSegs"
    },
    "description": "The node_netstat_Tcp_RetransSegs metric measures the number of TCP segments that have been retransmitted by a node due to packet loss or other transmission errors. This statistic is collected from the netstat command and provides insight into the reliability of network connections. High values may indicate issues with network connectivity, packet loss, or congestion, which can impact application performance and user experience. Potential implications for monitoring include: (1) Identifying nodes with high retransmission rates to investigate underlying network issues; (2) Correlating this metric with other network-related metrics to diagnose root causes of problems; (3) Setting alerts for threshold breaches to notify operations teams of potential network issues.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp6_InDatagrams"
    },
    "description": "The node_netstat_Udp6_InDatagrams metric measures the total number of UDP-6 datagrams received by the system and queued for processing. This value represents the amount of incoming IPv6 UDP traffic that has been successfully delivered to the kernel, but not yet processed by any application or service. A high value may indicate a potential issue with network congestion, firewall rules, or application performance, while a low value could suggest issues with packet loss or corruption. This metric can be used in monitoring and alerting to detect anomalies in UDP-6 traffic patterns, identify potential bottlenecks, or troubleshoot connectivity issues between nodes.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp6_InErrors"
    },
    "description": "The node_netstat_Udp6_InErrors metric measures the number of errors encountered while receiving UDPv6 packets on a node. This statistic is collected from the netstat command and represents the total count of errors that have occurred since the system was started or since the last reset. In the context of network operations, this metric can be used to identify potential issues with UDPv6 connectivity, such as misconfigured firewalls, routing problems, or faulty network hardware. It may also indicate a denial-of-service (DoS) attack targeting the node's UDPv6 services. Monitoring this metric can help operators detect and troubleshoot network-related issues in real-time, ensuring the reliability and performance of their systems.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp6_NoPorts"
    },
    "description": "The node_netstat_Udp6_NoPorts metric measures the number of UDPv6 sockets in a listening state without any assigned port numbers on the monitored node. This statistic is collected from the system's netstat output and provides insight into the network configuration and potential issues with socket management.\n\nIn monitoring or alerting, this metric can be used to detect situations where there are an unusually high number of UDPv6 sockets without ports, which might indicate a misconfiguration, resource exhaustion, or other networking problems. For example, it could trigger alerts when the count exceeds a certain threshold, indicating potential issues with network connectivity or socket management.\n\nIt is essential to note that this metric does not provide information about the specific applications using these sockets or their intended purposes.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp6_OutDatagrams"
    },
    "description": "The node_netstat_Udp6_OutDatagrams metric measures the total number of UDPv6 datagrams sent out by the system since it was last restarted or initialized. This metric is collected from the 'netstat' command and provides insight into the network traffic generated by the system, specifically for IPv6 UDP connections. It can be used to monitor and troubleshoot issues related to outgoing UDPv6 traffic, such as congestion, packet loss, or connectivity problems. Potential implications include identifying bottlenecks in network communication, detecting anomalies in traffic patterns, or triggering alerts when a threshold of outdatagrams is exceeded.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp6_RcvbufErrors"
    },
    "description": "The node_netstat_Udp6_RcvbufErrors metric measures the number of errors that occurred while receiving UDPv6 packets due to buffer overflow conditions on a node. This metric is collected from the netstat command and provides insight into the network stack's ability to handle incoming UDPv6 traffic. High values or increasing trends in this metric may indicate issues with network configuration, resource constraints, or software bugs affecting the system's capacity to process UDPv6 packets. Potential implications include dropped packets, increased latency, or even complete network unavailability. This metric can be used in monitoring and alerting to detect potential issues before they impact service availability.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp6_SndbufErrors"
    },
    "description": "The node_netstat_Udp6_SndbufErrors metric measures the number of errors encountered while sending UDPv6 packets due to insufficient buffer space on the system. This metric is collected from the 'netstat' command and provides insight into the network stack's performance and potential bottlenecks. High values or increasing trends in this metric may indicate issues with network resource allocation, packet loss, or congestion, which can impact application performance and user experience. It can be used to monitor and alert on UDPv6 traffic issues, optimize buffer sizes, and troubleshoot network connectivity problems.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_UdpLite6_InErrors"
    },
    "description": "The **node_netstat_UdpLite6_InErrors** metric tracks the total number of errors encountered when receiving UDP Lite IPv6 packets on a node, as reported by the system's netstat utility. In Prometheus, this metric helps SREs monitor the health and reliability of the node\u2019s IPv6 UDP Lite network stack. A rising or consistently high value indicates potential issues such as malformed packets, network misconfigurations, or hardware problems that could degrade network performance or cause packet loss.\n\n**Alerting guidance:**  \nSet an alert if the rate of increase in `node_netstat_UdpLite6_InErrors` exceeds a small threshold over a defined period, for example:  \n```\nrate(node_netstat_UdpLite6_InErrors[5m]) > 0\n```\nThis triggers when any UDP Lite IPv6 receive errors occur within 5 minutes, signaling a potential network problem requiring investigation.\n\n**Impact of values:**  \n- **Low or zero values:** Normal operation, no UDP Lite IPv6 receive errors detected.  \n- **Sustained non-zero or increasing values:** Indicates persistent packet reception errors, which may lead to degraded application performance, connectivity issues, or security vulnerabilities.\n\n**Example usage:**  \n- **Dashboard:** Plot the `rate(node_netstat_UdpLite6_InErrors[5m])` over time to visualize error trends and correlate with network events.  \n- **Alert rule:**  \n```yaml\nalert: UdpLite6ReceiveErrorsDetected\nexpr: rate(node_netstat_UdpLite6_InErrors[5m]) > 0\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"UDP Lite IPv6 receive errors detected on {{ $labels.instance }}\"\n  description: \"The node {{ $labels.instance }} has experienced UDP Lite IPv6 receive errors in the last 5 minutes, indicating possible network issues.\"\n```\nThis enables proactive detection and troubleshooting of network stack problems affecting UDP Lite IPv6 traffic.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_UdpLite_InErrors"
    },
    "description": "The node_netstat_UdpLite_InErrors metric measures the number of incoming errors for UDP-Lite packets on a node. This metric is collected from the Linux kernel's netstat statistics and represents the total count of errors encountered while receiving UDP-Lite packets. It can be used to monitor network connectivity issues, packet corruption, or other problems related to UDP-Lite communication. Potential implications include identifying nodes with high error rates, which may indicate a need for troubleshooting or maintenance. This metric can also serve as an input for alerting systems to notify teams of potential network issues.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp_InDatagrams"
    },
    "description": "The node_netstat_Udp_InDatagrams metric measures the total number of UDP datagrams received by the system from the network. This count includes all UDP packets that were successfully delivered to the transport layer for processing, but does not include any errors or retransmissions. In a typical scenario, this metric can be used as an indicator of network traffic volume and potential issues with UDP-based services. For example, if the value of node_netstat_Udp_InDatagrams is consistently high, it may indicate that the system is receiving an unusually large amount of UDP traffic, which could impact performance or lead to resource exhaustion. Conversely, a sudden drop in this metric might suggest a network connectivity issue or a problem with the UDP service itself. This metric can be used in conjunction with other metrics, such as node_netstat_Udp_OutDatagrams and node_netstat_Udp_Errors, to gain a more comprehensive understanding of UDP traffic patterns and troubleshoot related issues.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp_InErrors"
    },
    "description": "The node_netstat_Udp_InErrors metric measures the number of UDP packets that were received by the system but contained errors in their headers or payloads, resulting in them being discarded. This metric can be used to monitor network connectivity and potential issues with UDP-based services. High values may indicate problems with the network stack, firewall configurations, or issues with specific applications using UDP. It could also be a sign of a DoS (Denial-of-Service) attack targeting the system's UDP ports. In monitoring and alerting, this metric can be used to trigger alerts when the error rate exceeds a certain threshold, indicating potential service disruptions or security threats.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp_NoPorts"
    },
    "description": "The node_netstat_Udp_NoPorts metric measures the number of UDP connections that are in a state of no ports available on the node. This can indicate issues with network connectivity, firewall configurations, or resource constraints. Potential implications include: \n\n* Network congestion or saturation leading to dropped packets and failed connections.\n* Firewall rules blocking incoming or outgoing UDP traffic.\n* Insufficient system resources (e.g., memory, CPU) causing connection timeouts.\n\nThis metric can be used in monitoring and alerting to detect potential network issues, identify bottlenecks, and optimize resource allocation. It may also serve as a leading indicator for more severe problems, such as service unavailability or data loss.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp_OutDatagrams"
    },
    "description": "The node_netstat_Udp_OutDatagrams metric measures the number of UDP datagrams sent out by a node over a specified time interval. This statistic is collected from the /proc/net/snmp file on Linux systems and provides insight into the network traffic generated by UDP connections. In monitoring or alerting, this metric can be used to detect potential issues with UDP-based services, such as increased latency or packet loss due to high outgoing datagram rates. It may also indicate misconfigured or malfunctioning applications that are generating excessive UDP traffic.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp_RcvbufErrors"
    },
    "description": "The node_netstat_Udp_RcvbufErrors metric measures the number of UDP receive buffer errors on a node. This error occurs when the system's UDP receive buffer is full and cannot accept any more data, resulting in dropped packets. The metric can be used to detect potential issues with network connectivity or configuration, such as insufficient buffer size or high network traffic. It may also indicate problems with the underlying hardware or software components responsible for handling UDP traffic. In monitoring or alerting, this metric could trigger notifications when its value exceeds a certain threshold, indicating a need for investigation and potential corrective action to prevent data loss or other issues related to UDP communication.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_netstat_Udp_SndbufErrors"
    },
    "description": "The node_netstat_Udp_SndbufErrors metric measures the number of errors encountered while sending UDP packets due to insufficient send buffer space on a node. This metric is collected from the 'netstat' command and provides insight into network connectivity issues related to UDP packet transmission. Potential implications include: \n\n- Network congestion or saturation, leading to dropped packets and increased latency.\n- Insufficient system resources (e.g., memory) causing send buffer space exhaustion.\n- Misconfigured or malfunctioning network interfaces.\n\nThis metric can be used in monitoring and alerting to detect potential issues before they impact application performance. It may trigger alerts for operations teams to investigate and resolve the underlying causes, ensuring smooth communication between nodes and maintaining overall system reliability.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_address_assign_type",
      "device": "*"
    },
    "description": "The node_network_address_assign_type metric measures the type of address assignment used by a network device on the monitored node. This can be either 'dhcp' (Dynamic Host Configuration Protocol) or 'static', indicating whether the device's IP address is dynamically assigned by a DHCP server or manually configured, respectively.\n\nThis metric provides insight into how network devices are configured and managed within the infrastructure. It may be used to monitor for potential issues related to IP address assignment, such as unexpected changes in address assignment type or inconsistencies between expected and actual configurations.\n\nIn monitoring or alerting scenarios, this metric could be used to trigger notifications when a device's address assignment type is changed unexpectedly, indicating a possible configuration drift or security risk. Additionally, it can help identify devices that are not using static IP addresses as intended, potentially impacting network performance or reliability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_carrier",
      "device": "*"
    },
    "description": "The node_network_carrier metric measures whether a network interface on the node is currently active and connected to a carrier (e.g., a physical Ethernet cable). This metric indicates the presence or absence of a physical connection to a network, which can be useful for monitoring node connectivity and detecting potential issues with network cables or interfaces. Potential implications include alerting when a critical network interface goes down, identifying nodes that are not properly connected to the network, or triggering further investigation into network configuration or hardware problems.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_carrier_changes_total",
      "device": "*"
    },
    "description": "The node_network_carrier_changes_total metric measures the total number of network carrier changes on a node. This metric is likely related to the physical or virtual network interface's ability to detect and respond to link up/down events. It can be used to monitor the stability and reliability of network connections, potentially indicating issues with cabling, switch configurations, or other hardware-related problems. In monitoring or alerting, this metric could trigger notifications when a sudden spike in carrier changes occurs, suggesting a potential issue with the network infrastructure.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_carrier_down_changes_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times a network device's carrier status has transitioned from up to down or vice versa on a node. It indicates the frequency and stability of network connectivity issues. Potential implications include identifying nodes with frequent network outages, detecting potential hardware failures, or pinpointing configuration errors affecting network availability. This metric can be used in monitoring to set thresholds for alerting when excessive carrier status changes occur, indicating potential network instability or hardware issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_carrier_up_changes_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times a network device's carrier status has changed to 'up' since the node was started or restarted. It indicates the number of times the network connection has been established and is operational. This can be used in monitoring and alerting to detect issues with network connectivity, such as frequent carrier up/down cycles, which may indicate problems with the network interface, cable, or surrounding infrastructure.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_device_id",
      "device": "*"
    },
    "description": "The **node_network_device_id** metric provides a unique numeric identifier for each network device on the monitored node, as assigned by the operating system. In Prometheus, this metric helps correlate network interface-specific metrics by providing a stable reference to each device, even if device names change. \n\n**Purpose:**  \n- Track and differentiate network interfaces on a node.  \n- Detect changes in network device configurations, such as addition, removal, or renaming of interfaces.  \n- Correlate device-level metrics (e.g., traffic, errors) with a consistent device ID.\n\n**Alerting guidance:**  \nThis metric itself is an identifier and does not have meaningful numeric thresholds for alerting. Instead, alert on changes in the set of device IDs over time, which may indicate unexpected network interface changes. For example, alert if a known device ID disappears or a new unknown device ID appears, signaling potential hardware issues or configuration changes.\n\n**Impact of changes:**  \n- A sudden drop in device IDs may indicate a network interface failure or removal, potentially impacting network connectivity.  \n- New or unexpected device IDs may indicate hardware changes, misconfigurations, or unauthorized devices.\n\n**Example usage:**  \n- **Dashboard:** Use `node_network_device_id` alongside interface metrics (e.g., `node_network_receive_bytes_total`) to label and track traffic per device reliably.  \n- **Alert rule example:**  \n```yaml\nalert: NetworkDeviceChangeDetected\nexpr: changes(node_network_device_id[5m]) > 0\nfor: 10m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Network device configuration changed on {{ $labels.instance }}\"\n  description: \"A network device ID has appeared or disappeared in the last 5 minutes, indicating possible interface changes.\"\n```\nThis alert triggers if the set of network device IDs changes within 5 minutes, prompting investigation into network interface stability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_dormant",
      "device": "*"
    },
    "description": "The **node_network_dormant** metric indicates whether a specific network device on the node is in a dormant state (value = 1) or active (value = 0). A dormant state means the device is disabled or in low-power mode, often due to maintenance, configuration issues, or hardware problems. In Prometheus, this metric helps SREs detect network interfaces that are unexpectedly inactive, which can lead to degraded connectivity or reduced network throughput.\n\n**Alert Threshold:**  \nTrigger an alert if **node_network_dormant{device=\"*\"} == 1** for a sustained period (e.g., 5 minutes), indicating the interface is dormant and potentially impacting network availability.\n\n**Impact:**  \n- **High values (1):** The network device is dormant, possibly causing loss of connectivity or degraded network performance. Immediate investigation is recommended.  \n- **Low values (0):** The network device is active and functioning normally.\n\n**Example Alert Rule:**  \n```\nalert: NetworkDeviceDormant\nexpr: node_network_dormant{device!=\"lo\"} == 1\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Network device {{ $labels.device }} is dormant\"\n  description: \"The network device {{ $labels.device }} on node {{ $labels.instance }} has been dormant for over 5 minutes.\"\n```\n\n**Example Dashboard Usage:**  \nVisualize **node_network_dormant** per device as a binary status panel to quickly identify dormant interfaces, enabling rapid troubleshooting of network issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_flags",
      "device": "*"
    },
    "description": "The node_network_flags metric measures a network device property known as flags, which is a bitfield that can contain various information about the network interface. This metric provides insight into the configuration and operational state of the network interface on the node. Potential implications or usage in monitoring or alerting include detecting changes to network settings, identifying potential issues with network connectivity, or tracking the status of specific network features such as VLAN support or jumbo frames. However, without further context or information about the specific flags being reported, it is difficult to provide a more detailed explanation of what this metric represents.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_iface_id",
      "device": "*"
    },
    "description": "The **node_network_iface_id** metric provides a unique numeric identifier assigned by the operating system to each network interface on a node. It measures the interface ID as an integer value, which remains consistent for the lifetime of the interface. This metric helps differentiate and track individual network interfaces within the node. It is useful for correlating network metrics and monitoring interface-specific changes or issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_iface_link",
      "device": "*"
    },
    "description": "The **node_network_iface_link** metric reports the operational link status of a network interface on a Prometheus-monitored node, where the label **device** specifies the interface name (e.g., eth0, wlan0). It returns a value of **1** when the interface link is up (active and connected) and **0** when the link is down (disconnected or inactive). This metric is critical for SREs to monitor network connectivity at the interface level, quickly identifying link failures or hardware issues that could impact node communication.\n\n**Alerting guidance:**  \nTrigger an alert when **node_network_iface_link{device=\"eth0\"} == 0** for a sustained period (e.g., 5 minutes), indicating the interface link is down and potentially causing network outages or degraded service.\n\n**Impact of values:**  \n- **1 (up):** Interface is operational and ready to transmit/receive traffic.  \n- **0 (down):** Interface is disconnected or disabled, leading to loss of network connectivity on that interface.\n\n**Example alert rule:**  \n```yaml\nalert: NetworkInterfaceDown\nexpr: node_network_iface_link{device=\"eth0\"} == 0\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"Network interface eth0 is down on {{ $labels.instance }}\"\n  description: \"The network interface eth0 has been down for more than 5 minutes, which may impact node connectivity.\"\n```\n\n**Example dashboard usage:**  \nVisualize **node_network_iface_link** as a binary status panel per interface to quickly identify which interfaces are down, enabling rapid troubleshooting and proactive maintenance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_iface_link_mode",
      "device": "*"
    },
    "description": "The node_network_iface_link_mode metric measures the link mode of a network interface on a Prometheus node. This metric indicates whether the interface is up (link_up) or down (link_down), and can also indicate if the interface is in a suspended state (link_suspended). The possible values for this metric are: - link_up: The interface is operational and data transfer is allowed. - link_down: The interface is not operational, either due to hardware failure, cable disconnection, or other reasons. - link_suspended: The interface is temporarily disabled, often due to system configuration changes or maintenance operations. This metric can be used in monitoring and alerting to detect network connectivity issues, identify potential hardware problems, or track the status of network interfaces during maintenance windows.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_info",
      "address": "*",
      "adminstate": "*",
      "broadcast": "*",
      "device": "*",
      "duplex": "*",
      "ifalias": "*",
      "operstate": "*"
    },
    "description": "The **node_network_info** metric provides static metadata about each network interface on a node, sourced from the `/sys/class/net/<iface>` directory. Each unique combination of labels (such as `address`, `adminstate`, `broadcast`, `device`, `duplex`, `ifalias`, and `operstate`) represents a specific network interface and its configuration details. The metric value is always `1`, serving as a presence indicator rather than a numeric measurement.\n\n**Purpose:**  \nThis metric is primarily used to identify and enumerate network interfaces along with their key attributes (e.g., MAC address, administrative state, operational state, duplex mode). It enables correlation with other dynamic metrics (like traffic or error counters) by providing context about the interface\u2019s configuration and status.\n\n**Thresholds and Alerts:**  \nSince the metric value is constant (`1`), it is not suitable for threshold-based alerts directly. Instead, alerts should be based on label values, particularly `adminstate` and `operstate`. For example:  \n- Alert if `operstate` is `\"down\"` while `adminstate` is `\"up\"`, indicating an interface that should be operational but is not.  \n- Alert if `duplex` is `\"half\"` on interfaces expected to run in full duplex, which may indicate misconfiguration or degraded performance.\n\n**Impact:**  \nChanges in label values (e.g., `operstate` switching from `\"up\"` to `\"down\"`) reflect real changes in interface status that can impact network connectivity and performance. Monitoring these label changes helps detect interface failures or misconfigurations early.\n\n**Example Usage:**  \n- **Dashboard:** Display a table of all network interfaces with columns for `device`, `address` (MAC), `adminstate`, and `operstate` to provide a quick overview of interface health and configuration.  \n- **Alert Rule Example (PromQL):**  \n  ```promql\n  node_network_info{adminstate=\"up\", operstate=\"down\"} == 1\n  ```  \n  This expression triggers when any interface is administratively enabled but operationally down, signaling a potential network issue requiring investigation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_mtu_bytes",
      "device": "*"
    },
    "description": "The **node_network_mtu_bytes** metric reports the Maximum Transmission Unit (MTU) size, in bytes, configured for each network device on the node. MTU defines the largest packet size that can be sent over the interface without fragmentation. Monitoring this metric helps ensure network interfaces are correctly configured to optimize throughput and minimize packet loss or latency caused by fragmentation.\n\n**Purpose:**  \nIn Prometheus, this metric allows SREs to verify that network devices have appropriate MTU settings aligned with network policies and infrastructure requirements. Sudden changes or misconfigurations in MTU can lead to degraded network performance or connectivity issues.\n\n**Thresholds and Alerting:**  \n- Alert if MTU is below a minimum expected value (e.g., < 1400 bytes), which may cause excessive fragmentation and reduced performance.  \n- Alert if MTU is unexpectedly high or inconsistent with the network standard (e.g., > 9000 bytes on interfaces not supporting jumbo frames), which may cause packet drops or incompatibility.\n\n**Impact of Values:**  \n- **Low MTU:** Increased fragmentation, higher CPU overhead, potential throughput reduction, and latency spikes.  \n- **High MTU:** Potential incompatibility with network devices, leading to dropped packets or connectivity loss if jumbo frames are unsupported.\n\n**Example Usage:**  \n- **Dashboard:** Display MTU per device to quickly identify interfaces with non-standard MTU settings.  \n- **Alert Rule Example:**  \n```yaml\nalert: NetworkMTUOutOfRange  \nexpr: node_network_mtu_bytes{device!~\"lo\"} < 1400 or node_network_mtu_bytes{device!~\"lo\"} > 9000  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"MTU size out of expected range on {{ $labels.device }}\"  \n  description: \"The MTU for device {{ $labels.device }} is {{ $value }} bytes, which may cause network issues.\"  \n```\nUse this metric to proactively detect and correct MTU misconfigurations before they impact network reliability or application performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_name_assign_type",
      "device": "*"
    },
    "description": "The node_network_name_assign_type metric measures the type of assignment used for network device names on a given node. This can be either 'static' or 'dynamic'. A static assignment implies that the name is manually configured and remains unchanged, whereas a dynamic assignment suggests that the name is automatically generated or updated by the system. This information can be useful in monitoring and troubleshooting network connectivity issues, as it provides insight into how device names are being managed on the node. Potential implications include identifying potential misconfigurations or inconsistencies in network naming conventions, which could impact communication between devices or affect overall network performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_net_dev_group",
      "device": "*"
    },
    "description": "The **node_network_net_dev_group** metric indicates the group classification of a specific network device on a node. It categorizes devices based on their network interface grouping for management purposes. This metric is a label or identifier and does not have a numeric unit. It helps track device grouping configurations and supports monitoring and alerting related to network interface organization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_protocol_type",
      "device": "*"
    },
    "description": "The node_network_protocol_type metric measures the type of network protocol used by a node's network interface. This can be either IPv4 or IPv6, indicating whether the node is configured to communicate over the Internet Protocol version 4 or version 6. The implications of this metric in monitoring and alerting include identifying nodes that are not using the expected protocol for their environment, which could indicate misconfiguration or compatibility issues. For example, if a node is supposed to be IPv6-only but is reporting as IPv4, it may trigger an alert to investigate why the node is not adhering to its intended configuration.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_bytes_total",
      "device": "*"
    },
    "description": "The node_network_receive_bytes_total metric measures the total number of bytes received by a network device on the node. This metric is useful for monitoring and troubleshooting network connectivity issues, such as slow or dropped packets. It can be used to identify potential bottlenecks in network traffic, detect anomalies in data transfer rates, or track changes in overall network utilization over time. In operations, this metric might be used to trigger alerts when receive bytes exceed a certain threshold, indicating potential network congestion or errors.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_compressed_total",
      "device": "*"
    },
    "description": "The **node_network_receive_compressed_total** metric counts the total number of compressed packets received by a specific network device on the node. It is a cumulative counter measured in packets, not bytes. This metric helps track the volume of compressed network traffic handled by the device. Monitoring it can assist in analyzing network performance and compression effectiveness.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_drop_total",
      "device": "*"
    },
    "description": "The node_network_receive_drop_total metric measures the total number of packets that were received by a network device but were dropped due to errors or lack of buffer space. This metric is useful for identifying potential issues with network connectivity, packet loss, or resource constraints on the node. It can be used in monitoring and alerting to detect anomalies in network traffic patterns, such as sudden spikes in packet drops, which may indicate a hardware failure, configuration issue, or overload condition. By tracking this metric over time, operators can gain insights into the reliability and performance of their network infrastructure.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_errs_total",
      "device": "*"
    },
    "description": "The node_network_receive_errs_total metric measures the total number of errors encountered while receiving network packets on a node's network devices. This includes errors such as packet drops, buffer overflows, or other receive-related issues. It is a cumulative counter that increments each time an error occurs and can be used to detect potential network connectivity problems, identify bottlenecks, or troubleshoot issues related to data transfer between nodes. In monitoring and alerting, this metric can trigger alerts when the error count exceeds a certain threshold, indicating a possible issue with network configuration, hardware, or software.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_fifo_total",
      "device": "*"
    },
    "description": "The node_network_receive_fifo_total metric measures the total number of packets that have been received by a network device but are still waiting to be processed due to a full receive FIFO (First-In-First-Out) buffer. This can indicate network congestion or a misconfigured system, leading to potential packet loss and decreased network performance. It may be used in monitoring to detect when the receive FIFO is consistently near capacity, triggering further investigation into network configuration or resource allocation issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_frame_total",
      "device": "*"
    },
    "description": "The **node_network_receive_frame_total** metric counts the total number of Ethernet frames received by the specified network device on the node. It is a cumulative counter that increments by one for each frame received. The unit of measurement is the number of frames. This metric helps monitor network interface activity and diagnose issues like frame loss or network congestion.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_multicast_total",
      "device": "*"
    },
    "description": "The **node_network_receive_multicast_total** metric counts the total number of multicast packets received by a specific network device on the node. It is a cumulative counter measured in packets. This metric helps monitor multicast traffic volume and detect unusual spikes or drops that may indicate network issues. Tracking this metric aids in diagnosing multicast-related connectivity problems and network performance bottlenecks.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_nohandler_total",
      "device": "*"
    },
    "description": "The node_network_receive_nohandler_total metric measures the total number of network packets received by a node's network devices that were not handled by any protocol handler. This can indicate issues with packet processing or configuration problems on the node. Potential implications include: \n\n- Network congestion or saturation, leading to dropped packets.\n- Misconfigured network interfaces or protocols.\n- Hardware or software issues affecting packet handling.\n\nThis metric can be used in monitoring and alerting to detect potential network bottlenecks, misconfigurations, or hardware failures. It may also serve as a precursor to more severe issues such as data loss or corruption.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_receive_packets_total",
      "device": "*"
    },
    "description": "The **node_network_receive_packets_total** metric counts the total number of packets received by a specific network device on the monitored node. It is a cumulative counter that increases by one for each packet received. The unit of measurement is packets. This metric helps monitor network traffic volume and detect issues like packet loss or unusual reception patterns.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_speed_bytes",
      "device": "*"
    },
    "description": "The **node_network_speed_bytes** metric measures the configured maximum network speed of a specific network device, expressed in bytes per second. It reflects the hardware or interface speed capacity, not the current data transfer rate. This metric helps assess the potential throughput of the network interface for performance monitoring and capacity planning.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_bytes_total",
      "device": "*"
    },
    "description": "The **node_network_transmit_bytes_total** metric in Prometheus tracks the cumulative number of bytes transmitted by each network device on a node since the system started. It helps SREs monitor network output traffic volume per interface, enabling detection of unusual spikes or drops in transmitted data that may indicate network congestion, hardware issues, or misconfigurations.\n\n**Purpose:**  \n- Measure total outbound network traffic per device to assess network utilization and performance.  \n- Identify trends or sudden changes in transmission rates that could impact application responsiveness or data flow.\n\n**Alert Threshold Guidance:**  \n- Trigger alerts if the transmission rate (calculated as the derivative of this counter over time) exceeds a device-specific bandwidth capacity (e.g., >90% of interface max speed) for a sustained period (e.g., 5 minutes), indicating potential saturation.  \n- Alert if the transmission rate drops to near zero unexpectedly during normal operation, which may signal interface failure or network disconnection.\n\n**Impact of Values:**  \n- **High values or sustained high transmission rates:** May indicate heavy network usage, possible bottlenecks, or DDoS attacks, potentially causing increased latency or packet loss.  \n- **Low or zero values during expected traffic periods:** Could suggest network interface down, cable issues, or misrouted traffic, leading to service degradation.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-device transmission rate by calculating `rate(node_network_transmit_bytes_total[5m])` to visualize real-time outbound traffic trends and identify peak usage times.  \n- **Alert Rule Example:**  \n  ```\n  alert: HighNetworkTransmitUtilization\n  expr: rate(node_network_transmit_bytes_total[5m]) > (0.9 * <device_max_bandwidth_in_bytes_per_second>)\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High network transmit utilization on {{ $labels.device }}\"\n    description: \"Network transmit rate on device {{ $labels.device }} has exceeded 90% of its capacity for more than 5 minutes.\"\n  ```",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_carrier_total",
      "device": "*"
    },
    "description": "The node_network_transmit_carrier_total metric measures the total number of transmit carrier events on a network device. A transmit carrier event occurs when a network interface controller (NIC) is unable to send data due to a hardware or software issue. This can be caused by various factors such as cable problems, NIC driver issues, or configuration errors. The metric provides insight into potential network connectivity issues and can be used in monitoring and alerting to detect and troubleshoot transmit carrier events. It may indicate a problem with the network interface, the connected device, or the underlying infrastructure.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_colls_total",
      "device": "*"
    },
    "description": "The node_network_transmit_colls_total metric measures the total number of transmit collisions on a network device. Transmit collisions occur when two or more devices attempt to send data at the same time over the same network segment, resulting in errors and potential packet loss. This metric can be used to identify network congestion, misconfigured devices, or other issues that may impact network performance. It might be used in monitoring to detect sudden spikes in transmit collisions, which could indicate a problem with the network infrastructure. In alerting, this metric could trigger notifications when the number of transmit collisions exceeds a certain threshold, prompting operators to investigate and resolve the issue.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_compressed_total",
      "device": "*"
    },
    "description": "This metric measures the total number of compressed packets transmitted by a network device on the node. It represents the cumulative count of compressed data sent over the network interface, which can be used to monitor and troubleshoot network performance issues related to compression efficiency or packet transmission rates. Potential implications include identifying bottlenecks in network communication, detecting anomalies in compression ratios, or optimizing network configuration for better throughput.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_drop_total",
      "device": "*"
    },
    "description": "The node_network_transmit_drop_total metric measures the total number of packets that were transmitted by a network device but were not successfully delivered to their destination due to errors or other issues. This can be caused by various factors such as hardware failures, software bugs, or network congestion. In monitoring and alerting, this metric can be used to detect potential network bottlenecks or device failures, allowing for proactive maintenance and reducing the risk of service outages. It may also indicate a need for increased bandwidth or upgraded network infrastructure.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_errs_total",
      "device": "*"
    },
    "description": "The **node_network_transmit_errs_total** metric tracks the cumulative number of transmission errors encountered by a specific network device on a node, as collected by Prometheus. It reflects issues such as packet drops, collisions, or hardware faults during data transmission. \n\n**Purpose:** This metric helps SREs monitor the health and reliability of network interfaces by identifying transmission problems that could degrade network performance or cause connectivity failures.\n\n**Alert Threshold:** A common alert condition is when the rate of transmit errors exceeds a small threshold relative to total transmitted packets, for example, if the error rate is greater than 0.1% over a 5-minute window. This can be expressed as:\n\n```\nrate(node_network_transmit_errs_total{device=\"eth0\"}[5m]) / rate(node_network_transmit_packets_total{device=\"eth0\"}[5m]) > 0.001\n```\n\n**Impact of Values:**  \n- **Low or zero values:** Indicate healthy network transmission with minimal errors.  \n- **Sustained high values or spikes:** Suggest network interface issues such as faulty hardware, driver problems, or physical layer errors, potentially leading to degraded throughput or packet loss.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-device transmit error rate over time alongside total transmitted packets to visualize error trends and correlate with network traffic.  \n- **Alert Rule:** Trigger an alert if the transmit error rate exceeds 0.1% for more than 5 minutes on any critical network interface, prompting investigation or hardware checks.\n\nThis metric is essential for maintaining network reliability, especially in high-throughput or latency-sensitive environments.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_fifo_total",
      "device": "*"
    },
    "description": "The node_network_transmit_fifo_total metric measures the total number of packets that have been transmitted by a network device but are still waiting to be acknowledged or processed in the transmit FIFO (First-In-First-Out) buffer. This metric indicates potential network congestion, packet loss, or transmission errors. It can be used to monitor and alert on issues related to network throughput, latency, and reliability. High values of this metric may indicate a need for increased network bandwidth, improved network configuration, or troubleshooting of underlying network issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_packets_total",
      "device": "*"
    },
    "description": "The **node_network_transmit_packets_total** metric in Prometheus tracks the cumulative number of packets transmitted by each network device (interface) on a node since the system started. It helps SREs monitor network interface activity and detect issues such as packet loss, congestion, or hardware faults.\n\n**Purpose:**  \nThis metric provides insight into the volume of outbound network traffic per device, enabling correlation with network performance and error metrics. It is essential for identifying abnormal transmission patterns that may indicate network saturation or failures.\n\n**Alerting Thresholds:**  \n- A sudden drop or stagnation in the transmit packet count over a defined interval may indicate a network interface failure or disconnection.  \n- Conversely, an unusually high rate of packet transmission (e.g., sustained increase beyond typical baseline by 50-100%) could signal network congestion or a potential broadcast storm.  \n- Example threshold: Trigger an alert if the rate of transmitted packets per second falls below 10 packets/s for 5 minutes (possible interface down), or if it exceeds 10,000 packets/s continuously for 10 minutes (possible congestion).\n\n**Impact of Values:**  \n- **High values:** May indicate heavy outbound traffic, which could lead to network congestion, increased latency, or packet drops if the interface or network is saturated.  \n- **Low or zero values:** Could suggest the interface is down, disconnected, or misconfigured, resulting in no outbound traffic.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-device rate of transmitted packets using the Prometheus query:  \n  `rate(node_network_transmit_packets_total{device!=\"lo\"}[5m])`  \n  This shows the packets transmitted per second over the last 5 minutes, excluding the loopback interface.  \n- **Alert Rule Example:**  \n  ```yaml\n  - alert: NetworkInterfaceTransmitStall\n    expr: rate(node_network_transmit_packets_total[5m]) < 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Low transmit packet rate on network interface {{ $labels.device }}\"\n      description: \"The transmit packet rate on interface {{ $labels.device }} has been below 10 packets/s for over 5 minutes, indicating possible interface failure or disconnection.\"\n  ```",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_transmit_queue_length",
      "device": "*"
    },
    "description": "The node_network_transmit_queue_length metric measures the current length of the transmit queue for network devices on a given node. This queue represents the number of packets waiting to be transmitted over the network interface. A high value indicates that the network device is experiencing congestion, potentially leading to packet drops or delays in data transmission. This metric can be used to monitor and troubleshoot network performance issues, such as slow network speeds or packet loss. It may also serve as a precursor to more severe problems like network saturation or device overload.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_network_up",
      "device": "*"
    },
    "description": "The node_network_up metric measures whether a node's network interface is operational and reachable. It indicates whether the node can communicate with other nodes on the network. A value of 1 (true) signifies that the network interface is up and functioning correctly, while a value of 0 (false) indicates that it is down or unreachable. This metric can be used in monitoring to detect network connectivity issues, identify nodes that are not reachable, and trigger alerts for potential infrastructure problems. It may also be used as a dependency check for other services that rely on the node's network interface being operational.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_entries"
    },
    "description": "The node_nf_conntrack_entries metric measures the number of currently allocated flow entries for connection tracking in a Linux kernel's Netfilter subsystem. This metric is particularly relevant to systems that rely heavily on network filtering and firewall rules. A high or increasing value may indicate resource exhaustion, potential security vulnerabilities, or misconfigured firewall rules. It can be used to monitor and alert on issues related to connection tracking, such as DoS attacks or configuration errors. Additionally, this metric can help in identifying the root cause of performance degradation or network connectivity issues by pinpointing the number of active connections being tracked.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_entries_limit"
    },
    "description": "The node_nf_conntrack_entries_limit metric measures the maximum allowed size of the connection tracking table in the Linux kernel's Netfilter framework. This table is used to store information about active network connections, including source and destination IP addresses, ports, and protocol details. The limit represents the maximum number of entries that can be stored in this table before it starts dropping new connections due to lack of space. High or near-maximum values for this metric may indicate a potential issue with connection tracking, such as a denial-of-service (DoS) attack or an unexpected surge in network traffic. This metric is useful for monitoring and alerting purposes, particularly in environments where high volumes of network traffic are expected or where security threats are a concern.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_drop"
    },
    "description": "The node_nf_conntrack_stat_drop metric measures the number of packets dropped due to a failure in the connection tracking (conntrack) system on a Linux node. The conntrack system is responsible for managing network connections and ensuring that incoming traffic is properly associated with existing connections. When the conntrack system fails, it may drop packets, which can lead to connectivity issues or errors. This metric provides insight into the performance of the conntrack system and can be used to detect potential issues before they impact application availability. Potential implications for monitoring or alerting include: triggering alerts when packet drops exceed a certain threshold, investigating network configuration changes that may be contributing to conntrack failures, or correlating with other metrics (e.g., CPU utilization, memory usage) to identify root causes of the issue.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_early_drop"
    },
    "description": "The node_nf_conntrack_stat_early_drop metric measures the number of conntrack entries that were dropped by the Linux kernel's netfilter connection tracking system to make room for new connections when the maximum table size was reached. This occurs when the system is under a high volume of network traffic and the conntrack table becomes saturated, causing the kernel to drop older or less important connections to free up space for newer ones. This metric can be used in monitoring and alerting to detect potential issues with network congestion, resource exhaustion, or misconfigured systems that may lead to dropped connections. It can also serve as an indicator of the system's ability to handle high traffic loads and may require adjustments to the conntrack table size or other tuning parameters to prevent frequent drops.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_found"
    },
    "description": "The node_nf_conntrack_stat_found metric measures the number of entries in the Netfilter connection tracking system that were successfully searched by Prometheus. This metric is specific to Linux systems and provides insight into the performance and functionality of the connection tracking system, which is responsible for managing network connections on the host. A high value may indicate a large number of active connections or a need for increased resources to handle connection tracking tasks. Potential implications include monitoring for sudden spikes in connection counts, identifying potential bottlenecks in connection tracking, or alerting when the number of searched entries exceeds a certain threshold.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_ignore"
    },
    "description": "The node_nf_conntrack_stat_ignore metric measures the number of packets that are already connected to a conntrack entry in the Linux kernel's Netfilter connection tracking system. This metric indicates the number of packets that have been seen as part of an existing connection, and are therefore ignored by the conntrack system. In other words, it represents the number of packets that do not require new connection tracking entries to be created. High values for this metric may indicate issues with connection tracking or packet processing in the Linux kernel, potentially leading to performance problems or security vulnerabilities. This metric can be used in monitoring and alerting to detect anomalies in network traffic patterns, identify potential bottlenecks, or troubleshoot issues related to conntrack system performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_insert"
    },
    "description": "The node_nf_conntrack_stat_insert metric measures the number of new connection tracking entries inserted into the Netfilter connection tracking table on a Linux node. This metric is specific to nodes running Linux and utilizing the Netfilter firewall system. It can be used to monitor the rate at which new connections are being tracked, potentially indicating changes in network traffic patterns or issues with connection establishment. In monitoring or alerting contexts, this metric might be used to detect sudden spikes in connection tracking entries, which could indicate a denial-of-service (DoS) attack or other security incidents. However, without further context or additional metrics, it is unclear what specific thresholds or baselines should be applied for alerting purposes.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_insert_failed"
    },
    "description": "The node_nf_conntrack_stat_insert_failed metric measures the number of failed attempts to insert entries into the Netfilter connection tracking table on a Linux node. This can occur due to various reasons such as memory exhaustion, invalid or malformed packets, or other system resource constraints. The metric provides insight into the health and performance of the node's networking stack, specifically the connection tracking mechanism. It may be used in monitoring and alerting to detect potential issues with network connectivity, packet processing, or system resource utilization. This metric can help operations teams identify and troubleshoot problems related to failed connection tracking entries, ensuring the stability and security of the node.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_invalid"
    },
    "description": "The node_nf_conntrack_stat_invalid metric measures the number of packets that cannot be tracked by the Netfilter connection tracking system on a given node. This can occur due to various reasons such as invalid or malformed packet headers, missing or incorrect sequence numbers, or other errors in the packet processing pipeline. The metric provides insight into the health and performance of the network stack on the node, specifically highlighting potential issues with connection tracking and packet handling. It may be used in monitoring and alerting to detect anomalies in packet processing, identify potential security threats, or troubleshoot connectivity issues between nodes.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nf_conntrack_stat_search_restart"
    },
    "description": "The node_nf_conntrack_stat_search_restart metric measures the number of conntrack table lookups that had to be restarted due to hashtable resizes in a Linux kernel's Netfilter connection tracking system. This occurs when the conntrack table is resized, causing existing entries to be lost and requiring a restart of the lookup process. The metric provides insight into the performance and stability of the connection tracking system, which is critical for network security and traffic management. High values may indicate frequent hashtable resizes, potentially leading to dropped connections or increased latency. This metric can be used in monitoring and alerting to detect issues with conntrack table resizing, allowing for proactive measures to prevent service disruptions.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_nvme_info",
      "device": "*",
      "firmware_revision": "*",
      "model": "*",
      "serial": "*",
      "state": "*"
    },
    "description": "The **node_nvme_info** metric provides static metadata about NVMe devices on each node, including device name, firmware revision, model, serial number, and current state. It is a labeled metric with a constant value of 1, indicating the presence of the device rather than a numeric measurement. This metric does not measure performance or health directly but serves to identify and track NVMe device attributes for inventory and status purposes.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_os_info",
      "build_id": "*",
      "id": "*",
      "id_like": "*",
      "image_id": "*",
      "image_version": "*",
      "name": "*",
      "pretty_name": "*",
      "variant": "*",
      "variant_id": "*",
      "version": "*",
      "version_codename": "*",
      "version_id": "*"
    },
    "description": "The **node_os_info** metric in Prometheus provides static, descriptive labels about the operating system running on a monitored node. It includes detailed OS metadata such as `build_id`, `id`, `id_like`, `image_id`, `image_version`, `name`, `pretty_name`, `variant`, `variant_id`, `version`, `version_codename`, and `version_id`. These labels do not represent numeric values or dynamic measurements but serve as identifiers to help SREs understand the OS environment of each node.\n\n### Purpose\n- To uniquely identify and categorize nodes by their OS version and distribution.\n- To enable correlation of OS-specific issues with other performance or availability metrics.\n- To assist in tracking OS version consistency across a fleet of nodes.\n\n### Alerting Guidance\nSince **node_os_info** is a static metadata metric without numeric values, it does not have thresholds or direct alert triggers. Instead, alerting should be based on detecting changes or mismatches in OS labels across nodes, for example:\n- Alert if a node\u2019s OS version label differs from the expected or baseline version (indicating an unplanned upgrade or drift).\n- Alert if a node is running an OS version flagged as vulnerable or deprecated.\n- Alert if a new OS variant or version appears unexpectedly in the environment.\n\n### Impact of Changes\n- A change in OS labels (e.g., version or variant) can indicate an OS upgrade, rollback, or misconfiguration.\n- Mismatched or outdated OS versions may lead to compatibility issues, security vulnerabilities, or degraded performance.\n- Consistent OS labeling helps maintain homogeneous environments, simplifying troubleshooting and capacity planning.\n\n### Example Usage\n\n**Dashboard snippet:**  \nDisplay a table or filter grouping nodes by `version` and `pretty_name` to quickly identify OS distribution and versions across your infrastructure.\n\n```promql\nnode_os_info\n```\n\nUse label filters to segment nodes, e.g., `node_os_info{version=\"20.04\"}` to focus on Ubuntu 20.04 nodes.\n\n**Alert rule example:**  \nDetect nodes running an unexpected OS version (e.g., any node not running the approved version \"20.04\"):\n\n```yaml\nalert: UnexpectedOSVersion\nexpr: count by (instance) (node_os_info) unless node_os_info{version=\"20.04\"}\nfor: 10m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Node {{ $labels.instance }} is running an unexpected OS version\"\n  description: \"Node OS version {{ $labels.version }} does not match the approved version 20.04.\"\n```\n\nThis alert triggers if any node reports an OS version other than the approved baseline for more than 10 minutes, enabling proactive investigation.\n\n---\n\nIn summary, **node_os_info** is a critical metadata metric for OS identification and consistency checks rather than numeric monitoring. Use it to enforce OS version policies, detect configuration drift, and correlate OS changes with system behavior.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_os_support_end_timestamp_seconds"
    },
    "description": "This metric measures the end-of-life (EOL) date timestamp of the operating system (OS) running on a node. It indicates when the OS will no longer receive security updates or support from its vendor. This information can be used to identify nodes that are nearing EOL and require immediate attention for upgrade or replacement. In monitoring, this metric can trigger alerts to notify administrators of impending OS EOL dates, allowing them to plan and execute upgrades before potential security risks arise. Additionally, it can inform capacity planning and resource allocation decisions by identifying nodes with outdated operating systems that may be more prone to vulnerabilities.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_os_version",
      "id": "*",
      "id_like": "*",
      "name": "*"
    },
    "description": "The node_os_version metric measures the major and minor version of the operating system running on a given node. This information can be used to monitor for potential OS upgrades or patches that may impact the performance or stability of the node. It can also help in identifying nodes with outdated or unsupported OS versions, which could lead to security vulnerabilities or compatibility issues with other systems. In monitoring and alerting, this metric can be used to trigger notifications when a significant number of nodes are running an outdated OS version, indicating a potential risk to the overall system's stability and security.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_pressure_cpu_waiting_seconds_total"
    },
    "description": "**node_pressure_cpu_waiting_seconds_total** measures the cumulative time in seconds that processes on a node have spent waiting for CPU availability due to CPU contention. In Prometheus, this metric helps identify CPU pressure by quantifying how long workloads are delayed from execution because the CPU is fully utilized.\n\n**Purpose:**  \nThis metric is critical for detecting CPU bottlenecks at the node level. A rising or consistently high value indicates that processes are frequently stalled waiting for CPU time, which can degrade application performance and increase latency.\n\n**Thresholds for Alerting:**  \n- Alert when the rate of increase of `node_pressure_cpu_waiting_seconds_total` exceeds a defined threshold over a short interval (e.g., > 0.1 seconds per second over 5 minutes), indicating sustained CPU contention.  \n- Alternatively, alert if the metric\u2019s per-second increase remains high for multiple consecutive intervals, signaling persistent CPU pressure.\n\n**Impact of Values:**  \n- **Low or stable values:** Indicate that CPU resources are sufficient and processes are rarely delayed.  \n- **High or rapidly increasing values:** Suggest CPU saturation, leading to process queuing, slower response times, and potential service degradation. This may require scaling resources, optimizing workloads, or investigating CPU-intensive processes.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-second rate of `node_pressure_cpu_waiting_seconds_total` (`rate(node_pressure_cpu_waiting_seconds_total[5m])`) to visualize CPU wait time trends and identify spikes.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: HighCPUWaitTime\n  expr: rate(node_pressure_cpu_waiting_seconds_total[5m]) > 0.1\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High CPU wait time on node {{ $labels.instance }}\"\n    description: \"Processes have been waiting for CPU for more than 0.1 seconds per second over the last 5 minutes, indicating CPU pressure.\"\n  ```",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_pressure_io_stalled_seconds_total"
    },
    "description": "This metric measures the total time in seconds that no process could make progress due to Input/Output (IO) congestion on a node. It indicates the cumulative duration of IO stalls, which can be caused by various factors such as disk saturation, network congestion, or faulty storage devices. The metric is useful for identifying nodes with persistent IO issues, which may impact application performance and responsiveness. It can also serve as an input for alerting mechanisms to notify operations teams when a node's IO stall time exceeds a certain threshold, allowing for proactive intervention to prevent potential outages.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_pressure_io_waiting_seconds_total"
    },
    "description": "This metric measures the total time in seconds that processes have waited due to Input/Output (IO) congestion on a node. It represents the cumulative duration of IO wait times across all processes running on the node. This metric can be used to identify and troubleshoot IO-related performance issues, such as slow disk I/O or network bottlenecks. High values may indicate that the node's storage or network infrastructure is unable to keep up with the workload, leading to increased latency and decreased system responsiveness. Potential uses for this metric include monitoring average wait times over a specified period, setting alerts for excessive wait times, and correlating IO wait times with other metrics such as CPU usage, memory consumption, or disk I/O rates to diagnose root causes of performance issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_pressure_memory_stalled_seconds_total"
    },
    "description": "This metric measures the total time in seconds that no process could make progress due to memory congestion on a node. It indicates the duration of memory stalls caused by high memory usage or contention, which can lead to performance degradation and potential crashes. This metric is useful for identifying nodes with persistent memory issues, detecting trends in memory congestion over time, and triggering alerts when memory stalls exceed a certain threshold. It requires careful analysis in conjunction with other metrics, such as node_memory_MemFree_bytes and node_memory_MemTotal_bytes, to understand the underlying causes of memory pressure.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_pressure_memory_waiting_seconds_total"
    },
    "description": "This metric measures the total time in seconds that processes have waited for memory to become available on a node. It represents the cumulative duration of memory wait times across all processes running on the node. This metric can be used to identify potential memory bottlenecks, detect when the system is unable to allocate sufficient memory resources, and inform capacity planning decisions. In monitoring or alerting, this metric can trigger alerts when the waiting time exceeds a certain threshold, indicating that the system may require additional memory resources or optimization efforts.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_procs_blocked"
    },
    "description": "The node_procs_blocked metric measures the number of processes that are currently blocked and waiting for I/O operations to complete on a given node. This can include disk I/O, network I/O, or other types of input/output operations. A high value for this metric may indicate that the system is experiencing I/O bottlenecks, which can lead to performance issues and slow down application responsiveness. Potential implications for monitoring and alerting include setting up alerts when the number of blocked processes exceeds a certain threshold, indicating potential I/O congestion or hardware limitations. This metric can also be used in conjunction with other metrics, such as disk usage or network latency, to gain a more comprehensive understanding of system performance and identify areas for optimization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_procs_running"
    },
    "description": "The node_procs_running metric measures the number of processes in a runnable state on a given node. This includes processes that are currently executing and waiting for CPU resources to become available. A high value may indicate system overload or resource contention, while a low value could suggest underutilization or inefficient process management. Potential implications include alerting on sudden spikes or drops in the number of running processes, which can be indicative of issues such as resource exhaustion, process leaks, or configuration errors. This metric is useful for monitoring node-level performance and identifying potential bottlenecks.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_schedstat_running_seconds_total",
      "cpu": "*"
    },
    "description": "The node_schedstat_running_seconds_total metric measures the total number of seconds that CPU spent running a process on the node. This metric is based on the Linux kernel's scheduler statistics and provides insight into the time spent by the system in user mode (i.e., executing processes) versus other modes like idle, I/O wait, or system calls. It can be used to monitor the overall utilization of the CPU and identify potential bottlenecks or performance issues. For example, if this metric is consistently high, it may indicate that the node is under heavy load, and additional resources might be needed to handle the workload. Conversely, a low value could suggest underutilization, where the system has idle capacity that could be utilized more efficiently. This metric can also be used in conjunction with other metrics, such as CPU usage or memory consumption, to gain a deeper understanding of system performance and make informed decisions about resource allocation.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_schedstat_timeslices_total",
      "cpu": "*"
    },
    "description": "The node_schedstat_timeslices_total metric measures the total number of timeslices executed by the CPU since the last scrape. A timeslice is a unit of time allocated to a process or thread for execution on a CPU core. This metric provides insight into the CPU's utilization and scheduling efficiency, allowing operators to detect potential issues with system load, resource contention, or scheduling anomalies.\n\nPotential implications include:\n- Identifying underutilized or overutilized CPU resources, which can inform capacity planning and optimization efforts.\n- Detecting unusual patterns in timeslice allocation, which may indicate issues with process scheduling, thread management, or system configuration.\n- Correlating this metric with other performance indicators (e.g., node_cpu_usage_seconds_total) to gain a more comprehensive understanding of system resource utilization.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_schedstat_waiting_seconds_total",
      "cpu": "*"
    },
    "description": "The node_schedstat_waiting_seconds_total metric measures the total number of seconds spent by processing waiting for this CPU. This metric is a cumulative counter that tracks the time spent in the waiting state due to scheduling decisions. It provides insight into the CPU utilization and efficiency, helping operators identify potential bottlenecks or resource constraints. Potential implications include: (1) High values may indicate inefficient CPU usage, leading to increased latency or decreased throughput; (2) Sudden spikes might signal a sudden increase in workload or resource contention; (3) Long-term trends can help inform capacity planning and optimization efforts. This metric is particularly useful for monitoring and alerting on CPU utilization, especially in environments with high workloads or variable resource demands.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_scrape_collector_duration_seconds",
      "collector": "*"
    },
    "description": "The node_scrape_collector_duration_seconds metric measures the duration of a collector scrape in seconds for the node_exporter. This metric indicates how long it takes to collect and process metrics from various sources within the node. A high value may indicate issues with data collection, processing, or communication between components, potentially impacting monitoring accuracy and alerting responsiveness. It can be used to monitor the performance of the node_exporter, identify bottlenecks in the scraping process, and optimize data collection for better observability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_scrape_collector_success",
      "collector": "*"
    },
    "description": "The node_scrape_collector_success metric measures the success rate of collector scrapes in the node_exporter. It indicates whether a specific collector was able to successfully scrape data from the system. A value of 1 (or true) means the collector succeeded, while a value of 0 (or false) indicates failure. This metric can be used to monitor the health and reliability of collectors within the node_exporter, helping identify potential issues or bottlenecks in data collection. It may trigger alerts when a collector fails repeatedly, indicating a need for investigation or maintenance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_selinux_config_mode"
    },
    "description": "The node_selinux_config_mode metric measures the current SELinux enforcement mode configured on a given node. This metric indicates whether SELinux is enforcing its security policies (enforcing mode) or running in permissive mode, where it logs but does not enforce policy violations. The possible values for this metric are 'disabled', 'permissive', and 'enforcing'. In monitoring and alerting, this metric can be used to detect potential security risks if SELinux is disabled or set to permissive mode on a production node. It may also indicate misconfigurations or changes in the system's security posture over time.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_selinux_current_mode"
    },
    "description": "The node_selinux_current_mode metric measures the current SELinux (Security-Enhanced Linux) enforcement mode of a given node. This mode determines how strictly SELinux enforces access control and security policies on the system. The possible values for this metric include 'enforcing', 'permissive', or 'disabled'.\n\nIn monitoring, this metric can be used to detect potential security risks or misconfigurations. For instance, if the enforcement mode is set to 'permissive' when it should be 'enforcing', it may indicate a compromised system or an intentional security setting change. Similarly, if the mode is unexpectedly changed from its default value, it could signal a configuration drift or an attack.\n\nIn alerting, this metric can trigger notifications for unexpected changes in SELinux enforcement modes, ensuring that security teams are promptly notified of potential issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_selinux_enabled"
    },
    "description": "The node_selinux_enabled metric measures whether SELinux (Security-Enhanced Linux) is enabled on a given node. SELinux is a security feature that provides an additional layer of access control and isolation between system processes and resources. This metric indicates the current state of SELinux, with a value of 1 indicating it is enabled and 0 indicating it is disabled. In monitoring or alerting contexts, this metric can be used to detect potential security vulnerabilities or misconfigurations related to SELinux. For example, if SELinux is unexpectedly disabled on a critical node, an alert can be triggered to investigate the cause and restore the default configuration. Conversely, if SELinux is enabled but not functioning correctly, it may impact system performance or functionality, warranting further investigation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_FRAG6_inuse"
    },
    "description": "The node_sockstat_FRAG6_inuse metric measures the number of FRAG6 sockets on a node that are currently in an 'inuse' state. This typically indicates that these sockets are actively being used for communication between the node and other nodes or services, but have not yet been closed or terminated. The presence of a large number of such sockets may indicate resource contention, network congestion, or other issues affecting system performance.\n\nThis metric can be useful in monitoring and alerting scenarios to detect potential problems with socket usage on the node. For example, it could trigger alerts when the count exceeds a certain threshold, indicating that the node is experiencing high levels of socket activity that may impact its ability to handle new connections or requests.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_FRAG6_memory"
    },
    "description": "The node_sockstat_FRAG6_memory metric measures the number of FRAG6 sockets that are in a memory state on the monitored node. This type of socket is typically associated with fragmented packets and can indicate issues related to network congestion or packet corruption. A high value for this metric may suggest that the system is experiencing difficulties handling incoming traffic, potentially leading to performance degradation or even service unavailability. In monitoring or alerting contexts, this metric could be used to detect potential network bottlenecks or errors, triggering further investigation and corrective actions as needed.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_FRAG_inuse"
    },
    "description": "The node_sockstat_FRAG_inuse metric measures the number of Fragmentation (FRAG) sockets on a node that are currently in an 'inuse' state. This indicates that these sockets are actively engaged in network communication and have data being sent or received. FRAG sockets are used for packet fragmentation, which is the process of breaking down large packets into smaller ones to facilitate transmission over networks with limited MTU (Maximum Transmission Unit) sizes.\n\nIn monitoring and alerting contexts, this metric can be useful for identifying potential issues related to network congestion, packet loss, or other performance bottlenecks. For instance, if the number of FRAG sockets in an 'inuse' state consistently exceeds a certain threshold, it may indicate that the node's network resources are being overwhelmed, leading to decreased system responsiveness and potentially impacting user experience.\n\nTo utilize this metric effectively, operators can set up alerts based on specific thresholds or trends in the data. For example, they might configure an alert to trigger when the number of FRAG sockets in 'inuse' state exceeds 50% of the total available sockets over a certain period, indicating potential network resource exhaustion.\n\nIt's worth noting that this metric provides a snapshot view of the current socket usage and does not offer historical context. Therefore, it should be used in conjunction with other metrics that provide more comprehensive insights into system performance and behavior.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_FRAG_memory"
    },
    "description": "The **node_sockstat_FRAG_memory** metric measures the total amount of memory, in bytes, allocated to FRAG sockets on a node. FRAG sockets handle the fragmentation and reassembly of IP packets during network communication. Monitoring this metric helps identify excessive memory usage related to packet fragmentation, which may impact network performance. Alerts can be configured to trigger when memory consumption by FRAG sockets exceeds defined thresholds, indicating potential resource issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_RAW6_inuse"
    },
    "description": "The **node_sockstat_RAW6_inuse** metric reports the current number of RAW6 sockets actively in use on a node, collected via Prometheus from the node exporter. RAW6 sockets are IPv6 raw sockets used for low-level network communication, often for custom protocols or diagnostic tools.\n\n**Purpose:**  \nThis metric helps SREs monitor the utilization of RAW6 sockets to ensure the node\u2019s network stack is operating within expected parameters. A sudden increase may indicate abnormal network activity, potential socket leaks, or resource exhaustion, while consistently low values are typical under normal operation unless specific applications require RAW6 sockets.\n\n**Alert Threshold:**  \nSet an alert if **node_sockstat_RAW6_inuse** exceeds a threshold that approaches the system\u2019s maximum allowed RAW6 sockets (e.g., 80-90% of the socket limit). For example, if the system limit is 1024, alert when usage exceeds 900 sockets for more than 5 minutes:\n\n```\nnode_sockstat_RAW6_inuse > 900\n```\n\n**Impact of Values:**  \n- **High values:** May lead to socket allocation failures, degraded network performance, or application errors due to resource exhaustion. Investigate for socket leaks, misbehaving applications, or unusual network traffic.  \n- **Low or zero values:** Normal if no applications use RAW6 sockets; no immediate concern.\n\n**Example Usage:**  \n- **Dashboard:** Plot **node_sockstat_RAW6_inuse** over time alongside total socket limits to visualize trends and capacity.  \n- **Alert Rule Example (Prometheus):**\n\n```yaml\nalert: HighRAW6SocketUsage\nexpr: node_sockstat_RAW6_inuse > 900\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High RAW6 socket usage on {{ $labels.instance }}\"\n  description: \"RAW6 sockets in use ({{ $value }}) exceed 90% of system limit on {{ $labels.instance }} for over 5 minutes.\"\n```\n\nMonitoring this metric helps prevent network resource exhaustion and maintain node stability.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_RAW_inuse"
    },
    "description": "The node_sockstat_RAW_inuse metric measures the number of RAW sockets in the 'inuse' state on a given node. A RAW socket is a type of socket that allows direct access to the underlying network interface without any protocol overhead. The 'inuse' state indicates that these sockets are currently being used by the system for various purposes, such as handling incoming or outgoing network traffic.\n\nThis metric can be useful in monitoring and alerting scenarios where high levels of RAW socket usage may indicate issues with network performance, resource utilization, or potential security vulnerabilities. For example, an unusually high number of RAW sockets in use could suggest a denial-of-service (DoS) attack or a misconfigured application that is consuming excessive system resources.\n\nIn terms of usage, this metric can be used to set up alerts for threshold breaches, such as when the number of RAW sockets in use exceeds a certain percentage of total available sockets. It can also be used to monitor trends and anomalies in RAW socket usage over time, helping operators identify potential issues before they impact system performance or security.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_TCP6_inuse"
    },
    "description": "The node_sockstat_TCP6_inuse metric measures the number of TCPv6 sockets that are currently in the 'inuse' state on a given node. This state typically indicates that the socket is actively engaged in communication with another endpoint and has not yet been closed or terminated. The presence of a large number of such sockets may indicate issues related to network congestion, resource exhaustion, or misconfigured applications. Potential implications for monitoring include tracking changes in this metric over time to detect potential performance bottlenecks or anomalies. In alerting, thresholds could be set to trigger notifications when the count exceeds a certain threshold, indicating potential issues with TCPv6 socket usage on the node.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_TCP_alloc"
    },
    "description": "The node_sockstat_TCP_alloc metric measures the number of TCP sockets in the 'alloc' state on a given node. This state typically indicates that the socket is being allocated for use by an application or service, but has not yet been established or connected. In other words, it represents the number of pending TCP connections waiting to be accepted by the server. High values of this metric may indicate issues with connection establishment, such as a backlog overflow or a denial-of-service (DoS) attack. It can also be used to monitor the performance and scalability of network services, particularly those that rely heavily on TCP connections. Potential implications for monitoring or alerting include: - Triggering alerts when the number of alloc sockets exceeds a certain threshold, indicating potential connection issues or DoS attacks. - Correlating this metric with other metrics, such as node_sockstat_TCP_estab (established connections) and node_sockstat_TCP_close (closed connections), to gain insights into TCP connection lifecycle and performance. - Using this metric in conjunction with application-level metrics to identify bottlenecks or issues specific to certain services or applications.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_TCP_inuse"
    },
    "description": "The **node_sockstat_TCP_inuse** metric measures the current number of TCP sockets in the \"inuse\" state on a node. This count represents sockets actively allocated for communication but not yet closed. The metric is a simple integer value indicating real-time TCP socket usage. It helps monitor network load and detect potential socket exhaustion or performance issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_TCP_mem"
    },
    "description": "The node_sockstat_TCP_mem metric measures the number of Transmission Control Protocol (TCP) sockets on a node that are in the 'memory' state. This state typically indicates that the socket is being held in memory for an extended period, potentially consuming system resources and impacting performance. The metric can be used to monitor and detect potential issues related to TCP socket management, such as resource leaks or inefficient socket usage. It may also serve as a precursor to more severe problems like high CPU utilization, memory exhaustion, or network congestion. In monitoring and alerting, this metric could trigger notifications when the number of sockets in the 'memory' state exceeds a certain threshold, allowing operators to investigate and address potential issues before they escalate.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_TCP_mem_bytes"
    },
    "description": "The node_sockstat_TCP_mem_bytes metric measures the total number of TCP sockets on a node that are in the 'mem' state and have allocated memory bytes. This metric provides insight into the system's network resource utilization and can be used to detect potential issues related to socket creation, connection establishment, or memory allocation. Potential implications include identifying nodes with high socket counts, which may indicate performance bottlenecks or security vulnerabilities. This metric can be used in monitoring and alerting to trigger notifications when the number of TCP sockets in the 'mem' state exceeds a certain threshold, indicating potential issues that require attention from operations teams.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_TCP_orphan"
    },
    "description": "The node_sockstat_TCP_orphan metric measures the number of TCP sockets on a node that are in an orphaned state. Orphaned TCP sockets occur when a connection is closed by one end but not the other, leaving the socket open and unmanaged. This can lead to resource leaks and potential security vulnerabilities if left unchecked.\n\nIn monitoring or alerting, this metric can be used to detect and prevent issues related to orphaned connections. For example, an alert can be triggered when the number of orphaned TCP sockets exceeds a certain threshold, indicating a potential problem with connection management or network configuration. Additionally, this metric can be used in conjunction with other metrics, such as node_sockstat_TCP_active or node_sockstat_TCP_listen, to gain a more comprehensive understanding of the node's socket usage and identify areas for optimization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_TCP_tw"
    },
    "description": "The node_sockstat_TCP_tw metric measures the number of Transmission Control Protocol (TCP) sockets on a node that are in the 'tw' state. The 'tw' state typically represents TCP wait or timeout state, where a connection has been idle for an extended period and is waiting to be closed by either the client or server. This can indicate potential issues with network connectivity, application performance, or resource constraints. Monitoring this metric can help identify nodes experiencing high levels of idle connections, which may lead to increased latency, memory usage, or even node crashes. Potential uses include alerting on high TCP wait state counts, investigating root causes through log analysis and debugging, and optimizing system configurations for improved network efficiency.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_UDP6_inuse"
    },
    "description": "The node_sockstat_UDP6_inuse metric measures the number of UDPv6 sockets that are currently in the 'inuse' state on a given node. This state typically indicates that the socket is actively engaged in communication with another endpoint and has not yet been closed or reused. The metric provides insight into the current network activity and resource utilization on the node, specifically for UDPv6 traffic. It can be used to monitor and troubleshoot issues related to network connectivity, socket exhaustion, or resource bottlenecks. Potential implications include identifying nodes that are experiencing high levels of UDPv6 traffic, detecting potential denial-of-service (DoS) attacks, or optimizing system resources to improve overall performance.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_UDPLITE6_inuse"
    },
    "description": "The node_sockstat_UDPLITE6_inuse metric measures the number of UDP-Lite v6 sockets that are currently in the 'inuse' state. This means that these sockets have been established and are actively being used for communication between the local system and remote endpoints. The 'inuse' state typically indicates that data is being sent or received over these sockets, making them a critical component of network activity on the node.\n\nIn monitoring and alerting contexts, this metric can be useful for detecting potential issues with UDP-Lite v6 connectivity or resource utilization. For example, if the number of inuse UDPLITE6 sockets suddenly increases or remains consistently high over time, it may indicate a denial-of-service (DoS) attack or other network congestion issue.\n\nAdditionally, this metric can be used to monitor the overall health and performance of UDP-Lite v6 services on the node. By tracking changes in the number of inuse UDPLITE6 sockets, operators can gain insights into how these services are being utilized and identify potential bottlenecks or areas for optimization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_UDPLITE_inuse"
    },
    "description": "The node_sockstat_UDPLITE_inuse metric measures the number of UDP-Lite sockets that are currently in the 'inuse' state on a given node. This means that these sockets have been established and are actively being used for communication. The 'inuse' state typically indicates that the socket is engaged in an ongoing data transfer or connection, and it may be consuming system resources such as memory, CPU, or network bandwidth.\n\nThis metric can be useful for monitoring and alerting purposes to detect potential issues related to UDP-Lite socket usage on a node. For example, if this value consistently remains high over time, it could indicate that the node is experiencing resource constraints due to excessive UDP-Lite traffic. Conversely, if the value suddenly spikes or drops significantly, it may indicate an issue with the network connection, a misconfigured application, or even a security threat.\n\nIn terms of usage, this metric can be used in conjunction with other metrics to gain a more comprehensive understanding of system performance and resource utilization. For instance, combining node_sockstat_UDPLITE_inuse with metrics such as memory usage, CPU load, or network I/O can help identify potential bottlenecks or areas for optimization.\n\nIt is worth noting that the 'inuse' state does not necessarily imply any issues with the socket itself; it simply indicates that the socket is being used. Therefore, this metric should be interpreted in conjunction with other metrics and system logs to determine if there are any underlying problems.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_UDP_inuse"
    },
    "description": "The node_sockstat_UDP_inuse metric measures the number of UDP sockets that are currently in the 'inuse' state on a given node. This means that these sockets have been established and are actively being used for communication between the node and other endpoints, but they have not yet been closed or terminated. The 'inuse' state typically indicates that the socket is engaged in an ongoing conversation or data transfer with another peer. High values of this metric may indicate issues such as network congestion, resource starvation, or misconfigured applications that are creating excessive UDP connections. This metric can be used to monitor and alert on potential issues related to UDP socket usage, such as identifying nodes with high numbers of in-use UDP sockets, which could impact performance or lead to resource exhaustion.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_UDP_mem"
    },
    "description": "The node_sockstat_UDP_mem metric measures the number of UDP sockets on a node that are in the 'mem' state. This typically indicates that the socket is being held in memory and has not been released back to the operating system for reuse or recycling. The presence of a large number of UDP sockets in this state may indicate resource exhaustion, inefficient network usage, or other issues that could impact node performance. Potential implications include increased memory usage, decreased network throughput, or even node crashes due to excessive resource consumption. This metric can be used in monitoring and alerting to detect potential issues before they escalate into major problems.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_UDP_mem_bytes"
    },
    "description": "The **node_sockstat_UDP_mem_bytes** metric measures the total amount of memory, in bytes, currently used by UDP sockets on a node for buffering data. It reflects the memory allocated to UDP socket buffers, not the number of sockets. Monitoring this metric helps identify excessive memory usage by UDP sockets, which may indicate resource leaks or abnormal network activity. Sudden increases can signal potential issues such as denial-of-service attacks or misconfigured applications.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_sockstat_sockets_used"
    },
    "description": "The **node_sockstat_sockets_used** metric reports the current number of IPv4 sockets in use on a node, including all socket states (e.g., listening, established, syn-sent, fin-wait). This metric helps SREs monitor network socket consumption to identify abnormal socket usage patterns that may indicate resource exhaustion, application issues, or potential security risks.\n\n**Purpose:**  \nIn Prometheus, this metric is used to track socket utilization trends over time, enabling detection of sudden spikes or sustained high usage that could degrade network performance or cause socket allocation failures.\n\n**Alerting Thresholds:**  \nSet alert thresholds based on your node\u2019s capacity and typical socket usage patterns. For example:  \n- **Warning:** when `node_sockstat_sockets_used` exceeds 80% of the system\u2019s maximum allowed sockets (e.g., if max sockets = 100000, alert if > 80000).  \n- **Critical:** when usage exceeds 90-95% of max sockets, indicating imminent risk of socket exhaustion.\n\n**Impact of Values:**  \n- **High values:** May lead to socket allocation failures, degraded application performance, or inability to accept new connections. Could indicate a socket leak, DDoS attack, or misconfigured application.  \n- **Low values:** Generally normal, but sudden drops might indicate network service restarts or failures.\n\n**Example Usage:**  \n- **Dashboard:** Plot `node_sockstat_sockets_used` over time alongside `node_sockstat_sockets_max` (if available) to visualize socket usage relative to capacity.  \n- **Alert Rule (PromQL):**  \n  ```promql\n  node_sockstat_sockets_used / node_sockstat_sockets_max > 0.8\n  ```  \n  This triggers an alert when socket usage exceeds 80% of the maximum allowed sockets.\n\nBy monitoring this metric with appropriate thresholds, SREs can proactively manage socket resources, prevent outages, and maintain network service reliability.  \n\n*Note: This metric only accounts for IPv4 sockets; consider complementary metrics for IPv6 socket monitoring.*",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_softnet_backlog_len",
      "cpu": "*"
    },
    "description": "The node_softnet_backlog_len metric measures the length of the softnet backlog on a Linux system. The softnet backlog is a buffer that holds incoming network packets while they are being processed by the kernel. This metric indicates the number of packets waiting to be processed, which can be an indicator of network congestion or processing delays. High values may indicate issues with network throughput, packet loss, or kernel resource constraints. Potential implications for monitoring and alerting include: triggering alerts when the backlog exceeds a certain threshold, investigating network configuration or kernel tuning issues, or correlating this metric with other network-related metrics to identify root causes of performance degradation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_softnet_cpu_collision_total",
      "cpu": "*"
    },
    "description": "The node_softnet_cpu_collision_total metric measures the total number of CPU collisions that occurred while obtaining a device lock for transmission on the node. This metric is specific to Linux systems and is related to the Softnet Device Driver, which manages network packet processing. A high value for this metric may indicate contention or resource constraints in the system's CPU resources, potentially leading to performance degradation or dropped packets. It can be used in monitoring and alerting to detect issues with network I/O or CPU saturation, allowing operators to investigate and optimize their systems accordingly.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_softnet_dropped_total",
      "cpu": "*"
    },
    "description": "The **node_softnet_dropped_total** metric in Prometheus tracks the cumulative number of network packets dropped by the Linux softnet layer on each CPU core (indicated by the `cpu` label). This metric helps SREs monitor the kernel\u2019s ability to process incoming network packets and identify potential network stack bottlenecks or resource exhaustion.\n\n**Purpose:**  \nIt measures packet drops occurring after the network interface but before packets reach the application, often caused by CPU overload, interrupt handling delays, or network congestion.\n\n**Alerting Thresholds:**  \nA sustained increase in dropped packets is a strong indicator of network processing issues. A practical alert threshold could be:  \n- Alert if `rate(node_softnet_dropped_total[cpu=\"*\"][5m]) > 100` packets per second per CPU core for more than 5 minutes.  \nAdjust thresholds based on baseline traffic and system capacity.\n\n**Impact of Values:**  \n- **Low or zero values:** Normal operation, indicating the system is handling network traffic efficiently.  \n- **High or increasing values:** Potential packet loss leading to degraded application performance, increased latency, or data loss, especially critical in latency-sensitive or high-throughput environments.\n\n**Example Usage:**  \n- **Alert Rule:**  \n```yaml\nalert: HighSoftnetPacketDrops  \nexpr: sum by(cpu) (rate(node_softnet_dropped_total[5m])) > 100  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"High packet drops detected on CPU {{ $labels.cpu }}\"  \n  description: \"Softnet packet drops exceed 100 packets/sec on CPU {{ $labels.cpu }} for over 5 minutes, indicating possible network stack congestion.\"  \n```\n\n- **Dashboard Panel Query:**  \n```promql\nsum by(cpu) (rate(node_softnet_dropped_total[1m]))\n```\nThis query shows the per-CPU packet drop rate over the last minute, helping visualize trends and identify problematic CPUs.\n\nMonitoring this metric enables proactive detection of network processing issues, allowing timely intervention such as tuning network parameters, scaling resources, or investigating hardware problems.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_softnet_flow_limit_count_total",
      "cpu": "*"
    },
    "description": "The node_softnet_flow_limit_count_total metric measures the total number of times the flow limit has been reached on a given node. This metric is likely related to network traffic management and may indicate potential congestion or bottlenecks within the system. It could be used in monitoring to detect when the flow limit is consistently being exceeded, potentially leading to degraded performance or even outages. In alerting, this metric might trigger notifications when the count exceeds a certain threshold, prompting operators to investigate and take corrective action to manage network traffic and prevent future issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_softnet_processed_total",
      "cpu": "*"
    },
    "description": "The **node_softnet_processed_total** metric counts the total number of network packets processed by the Linux softnet layer on a specific CPU. It is a cumulative counter measured in packets. This metric reflects the volume of packets handled by the kernel\u2019s network processing queue. Monitoring it helps identify changes in network load or potential processing bottlenecks per CPU.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_softnet_received_rps_total",
      "cpu": "*"
    },
    "description": "The **node_softnet_received_rps_total** metric, labeled by CPU (e.g., cpu=\"0\", cpu=\"1\"), measures the rate of packets per second processed by the Linux kernel's softnet layer on each CPU core. This metric is collected by Prometheus via the node exporter and reflects the incoming network packet processing load distributed across CPUs.\n\n**Purpose:**  \nIt helps SREs monitor how efficiently the system handles incoming network traffic at the kernel level, enabling detection of CPU bottlenecks or packet drops caused by network stack congestion.\n\n**Thresholds and Alerting:**  \nA sustained high value (e.g., consistently above 100,000 packets per second per CPU, depending on hardware capacity) may indicate heavy network load or potential softnet backlog buildup, risking packet drops and degraded network performance. Conversely, unusually low values during expected high traffic periods might signal network interface issues or misconfigurations.\n\n**Impact:**  \n- **High values:** May lead to increased CPU utilization, softnet backlog growth, and packet drops, causing network latency or throughput degradation.  \n- **Low values:** Could indicate underutilized network capacity or potential network interface problems.\n\n**Example Alert Rule:**  \n```yaml\nalert: HighSoftnetReceivedRPS\nexpr: sum by(cpu) (rate(node_softnet_received_rps_total[1m])) > 100000\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High incoming packet rate on CPU {{ $labels.cpu }}\"\n  description: \"The softnet received packets per second on CPU {{ $labels.cpu }} has exceeded 100,000 for more than 5 minutes, indicating potential network processing bottlenecks.\"\n```\n\n**Example Dashboard Usage:**  \nPlot `rate(node_softnet_received_rps_total[1m])` per CPU to visualize incoming packet processing load distribution, helping identify CPUs under heavy network processing pressure and enabling capacity planning or tuning of network stack parameters.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_softnet_times_squeezed_total",
      "cpu": "*"
    },
    "description": "The node_softnet_times_squeezed_total metric measures the total number of times the Linux softnet (software network) processing has run out of quota on a given node. This quota is typically set to manage and limit the amount of network packets that can be processed by the system, preventing potential denial-of-service (DoS) attacks or overwhelming the system with excessive network traffic.\n\nThis metric can be used in monitoring and alerting to detect situations where the system's network processing capabilities are being overwhelmed, potentially leading to packet drops, delays, or other network-related issues. It may indicate a need for increased resources, such as more memory or CPU power, or adjustments to the softnet quota settings.\n\nIn terms of potential implications, this metric can be used to:\n- Identify nodes that are consistently running out of quota, indicating potential resource bottlenecks or misconfigurations.\n- Trigger alerts when the number of times processing packets ran out of quota exceeds a certain threshold, signaling a need for immediate attention from operations teams.\n- Inform capacity planning and resource allocation decisions by providing insights into the system's network processing capabilities.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_textfile_scrape_error"
    },
    "description": "The node_textfile_scrape_error metric measures the occurrence of errors during file scraping operations on a Prometheus node. It indicates whether there was an error opening or reading a file (value: 1) or not (value: 0). This metric can be used to monitor and troubleshoot issues related to text file scraping, such as configuration errors, permission issues, or network connectivity problems. Potential implications include identifying nodes with persistent scraping errors, which may indicate underlying infrastructure or configuration issues that require attention from operations teams.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_time_clocksource_available_info",
      "clocksource": "*",
      "device": "*"
    },
    "description": "This metric indicates whether a specific clock source and device are currently available on the node, represented as a binary value (1 for available, 0 for unavailable). It reflects the system's ability to access and use clock sources listed under '/sys/devices/system/clocksource'. Monitoring this metric helps identify issues with hardware or software clock sources that could affect timekeeping accuracy. Alerts can be triggered if a clock source becomes unavailable, enabling timely troubleshooting.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_time_clocksource_current_info",
      "clocksource": "*",
      "device": "*"
    },
    "description": "The **node_time_clocksource_current_info** metric reports the current clock source used by the system\u2019s kernel for timekeeping, as read from `/sys/devices/system/clocksource/clocksource0/current_clocksource`. The metric labels include the clocksource name (e.g., `tsc`, `hpet`, `acpi_pm`) and device, allowing you to identify which hardware or software timer is active.\n\n**Purpose:**  \nThis metric helps SREs monitor the system\u2019s timekeeping mechanism, which is critical for accurate timestamps, scheduling, and distributed system coordination. Changes in the clock source can indicate kernel or hardware issues, or configuration changes that may affect system stability.\n\n**Alerting Guidance:**  \n- **Thresholds:** Alert if the clocksource changes unexpectedly from a known stable source to an unknown or less reliable one. For example, if the clocksource switches from `tsc` (high-resolution CPU timer) to `acpi_pm` (lower resolution), it may indicate hardware or kernel problems.  \n- Since this metric is informational (string labels), alerts should be based on detecting changes or unexpected values rather than numeric thresholds.\n\n**Impact:**  \n- A stable, high-resolution clocksource (e.g., `tsc`) ensures precise timekeeping and minimal clock drift.  \n- Switching to a lower-quality clocksource can cause time drift, inaccurate timestamps, and issues in time-sensitive applications like distributed databases or logging systems.\n\n**Example Usage:**  \n- **Dashboard:** Display the current clocksource as a label or single stat panel to quickly verify the active timer on each node.  \n- **Alert Rule (PromQL):**  \n```promql\nchanges(node_time_clocksource_current_info[5m]) > 0\nand on(instance) node_time_clocksource_current_info{clocksource!=\"tsc\"}\n```\nThis alerts if the clocksource changes within 5 minutes and the new source is not `tsc`, indicating a potential problem requiring investigation.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_time_seconds"
    },
    "description": "The node_time_seconds metric measures the system time in seconds since the Unix epoch (January 1, 1970). This metric represents the current system clock time and can be used to monitor the accuracy of the system's clock. It may also indicate potential issues with the system's time synchronization or clock drift. In monitoring or alerting, this metric could be used to detect anomalies in system time, such as sudden changes or deviations from expected values. For example, an alert could be triggered if the system time deviates by more than a certain threshold (e.g., 1 minute) from the expected value. Additionally, this metric can be used to correlate with other metrics that rely on accurate system time, such as log timestamps or network communication timing.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_time_zone_offset_seconds",
      "time_zone": "*"
    },
    "description": "The node_time_zone_offset_seconds metric measures the difference in seconds between the system's local time and Coordinated Universal Time (UTC). This offset is typically determined by the system's time zone setting. It can be used to detect potential issues with the system clock, such as drift or incorrect configuration. In monitoring or alerting, this metric could be used to trigger alerts when the offset exceeds a certain threshold, indicating a possible problem with the system's timekeeping.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_estimated_error_seconds"
    },
    "description": "The node_timex_estimated_error_seconds metric measures the estimated error in seconds for the system clock on a given node. This metric is based on the Timex algorithm, which estimates the error between the system clock and a reference time source. The estimated error is calculated as the maximum absolute difference between the system clock and the reference time source over a certain period of time. This metric can be used to detect potential issues with the system clock, such as drift or synchronization problems, which may impact the accuracy of timestamped events or logs. In monitoring or alerting, this metric can be used to trigger alerts when the estimated error exceeds a certain threshold, indicating a potential issue with the system clock.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_frequency_adjustment_ratio"
    },
    "description": "The node_timex_frequency_adjustment_ratio metric measures the ratio of the frequency adjustments made by the Linux kernel's timekeeping mechanism (timex) to the actual system clock frequency. This value indicates how often the kernel has adjusted the system clock to maintain synchronization with a reference clock, such as NTP. A higher ratio may indicate issues with the system clock or network connectivity, potentially leading to timing-related problems in applications that rely on precise timekeeping.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_loop_time_constant"
    },
    "description": "The node_timex_loop_time_constant metric measures the time constant of the phase-locked loop (PLL) in the Linux kernel's Timekeeping subsystem, specifically within the NTP (Network Time Protocol) implementation. This value represents the average time it takes for the PLL to adjust its frequency and lock onto a reference clock source. A lower time constant indicates faster convergence and better synchronization with external clocks. In monitoring or alerting, this metric can be used to detect potential issues with the system's timekeeping accuracy, such as increased jitter or drift from expected values. It may also indicate problems with the NTP configuration or connectivity. If the value is consistently high or increasing, it could suggest a hardware or software issue affecting the system's clock synchronization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_maxerror_seconds"
    },
    "description": "The node_timex_maxerror_seconds metric measures the maximum error in seconds observed by the system clock on a given node. This metric is related to the timex module, which is used for timekeeping and synchronization. A high value indicates that the system clock has deviated significantly from the expected time, potentially causing issues with scheduling, timing-dependent operations, or data consistency. This metric can be used in monitoring to detect potential problems with time synchronization, such as NTP (Network Time Protocol) configuration errors or hardware-related issues. It may also trigger alerts for nodes with consistently high maximum error values, indicating a need for investigation and corrective action.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_offset_seconds"
    },
    "description": "The node_timex_offset_seconds metric measures the time offset in seconds between the local system clock and a reference clock, typically used as a source of truth for timekeeping. This metric is crucial in ensuring that the system's clock remains synchronized with a reliable external time source. A non-zero value indicates a discrepancy between the local system clock and the reference clock, which can have significant implications on various aspects of system operation, including but not limited to: scheduling tasks, processing timestamps, and maintaining data integrity. Potential usage includes monitoring for large offsets (e.g., > 1 second) to detect potential issues with time synchronization, alerting when offsets exceed a certain threshold, or using this metric as an input for more complex calculations in observability pipelines.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_calibration_total"
    },
    "description": "The node_timex_pps_calibration_total metric measures the total count of pulse per second (PPS) calibration intervals performed by a Prometheus node. This metric is related to the TimeeX PPS calibration mechanism, which ensures that the node's clock remains synchronized with an external reference time source. The calibration process involves adjusting the node's clock to match the external time source's frequency and phase. A high count of calibration intervals may indicate issues with the node's clock synchronization or the quality of the external time source. This metric can be used in monitoring and alerting to detect potential problems with timekeeping, such as drifts or losses of synchronization. It can also serve as a diagnostic tool for troubleshooting PPS-related issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_error_total"
    },
    "description": "The node_timex_pps_error_total metric measures the total count of pulse per second (PPS) calibration errors encountered by a Prometheus node. This metric is related to the Timebase Corrector (TIMEX) and PPS signal used for time synchronization. A high value indicates potential issues with the system's ability to accurately synchronize its clock, which can impact monitoring and alerting accuracy. Potential implications include: (1) Investigating the root cause of the calibration errors, such as faulty hardware or misconfigured settings. (2) Ensuring that the node's clock is properly synchronized with other systems in the infrastructure. (3) Implementing additional monitoring to detect and respond to PPS signal issues before they impact system performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_frequency_hertz"
    },
    "description": "The node_timex_pps_frequency_hertz metric measures the frequency of Pulse Per Second (PPS) signals generated by the system's Timebase-64 (timex) driver. This metric is typically used to monitor the accuracy and stability of the system clock, which is critical for various applications such as synchronization, timing, and high-performance computing.\n\nIn a healthy system, this value should be close to 1 Hz, indicating that one PPS signal is generated per second. Any deviation from this expected value may indicate issues with the timex driver or the underlying hardware.\n\nThis metric can be used in monitoring and alerting to detect potential problems such as:\n- Timex driver errors or crashes\n- Hardware issues affecting the system clock\n- Synchronization problems due to incorrect PPS signal generation\n\nIt is essential to note that this metric requires proper configuration of the timex driver and may not be applicable to all systems. If unsure, consult the relevant documentation for your specific use case.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_jitter_seconds"
    },
    "description": "The node_timex_pps_jitter_seconds metric measures the jitter (variability or dispersion) of pulse per second (PPS) signals received by the node's Time eXtended (timex) system. PPS signals are used to synchronize clocks and provide a precise timing reference. Jitter in PPS signals can indicate issues with the signal source, transmission medium, or receiver, potentially affecting clock synchronization accuracy and overall system reliability. This metric can be used to monitor the stability of PPS signals and detect anomalies that may require investigation or corrective action.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_jitter_total"
    },
    "description": "The node_timex_pps_jitter_total metric measures the total count of events where the pulse per second (PPS) jitter limit has been exceeded on a node. PPS jitter refers to the deviation in the time interval between consecutive pulses. This metric can be used to identify potential issues with the node's timing and synchronization, such as hardware or software problems affecting the PPS signal. High values of this metric may indicate a need for further investigation into the node's timing configuration or hardware health. It could also trigger alerts for operations teams to take corrective action before it affects system performance or data integrity.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_shift_seconds"
    },
    "description": "The node_timex_pps_shift_seconds metric measures the duration of pulse per second (PPS) interval shifts in seconds on a Prometheus node. This metric is related to the Pulse Per Second (PPS) signal, which is used as a reference clock for timekeeping. The PPS signal is typically generated by a hardware timer or a dedicated chip and is used to synchronize clocks across systems. A shift in the PPS interval can indicate issues with the system's clock synchronization, such as drifts or changes in the system's clock frequency. This metric can be used to monitor the stability of the system's timekeeping and detect potential problems that may affect the accuracy of timestamps or scheduling. Potential implications for monitoring or alerting include detecting sudden changes in PPS interval duration, which could indicate hardware issues, software bugs, or configuration errors.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_stability_exceeded_total"
    },
    "description": "This metric measures the total count of events where the pulse per second (PPS) stability limit has been exceeded on a node. The PPS stability limit is a threshold that indicates whether the node's clock synchronization with a reference time source (such as NTP or a hardware-based PPS signal) is stable and accurate. When this limit is exceeded, it may indicate issues with the node's clock synchronization, such as network connectivity problems, configuration errors, or hardware malfunctions. This metric can be used to detect potential issues with clock synchronization, which can have significant implications for system reliability, data integrity, and compliance in environments where precise timing is critical (e.g., financial transactions, scientific simulations). It may also serve as a leading indicator of more severe problems, such as node crashes or data corruption. In monitoring and alerting, this metric can be used to trigger alerts when the count exceeds a certain threshold, indicating that corrective action is needed to prevent potential issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_pps_stability_hertz"
    },
    "description": "The node_timex_pps_stability_hertz metric measures the average frequency changes of the Pulse Per Second (PPS) signal over a recent time window. This metric is indicative of the stability and accuracy of the system's clock synchronization with an external reference, such as a GPS signal or a network-based timing source. A high value indicates that the PPS signal has been stable and consistent, while a low value suggests that there have been significant frequency changes, potentially indicating issues with the timing source or the system's ability to synchronize with it. This metric can be used in monitoring and alerting to detect potential problems with clock synchronization, which may impact system performance, data integrity, or compliance with regulatory requirements.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_status"
    },
    "description": "The **node_timex_status** metric reports the current status flags of the Linux Timex subsystem on a node, representing various timekeeping conditions such as clock synchronization state, frequency adjustments, and error states. Each bit in the status value corresponds to a specific condition (e.g., PLL, FLL, or PPS synchronization active, or time offset errors). Monitoring this metric helps ensure the node\u2019s system clock remains accurate and stable, which is critical for distributed systems relying on precise time.\n\n**Alerting guidance:**  \nTrigger an alert if **node_timex_status** indicates any error or warning bits are set, such as loss of synchronization or clock discipline failure. For example, alert if `(node_timex_status & ERROR_BITS) != 0`, where `ERROR_BITS` is a bitmask of known problematic flags (e.g., bits indicating clock unsynchronized or time adjustment failures). Thresholds depend on your environment but typically any non-zero error bit should warrant investigation.\n\n**Impact:**  \n- **High values (with error bits set):** Indicate timekeeping issues that can cause problems like inconsistent timestamps, failed distributed coordination, or degraded system performance.  \n- **Low or zero values:** Indicate normal, stable time synchronization.\n\n**Example alert rule:**  \n```yaml\nalert: NodeTimexStatusError\nexpr: (node_timex_status & 0x1F) != 0\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Node {{ $labels.instance }} Timex status error\"\n  description: \"The node_timex_status metric indicates timekeeping errors on node {{ $labels.instance }}. Investigate clock synchronization and system time stability.\"\n```\n\n**Example dashboard usage:**  \nDisplay the raw **node_timex_status** value alongside decoded bit flags to quickly identify which timekeeping conditions are active, enabling rapid diagnosis of timing issues affecting the node.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_sync_status"
    },
    "description": "The node_timex_sync_status metric measures whether a node's system clock is synchronized to a reliable time source, such as an NTP (Network Time Protocol) server. A value of 1 indicates that the clock is successfully synchronized, while a value of 0 suggests that synchronization has failed or is not occurring. This metric can be used in monitoring and alerting to detect potential issues with system clock accuracy, which may impact various aspects of node operation, including logging, auditing, and data processing. It can also serve as an indicator for more severe problems, such as hardware failures or network connectivity issues.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_tai_offset_seconds"
    },
    "description": "The node_timex_tai_offset_seconds metric measures the offset between the International Atomic Time (TAI) and the system time on a given node. TAI is a modern continuation of International Atomic Time (TAI), which is a time standard that is intended to be independent of human intervention. This metric can be used to detect potential issues with the system clock, such as drift or synchronization problems. In monitoring or alerting, this metric might be used to trigger alerts when the offset exceeds a certain threshold, indicating a possible issue with the system's timekeeping. It may also be useful for debugging purposes, such as identifying nodes that are not synchronizing correctly with a reference time source.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_timex_tick_seconds"
    },
    "description": "The **node_timex_tick_seconds** metric measures the duration (in seconds) between consecutive system clock ticks as reported by the node's timekeeping subsystem. It reflects the stability and accuracy of the system clock's tick interval, which is critical for time-sensitive operations such as logging, event ordering, and distributed coordination.\n\n**Purpose:**  \nThis metric helps SREs monitor the consistency of the system clock's tick rate. Deviations from the expected tick interval can indicate clock drift, synchronization issues with NTP/PTP servers, or underlying hardware problems affecting timekeeping.\n\n**Thresholds and Alerting:**  \n- Normal tick interval is typically stable and close to a known baseline (e.g., around 0.01 seconds, depending on system configuration).  \n- Alert if **node_timex_tick_seconds** deviates by more than \u00b110% from the baseline for a sustained period (e.g., 5 minutes).  \n- Example alert condition:  \n  `abs(node_timex_tick_seconds - baseline_tick) > 0.001` sustained over 5 minutes.  \n- Such deviations may warrant alerts labeled as \"Clock tick interval instability\" or \"Potential clock synchronization failure.\"\n\n**Impact of Values:**  \n- **High values:** Longer tick intervals can cause time to appear to move slower than real time, potentially leading to delayed event processing, inaccurate timestamps, and issues in time-dependent applications.  \n- **Low values:** Shorter tick intervals may indicate clock jitter or instability, causing inconsistent time progression and possible synchronization errors.\n\n**Example Usage:**  \n- **Dashboard:** Plot `node_timex_tick_seconds` over time alongside NTP synchronization status to correlate clock stability with sync health.  \n- **Alert Rule Example (Prometheus):**  \n  ```yaml\n  alert: ClockTickIntervalUnstable\n  expr: abs(node_timex_tick_seconds - 0.01) > 0.001\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"System clock tick interval unstable on {{ $labels.instance }}\"\n    description: \"The node_timex_tick_seconds metric has deviated from the expected 0.01s tick interval for over 5 minutes, indicating potential clock synchronization issues.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_udp_queues",
      "ip": "*",
      "queue": "*"
    },
    "description": "The node_udp_queues metric measures the number of allocated memory in the kernel for UDP datagrams in bytes. This metric indicates the amount of system resources being utilized by the kernel to handle incoming and outgoing UDP traffic. High values may indicate network congestion or inefficient use of system resources, potentially leading to performance degradation or even crashes. It can be used in monitoring to detect anomalies in UDP traffic patterns, alerting teams to investigate and optimize their systems for better resource utilization.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_uname_info",
      "domainname": "*",
      "machine": "*",
      "nodename": "*",
      "release": "*",
      "sysname": "*",
      "version": "*"
    },
    "description": "The **node_uname_info** metric in Prometheus exposes static system information obtained from the Unix `uname` system call on each monitored node. It provides a constant value of `1` with labels that describe the system's identity and environment, including:\n\n- **sysname**: The operating system name (e.g., Linux).\n- **nodename**: The network node hostname.\n- **release**: The OS release version.\n- **version**: The OS version string.\n- **machine**: The hardware architecture (e.g., x86_64).\n- **domainname**: The NIS or DNS domain name.\n\nThis metric does not represent a numeric measurement with units but serves as a set of descriptive labels to uniquely identify the system's kernel and environment details at scrape time.\n\nBecause these values are expected to remain constant for a given node, any unexpected changes\u2014such as a sudden change in `nodename`, `release`, or `machine`\u2014may indicate system reconfiguration, unauthorized modifications, or potential misconfigurations. Alerting on such changes can help detect unintended system updates, node replacements, or security incidents. Monitoring this metric is useful for inventory tracking, verifying node consistency, and ensuring compliance with expected system configurations.",
    "metric_type": "gauge",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_vmstat_oom_kill"
    },
    "description": "The node_vmstat_oom_kill metric measures the number of times the Out-Of-Memory (OOM) killer has been invoked on a node. The OOM killer is a mechanism that terminates processes to free up memory when the system runs out of available memory. This metric can be used to detect and diagnose memory-related issues, such as high memory usage or memory leaks, which may lead to the invocation of the OOM killer. Potential implications include identifying nodes with frequent OOM killer invocations, indicating potential memory bottlenecks or resource constraints. This metric can be used in monitoring and alerting to trigger notifications when the OOM killer is invoked excessively, allowing operators to investigate and address underlying issues before they impact system performance or availability.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_vmstat_pgfault"
    },
    "description": "The node_vmstat_pgfault metric measures the number of page faults that occurred on the system since it was booted. A page fault occurs when a process attempts to access a memory page that is not currently in physical RAM and must be retrieved from disk or another storage device. This can indicate issues with memory usage, disk I/O performance, or inefficient application behavior. Potential implications for monitoring include: \n\n* Identifying memory bottlenecks or resource constraints.\n* Detecting potential issues with application performance or optimization.\n* Triggering alerts when page faults exceed a certain threshold, indicating system instability or resource exhaustion.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_vmstat_pgmajfault"
    },
    "description": "The node_vmstat_pgmajfault metric measures the number of major page faults that occurred on the system since it was booted. A major page fault occurs when a process attempts to access a memory page that is not in physical RAM and must be retrieved from disk, resulting in significant performance degradation. This metric can be used to detect potential issues with memory usage or disk I/O bottlenecks, triggering alerts for operations teams to investigate and optimize system resources.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_vmstat_pgpgin"
    },
    "description": "The node_vmstat_pgpgin metric measures the number of pages that have been read into memory from disk since boot time. This value represents a cumulative count and can be used to monitor the system's page-in activity, which may indicate issues with memory availability or disk I/O performance. Potential implications include identifying systems under heavy load, detecting potential memory leaks, or triggering alerts when this metric exceeds a certain threshold. It is essential to consider this metric in conjunction with other relevant metrics, such as node_vmstat_pgpgout and node_memory_MemAvailable, for a comprehensive understanding of the system's memory usage.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_vmstat_pgpgout"
    },
    "description": "The **node_vmstat_pgpgout** metric in Prometheus tracks the number of memory pages written from the page cache to disk per second. This metric reflects the system\u2019s disk write activity caused by memory pressure or page eviction, which can impact overall system performance. High values indicate frequent page writes to disk, often due to insufficient RAM or heavy memory usage, leading to increased disk I/O latency and potential application slowdowns. Conversely, consistently low values suggest stable memory usage with minimal paging.\n\n**Threshold guidance:**  \nSet alert thresholds based on baseline observations for your environment. For example, trigger a warning if **node_vmstat_pgpgout** exceeds 1000 pages/sec sustained over 5 minutes, and a critical alert if it surpasses 5000 pages/sec, indicating severe memory pressure or disk bottlenecks.\n\n**Impact:**  \n- **High values:** May cause increased latency, degraded application performance, and potential system instability due to excessive disk writes.  \n- **Low values:** Generally indicate healthy memory usage and minimal paging activity.\n\n**Example alert rule:**  \n```yaml\nalert: HighPageCacheWrites  \nexpr: rate(node_vmstat_pgpgout[5m]) > 1000  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"High page cache write rate detected\"  \n  description: \"node_vmstat_pgpgout is above 1000 pages/sec for more than 5 minutes, indicating potential memory pressure or disk I/O issues.\"  \n```\n\n**Dashboard usage:**  \nPlot the rate of **node_vmstat_pgpgout** over time to identify trends or spikes in page cache writes. Correlate with other metrics like memory usage, disk I/O, and application latency to diagnose root causes of performance degradation.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_vmstat_pswpin"
    },
    "description": "The node_vmstat_pswpin metric measures the number of pages that have been swapped in from disk to RAM. This value represents the total number of page faults that occurred due to a lack of available memory, resulting in data being retrieved from disk and brought into RAM. High values may indicate memory pressure or inefficient system resource utilization, potentially leading to performance degradation or even system crashes. Monitoring this metric can help identify issues related to memory usage, such as high memory consumption by specific processes or applications, and inform decisions on resource allocation, caching strategies, or even hardware upgrades.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_vmstat_pswpout"
    },
    "description": "The **node_vmstat_pswpout** metric in Prometheus measures the number of memory pages swapped out from physical RAM to disk per second. It reflects how frequently the system is offloading memory contents to swap space, typically due to memory pressure or insufficient available RAM. \n\n**Purpose:**  \nThis metric helps SREs monitor memory pressure and system performance by indicating when the system relies on disk-based swap, which is significantly slower than RAM access.\n\n**Thresholds and Alerting:**  \nA sustained **node_vmstat_pswpout** rate above 10-20 pages per second may indicate problematic memory pressure and warrant investigation. Alert thresholds should be tailored to your environment but a common alert rule might trigger if the metric exceeds 15 pages/sec for more than 5 minutes, signaling potential performance degradation.\n\n**Impact of Values:**  \n- **High values:** Indicate active swapping, which can cause increased latency, slow application response times, and overall system performance degradation due to slow disk I/O. This often suggests insufficient physical memory or memory leaks.  \n- **Low or zero values:** Indicate that the system is not swapping, which is generally desirable and implies adequate memory availability.\n\n**Example Usage:**  \n- **Dashboard:** Plot `node_vmstat_pswpout` as a time series graph to visualize swap activity trends over time, correlating spikes with application performance issues.  \n- **Alert Rule (PromQL):**  \n  ```\n  avg_over_time(node_vmstat_pswpout[5m]) > 15\n  ```  \n  This rule triggers an alert if the average swap-out rate exceeds 15 pages per second over a 5-minute window, prompting investigation into memory usage and potential remediation actions such as adding RAM or optimizing workloads.",
    "metric_type": "untyped",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_allocation_btree_compares_total",
      "device": "*"
    },
    "description": "The node_xfs_allocation_btree_compares_total metric measures the total number of allocation B-tree compares performed by a filesystem on a node. This metric is specific to XFS file systems and indicates the number of times the allocation B-tree has been compared during operations such as file creation, deletion, or resizing. A high value for this metric may indicate inefficient disk usage, slow performance, or potential issues with the file system's metadata management. It can be used in monitoring to detect anomalies in file system behavior, identify bottlenecks, and trigger alerts when thresholds are exceeded.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_allocation_btree_lookups_total",
      "device": "*"
    },
    "description": "The node_xfs_allocation_btree_lookups_total metric measures the total number of allocation B-tree lookups performed by a filesystem on a node. This metric is specific to XFS file systems and represents the number of times the allocation B-tree has been searched for available space or to locate a specific block. High values may indicate inefficient disk usage, fragmentation issues, or other performance bottlenecks within the file system. Potential implications include monitoring for sudden spikes in lookups, which could signal underlying storage problems or configuration issues. This metric can be used in conjunction with other metrics, such as node_xfs_allocation_btree_inserts_total and node_xfs_allocation_btree_deletes_total, to gain a more comprehensive understanding of the file system's performance and identify potential areas for optimization.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_allocation_btree_records_deleted_total",
      "device": "*"
    },
    "description": "This metric measures the total number of allocation B-tree records deleted for a filesystem on a node. Allocation B-trees are data structures used by XFS file systems to manage free space and allocate new inodes. The deletion of these records can indicate changes in disk usage patterns, potential issues with disk fragmentation, or even signs of disk corruption. This metric can be used to monitor the health and performance of XFS file systems on a node, particularly in scenarios where disk space is limited or highly utilized. It may also serve as an indicator for more detailed analysis or troubleshooting when combined with other metrics, such as disk usage, inode counts, or error rates.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_allocation_btree_records_inserted_total",
      "device": "*"
    },
    "description": "This metric measures the total number of allocation B-tree records inserted for a filesystem on a node. Allocation B-trees are data structures used by XFS to manage file system metadata, such as inodes and block allocations. The insertion of new records into these trees can indicate changes in file system usage patterns or potential issues with disk space management. This metric may be useful for monitoring the rate at which allocation B-tree records are being inserted, potentially indicating issues with file system performance or capacity planning. It could also be used to alert on sudden spikes in record insertions, which might suggest a problem with the file system's metadata management.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_map_btree_compares_total",
      "device": "*"
    },
    "description": "The **node_xfs_block_map_btree_compares_total** metric counts the total number of B-tree comparison operations performed on block maps within the XFS filesystem on a node. It is a cumulative counter measured in the number of compare operations. This metric helps track how often the filesystem's B-tree structures are accessed or traversed during block allocation and lookup. Sudden increases may indicate filesystem performance issues or underlying storage problems.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_map_btree_lookups_total",
      "device": "*"
    },
    "description": "Total number of block map B-tree lookups performed by the XFS filesystem on the specified device. This counter metric represents the cumulative count of lookup operations in the block mapping B-tree, a key data structure used by XFS to efficiently translate file offsets to physical disk blocks. Values are unitless counts that increase monotonically since system start or metric reset. The data is sourced from the XFS kernel filesystem driver via node exporter\u2019s filesystem metrics. An unusually high or rapidly increasing rate of lookups may indicate heavy metadata access or fragmentation, potentially leading to increased disk I/O latency or degraded filesystem performance, and should trigger alerts for further investigation.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_map_btree_records_deleted_total",
      "device": "*"
    },
    "description": "This metric counts the total number of block map B-tree records deleted by the XFS file system on a specific device. These records track block allocations within the filesystem's metadata. The value is a cumulative count (unit: records) since the system started. Monitoring this metric helps identify unusual deletion activity that may signal filesystem issues or misconfigurations.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_map_btree_records_inserted_total",
      "device": "*"
    },
    "description": "This metric measures the total number of block map B-tree records inserted for a filesystem on a node. The XFS file system uses a B-tree data structure to manage its metadata, including block maps that track which blocks are allocated to files and directories. When new allocations occur or existing ones change, these records are updated in the B-tree. This metric counts the cumulative number of such insertions over time, providing insight into the filesystem's metadata management activity.\n\nIn monitoring and alerting contexts, this metric can be used to detect potential issues with XFS file system performance, such as excessive metadata updates that could indicate a problem with disk space allocation or usage patterns. It may also serve as an input for capacity planning, helping operators anticipate future storage needs based on observed insertion rates.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_mapping_extent_list_compares_total",
      "device": "*"
    },
    "description": "This metric measures the total number of extent list compares performed by the XFS filesystem on a node. An extent is a contiguous block of disk space allocated to a file or directory. The extent list compare operation is used to validate the consistency of the extent tree, which is a data structure used by XFS to manage file system metadata. High values for this metric may indicate issues with file system integrity, such as corruption or inconsistencies in the extent tree. Potential implications include: (1) File system corruption or data loss due to incorrect extent list compares; (2) Performance degradation due to frequent extent list compare operations; (3) Inaccurate monitoring of file system health if this metric is not properly accounted for. This metric can be used in monitoring and alerting to detect potential issues with the XFS filesystem, such as setting up alerts when the rate of extent list compares exceeds a certain threshold or when the total count reaches a specific value.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_mapping_extent_list_deletions_total",
      "device": "*"
    },
    "description": "This metric measures the total number of extent list deletions for a filesystem on a node. An extent is a contiguous block of disk space allocated to a file system. Extent list deletions occur when an extent is no longer needed and its corresponding entry in the extent tree is removed. This metric can be used to monitor the health and performance of the file system, particularly in scenarios where frequent extent list deletions may indicate issues such as high disk usage, fragmentation, or inefficient allocation strategies. Potential implications include identifying potential storage bottlenecks, detecting anomalies in file system behavior, or triggering alerts for impending storage capacity issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_mapping_extent_list_insertions_total",
      "device": "*"
    },
    "description": "This metric measures the total number of extent list insertions for a filesystem on a node. An extent is a contiguous block of disk space allocated to a file system. The extent list insertion count represents the number of times new extents have been added to the file system's mapping, indicating changes in storage allocation and usage. This metric can be used to monitor and alert on potential issues such as rapid growth or fragmentation within the file system, which may impact performance or lead to data corruption. It can also serve as a precursor to more severe problems like disk full conditions or quota exhaustion.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_mapping_extent_list_lookups_total",
      "device": "*"
    },
    "description": "This metric measures the total number of extent list lookups performed by a filesystem on a node. An extent is a contiguous block of disk space allocated to a file system for efficient storage and retrieval. The lookup operation involves searching for an extent in the extent tree, which is a data structure used to manage extents within a file system. High values of this metric may indicate inefficient file system operations or resource contention issues, potentially leading to performance degradation or even crashes. This metric can be used to monitor file system health and identify potential bottlenecks, allowing for proactive measures such as adjusting disk I/O settings, upgrading storage capacity, or optimizing file system configuration.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_mapping_reads_total",
      "device": "*"
    },
    "description": "The node_xfs_block_mapping_reads_total metric measures the total number of block map operations for read requests on a filesystem managed by XFS (a high-performance journaling file system) on a given node. This metric is incremented each time a read operation requires a block mapping, which can occur when data is being accessed from disk. It provides insight into the frequency and volume of read requests that require block mappings, potentially indicating issues with disk performance, filesystem configuration, or application behavior. This metric can be used to monitor and alert on potential issues such as high latency, slow disk I/O, or inefficient use of system resources.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_mapping_unmaps_total",
      "device": "*"
    },
    "description": "The node_xfs_block_mapping_unmaps_total metric measures the total number of block unmap operations (deletes) for a filesystem on a given node. This metric indicates the number of times blocks have been unmapped from the filesystem's mapping table, which can be an indicator of disk space reclaiming or garbage collection activities. High values may indicate issues with disk space management, file system corruption, or other underlying problems that require further investigation. Potential usage in monitoring includes tracking this metric over time to identify trends and anomalies, setting thresholds for alerting when unmap operations exceed a certain threshold, and correlating it with other metrics such as disk usage, inode counts, or error rates to diagnose root causes of issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_block_mapping_writes_total",
      "device": "*"
    },
    "description": "This metric measures the total number of block mapping write operations for a filesystem on a node. It represents the cumulative count of writes to disk blocks that require remapping due to changes in file system metadata or other factors. This can be an indicator of high disk usage, potential performance bottlenecks, or issues with file system consistency. Potential implications include monitoring for sudden spikes in this metric, which could indicate a problem with disk space management, data corruption, or other underlying issues. It may also be used to set thresholds for alerting on excessive write operations, helping operators identify and address potential problems before they impact system availability.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_directory_operation_create_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times a new directory entry was created for a filesystem on a node. It represents the cumulative count of create operations performed by the XFS file system on directories. This metric can be used to monitor and troubleshoot issues related to directory creation, such as high rates of directory creation that may indicate a problem with the application or service using the directory, or potential issues with disk space or performance. It can also be used in conjunction with other metrics, such as node_disk_io_time_seconds_total, to understand the impact of directory creation on overall system performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_directory_operation_getdents_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times the directory getdents operation was performed for a filesystem on a node. The getdents system call is used to read directory entries from a file system. It can be an indicator of disk I/O activity, especially in scenarios where many files are being accessed or created within a short period. High values might suggest issues with disk performance, such as slow disk speeds, high latency, or resource contention. This metric could be used to monitor and alert on potential disk bottlenecks, helping operators identify and address the root cause of performance degradation.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_directory_operation_lookup_total",
      "device": "*"
    },
    "description": "This metric measures the total number of file name directory lookups that miss the operating system's directory name lookup cache on a node. It indicates the frequency at which the system has to perform additional lookups for directory names, potentially impacting performance and latency in file operations. This metric can be used to monitor and optimize storage performance by identifying nodes with high directory lookup rates, indicating potential issues with caching or configuration. It may also serve as an indicator of underlying hardware or software limitations affecting file system efficiency.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_directory_operation_remove_total",
      "device": "*"
    },
    "description": "This metric, **node_xfs_directory_operation_remove_total**, counts the total number of directory entry removal operations performed on the XFS filesystem of a node, aggregated per device. It helps SREs monitor filesystem activity related to directory deletions, which can indicate normal cleanup processes or abnormal behavior such as excessive file churn or potential application issues.\n\n**Purpose:**  \nTrack the frequency of directory removal operations to detect unusual filesystem activity that may impact performance or stability.\n\n**Alert Thresholds:**  \n- Trigger an alert if the rate of directory removals exceeds a sustained threshold, for example, a rate greater than 100 removals per minute over 5 minutes (`rate(node_xfs_directory_operation_remove_total[5m]) > 100`).  \n- This threshold should be adjusted based on baseline activity for the specific environment.\n\n**Impact of Values:**  \n- **High values:** May indicate excessive directory deletions, possibly due to application bugs, misconfigured cleanup jobs, or malicious activity, potentially leading to increased I/O load and filesystem fragmentation.  \n- **Low or zero values:** Normal in stable systems with little directory churn; however, a sudden drop from a previously high baseline might indicate stalled cleanup processes or filesystem issues.\n\n**Example Usage:**  \n- **Alert Rule:**  \n  ```yaml\n  alert: HighXfsDirectoryRemovals\n  expr: rate(node_xfs_directory_operation_remove_total[5m]) > 100\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of XFS directory removals on {{ $labels.device }}\"\n    description: \"The rate of directory removal operations on device {{ $labels.device }} has exceeded 100 per minute for over 5 minutes, which may indicate abnormal filesystem activity.\"\n  ```\n\n- **Dashboard Panel:**  \n  Plot the 5-minute rate of `node_xfs_directory_operation_remove_total` per device to visualize trends and detect spikes in directory removal activity, correlating with other filesystem and system metrics such as disk I/O and CPU load for comprehensive analysis.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_extent_allocation_blocks_allocated_total",
      "device": "*"
    },
    "description": "The node_xfs_extent_allocation_blocks_allocated_total metric measures the total number of blocks allocated for a filesystem on a given node. This metric is specific to XFS file systems and represents the amount of disk space allocated for storing data in extents. An extent is a contiguous block of free space that can be allocated to store files. The value of this metric indicates how much storage capacity has been committed to the file system, which can help operators understand the current state of disk utilization and plan for future growth. Potential implications or usage in monitoring or alerting include: - Triggering alerts when the allocated block count exceeds a certain threshold, indicating potential storage issues. - Monitoring the rate at which blocks are being allocated to detect anomalies or performance bottlenecks. - Correlating this metric with other disk-related metrics (e.g., node_disk_usage) to gain a comprehensive understanding of storage utilization and capacity planning.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_extent_allocation_blocks_freed_total",
      "device": "*"
    },
    "description": "This metric measures the total number of blocks freed for a filesystem on a node, as reported by XFS. It represents the cumulative count of blocks that have been released back to the filesystem's free space pool since the last scrape. This metric can be used to monitor and alert on issues related to disk space utilization, such as sudden spikes in block allocation or unexpected block deallocation events. It may also indicate potential problems with file system health, data corruption, or other underlying storage issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_extent_allocation_extents_allocated_total",
      "device": "*"
    },
    "description": "This metric measures the total number of extents allocated for a filesystem on a node. An extent is a contiguous block of disk space that can be used to store file data. The allocation of extents is a critical aspect of filesystem performance and management. High values may indicate inefficient use of disk space, while low values could suggest insufficient storage capacity. This metric can be used in monitoring to track changes in filesystem usage over time, identify potential issues with storage allocation, or trigger alerts when thresholds are exceeded.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_extent_allocation_extents_freed_total",
      "device": "*"
    },
    "description": "This metric measures the total number of extents freed for a filesystem on a node. An extent is a contiguous block of disk space allocated to a file system. The metric indicates the cumulative count of extents that have been released back to the file system's free space pool since the last scrape. This can be an indicator of file system usage patterns, such as frequent creation and deletion of large files or directories. It may also signal potential issues with file system fragmentation or disk space management. In monitoring or alerting, this metric could be used to detect sudden spikes in extent allocation and deallocation, which might indicate performance bottlenecks or resource exhaustion. Additionally, it can serve as a precursor to more severe issues like disk full conditions or file system crashes.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_inode_operation_attempts_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times the operating system (OS) has attempted to access an XFS inode from the inode cache. An inode is a data structure used by file systems like XFS to store metadata about files and directories. The inode cache is a mechanism that stores recently accessed inodes in memory for faster lookup. This metric can be used to monitor the performance of the file system, specifically the efficiency of the inode cache. High values may indicate issues with inode cache effectiveness or potential bottlenecks in file system access. It could also be an indicator of high disk I/O activity if the inode cache is being frequently replenished due to frequent changes in the file system structure.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_inode_operation_attribute_changes_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times the operating system (OS) has explicitly changed the attributes of an XFS inode. An inode is a data structure used by file systems to represent files and directories. Attribute changes can include updates to metadata such as permissions, timestamps, or ownership. This metric may indicate issues with file system performance, configuration problems, or potential security risks if attribute changes are occurring unexpectedly.\n\nPotential implications for monitoring or alerting include:\n- High rates of attribute changes could indicate a misconfigured file system or a malicious actor modifying file attributes.\n- Low rates of attribute changes might suggest an underutilized or inefficiently configured file system.\n- Correlation with other metrics, such as disk usage or network traffic, can provide further insights into the root cause of attribute changes.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_inode_operation_duplicates_total",
      "device": "*"
    },
    "description": "This metric, **node_xfs_inode_operation_duplicates_total**, tracks the cumulative count of attempts by the operating system to add an XFS inode to the inode cache when that inode was already present, indicating duplicate insertions. In Prometheus, it helps SREs monitor inode cache behavior on XFS file systems, highlighting potential contention or inefficiencies in inode management.\n\n**Purpose:**  \nHigh values suggest frequent concurrent access or race conditions in inode caching, which can lead to increased CPU usage, latency in file system operations, or even stability issues if inode cache consistency is compromised.\n\n**Thresholds & Alerting:**  \n- A low or zero rate is normal under typical workloads.  \n- An alert should be triggered if the rate of increase exceeds a small threshold over a sustained period (e.g., more than 10 duplicates per minute over 5 minutes), indicating abnormal inode cache contention.\n\n**Impact:**  \n- **High values:** May cause performance degradation due to redundant inode cache operations, increased lock contention, or potential file system instability.  \n- **Low or zero values:** Indicate normal inode cache behavior with minimal contention.\n\n**Example Alert Rule:**  \n```yaml\nalert: HighXFSInodeDuplicateOperations  \nexpr: rate(node_xfs_inode_operation_duplicates_total[5m]) > 10  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"High rate of XFS inode duplicate cache insertions on {{ $labels.device }}\"  \n  description: \"The inode cache on device {{ $labels.device }} is experiencing frequent duplicate insertions, which may indicate inode cache contention or file system issues.\"  \n```\n\n**Example Dashboard Usage:**  \nPlot the per-device rate of `node_xfs_inode_operation_duplicates_total` over time to identify spikes or trends in inode cache duplication, correlating with file system performance metrics to diagnose potential issues.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_inode_operation_found_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times the operating system (OS) looked for and found an XFS inode in the inode cache. An inode is a data structure used by file systems to manage files and directories. The inode cache is a mechanism that stores recently accessed inodes to improve performance by reducing the need for disk I/O operations. This metric can be used to monitor the efficiency of the inode cache and identify potential issues with file system performance. High values may indicate inefficient use of the inode cache, leading to increased disk I/O and potentially impacting application performance. It could also be an indicator that the system is running low on memory or experiencing high CPU usage due to inode cache thrashing.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_inode_operation_missed_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times the operating system (OS) looked for an XFS inode in its cache but failed to find it. An inode is a data structure used by file systems like XFS to manage files and directories. When the OS attempts to access an inode, it first checks if it's already cached in memory. If not found, this metric increments. High values may indicate issues with inode caching, such as cache misses due to inadequate cache size or inefficient cache eviction policies. This could lead to increased latency and decreased system performance. Potential uses for this metric include monitoring inode cache health, identifying potential bottlenecks, and optimizing inode caching configurations.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_inode_operation_reclaims_total",
      "device": "*"
    },
    "description": "This metric measures the number of times the operating system (OS) reclaimed an XFS inode from the inode cache to free memory for another purpose. An inode is a data structure used by file systems like XFS to manage files and directories. The inode cache stores recently accessed inodes to improve performance by reducing disk I/O. When the OS needs to free up memory, it may reclaim an inode from the cache if there are not enough available inodes or if the system is running low on memory. This metric can be used to monitor the frequency of inode reclaims, which may indicate issues with memory pressure, inode cache efficiency, or file system performance. Potential implications include: high inode reclaims could lead to increased disk I/O, decreased file system performance, and potential data corruption if inodes are not properly managed. In monitoring or alerting, this metric can be used to detect abnormal inode reclaim rates, which may trigger further investigation into memory usage, inode cache efficiency, or file system configuration.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_inode_operation_recycled_total",
      "device": "*"
    },
    "description": "This metric measures the total number of times the operating system (OS) found an XFS inode in its cache but was unable to utilize it due to recycling. An inode is a data structure used by file systems like XFS to manage files and directories. When an inode is recycled, it means that the OS has reclaimed the inode's resources, likely because the associated file or directory has been deleted or modified. The metric indicates potential issues with inode management, cache efficiency, or resource utilization within the system. It may be used in monitoring to detect anomalies in inode recycling rates, which could indicate problems such as high disk usage, inefficient caching, or even security concerns like privilege escalation attacks that target file system resources.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_read_calls_total",
      "device": "*"
    },
    "description": "The node_xfs_read_calls_total metric measures the total number of read(2) system calls made to files in a filesystem on the node. This metric can be used to monitor and troubleshoot file system performance issues, such as high latency or I/O wait times. It may indicate problems with disk storage, file system configuration, or application behavior. Potential implications include identifying bottlenecks in data access patterns, detecting anomalies in read operations, or correlating with other metrics like node_disk_read_bytes_total to understand the actual data transferred.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_vnode_active_total",
      "device": "*"
    },
    "description": "The node_xfs_vnode_active_total metric measures the number of in-use vnodes (virtual nodes) for a filesystem on a node. Vnodes are data structures used by XFS to manage file metadata and free space. An active vnode is one that has been allocated for use by an open file, directory, or other filesystem object. This metric indicates the total count of such vnodes across all filesystems on the node, excluding those on free lists. High values may indicate inefficient memory usage, while low values could suggest underutilization of available resources. Potential implications include monitoring for sudden spikes in vnode counts to detect potential issues with file system performance or resource exhaustion. This metric can be used in conjunction with other XFS metrics (e.g., node_xfs_vnode_total) to gain a more comprehensive understanding of filesystem health and resource utilization.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_vnode_allocate_total",
      "device": "*"
    },
    "description": "The node_xfs_vnode_allocate_total metric measures the total number of times the vn_alloc function was called for a filesystem on the node. This function is responsible for allocating new inodes (vnodes) to store file metadata. The count represents the cumulative number of allocation requests made by the XFS filesystem, which can be indicative of storage capacity issues or inode exhaustion. Potential implications include monitoring for sudden spikes in allocations, which may indicate a problem with disk space management or inode availability. This metric could be used in alerting to notify administrators when inode allocation rates exceed a certain threshold, potentially preventing file system crashes due to inode depletion.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_vnode_get_total",
      "device": "*"
    },
    "description": "The **node_xfs_vnode_get_total** metric counts the cumulative number of times the XFS filesystem\u2019s `vn_get` function has been called on the node, across all devices (`device=\"*\"`). This function is responsible for retrieving vnode (inode) structures, which represent files and directories in the filesystem. Monitoring this metric helps SREs understand filesystem load and inode access patterns.\n\n**Purpose:**  \n- Track filesystem vnode retrieval activity to identify high inode lookup rates that may indicate heavy filesystem usage or inefficient application behavior.  \n- Detect potential performance bottlenecks related to inode management or excessive file metadata operations.\n\n**Threshold Guidance:**  \n- A sudden spike or sustained high rate of `vn_get` calls (e.g., exceeding 10,000 calls per minute per device) may signal abnormal filesystem activity or application inefficiencies.  \n- Thresholds should be tuned based on baseline metrics for your environment; alert if the per-minute rate increases by more than 50% above normal or crosses an absolute threshold relevant to your workload.\n\n**Impact of Values:**  \n- **High values:** Indicate heavy inode access, which can lead to increased CPU usage, filesystem contention, or degraded performance. May suggest inefficient file access patterns or a need to optimize application I/O.  \n- **Low or stable values:** Typically indicate normal filesystem vnode usage and healthy inode management.\n\n**Example Usage:**  \n- **Prometheus query to calculate per-minute rate:**  \n  `rate(node_xfs_vnode_get_total[1m])`  \n- **Alert rule example:**  \n  ```yaml\n  alert: HighXFSVnodeGetRate\n  expr: rate(node_xfs_vnode_get_total[1m]) > 10000\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High XFS vnode get rate detected on {{ $labels.device }}\"\n    description: \"The rate of vn_get calls on device {{ $labels.device }} has exceeded 10,000 per minute for over 5 minutes, indicating potential filesystem stress or inefficient inode access.\"\n  ```\n- **Dashboard visualization:**  \n  Plot `rate(node_xfs_vnode_get_total[1m])` over time per device to identify trends, spikes, or anomalies in vnode retrieval activity. Use this alongside CPU and I/O metrics to correlate filesystem load with system performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_vnode_hold_total",
      "device": "*"
    },
    "description": "The node_xfs_vnode_hold_total metric measures the total number of times the vn_hold function has been called for a filesystem on the node. This function is used to acquire a hold on a vnode (a virtual inode) in an XFS file system, which can be indicative of resource contention or locking issues within the file system. High values may indicate that there are ongoing file system operations causing locks to be held for extended periods, potentially leading to performance degradation or even file system crashes. This metric can be used to monitor and alert on potential file system bottlenecks, allowing operators to investigate and resolve underlying issues before they impact system availability.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_vnode_reclaim_total",
      "device": "*"
    },
    "description": "The node_xfs_vnode_reclaim_total metric measures the total number of times the vn_reclaim function was called for a filesystem on the node. This function is responsible for reclaiming inodes and data blocks from deleted files or directories. High values may indicate issues with file system health, such as high rates of file deletion or corruption, which can lead to performance degradation or even file system crashes. Potential usage includes monitoring this metric to detect anomalies in file system activity, setting alerts for unusually high reclamation counts, or using it as a component in more complex metrics that track overall file system health and performance.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_vnode_release_total",
      "device": "*"
    },
    "description": "The node_xfs_vnode_release_total metric measures the total number of times the vn_rele function was called for a filesystem on the node. This function is responsible for releasing file system nodes that are no longer in use. The metric can be used to monitor and detect potential issues related to file system resource utilization, such as memory leaks or inefficient file handling. It may also indicate problems with file system configuration or kernel-level issues. Potential implications include identifying bottlenecks in file system operations, detecting anomalies in file system usage patterns, and triggering alerts for potential file system crashes or data corruption.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_vnode_remove_total",
      "device": "*"
    },
    "description": "The node_xfs_vnode_remove_total metric measures the total number of times the vn_remove function is called for a filesystem on the node. This function is responsible for removing inodes from the filesystem's inode cache. The metric can be used to monitor and troubleshoot issues related to inode removal, such as high latency or errors during file system operations. Potential implications include identifying bottlenecks in file system performance, detecting inode cache exhaustion, or pinpointing issues with vn_remove function calls. This metric is particularly useful for monitoring systems that rely heavily on XFS filesystems, such as large-scale storage environments or high-performance computing clusters.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "node_xfs_write_calls_total",
      "device": "*"
    },
    "description": "The node_xfs_write_calls_total metric measures the total number of write system calls made to files in a filesystem on the node. This metric can be used to monitor and troubleshoot file system performance issues, such as high latency or slow write operations. It may indicate potential problems with disk I/O, storage capacity, or file system configuration. In monitoring or alerting, this metric could be used to detect sudden spikes in write calls, which might signal a resource bottleneck or an application issue causing excessive writes.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "promhttp_metric_handler_errors_total",
      "cause": "*"
    },
    "description": "The promhttp_metric_handler_errors_total metric measures the total number of internal errors encountered by the Prometheus HTTP handler. This includes errors that occur during the processing and serving of metrics, such as invalid or malformed requests, authentication failures, or issues with data storage. The metric is incremented each time an error occurs, providing a cumulative count of errors over time. This information can be used to monitor the health and reliability of the Prometheus server, identify potential issues with data collection or storage, and trigger alerts when error rates exceed acceptable thresholds.",
    "metric_type": "counter",
    "exporter": "NodeExporter"
  },
  {
    "query": {
      "MetricName": "blackbox_exporter_build_info",
      "branch": "*",
      "goarch": "*",
      "goos": "*",
      "goversion": "*",
      "revision": "*",
      "tags": "*",
      "version": "*"
    },
    "description": "The **blackbox_exporter_build_info** metric exposes static metadata about the build environment and version of the blackbox exporter binary currently running. It includes labels such as `version`, `revision`, `branch`, `goarch`, `goos`, `goversion`, and `tags` that identify the exact build configuration. This metric does not change over time and does not have numeric values to monitor thresholds or performance impact.\n\n**Purpose:**  \nUse this metric in Prometheus to verify and track which version and build of the blackbox exporter is deployed across your infrastructure. It helps ensure consistency, detect outdated or mismatched versions, and supports troubleshooting by providing build context.\n\n**Alerting Guidance:**  \nSince this metric is static and informational, alerts should be based on changes or mismatches in the `version` or `revision` labels across your fleet. For example, trigger an alert if any instance is running a version older than the current approved release, indicating a need for upgrade. No numeric thresholds apply.\n\n**Impact:**  \nThere are no high or low values to interpret. The impact of this metric lies in identifying outdated or inconsistent builds that may cause compatibility or security issues.\n\n**Example Usage:**  \n- **Alert rule:**  \n  Alert if any blackbox exporter instance is not running the approved version:  \n  ```promql\n  count by (instance) (blackbox_exporter_build_info{version!=\"v0.22.0\"}) > 0\n  ```  \n- **Dashboard:**  \n  Display a table listing all instances with their `version`, `revision`, and `branch` labels to quickly identify which versions are deployed and spot discrepancies.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "blackbox_exporter_config_last_reload_success_timestamp_seconds"
    },
    "description": "The blackbox_exporter_config_last_reload_success_timestamp_seconds metric measures the timestamp of the last successful configuration reload event in the Blackbox Exporter. This metric indicates when the exporter successfully reloaded its configuration from a remote source, such as a YAML file or an external service discovery mechanism. The value represents the number of seconds since the Unix epoch (January 1, 1970) and can be used to track the frequency and timing of successful reloads. Potential implications for monitoring include: \n\n- Identifying configuration changes and their impact on the exporter's behavior.\n- Detecting issues with remote configuration sources or network connectivity.\n- Triggering alerts when reload failures occur, indicating potential misconfigurations or service disruptions.\n\nThis metric can be used in conjunction with other metrics, such as blackbox_exporter_config_last_reload_failure_timestamp_seconds, to provide a more comprehensive view of the exporter's configuration management and reliability.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "blackbox_exporter_config_last_reload_successful"
    },
    "description": "The blackbox_exporter_config_last_reload_successful metric measures whether the last reload of the Blackbox exporter configuration was successful. This metric indicates whether the Blackbox exporter has successfully loaded its configuration from the specified source (e.g., a file or a remote endpoint) after a reload operation. A value of 1 indicates success, while a value of 0 indicates failure. This metric can be used to monitor the reliability and stability of the Blackbox exporter's configuration management process. Potential implications include identifying issues with configuration files, network connectivity, or authentication mechanisms that may prevent successful reloads. In monitoring or alerting scenarios, this metric can trigger notifications when the last reload fails, allowing operators to investigate and resolve configuration-related problems promptly.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "blackbox_module_unknown_total"
    },
    "description": "The **blackbox_module_unknown_total** metric counts the total number of probe requests made to unknown or undefined modules in the Prometheus Blackbox Exporter. It is a cumulative counter measured in units of probe attempts. This metric indicates that the Blackbox Exporter received a module name it does not recognize or that is not configured. Monitoring this metric helps identify misconfigurations or missing modules in probe setups.",
    "metric_type": "counter",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_cpu_seconds_total"
    },
    "description": "The process_cpu_seconds_total metric measures the total amount of CPU time spent by a process in seconds, including both user and system CPU time. This metric is useful for monitoring the resource utilization of individual processes or containers within an infrastructure. It can be used to identify performance bottlenecks, detect anomalies in CPU usage patterns, and optimize resource allocation. For example, if this metric exceeds a certain threshold, it may indicate that a process is consuming excessive resources, potentially impacting system responsiveness or causing other issues. This information can be leveraged by operations teams to investigate and address the root cause of high CPU utilization, ensuring smooth operation of critical systems.",
    "metric_type": "counter",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_max_fds"
    },
    "description": "The process_max_fds metric measures the maximum number of open file descriptors allowed for a given process. This value is typically set by the operating system and can be influenced by factors such as the process's resource limits, configuration files, or environment variables. In Prometheus, this metric is collected from the blackbox exporter, which probes the target system to gather information about its running processes. The process_max_fds metric can be used in monitoring and alerting to detect potential issues related to file descriptor exhaustion, which can lead to process crashes or resource starvation. For example, if the value of process_max_fds exceeds a certain threshold (e.g., 90% of the maximum allowed), it may indicate that the system is running low on available file descriptors, potentially causing issues with process creation, network connections, or other I/O-intensive operations. This metric can be used in conjunction with other metrics, such as process_open_fds, to gain a more comprehensive understanding of the system's resource utilization and identify potential bottlenecks.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_network_receive_bytes_total"
    },
    "description": "The **process_network_receive_bytes_total** metric measures the total number of bytes a specific process has received over the network since it started. It is expressed as a cumulative count in bytes. This metric helps monitor the network input volume for individual processes, aiding in performance analysis and troubleshooting. It is typically collected via Prometheus exporters that track process-level network activity.",
    "metric_type": "counter",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_network_transmit_bytes_total"
    },
    "description": "This metric measures the total number of bytes transmitted by a process over the network. It is collected by Prometheus using the blackbox exporter and represents the cumulative sum of bytes sent since the last scrape. This metric can be used to monitor network activity, detect potential issues such as high bandwidth usage or network congestion, and identify processes that are consuming excessive network resources. It may also be useful for capacity planning and forecasting future network requirements.",
    "metric_type": "counter",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_open_fds"
    },
    "description": "The **process_open_fds** metric tracks the number of open file descriptors held by a specific process or service, collected via Prometheus (commonly through the node_exporter or blackbox exporter). This metric helps SREs monitor resource utilization related to file handles, which are critical for managing files, network sockets, and other I/O resources.\n\n**Purpose:**  \nMonitoring **process_open_fds** enables detection of abnormal resource consumption patterns that may indicate leaks, inefficient resource management, or potential security issues such as file descriptor exhaustion attacks.\n\n**Thresholds and Alerting:**  \nA typical alert threshold is set relative to the system or process limits. For example, if a process approaches 80-90% of its maximum allowed open file descriptors (often defined by `ulimit -n`), it should trigger a warning alert. A critical alert might be triggered if usage exceeds 95%, indicating imminent resource exhaustion that could cause process failure or degraded performance.\n\n**Impact of Values:**  \n- **High values:** May signal resource leaks, excessive open connections, or file handles not being properly closed, potentially leading to system instability or denial of service.  \n- **Low or normal values:** Indicate healthy resource usage within expected operational parameters.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph of `process_open_fds` alongside the process\u2019s maximum file descriptor limit to visualize trends and headroom.  \n- **Alert Rule (Prometheus):**  \n  ```yaml\n  alert: HighOpenFileDescriptors\n  expr: process_open_fds / process_max_fds > 0.9\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High number of open file descriptors for {{ $labels.process }}\"\n    description: \"The process {{ $labels.process }} has used more than 90% of its allowed open file descriptors for over 5 minutes.\"\n  ```\n\nBy monitoring **process_open_fds** with appropriate thresholds and alerting, SREs can proactively prevent resource exhaustion, maintain system stability, and quickly investigate potential security concerns.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_resident_memory_bytes"
    },
    "description": "The process_resident_memory_bytes metric measures the amount of memory that a process is currently using, excluding any swapped-out or paged memory. This value represents the portion of the process's total memory allocation that is actively being used by the system and is not stored on disk. It can be used to monitor the memory usage of individual processes, identify potential memory leaks, and detect when a process is consuming excessive resources. In monitoring and alerting, this metric can be used to set thresholds for normal or abnormal memory usage, trigger alerts when a process exceeds a certain memory threshold, or investigate performance issues related to memory consumption.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_start_time_seconds"
    },
    "description": "The **process_start_time_seconds** metric records the Unix timestamp (in seconds since January 1, 1970) when a process began running. In Prometheus, this metric helps SREs track process lifecycles by indicating the exact start time of each monitored process. It is primarily used to detect unexpected restarts or prolonged downtime by comparing the current time against the process start time.\n\n**Alerting guidance:**  \nSet an alert if the process start time is too recent or too old relative to expectations. For example, if a critical service restarts unexpectedly, the start time will be very recent, which may indicate instability. Conversely, if the process start time is too far in the past without a restart, it may suggest the process is stale or hung. A typical alert threshold might be:  \n- Alert if `time() - process_start_time_seconds < 300` seconds (process restarted within last 5 minutes, indicating frequent restarts)  \n- Alert if `time() - process_start_time_seconds > 86400` seconds (process has been running for more than 24 hours without restart, if restarts are expected regularly)\n\n**Impact of values:**  \n- **Low values (recent start times):** May indicate frequent restarts or crashes, potentially causing service instability.  \n- **High values (older start times):** Suggest stable, long-running processes, but could also mean the process is stuck or not refreshing as expected.\n\n**Example usage:**  \n- **Dashboard:** Display `process_start_time_seconds` alongside `time()` to visualize process uptime and detect restarts.  \n- **Alert rule example:**  \n```yaml\nalert: FrequentProcessRestart  \nexpr: time() - process_start_time_seconds < 300  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"Process restarted recently\"  \n  description: \"Process {{ $labels.instance }} restarted within the last 5 minutes, indicating potential instability.\"  \n```\n\nThis metric enables proactive monitoring of process health by correlating start times with system events and other metrics.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_virtual_memory_bytes"
    },
    "description": "The **process_virtual_memory_bytes** metric in Prometheus represents the total amount of virtual memory (in bytes) currently allocated to a process, including both physical RAM and swap space. This metric helps SREs monitor the memory footprint of a process beyond just physical memory usage, capturing all address space reserved by the process.\n\n**Purpose:**  \nUse this metric to track the overall virtual memory consumption of critical processes, detect abnormal growth patterns that may indicate memory leaks, and understand memory pressure that could affect system stability.\n\n**Thresholds and Alerts:**  \nSet alert thresholds based on the expected memory profile of your application. For example, trigger a warning if **process_virtual_memory_bytes** exceeds 80% of the process\u2019s configured memory limit or a fixed byte value (e.g., 4GB). Sudden increases or sustained high values should prompt investigation.\n\n**Impact of Values:**  \n- **High values:** May indicate memory leaks, inefficient memory usage, or excessive swapping, potentially leading to degraded performance or out-of-memory errors.  \n- **Low or stable values:** Suggest normal memory usage and stable process behavior.\n\n**Example Alert Rule:**  \n```yaml\nalert: HighVirtualMemoryUsage\nexpr: process_virtual_memory_bytes > 4e9  # 4 GB threshold\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Process virtual memory usage is high\"\n  description: \"The process is using more than 4GB of virtual memory for over 5 minutes.\"\n```\n\n**Example Dashboard Usage:**  \nPlot **process_virtual_memory_bytes** over time alongside CPU usage and disk I/O metrics to correlate memory usage spikes with CPU load or swapping activity, aiding in diagnosing performance bottlenecks or memory leaks.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "process_virtual_memory_max_bytes"
    },
    "description": "This metric, **process_virtual_memory_max_bytes**, represents the maximum virtual memory size (in bytes) that a process is allowed to allocate, as enforced by the operating system. It reflects the upper limit on the process's virtual address space, combining physical RAM and swap/disk-backed memory. Monitoring this metric helps SREs understand the memory constraints imposed on a process, which can affect its ability to handle workload spikes or memory-intensive operations.\n\n**Purpose:**  \nIn Prometheus, this metric is used to track the configured virtual memory limit of a process, enabling detection of configuration changes or resource restrictions that might impact process stability or performance.\n\n**Alert Thresholds:**  \n- Alert if the limit is unexpectedly low or decreases significantly, which may cause the process to fail memory allocations or crash.  \n- Alert if the limit is unusually high or unbounded, potentially masking runaway memory usage or leaks.\n\nA practical alert example:  \n```\nalert: VirtualMemoryLimitLow  \nexpr: process_virtual_memory_max_bytes < 1e9  # less than 1 GB  \nfor: 5m  \nlabels: { severity=\"warning\" }  \nannotations: { summary=\"Process virtual memory limit is critically low\", description=\"The process virtual memory max bytes is below 1 GB, which may cause memory allocation failures.\" }\n```\n\n**Impact of Values:**  \n- **High values:** Indicate a large or unlimited virtual memory limit, which allows the process to allocate more memory but may hide memory leaks or lead to system-wide resource exhaustion if unchecked.  \n- **Low values:** Restrict the process's memory usage, potentially causing allocation failures, crashes, or degraded performance under load.\n\n**Dashboard Example:**  \nVisualize this metric alongside actual virtual memory usage (`process_virtual_memory_bytes`) to correlate limits with consumption. For example, a graph showing both metrics over time helps identify when usage approaches or hits the limit, signaling potential risk of OOM (Out Of Memory) errors.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "promhttp_metric_handler_requests_in_flight"
    },
    "description": "The promhttp_metric_handler_requests_in_flight metric measures the current number of active HTTP requests being served by the Prometheus server's /metrics endpoint. This includes both scrape and user-requests. It provides insight into the server's capacity to handle incoming requests, which can be useful for monitoring and alerting purposes. High values may indicate a high load on the server or potential issues with request queuing. Conversely, low values might suggest underutilization of resources. This metric is particularly relevant in environments where Prometheus serves as a central metrics repository or when scraping large numbers of targets.",
    "metric_type": "gauge",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "promhttp_metric_handler_requests_total",
      "code": "*"
    },
    "description": "The promhttp_metric_handler_requests_total metric measures the total number of HTTP requests made to the Prometheus server's /metrics endpoint by the blackbox exporter. This includes all successful and failed requests, categorized by their respective HTTP status codes (e.g., 200 OK, 404 Not Found, etc.). The metric provides insight into the health and accessibility of the Prometheus server from the perspective of the blackbox exporter. Potential implications for monitoring or alerting include: identifying issues with the Prometheus server's availability or configuration, detecting anomalies in request patterns that may indicate security vulnerabilities or misconfigured exporters, and optimizing the performance of the blackbox exporter by adjusting its scrape interval or timeout settings.",
    "metric_type": "counter",
    "exporter": "BlackboxExporter"
  },
  {
    "query": {
      "MetricName": "kube_certificatesigningrequest_annotations"
    },
    "description": "The **kube_certificatesigningrequest_annotations** metric exposes Kubernetes CertificateSigningRequest (CSR) annotations as Prometheus labels, enabling detailed monitoring of metadata attached to each CSR. This metric helps SREs track CSR lifecycle events by revealing patterns or anomalies in annotation usage, such as unusual requester identities or unexpected certificate purposes.\n\n**Purpose:**  \nMonitor the presence and variation of annotations on CSRs to detect abnormal or unauthorized certificate requests, which could indicate security issues or misconfigurations in certificate issuance workflows.\n\n**Alerting Guidance:**  \nSet alerts based on unusual spikes or drops in the count of CSRs with specific annotations, or the appearance of unexpected annotation keys/values. For example, alert if the number of CSRs with a critical annotation (e.g., `requester=unknown`) exceeds a threshold (e.g., > 5 within 5 minutes), indicating potential unauthorized certificate requests.\n\n**Impact of Values:**  \n- **High values:** A sudden increase in CSRs with certain annotations may signal a surge in certificate requests, possibly due to automated processes, misconfiguration, or an attack vector attempting to obtain certificates.  \n- **Low or zero values:** May indicate normal operation or, if expected annotations disappear, a failure in the CSR generation process or changes in cluster behavior.\n\n**Example Usage:**  \nTo alert on an unusual number of CSRs from unknown requesters within 5 minutes:\n\n```yaml\nalert: HighUnknownCSRRequests\nexpr: sum by (requester) (kube_certificatesigningrequest_annotations{requester=\"unknown\"}) > 5\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High number of CSRs from unknown requesters\"\n  description: \"More than 5 CSRs with 'requester=unknown' annotation detected in the last 5 minutes.\"\n```\n\nIn dashboards, this metric can be visualized as a time series grouped by annotation keys (e.g., `requester`, `certificate_usage`) to identify trends or anomalies in CSR requests over time.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_certificatesigningrequest_labels"
    },
    "description": "The **kube_certificatesigningrequest_labels** metric exposes the set of labels attached to Kubernetes CertificateSigningRequest (CSR) objects as Prometheus labels. This metric enables SREs to monitor and filter CSRs based on attributes such as namespace, requester identity, or purpose, facilitating detailed tracking of certificate request patterns and statuses within the cluster.\n\n**Purpose:**  \nUse this metric to identify and categorize CSRs by their labels, helping to detect unusual spikes in certificate requests, stalled or failed CSRs, or unexpected label values that may indicate misconfigurations or security issues.\n\n**Alerting Guidance:**  \nWhile this metric itself is label metadata and does not have a numeric value, it should be combined with other CSR status metrics (e.g., `kube_certificatesigningrequest_status_condition`) to trigger alerts. For example, alert if the count of CSRs with a \"Pending\" status and specific labels (e.g., from a critical namespace or service account) exceeds a threshold (e.g., > 10 for more than 10 minutes), indicating potential certificate approval bottlenecks.\n\n**Impact of High or Low Values:**  \n- A sudden increase in CSRs with certain labels may indicate automated processes requesting certificates excessively, possibly signaling misconfiguration or abuse.  \n- A low or zero count of CSRs in expected namespaces might suggest certificate issuance failures or missing workloads requiring certificates.\n\n**Example Usage:**  \nTo alert on a high number of pending CSRs in the \"production\" namespace:  \n```promql\ncount by (namespace) (\n  kube_certificatesigningrequest_labels{namespace=\"production\"} \n  and on (csr) kube_certificatesigningrequest_status_condition{condition=\"Pending\"}\n) > 10\n```\n\nThis can be visualized in a dashboard showing CSR counts grouped by labels such as namespace or requester, enabling quick identification of abnormal certificate request activity.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_certificatesigningrequest_created",
      "certificatesigningrequest": "*",
      "signer_name": "*"
    },
    "description": "This metric counts the total number of Certificate Signing Requests (CSRs) created in a Kubernetes cluster. Each increment represents one new CSR object generated, regardless of the requesting component. It is measured as a cumulative count (unit: requests). This metric helps track the volume of certificate requests for cluster authentication and security management.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_certificatesigningrequest_condition",
      "certificatesigningrequest": "*",
      "signer_name": "*",
      "condition": "*"
    },
    "description": "This metric counts the number of Kubernetes CertificateSigningRequest (CSR) conditions, labeled by CSR name, signer name, and condition type. Each condition represents the current status of a CSR, such as \"Pending,\" \"Approved,\" or \"Denied.\" The metric is a simple integer count of CSRs in each condition state. It helps monitor the progress and outcome of certificate requests within the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_certificatesigningrequest_cert_length",
      "certificatesigningrequest": "*",
      "signer_name": "*"
    },
    "description": "The kube_certificatesigningrequest_cert_length metric measures the length of the issued certificate in bytes for each Certificate Signing Request (CSR) processed by the Kubernetes cluster's certificate authority. This metric can be used to monitor and troubleshoot issues related to certificate issuance, such as detecting unusually large certificates that may indicate a security vulnerability or configuration error. It can also help identify potential performance bottlenecks caused by excessive certificate size, allowing for proactive optimization of the certificate signing process.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_configmap_annotations"
    },
    "description": "The **kube_configmap_annotations** metric captures the key-value annotations attached to Kubernetes ConfigMap resources as Prometheus labels. It does not measure a numeric value but represents metadata for each ConfigMap in the cluster. This enables querying and filtering ConfigMaps based on their annotations within Prometheus. It is useful for monitoring annotation changes, verifying expected metadata, and correlating ConfigMap annotations with cluster events.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_configmap_labels"
    },
    "description": "The **kube_configmap_labels** metric exposes the set of Kubernetes labels assigned to each ConfigMap in a cluster as Prometheus labels. It does not measure a numeric value but provides metadata for each ConfigMap resource. This enables filtering, grouping, and alerting based on ConfigMap label values within Prometheus. It is useful for tracking configuration changes, identifying misconfigurations, and organizing ConfigMaps by their labels.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_configmap_info",
      "namespace": "*",
      "configmap": "*"
    },
    "description": "The **kube_configmap_info** metric provides metadata about ConfigMaps in a Kubernetes cluster, including their names, namespaces, labels, and annotations. It reports these details as dimension labels without numerical values. This metric helps track the existence and attributes of ConfigMaps but does not measure their size or error counts. It is useful for monitoring configuration data consistency and identifying changes across ConfigMaps.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_configmap_created",
      "namespace": "*",
      "configmap": "*"
    },
    "description": "This metric measures the Unix creation timestamp of a Kubernetes ConfigMap resource. It indicates when the ConfigMap was created in the cluster. The metric can be used to monitor and track changes to ConfigMaps over time, such as when new configurations are applied or updated. In operations, this metric might be useful for identifying potential issues related to configuration drift or inconsistencies between different environments. For example, if a ConfigMap is recreated unexpectedly, it could indicate a misconfiguration or an issue with the deployment process. This metric can also be used in conjunction with other metrics, such as kube_configmap_updated, to gain a more comprehensive understanding of ConfigMap lifecycle events.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_configmap_metadata_resource_version",
      "namespace": "*",
      "configmap": "*"
    },
    "description": "The kube_configmap_metadata_resource_version metric measures the resource version of a specific configmap in the Kubernetes cluster. This metric represents a unique identifier for each version of the configmap, allowing for tracking and comparison of changes made to the configmap over time. In monitoring or alerting, this metric can be used to detect unexpected changes to configmaps, such as unauthorized modifications or configuration drift. It can also be utilized to track the evolution of configmaps across different environments or deployments, facilitating better understanding and management of application configurations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_created",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "The **kube_daemonset_created** metric records the Unix timestamp (in seconds) when a specific DaemonSet was created in a Kubernetes cluster. It provides the exact creation time of the DaemonSet resource within the given namespace. This metric helps track the age of DaemonSets and identify newly created ones. It does not directly reflect cluster performance or resource usage.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_current_number_scheduled",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric measures the number of nodes in a Kubernetes cluster that are running at least one daemon pod and are supposed to be running them according to their configuration. In other words, it tracks the number of nodes where a daemonset is successfully scheduled and running. This information can be useful for monitoring and troubleshooting purposes, such as identifying underutilized or misconfigured nodes, detecting issues with daemonset deployments, or verifying that all required nodes are running the expected pods. It may also serve as a precursor to more in-depth analysis of node resource utilization, pod performance, or other related metrics.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_desired_number_scheduled",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric measures the desired number of nodes that should be running a daemon pod in a Kubernetes cluster. It represents the intended state of the daemonset deployment, indicating how many nodes are expected to have the daemon pod scheduled and running. This metric can be used to monitor the consistency between the desired state and the actual state of the daemonset deployment. Potential implications or usage in monitoring or alerting include: (1) detecting discrepancies between the desired and actual number of nodes running the daemon pod, which may indicate issues with node availability, resource constraints, or misconfigured daemonsets; (2) triggering alerts when the desired number of nodes is not met, allowing operators to investigate and resolve the underlying cause; (3) using this metric in conjunction with other metrics, such as kube_daemonset_status_current_number_scheduled, to gain a more comprehensive understanding of the daemonset deployment's health and performance.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_number_available",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric reports the number of nodes in a Kubernetes cluster where the specified daemonset has at least one pod in the \"Available\" state. It is a count of nodes successfully running the daemonset's pods. The unit is an integer representing the number of nodes. Monitoring this metric helps verify that daemonset pods are properly deployed and available across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_number_misscheduled",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric, `kube_daemonset_status_number_misscheduled`, tracks the number of nodes in a Kubernetes cluster that are running daemonset pods they should not be running. Daemonsets are intended to run specific pods on designated nodes based on labels, taints, or node selectors. A non-zero value indicates misconfiguration\u2014such as incorrect node selectors, label mismatches, or scheduling errors\u2014that causes daemon pods to run on unintended nodes.\n\n**Purpose:**  \nUse this metric in Prometheus to detect and alert on daemonset pods deployed to incorrect nodes, which can lead to resource contention, security risks, or operational inconsistencies.\n\n**Alert Threshold:**  \nTrigger an alert if `kube_daemonset_status_number_misscheduled > 0` for more than 5 minutes, indicating persistent mis-scheduling that requires investigation.\n\n**Impact:**  \n- **High values:** Suggest widespread misconfiguration or cluster state issues, potentially causing unnecessary resource usage, degraded node performance, or violation of node role constraints.  \n- **Zero value:** Indicates daemonsets are correctly scheduled, aligning with intended cluster design.\n\n**Example Alert Rule:**  \n```yaml\nalert: DaemonsetMisscheduled\nexpr: kube_daemonset_status_number_misscheduled > 0\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Daemonset pods are running on unintended nodes\"\n  description: \"Detected {{ $value }} mis-scheduled daemonset pods in namespace {{ $labels.namespace }} for daemonset {{ $labels.daemonset }}. Investigate node selectors and labels.\"\n```\n\n**Example Dashboard Usage:**  \nDisplay this metric as a gauge or time series per daemonset and namespace to quickly identify which daemonsets have mis-scheduled pods and track trends over time, enabling proactive remediation before impacting cluster stability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_number_ready",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric, `kube_daemonset_status_number_ready`, reports the number of nodes in a Kubernetes cluster on which the specified DaemonSet has at least one pod instance in the \"Ready\" state. The value is an integer count representing how many nodes currently have ready DaemonSet pods. It is collected from the Kubernetes API server via the kube-state-metrics component, which monitors the status of Kubernetes objects. The metric\u2019s unit is a simple count of nodes with ready pods for the given DaemonSet in the specified namespace.\n\nAn unusually low value\u2014such as fewer ready nodes than expected based on the total number of nodes targeted by the DaemonSet\u2014may indicate issues such as pod startup failures, scheduling problems, or node readiness problems. Such discrepancies should trigger alerts to notify administrators of potential disruptions in the DaemonSet\u2019s deployment, as this could impact the intended functionality provided by the DaemonSet across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_number_unavailable",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric measures the number of nodes in a Kubernetes cluster that are expected to be running a daemonset pod but do not have any available instances of the pod. A node is considered unavailable if none of its replicas of the daemonset pod are running and available, indicating potential issues with the node's health or resource availability. This metric can be used to identify nodes that are experiencing problems and require attention from operations teams. It may also serve as a precursor to more severe issues such as node failures or cluster instability. In monitoring and alerting, this metric can trigger alerts when the number of unavailable nodes exceeds a certain threshold, allowing for proactive measures to prevent cascading failures within the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_observed_generation",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric measures the most recent generation observed by the daemon set controller in a Kubernetes cluster. It indicates the current state of the daemon set's configuration and deployment. A higher value may indicate that the daemon set has been updated or reconfigured, potentially leading to changes in its behavior or performance. This metric can be used to monitor the health and consistency of daemon sets across the cluster, helping to detect issues related to misconfiguration, rollout failures, or other deployment-related problems. It can also serve as a trigger for alerting when unexpected generation changes occur, allowing operators to investigate and address potential issues promptly.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_status_updated_number_scheduled",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "This metric counts the number of nodes in a Kubernetes cluster that are running daemonset pods updated to the latest version. It reflects how many nodes have successfully scheduled and are actively running the current daemonset pods. The unit is a simple count of nodes. This metric helps track the progress and consistency of daemonset updates across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_metadata_generation",
      "namespace": "*",
      "daemonset": "*"
    },
    "description": "The kube_daemonset_metadata_generation metric measures the sequence number representing a specific generation of the desired state for DaemonSets in a Kubernetes cluster. This metric indicates the current version of the DaemonSet's metadata, which is used to track changes and ensure consistency between the desired and actual states. It can be used to detect potential issues such as configuration drift or unexpected updates to the DaemonSet. In monitoring and alerting, this metric can help identify when a DaemonSet's metadata has changed unexpectedly, potentially indicating a problem with the cluster's configuration or deployment process.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_annotations"
    },
    "description": "The kube_daemonset_annotations metric measures the Kubernetes annotations converted to Prometheus labels for daemonsets in a cluster. This metric provides insight into the configuration and metadata associated with daemonsets, which are essential components of Kubernetes clusters responsible for running system-level tasks. Annotations on daemonsets can contain various information such as deployment strategies, resource requirements, or custom configurations. By monitoring this metric, operators can gain visibility into how daemonsets are configured and managed within their cluster, enabling them to troubleshoot issues related to daemonset deployments, scaling, or resource utilization more effectively. Potential implications for operations include identifying misconfigured daemonsets, detecting anomalies in annotation values over time, or correlating annotation changes with specific events or incidents in the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_daemonset_labels"
    },
    "description": "The kube_daemonset_labels metric represents a collection of key-value pairs that describe the characteristics or attributes of Kubernetes DaemonSets. These labels are used to organize and categorize DaemonSets based on their configuration, environment, or other relevant factors. The metric measures the presence and values of these labels for each DaemonSet in the cluster.\n\nIn monitoring and alerting, this metric can be useful for identifying issues related to specific DaemonSets or their configurations. For example, if a particular label is consistently missing or has an unexpected value across multiple DaemonSets, it may indicate a problem with the deployment process or configuration management. Additionally, this metric can help in filtering or grouping metrics and logs based on DaemonSet labels, making it easier to analyze and troubleshoot issues.\n\nIt's worth noting that the kube_daemonset_labels metric is a conversion of Kubernetes labels to Prometheus labels, which means it provides a way to access and monitor these labels using Prometheus tools and queries.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_created",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "The kube_deployment_created metric measures the Unix timestamp when a Kubernetes deployment was created. This metric can be used to monitor and track changes in deployments over time, such as identifying when new deployments were created or updated. It may also be useful for alerting on stale or outdated deployments that have not been updated recently. However, without additional context or information about the specific deployment or cluster, it is difficult to determine the exact implications of this metric.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_status_replicas",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "The kube_deployment_status_replicas metric measures the actual number of replicas for a given Kubernetes deployment, which is the number of running instances of a pod that make up the deployment. This metric can be used to monitor and ensure that the desired replica count is being maintained by the deployment controller. Potential implications or usage in monitoring or alerting include: detecting under/over-provisioning of resources, identifying scaling issues, or verifying successful deployment rollouts. It's essential to note that this metric does not account for pending or failed replicas, so it should be used in conjunction with other metrics like kube_deployment_status_replicas_pending and kube_deployment_status_replicas_unavailable for a more comprehensive view.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_status_replicas_ready",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "This metric, **kube_deployment_status_replicas_ready**, reports the current number of ready replicas in a Kubernetes deployment within a specified namespace. A replica is considered ready when its pod is running, has passed readiness probes, and is able to serve traffic. This metric is essential for monitoring deployment health and ensuring that the desired number of pods are available to handle workloads.\n\n**Purpose:**  \nUse this metric in Prometheus to track whether deployments have the expected number of ready replicas, reflecting successful rollouts and stable application availability.\n\n**Alerting Thresholds:**  \n- Trigger an alert if the number of ready replicas falls below the desired replicas (typically defined by `kube_deployment_spec_replicas`) for more than 5 minutes.  \n- For example, alert if `kube_deployment_status_replicas_ready{namespace=\"X\", deployment=\"Y\"} < kube_deployment_spec_replicas{namespace=\"X\", deployment=\"Y\"}` continuously for 5 minutes, indicating potential pod failures or rollout issues.\n\n**Impact of Values:**  \n- **Low values:** Indicate that some pods are not ready, which can lead to reduced application capacity, degraded performance, or downtime. This may be caused by pod crashes, failed readiness probes, or resource constraints.  \n- **High values:** Typically match the desired replica count; values higher than desired replicas are unusual and may indicate stale metrics or misconfigurations.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing `kube_deployment_status_replicas_ready` alongside `kube_deployment_spec_replicas` over time to visualize deployment health and readiness trends.  \n- **Alert Rule Example (PromQL):**  \n  ```\n  kube_deployment_status_replicas_ready{namespace=\"prod\", deployment=\"web-app\"} < kube_deployment_spec_replicas{namespace=\"prod\", deployment=\"web-app\"}\n  ```\n  This alert fires if the ready replicas are fewer than desired, signaling a potential issue requiring investigation.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_status_replicas_available",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "This metric measures the number of available replicas for a deployment in a Kubernetes cluster. A replica is an instance of a pod that runs a container from a deployment. The metric indicates how many replicas are currently running and available to serve traffic, excluding any replicas that may be pending or unavailable due to issues such as node unavailability or container crashes. This information can be used to monitor the health and scalability of deployments in the cluster. Potential implications include identifying underutilized resources, detecting deployment failures, and triggering alerts for replica availability issues. It can also be used to inform scaling decisions by comparing the available replicas with the desired state specified in the deployment configuration.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_status_replicas_unavailable",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "This metric measures the number of unavailable replicas per deployment in a Kubernetes cluster. An unavailable replica is a pod that is not running or is not ready to serve traffic due to various reasons such as container crashes, network issues, or configuration errors. This metric provides insight into the health and availability of deployments in the cluster, helping operators identify potential issues before they impact application performance or user experience. It can be used to trigger alerts when a deployment's unavailable replicas exceed a certain threshold, indicating a need for manual intervention or automated remediation actions such as rolling updates or restarts.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_status_replicas_updated",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "This metric measures the number of replicas updated for each deployment in a Kubernetes cluster. It indicates the number of pods that have been successfully updated to match the desired state specified in the deployment configuration. This metric can be used to monitor the health and scalability of deployments, identify potential issues with rolling updates or replica scaling, and optimize deployment strategies. Potential implications include: (1) detecting failed updates or stuck replicas, which may indicate configuration errors or resource constraints; (2) monitoring the rate at which replicas are updated, which can help identify performance bottlenecks or inefficient update processes; (3) correlating this metric with other metrics, such as CPU or memory usage, to understand the impact of deployment updates on cluster resources. If the number of updated replicas is consistently lower than expected, it may indicate issues with the deployment configuration, pod scheduling, or resource availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_status_observed_generation",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "The kube_deployment_status_observed_generation metric measures the generation number of a Kubernetes deployment as observed by the deployment controller. This value represents the current state of the deployment and is used to track changes made to the deployment configuration. A higher generation number indicates that the deployment has been updated or modified since the last observed generation. This metric can be used in monitoring and alerting to detect potential issues with deployment updates, such as failed rollouts or stuck deployments. It can also be used to trigger notifications when a deployment is updated, allowing operators to review and verify the changes made.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_status_condition",
      "namespace": "*",
      "deployment": "*",
      "condition": "*",
      "status": "*"
    },
    "description": "The kube_deployment_status_condition metric measures the current status conditions of a Kubernetes deployment. This includes various states such as 'Available', 'Progressing', and 'Degraded'. The metric provides a snapshot of the deployment's overall health and readiness for production use. Potential implications or usage in monitoring or alerting include: triggering alerts when a deployment is not available, detecting issues with rolling updates, or identifying resource constraints impacting deployment progress. This metric can be used to inform DevOps teams about the status of their deployments, enabling them to take corrective actions to ensure smooth operation and minimize downtime.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_spec_replicas",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "This metric measures the number of desired pods for a deployment in a Kubernetes cluster. It represents the intended state of the deployment, indicating how many replicas should be running at any given time. This value is typically set by the user or automated through configuration management tools and can be used to monitor and ensure that the actual number of running pods matches the desired state. Potential implications for monitoring include tracking changes in this metric over time to detect potential issues with deployment scaling, while alerting could be triggered when there are discrepancies between the desired and actual pod counts.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_spec_paused",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "The kube_deployment_spec_paused metric measures whether a Kubernetes deployment is currently paused and will not be processed by the deployment controller. This pause can be intentionally set by an administrator to prevent automatic updates or rollouts of the deployment. A value of 'true' indicates that the deployment is paused, while a value of 'false' indicates it is active and being managed by the deployment controller. This metric can be used in monitoring and alerting to detect situations where deployments are unexpectedly paused, potentially causing delays or disruptions to application services. It may also be useful for identifying resource constraints or other issues that require manual intervention to resolve.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_spec_strategy_rollingupdate_max_unavailable",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "This metric, **kube_deployment_spec_strategy_rollingupdate_max_unavailable**, represents the configured maximum number or percentage of pods that can be unavailable simultaneously during a rolling update of a Kubernetes deployment. It reflects the deployment\u2019s update strategy setting (`maxUnavailable`), controlling how aggressively pods are taken down and replaced to balance availability and rollout speed.\n\n**Purpose:**  \nIn Prometheus, this metric helps SREs understand the deployment\u2019s tolerance for downtime during updates. It sets the expected upper limit of unavailable pods allowed without impacting service availability.\n\n**Alert Threshold:**  \nAn alert should trigger if the actual number of unavailable pods exceeds this configured maximum value, indicating that the deployment is experiencing more downtime than intended, which could signal rollout issues or infrastructure problems.\n\n**Impact of Values:**  \n- **High values** (e.g., 50% or more) allow faster rollouts but increase the risk of service disruption due to more pods being down simultaneously.  \n- **Low values** (e.g., 0-10%) prioritize availability but slow down the update process, potentially delaying critical fixes or features.\n\n**Example Usage:**  \n- **Dashboard:** Display this metric alongside the current number of unavailable pods to visualize if the deployment is within its configured limits during updates.  \n- **Alert Rule (PromQL):**  \n  ```\n  kube_deployment_status_replicas_unavailable{namespace=\"$namespace\", deployment=\"$deployment\"} > kube_deployment_spec_strategy_rollingupdate_max_unavailable{namespace=\"$namespace\", deployment=\"$deployment\"}\n  ```\n  This alert fires when the actual unavailable pods exceed the configured maxUnavailable, prompting investigation into rollout health.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_spec_strategy_rollingupdate_max_surge",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "This metric measures the maximum number of replicas that can be scheduled above the desired number of replicas during a rolling update of a deployment in a Kubernetes cluster. It represents the surge capacity allowed for the deployment to scale up temporarily while replacing old pods with new ones. A higher value indicates a more aggressive scaling strategy, which may lead to faster rollout times but also increases the risk of cascading failures if not properly managed. This metric can be used to monitor and alert on potential issues related to deployment rollouts, such as unexpected spikes in resource utilization or errors due to excessive scaling.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_metadata_generation",
      "namespace": "*",
      "deployment": "*"
    },
    "description": "The kube_deployment_metadata_generation metric measures the sequence number representing a specific generation of the desired state for a Kubernetes deployment. This metric indicates the number of times the deployment's configuration has been updated or changed since its initial creation. A higher value signifies that the deployment's configuration has undergone multiple updates, potentially indicating changes in the application's requirements or infrastructure adjustments. In monitoring and alerting, this metric can be used to detect sudden spikes in generation numbers, which may indicate issues with deployment rollouts, configuration drift, or other problems that require immediate attention.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_annotations"
    },
    "description": "The kube_deployment_annotations metric measures the Kubernetes annotations associated with a deployment, which are converted to Prometheus labels for monitoring and analysis purposes. Annotations in Kubernetes provide additional metadata about an object, such as configuration settings or custom attributes. This metric can be used to monitor and troubleshoot deployments by tracking changes to these annotations over time. For example, it could help identify when a specific annotation is updated or removed, which might indicate a configuration change or issue with the deployment. In monitoring and alerting, this metric can be utilized to set up alerts based on specific annotation values or changes, enabling operators to quickly respond to potential issues.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_deployment_labels"
    },
    "description": "The **kube_deployment_labels** metric exposes the set of labels assigned to each Kubernetes deployment as Prometheus labels. It does not measure a numeric value but provides metadata by mapping deployment label key-value pairs for identification and filtering purposes. This metric is collected by querying the Kubernetes API and is useful for monitoring deployment configurations and grouping metrics by deployment attributes.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_endpoint_info",
      "namespace": "*",
      "endpoint": "*"
    },
    "description": "The **kube_endpoint_info** metric in Prometheus provides detailed metadata about Kubernetes endpoints, which are IP and port combinations exposed by pods to the network. Each metric sample includes labels such as the pod name, namespace, node, IP address, port, and protocol (TCP/UDP), enabling SREs to track the presence and configuration of endpoints across the cluster.\n\n**Purpose:**  \nThis metric helps SREs verify that expected endpoints are correctly registered and available, facilitating monitoring of service discovery and network connectivity within the cluster.\n\n**Alerting Guidance:**  \nSince **kube_endpoint_info** is an informational metric (it does not have a numeric value but rather presence via labels), alerts should be based on the absence or unexpected changes in endpoint counts or labels. For example:  \n- Alert if the number of endpoints for a critical service drops below a defined threshold (e.g., fewer than 2 endpoints for a highly available service).  \n- Alert if endpoints appear in unexpected namespaces or with unexpected IP addresses, indicating possible misconfiguration or security issues.\n\n**Impact of Changes:**  \n- A **decrease** in the number of endpoints may indicate pod failures, network issues, or service disruptions, potentially causing application downtime or degraded performance.  \n- An **increase** or appearance of unexpected endpoints could signal misconfigurations, unauthorized pods, or security risks.\n\n**Example Usage:**  \nTo monitor the number of endpoints per service namespace and alert if it falls below 2, you can use a Prometheus query like:  \n```\ncount(kube_endpoint_info) by (namespace, endpoint)\n```\nand create an alert rule:  \n```\nalert: LowEndpointCount  \nexpr: count(kube_endpoint_info{namespace=\"critical-namespace\"}) by (endpoint) < 2  \nfor: 5m  \nlabels: { severity=\"warning\" }  \nannotations: { summary=\"Low number of endpoints for {{ $labels.endpoint }} in {{ $labels.namespace }}\" }\n```\nIn dashboards, this metric can be visualized as a table or heatmap showing endpoint counts per namespace and service, helping quickly identify missing or unexpected endpoints.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_endpoint_created",
      "namespace": "*",
      "endpoint": "*"
    },
    "description": "The **kube_endpoint_created** metric records the Unix timestamp (in seconds) when a specific Kubernetes endpoint was created. It provides a precise creation time for each endpoint within the given namespace. This metric helps track the lifecycle and age of endpoints, enabling monitoring of new endpoint additions and analysis of endpoint creation trends over time.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_endpoint_annotations"
    },
    "description": "The **kube_endpoint_annotations** metric exposes Kubernetes endpoint annotations as Prometheus labels, enabling SREs to monitor custom metadata attached to endpoint objects. This metric helps track configuration changes, detect misconfigurations, and ensure endpoint annotations comply with organizational policies. Since annotations are key-value pairs without inherent numeric values, this metric is best used to identify the presence, absence, or unexpected changes of specific annotation keys or values rather than threshold-based alerts on numeric values.\n\n**Alerting guidance:**  \nSet alerts based on the absence or unexpected modification of critical annotations. For example, trigger an alert if a required annotation (e.g., `service.beta.kubernetes.io/aws-load-balancer-internal`) is missing from any endpoint, or if annotation values deviate from expected patterns, indicating potential misconfiguration.\n\n**Impact of changes:**  \n- **Missing or incorrect annotations** can lead to misrouted traffic, security policy violations, or failure of integrations relying on annotation metadata.  \n- **Unexpected annotation changes** may signal unauthorized modifications or deployment issues affecting endpoint behavior.\n\n**Example usage:**  \n- **Dashboard:** Display a table or heatmap showing endpoints and their key annotations, highlighting missing or inconsistent values.  \n- **Alert rule example:**  \n```yaml\nalert: MissingCriticalEndpointAnnotation  \nexpr: count by (endpoint) (kube_endpoint_annotations{annotation_key=\"service.beta.kubernetes.io/aws-load-balancer-internal\"} == \"\") > 0  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"Endpoint {{ $labels.endpoint }} is missing critical annotation\"  \n  description: \"The endpoint {{ $labels.endpoint }} lacks the required AWS internal load balancer annotation, which may affect traffic routing.\"  \n```\n\nThis approach ensures SREs can proactively detect and respond to annotation-related issues impacting endpoint functionality.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_endpoint_labels"
    },
    "description": "The **kube_endpoint_labels** metric exposes the set of Kubernetes labels assigned to each endpoint (such as pods or services) in the cluster, represented as Prometheus labels. Its primary purpose is to enable filtering, grouping, and aggregation of metrics based on endpoint metadata, facilitating more granular monitoring and alerting tied to specific workloads or services.\n\n**Purpose:**  \n- Provides a dynamic mapping of endpoint labels to Prometheus labels, allowing SREs to create custom queries and alerts based on the presence, absence, or changes of specific labels on endpoints.  \n- Enables correlation of endpoint health and behavior with deployment metadata, improving root cause analysis and capacity planning.\n\n**Alerting Guidance:**  \n- Since this metric is label metadata rather than a numeric value, alerts should focus on changes in label counts or the presence/absence of critical labels.  \n- Example threshold: Alert if the number of endpoints with a required label (e.g., `app=payment-service`) drops below an expected minimum, indicating potential pod failures or misconfigurations.  \n- Example alert condition: `count(kube_endpoint_labels{label_app=\"payment-service\"}) < 3` triggers when fewer than 3 endpoints have the `app=payment-service` label, possibly signaling service degradation.\n\n**Impact of Values:**  \n- A **high count** of endpoints with a specific label typically indicates healthy scaling or deployment of that service.  \n- A **low or zero count** may indicate pod crashes, failed deployments, or label misconfigurations, potentially leading to service unavailability or degraded performance.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the number of endpoints per service by grouping `kube_endpoint_labels` by `label_app` to monitor service scaling and detect anomalies.  \n- **Alert Rule:**  \n  ```yaml\n  alert: PaymentServiceEndpointsLow\n  expr: count(kube_endpoint_labels{label_app=\"payment-service\"}) < 3\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"Low number of payment-service endpoints\"\n    description: \"Less than 3 endpoints with label app=payment-service detected for over 5 minutes, indicating potential pod failures or deployment issues.\"\n  ```\nThis approach ensures that SREs can leverage **kube_endpoint_labels** to maintain service reliability by monitoring endpoint label presence and counts effectively.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_endpoint_address",
      "namespace": "*",
      "endpoint": "*",
      "port_protocol": "*",
      "port_number": "*",
      "port_name": "*",
      "ip": "*",
      "ready": "*"
    },
    "description": "The **kube_endpoint_address** metric in Prometheus tracks the status of individual IP addresses associated with Kubernetes Endpoints, labeled by namespace, endpoint name, port protocol, port number, port name, IP address, and readiness state (`ready=\"true\"` or `ready=\"false\"`). It indicates whether each endpoint IP is currently considered ready (available) or not, reflecting the health and connectivity of service backends.\n\n**Purpose:**  \nThis metric helps SREs monitor the real-time availability of service endpoints, enabling detection of issues such as pod failures, network partitions, or misconfigurations that cause endpoints to become unready.\n\n**Alert Thresholds:**  \nA common alert condition is when the count of ready endpoints for a critical service drops below a defined threshold (e.g., zero or less than the expected number of replicas) for a sustained period (e.g., 5 minutes). For example:  \n```\nsum by (namespace, endpoint) (kube_endpoint_address{ready=\"true\"}) < expected_endpoint_count\n```\nThis indicates that no healthy backend is available to serve traffic, warranting immediate investigation.\n\n**Impact of Values:**  \n- **High ready endpoint count:** Indicates healthy, available backends, ensuring service reliability and load distribution.  \n- **Low or zero ready endpoint count:** Signals potential service disruption, degraded performance, or outages due to unavailable pods or network issues.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing the number of ready vs. unready endpoints per namespace or service over time to quickly identify trends or sudden drops in availability.  \n- **Alert Rule:**  \n```yaml\nalert: EndpointUnavailable\nexpr: sum by (namespace, endpoint) (kube_endpoint_address{ready=\"true\"}) == 0\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"No ready endpoints for {{ $labels.namespace }}/{{ $labels.endpoint }}\"\n  description: \"All endpoints for service {{ $labels.endpoint }} in namespace {{ $labels.namespace }} are unready for more than 5 minutes.\"\n```\nThis alert triggers when no ready endpoints exist for a service, indicating a critical outage affecting service availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_endpoint_ports",
      "namespace": "*",
      "endpoint": "*",
      "port_name": "*",
      "port_protocol": "*",
      "port_number": "*"
    },
    "description": "The **kube_endpoint_ports** metric in Prometheus reports the ports exposed by Kubernetes Endpoints, detailing the mapping between Services and their backing Pods. Each time series is labeled by namespace, endpoint (Service), port name, protocol, and port number, allowing SREs to verify that Services correctly expose the expected ports on their associated Pods.\n\n**Purpose:**  \nThis metric helps SREs monitor the network connectivity layer within a Kubernetes cluster by confirming that Services have active endpoints with the correct port configurations. It is useful for detecting misconfigurations such as missing or orphaned endpoints, unexpected port changes, or discrepancies between Service definitions and Pod readiness.\n\n**Alerting thresholds:**  \n- **Low or zero values:** If the count of exposed ports for a critical Service endpoint drops to zero or below an expected threshold for a sustained period (e.g., 5 minutes), it may indicate that Pods are not ready or endpoints are missing, triggering an alert for potential service disruption.  \n- **Unexpected port changes:** Sudden changes in port numbers or protocols can signal misconfiguration or deployment errors and should be flagged.\n\n**Impact of values:**  \n- **High values:** Typically, a higher number of ports per endpoint is normal if the Service exposes multiple ports; however, an unexpected increase might indicate configuration drift or security risks.  \n- **Low or zero values:** Indicates that no Pods are currently serving the Service on the expected ports, potentially causing application downtime or degraded functionality.\n\n**Example usage:**  \n- **Dashboard:** Display a panel showing the count of active ports per Service endpoint, grouped by namespace and port name, to quickly identify Services with missing or extra ports.  \n- **Alert rule example:**  \n```yaml\nalert: KubernetesServiceEndpointPortMissing  \nexpr: kube_endpoint_ports == 0  \nfor: 5m  \nlabels:  \n  severity: critical  \nannotations:  \n  summary: \"No active ports detected for Service {{ $labels.endpoint }} in namespace {{ $labels.namespace }}\"  \n  description: \"The Service {{ $labels.endpoint }} in namespace {{ $labels.namespace }} has no active endpoints exposing port {{ $labels.port_number }} ({{ $labels.port_name }}). This may indicate Pod readiness issues or network misconfiguration.\"  \n```\n\n**Note:** Since **kube_endpoint_ports** has been deprecated since Kubernetes metrics server v2.14.0, it is recommended to transition to metrics based on the EndpointSlice API for more accurate and current endpoint port information.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_ingress_info",
      "namespace": "*",
      "ingress": "*",
      "ingressclass": "*"
    },
    "description": "The kube_ingress_info metric measures information about ingress resources in a Kubernetes cluster. It provides details such as ingress name, namespace, class, and status. This metric can be used to monitor the health and configuration of ingress resources, which are crucial for exposing services outside the cluster through HTTP or HTTPS protocols. Potential implications include identifying misconfigured or malfunctioning ingress resources, detecting changes in ingress configurations, and triggering alerts when ingress resources become unavailable or experience errors. This information can be leveraged by operations teams to troubleshoot issues, optimize ingress resource utilization, and ensure smooth service exposure.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_ingress_annotations"
    },
    "description": "The kube_ingress_annotations metric measures the Kubernetes annotations converted to Prometheus labels for ingress resources in a cluster. This metric provides visibility into the configuration and settings applied to ingress objects, such as path rewriting rules, TLS termination configurations, or other custom annotations. It can be used to monitor and troubleshoot issues related to ingress traffic routing, SSL/TLS certificate management, or other annotation-driven behaviors. Potential implications include identifying misconfigured or outdated annotations that may impact application availability or security. This metric can also inform capacity planning by highlighting the types of annotations being applied across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_ingress_labels"
    },
    "description": "The **kube_ingress_labels** metric exposes the set of Kubernetes labels attached to each ingress resource in the cluster, mapped as Prometheus labels. Its primary purpose is to enable filtering, grouping, and correlation of ingress-related metrics based on these labels, facilitating fine-grained monitoring and alerting of ingress behavior and health.\n\nSince **kube_ingress_labels** itself is a label-mapping metric rather than a numeric measurement, it does not have direct threshold values or high/low numeric impacts. Instead, it serves as a dimension to segment other ingress metrics (e.g., request rates, error rates, latency) by ingress labels such as environment, app, or team.\n\n**Usage guidance:**\n- Use **kube_ingress_labels** to create label selectors in Prometheus queries, enabling targeted monitoring of specific ingress resources.\n- Define alert rules on ingress metrics filtered by these labels to detect anomalies, such as increased error rates or traffic drops on ingresses with critical labels.\n- Monitor changes in label values over time to detect misconfigurations or unauthorized modifications.\n\n**Example alert rule:**\n```yaml\nalert: HighIngressErrorRate\nexpr: rate(nginx_ingress_controller_requests{status=~\"5..\", ingress=~\".+\"}[5m]) > 0.05\n  and on(ingress) kube_ingress_labels{environment=\"production\"}\nfor: 10m\nlabels:\n  severity: critical\nannotations:\n  summary: \"High 5xx error rate on production ingress {{ $labels.ingress }}\"\n  description: \"Ingress {{ $labels.ingress }} in production is returning >5% 5xx errors over the last 10 minutes.\"\n```\n\n**Example dashboard usage:**\n- Use **kube_ingress_labels** to group ingress request metrics by labels like `app` or `environment` to visualize traffic patterns and error rates per ingress group.\n- Track ingress health trends filtered by labels to quickly identify problematic ingress resources within specific teams or environments.\n\nIn summary, **kube_ingress_labels** is a foundational metric for enriching ingress monitoring with contextual label data, enabling precise alerting and insightful dashboards based on ingress metadata.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_ingress_created",
      "namespace": "*",
      "ingress": "*"
    },
    "description": "The **kube_ingress_created** metric records the Unix timestamp when each ingress resource was created in the Kubernetes cluster. It helps SREs track the age and creation timing of ingress objects, enabling visibility into deployment patterns and configuration changes. \n\n**Purpose:**  \n- Monitor when ingress resources are created to detect recent changes that might impact traffic routing or application availability.  \n- Correlate ingress creation events with incidents or performance issues.  \n- Support capacity planning by observing the rate of new ingress deployments over time.\n\n**Alerting Guidance:**  \n- Alert if new ingress resources are created unexpectedly or too frequently within a short time window, which may indicate automation errors or misconfigurations.  \n- Example threshold: Trigger an alert if more than 5 ingress resources are created within 10 minutes in a critical namespace, signaling potential deployment storms or configuration churn.\n\n**Impact of Values:**  \n- **High values (recent timestamps):** Indicate new ingress objects have been created recently; frequent spikes may suggest rapid changes or instability.  \n- **Low values (older timestamps):** Suggest stable ingress configurations with no recent changes; a sudden drop in new ingress creations might indicate stalled deployments or automation failures.\n\n**Example Usage:**  \n- **Dashboard:** Display a heatmap or timeline of ingress creation timestamps per namespace to visualize deployment activity and identify unusual spikes.  \n- **Alert Rule (PromQL):**  \n  ```\n  count_over_time(kube_ingress_created[10m]) > 5\n  ```  \n  This alerts if more than 5 ingress resources are created within 10 minutes, prompting investigation into rapid ingress changes.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_ingress_metadata_resource_version",
      "namespace": "*",
      "ingress": "*"
    },
    "description": "The kube_ingress_metadata_resource_version metric measures the resource version of a specific ingress in a Kubernetes cluster. This metric represents a unique identifier for each ingress object, indicating changes to its configuration or metadata over time. It can be used to track updates to ingress resources, such as changes to annotations, labels, or other metadata. In monitoring and alerting, this metric can help identify when an ingress resource has been modified unexpectedly, potentially impacting traffic routing or application availability. This information can be useful for debugging purposes, ensuring that ingress configurations are up-to-date and consistent across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_ingress_path",
      "namespace": "*",
      "ingress": "*",
      "host": "*",
      "path": "*",
      "service_name": "*",
      "service_port": "*"
    },
    "description": "The **kube_ingress_path** metric in Prometheus provides detailed information about ingress paths configured in a Kubernetes cluster, including the namespace, ingress resource, host, path, backend service name, and service port. It is primarily used to monitor the routing configuration of ingress controllers and verify that traffic is correctly directed to backend services.\n\n**Purpose:**  \nThis metric helps SREs ensure that ingress rules are properly set up and that backend services are reachable through their defined paths. It can reveal misconfigurations such as missing or incorrect paths, unresponsive backend services, or unexpected changes in ingress routing.\n\n**Alerting Thresholds:**  \n- Alert if the number of ingress paths with no healthy backend endpoints exceeds zero for more than 5 minutes, indicating potential service outages or misconfigurations.  \n- Alert if the count of ingress paths suddenly drops to zero, which may indicate ingress controller failures or deleted ingress resources.  \n- Alert on anomalous spikes or drops in ingress path usage that deviate significantly (e.g., >50%) from baseline traffic patterns, signaling possible routing issues or attacks.\n\n**Impact of Values:**  \n- **High values** (many ingress paths) indicate complex routing configurations, which may increase ingress controller load and require capacity planning.  \n- **Low or zero values** may indicate missing ingress rules or failures in ingress controller synchronization, potentially causing service unavailability.\n\n**Example Usage:**  \n- **Dashboard:** Visualize the number of ingress paths per namespace and service, highlighting paths with no healthy backends to quickly identify routing issues.  \n- **Alert Rule Example:**  \n```yaml\nalert: IngressPathBackendUnavailable  \nexpr: kube_ingress_path{namespace!=\"\", service_name!=\"\"} and on(namespace, ingress, host, path, service_name, service_port) absent(kube_endpoint_address_available{namespace=~\".*\", endpoint=~\".*\"})  \nfor: 5m  \nlabels:  \n  severity: critical  \nannotations:  \n  summary: \"Ingress path {{ $labels.ingress }} in namespace {{ $labels.namespace }} has no available backend endpoints\"  \n  description: \"The ingress path {{ $labels.path }} on host {{ $labels.host }} routes to service {{ $labels.service_name }} on port {{ $labels.service_port }}, but no healthy backend endpoints are available for over 5 minutes.\"  \n```\n\nThis enables proactive detection and resolution of ingress routing issues to maintain reliable traffic flow within the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_ingress_tls",
      "namespace": "*",
      "ingress": "*",
      "tls_host": "*",
      "secret": "*"
    },
    "description": "The kube_ingress_tls metric measures information related to Ingress TLS (Transport Layer Security) configurations in a Kubernetes environment. Specifically, it tracks host and secret details for each Ingress resource that has TLS enabled. This includes the hostname or domain name associated with the TLS configuration and the Secret object used to store the TLS certificate and key. The metric provides visibility into the TLS settings of Ingress resources, which is crucial for ensuring secure communication between clients and servers. Potential implications include monitoring for expired or about-to-expire certificates, detecting misconfigured TLS settings, and verifying that the correct certificates are being used for each Ingress host. This information can be used in operations to identify potential security risks, troubleshoot connectivity issues, and maintain compliance with security best practices.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_lease_owner",
      "lease": "*",
      "owner_kind": "*",
      "owner_name": "*",
      "namespace": "*",
      "lease_holder": "*"
    },
    "description": "The **kube_lease_owner** metric identifies the owner of a specific Lease resource in a Kubernetes cluster, including details such as the owner\u2019s kind, name, namespace, and the current lease holder. A Lease is a Kubernetes object used to coordinate resource locking and leader election, ensuring only one entity holds the lock at a time. This metric provides string labels describing the Lease and its owner but does not measure a numeric value or unit. It is useful for tracking which component currently holds a Lease to help diagnose resource contention or leader election status.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_lease_renew_time",
      "lease": "*",
      "namespace": "*"
    },
    "description": "The kube_lease_renew_time metric measures the time it takes for a Kubernetes node to renew its lease in the etcd cluster. This lease is essential for maintaining the node's membership and ensuring that it remains part of the cluster. A high or increasing renewal time can indicate issues with etcd, network connectivity problems between nodes, or even hardware failures on the node. Monitoring this metric can help identify potential problems before they impact cluster availability or performance. It may be used in alerting to notify administrators when lease renewals exceed a certain threshold, triggering further investigation and corrective action.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_mutatingwebhookconfiguration_info",
      "namespace": "*",
      "mutatingwebhookconfiguration": "*"
    },
    "description": "The **kube_mutatingwebhookconfiguration_info** metric in Prometheus provides metadata about each MutatingWebhookConfiguration resource in the Kubernetes cluster, including its name and namespace. It is a gauge metric that indicates the presence of these webhook configurations but does not count or measure their activity directly. This metric helps SREs track which mutating webhooks are currently deployed and detect unexpected additions, removals, or changes in webhook configurations.\n\n**Purpose:**  \n- To monitor the existence and identity of mutating webhook configurations across namespaces.  \n- To detect configuration drift or unauthorized changes that could impact admission control behavior or cluster security.\n\n**Alerting guidance:**  \n- Trigger an alert if the number of expected mutating webhook configurations changes unexpectedly (e.g., a webhook is deleted or a new unknown webhook appears).  \n- Example threshold: Alert if the count of mutatingwebhookconfiguration resources deviates from a known baseline by \u00b11 or more.  \n- Since this metric is presence-based, alerts are typically based on changes in the set of webhook names rather than numeric thresholds.\n\n**Impact of values:**  \n- **High value (more webhook configurations than expected):** May indicate unauthorized or misconfigured webhooks that could mutate requests unexpectedly, potentially causing security or stability issues.  \n- **Low value (fewer webhook configurations than expected):** Could mean critical mutating webhooks are missing, leading to missing admission controls or policy enforcement.\n\n**Example usage:**  \n- **Dashboard:** Display a list or count of current mutating webhook configurations by namespace to provide visibility into active admission controllers.  \n- **Alert rule example (PromQL):**  \n  ```promql\n  count(kube_mutatingwebhookconfiguration_info) != <expected_count>\n  ```  \n  Replace `<expected_count>` with the known number of mutating webhook configurations your cluster should have. This alert fires if the count changes, indicating a configuration change that requires investigation.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_mutatingwebhookconfiguration_created",
      "namespace": "*",
      "mutatingwebhookconfiguration": "*"
    },
    "description": "This metric, **kube_mutatingwebhookconfiguration_created**, records the Unix timestamp when each MutatingWebhookConfiguration resource was created in the Kubernetes cluster, labeled by namespace and configuration name. It helps SREs track the age and creation events of mutating webhook configurations, which are critical for admission control and cluster security.\n\n**Purpose:**  \n- Monitor when mutating webhook configurations are introduced or updated.  \n- Audit changes to admission control policies that could affect workload behavior or security posture.\n\n**Alerting guidance:**  \n- Trigger an alert on the creation of a new MutatingWebhookConfiguration by detecting a recent timestamp (e.g., within the last 5 minutes), indicating a configuration change that may require review.  \n- Example alert threshold: alert if `time() - kube_mutatingwebhookconfiguration_created{namespace=\"*\", mutatingwebhookconfiguration=\"*\"} < 300` seconds (5 minutes), signaling a newly created webhook.\n\n**Impact of values:**  \n- **Low values (recent timestamps):** Indicate a newly created webhook configuration, which may introduce new admission policies or security implications. Immediate review is recommended.  \n- **High values (older timestamps):** Represent stable, long-standing webhook configurations, implying no recent changes.\n\n**Example usage:**  \n- **Alert rule:**  \n  ```yaml\n  alert: NewMutatingWebhookConfigurationCreated\n  expr: time() - kube_mutatingwebhookconfiguration_created < 300\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"New MutatingWebhookConfiguration created in the last 5 minutes\"\n    description: \"A new mutating webhook configuration '{{ $labels.mutatingwebhookconfiguration }}' was created in namespace '{{ $labels.namespace }}'. Review this change to ensure it aligns with security policies.\"\n  ```\n\n- **Dashboard panel:**  \n  Display the age of each mutating webhook configuration as `time() - kube_mutatingwebhookconfiguration_created` in hours or days, highlighting recently created webhooks to quickly identify recent changes in admission control.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_mutatingwebhookconfiguration_metadata_resource_version",
      "namespace": "*",
      "mutatingwebhookconfiguration": "*"
    },
    "description": "This metric represents the resource version of a specific MutatingWebhookConfiguration in the Kubernetes cluster. It measures the current state of the configuration's metadata, indicating whether it has been updated or modified since its last recorded version. The metric can be used to detect changes in the webhook configuration, such as unexpected updates or rollbacks, which may impact the behavior of pods and services within the cluster. Potential implications include identifying misconfigurations, detecting security vulnerabilities, or triggering alerts when a critical configuration change occurs.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_mutatingwebhookconfiguration_webhook_clientconfig_service",
      "namespace": "*",
      "mutatingwebhookconfiguration": "*",
      "webhook_name": "*",
      "service_name": "*",
      "service_namespace": "*"
    },
    "description": "This metric, `kube_mutatingwebhookconfiguration_webhook_clientconfig_service`, identifies the Kubernetes Service that the API server uses to connect to each mutating webhook configuration\u2019s webhook. It provides visibility into which backend service handles admission requests for mutation before objects are persisted in the cluster. \n\n**Purpose:**  \nUse this metric to verify that the API server is correctly routing admission requests to the intended webhook service. It helps detect misconfigurations, service unavailability, or unexpected changes in webhook client configurations that could disrupt pod or object mutation workflows.\n\n**Alerting guidance:**  \nThis metric itself is a label-mapping metric (indicating service identity) and does not have a numeric value to threshold directly. Instead, alert on related metrics such as webhook call failures, latency, or absence of expected webhook service mappings. For example, alert if the webhook service label disappears or changes unexpectedly, or if webhook call failure rates (e.g., `kube_admissionwebhook_admission_duration_seconds` or `kube_admissionwebhook_admission_errors_total`) exceed a threshold (e.g., >5% error rate over 5 minutes).\n\n**Impact of anomalies:**  \n- Missing or changed service labels may indicate webhook misconfiguration or service deletion, potentially causing admission failures or unintended behavior.  \n- Persistent failure or latency in webhook calls routed to the service can delay or block pod creation/modification, impacting cluster stability and application deployments.\n\n**Example usage:**  \nIn a dashboard, display this metric alongside webhook call success/failure rates to correlate service identity with webhook health. For alerting, create a Prometheus alert that triggers if the webhook service label for a known mutating webhook disappears or if error rates for that webhook\u2019s calls exceed a threshold:\n\n```yaml\n# Example alert: webhook service missing\nalert: MutatingWebhookServiceMissing\nexpr: absent(kube_mutatingwebhookconfiguration_webhook_clientconfig_service{mutatingwebhookconfiguration=\"my-webhook\"})\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"Mutating webhook service missing for 'my-webhook'\"\n  description: \"The API server no longer routes 'my-webhook' to any service, indicating possible misconfiguration or service outage.\"\n\n# Example alert: high webhook error rate\nalert: MutatingWebhookErrorRateHigh\nexpr: rate(kube_admissionwebhook_admission_errors_total{webhook_name=\"my-webhook\"}[5m]) / rate(kube_admissionwebhook_admission_duration_seconds_count{webhook_name=\"my-webhook\"}[5m]) > 0.05\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High error rate for mutating webhook 'my-webhook'\"\n  description: \"More than 5% of admission requests to 'my-webhook' are failing over the last 5 minutes.\"\n```\n\nUse this metric as a foundational reference to ensure webhook client configurations remain stable and correctly mapped to services, enabling reliable admission control mutation workflows.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_namespace_created",
      "namespace": "*"
    },
    "description": "The kube_namespace_created metric measures the Unix creation timestamp for a Kubernetes namespace. This metric indicates when a namespace was created in the cluster. It can be used to monitor and track changes to namespaces over time, such as identifying newly created namespaces or detecting anomalies in namespace creation patterns. In monitoring or alerting scenarios, this metric could be used to trigger notifications when a new namespace is created outside of regular business hours or by an unknown user, indicating potential security risks or operational issues.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_namespace_annotations"
    },
    "description": "The kube_namespace_annotations metric measures the Kubernetes annotations associated with each namespace in a cluster, converted to Prometheus labels for easier monitoring and analysis. This metric provides insights into the configuration and metadata of namespaces, which can be useful for identifying potential issues or misconfigurations. For example, it can help detect if a namespace is missing essential annotations required for proper functioning or if there are inconsistencies in annotation values across different namespaces. In operations, this metric can be used to create alerts for namespace-related issues, such as missing or invalid annotations, and to track changes in namespace configurations over time.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_namespace_labels"
    },
    "description": "The **kube_namespace_labels** metric exposes the Kubernetes labels assigned to each namespace as Prometheus labels. It is a gauge metric with a constant value of 1 for each namespace-label pair, serving as a metadata indicator rather than a numeric measurement. The data source for this metric is the Kubernetes API server, which provides the current set of labels for all namespaces in the cluster. This metric enables Prometheus to associate namespace-specific metadata with other metrics for filtering, grouping, or aggregation purposes. An unusual value or absence of expected labels (e.g., missing critical labels like environment or team) should trigger an alert, as it may indicate misconfiguration, namespace creation without proper labeling, or drift from organizational policies, potentially impacting monitoring accuracy and resource management.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_namespace_status_phase",
      "namespace": "*",
      "phase": "*"
    },
    "description": "The kube_namespace_status_phase metric measures the current status phase of a Kubernetes namespace. This phase indicates whether the namespace is in a stable state (e.g., Active) or if it's experiencing issues (e.g., Terminating). The possible phases include: Active, Pending, Failed, Unknown, and Terminating. This metric can be used to monitor namespace health and detect potential problems such as namespace deletion failures or unexpected phase transitions. It may also be used in alerting scenarios to notify administrators of namespace status changes that require attention.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_namespace_status_condition"
    },
    "description": "The **kube_namespace_status_condition** metric in Prometheus reports the current status conditions of Kubernetes namespaces, such as whether a namespace is Active, Terminating, or has any failure conditions. Each condition is represented as a gauge with a value of 1 (true) or 0 (false), indicating the presence or absence of that condition for a given namespace.\n\n**Purpose:**  \nThis metric helps SREs monitor the health and lifecycle state of namespaces, enabling early detection of issues like namespaces stuck in Terminating state or namespaces with failed readiness conditions that could impact workloads deployed within them.\n\n**Alerting Thresholds:**  \n- Trigger an alert if any namespace has the condition `Terminating` set to 1 for longer than a defined period (e.g., 5 minutes), indicating a namespace stuck in termination.  \n- Alert if the `NamespaceFailure` or any custom failure condition is 1, signaling a namespace-level problem.  \n- Alert if the `Active` condition is 0, meaning the namespace is not fully active and may not be ready to host workloads.\n\n**Impact of Values:**  \n- A value of 1 for `Active` means the namespace is healthy and ready.  \n- A value of 1 for `Terminating` or failure conditions indicates potential issues that can prevent deployments, cause resource leaks, or affect application availability.  \n- Persistent non-zero failure conditions require investigation to avoid cascading failures in workloads.\n\n**Example Alert Rule:**  \n```yaml\nalert: NamespaceStuckTerminating  \nexpr: kube_namespace_status_condition{condition=\"Terminating\"} == 1  \n  and on(namespace) (time() - kube_namespace_status_condition{condition=\"Terminating\"}[5m]) > 300  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"Namespace {{ $labels.namespace }} is stuck terminating\"  \n  description: \"The namespace {{ $labels.namespace }} has been in Terminating state for over 5 minutes.\"  \n```\n\n**Example Dashboard Usage:**  \nVisualize the `kube_namespace_status_condition` metric grouped by namespace and condition to quickly identify namespaces that are not Active or are stuck Terminating. Use color coding to highlight namespaces with failure conditions, enabling rapid operational response.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_networkpolicy_created",
      "namespace": "*",
      "networkpolicy": "*"
    },
    "description": "The **kube_networkpolicy_created** metric records the Unix timestamp when each Kubernetes NetworkPolicy resource was created, labeled by namespace and networkpolicy name. In Prometheus, this metric helps SREs track the age and changes of network policies over time, enabling visibility into network security posture and configuration drift.\n\n**Purpose:**  \n- Monitor the creation time of network policies to detect recent additions or updates.  \n- Identify stale or potentially outdated policies that may pose security risks if not reviewed or rotated regularly.  \n- Correlate network policy changes with deployment events or security incidents.\n\n**Alerting Guidance:**  \n- Trigger an alert if a new network policy is created unexpectedly outside of planned deployment windows, indicating potential unauthorized changes.  \n- Alert if no network policy has been created or updated for an extended period (e.g., 90 days), suggesting possible neglect or outdated security controls.\n\n**Impact of Values:**  \n- **High values (recent timestamps):** Indicate newly created or updated network policies, which may require validation or audit.  \n- **Low values (old timestamps):** Suggest policies that have not changed for a long time, potentially increasing risk if cluster requirements have evolved.\n\n**Example Alert Rule:**  \n```yaml\nalert: UnexpectedNetworkPolicyCreation  \nexpr: time() - kube_networkpolicy_created{namespace=\"prod\"} < 3600  \nfor: 10m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"New NetworkPolicy created in the last hour in production namespace\"  \n  description: \"A network policy '{{ $labels.networkpolicy }}' was created less than an hour ago in namespace '{{ $labels.namespace }}'. Verify this change is authorized.\"  \n```\n\n**Example Dashboard Usage:**  \n- Display the age of each network policy by calculating `time() - kube_networkpolicy_created` to highlight recently created or stale policies.  \n- Use heatmaps or tables grouped by namespace to visualize network policy creation trends and identify anomalies.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_networkpolicy_annotations"
    },
    "description": "The **kube_networkpolicy_annotations** metric counts the annotations applied to Kubernetes NetworkPolicy objects, represented as Prometheus labels. It provides insight into the metadata associated with each network policy, helping to track configuration details. This metric is useful for monitoring changes or inconsistencies in network policy annotations that may affect policy enforcement and pod communication. It is a dimensionless count reflecting the presence of specific annotation keys and values.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_networkpolicy_labels"
    },
    "description": "The **kube_networkpolicy_labels** metric exposes the set of labels assigned to each Kubernetes NetworkPolicy as Prometheus labels. Its primary purpose is to provide visibility into the metadata of network policies applied within the cluster, enabling correlation of network policy configurations with other metrics for monitoring and troubleshooting.\n\n**Purpose in Prometheus:**  \nThis metric does not represent a numeric value but rather label metadata, allowing SREs to track which network policies exist, their labels, and how they relate to pods and namespaces. It is useful for detecting configuration changes, ensuring compliance with labeling standards, and correlating network policies with network traffic or security incidents.\n\n**Thresholds and Alerting:**  \nSince this metric is label-based and not numeric, alerting should focus on changes or anomalies in the presence or absence of expected labels rather than threshold values. For example:  \n- Alert if a critical network policy label (e.g., `security=restricted`) is missing from any namespace or pod where it is required.  \n- Alert on unexpected addition or removal of network policy labels that could indicate misconfiguration or drift.\n\n**Impact of Changes:**  \n- **Missing or incorrect labels:** May indicate misconfigured network policies, potentially leading to unintended network access or blocked traffic, impacting application security and connectivity.  \n- **Unexpected label changes:** Could signal unauthorized modifications or deployment issues, increasing security risk or causing service disruptions.\n\n**Example Usage:**  \n- **Dashboard:** Use `kube_networkpolicy_labels` to build a table or heatmap showing network policies by namespace and their labels, highlighting missing or inconsistent labels across environments.  \n- **Alert Rule Example:**  \n```yaml\nalert: MissingCriticalNetworkPolicyLabel  \nexpr: count by(namespace) (kube_networkpolicy_labels{label_security!=\"restricted\"}) > 0  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"Namespace {{ $labels.namespace }} missing critical 'security=restricted' network policy label\"  \n  description: \"One or more network policies in namespace {{ $labels.namespace }} lack the required 'security=restricted' label, which may expose pods to unintended network access.\"  \n```\n\nThis approach helps SREs maintain network policy compliance, detect configuration drift, and correlate network policy metadata with cluster behavior for improved security and reliability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_networkpolicy_spec_ingress_rules",
      "namespace": "*",
      "networkpolicy": "*"
    },
    "description": "This metric measures the number of ingress rules specified in a Kubernetes network policy. Ingress rules define which incoming traffic is allowed to enter a pod or a namespace. A higher value indicates that the network policy has more restrictive access controls, potentially limiting the flow of incoming traffic. This metric can be used to monitor and troubleshoot issues related to network connectivity and security within a Kubernetes cluster. It may also serve as an indicator for potential misconfigurations or overly permissive policies. In monitoring and alerting, this metric could trigger alerts when the number of ingress rules exceeds a certain threshold, indicating a possible security risk or performance bottleneck.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_networkpolicy_spec_egress_rules",
      "namespace": "*",
      "networkpolicy": "*"
    },
    "description": "This metric measures the number of egress rules defined in a Kubernetes network policy. Egress rules specify which traffic is allowed to exit from a pod or a namespace. An increase in this metric may indicate changes to network policies, potentially impacting application connectivity or security configurations. It can be used in monitoring and alerting to detect unauthorized modifications to network policies, ensure compliance with security standards, or troubleshoot issues related to egress traffic restrictions.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_annotations"
    },
    "description": "The **kube_node_annotations** metric exposes Kubernetes annotations applied to each node as Prometheus labels, enabling SREs to monitor and analyze node-specific metadata directly within Prometheus. This metric is primarily used to verify the presence, absence, or changes of critical annotations that may affect node behavior, configuration, or compliance requirements. Since annotations are key-value pairs without inherent numeric values, alert thresholds are typically based on the presence or absence of specific annotation keys or changes in their values rather than numeric thresholds. For example, an alert can be triggered if a required annotation (e.g., `node.kubernetes.io/maintenance`) is missing or altered unexpectedly, indicating potential misconfiguration or drift. High cardinality (many unique annotations) may impact Prometheus performance and should be monitored. A practical use case includes creating dashboards that display nodes grouped by annotation values (e.g., environment or role) or alerting when critical annotations are removed or modified. Example alert rule snippet:  \n```\nalert: MissingNodeAnnotation  \nexpr: absent(kube_node_annotations{annotation_key=\"node.kubernetes.io/maintenance\"})  \nfor: 5m  \nlabels: { severity=\"warning\" }  \nannotations: { summary=\"Node missing maintenance annotation\", description=\"Node {{ $labels.node }} lacks the required maintenance annotation.\" }  \n```  \nThis enables proactive detection of annotation-related issues affecting node management and cluster operations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_created",
      "node": "*"
    },
    "description": "The **kube_node_created** metric records the Unix timestamp of when each Kubernetes node was initially created. In Prometheus, this metric helps SREs track node lifecycle events by providing a precise creation time per node, enabling detection of new or unexpectedly recreated nodes. \n\n**Purpose:**  \n- Monitor the age of nodes to identify newly added nodes or nodes that have been recently recreated (e.g., due to failures or upgrades).  \n- Detect anomalies such as nodes being created outside of scheduled maintenance windows or unusually frequent node churn, which may indicate cluster instability.\n\n**Thresholds and Alerting:**  \n- Alert when a node\u2019s creation timestamp is within a recent time window (e.g., less than 1 hour old), indicating a new node has appeared unexpectedly.  \n- Alert if multiple nodes are created within a short period, exceeding normal scaling or maintenance patterns.  \n- Example threshold: `time() - kube_node_created{node=\"<node_name>\"} < 3600` triggers if the node was created less than 1 hour ago.\n\n**Impact of Values:**  \n- **Low values (recent timestamps):** Indicate new or recently recreated nodes; frequent occurrences may signal instability or automated scaling events.  \n- **High values (older timestamps):** Indicate stable, long-running nodes; a sudden drop to low values for many nodes may suggest mass node replacements or cluster disruptions.\n\n**Example Usage:**  \n- **Alert Rule:**  \n  ```yaml\n  alert: UnexpectedNewNode\n  expr: time() - kube_node_created < 3600\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"New Kubernetes node detected\"\n    description: \"Node {{ $labels.node }} was created less than 1 hour ago, which may indicate unexpected scaling or node replacement.\"\n  ```\n- **Dashboard Panel:**  \n  Display node age in hours:  \n  ```promql\n  (time() - kube_node_created{node=~\".+\"}) / 3600\n  ```  \n  This visualizes node age, helping identify recently created nodes and monitor cluster stability over time.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_deletion_timestamp"
    },
    "description": "The kube_node_deletion_timestamp metric measures the Unix timestamp when a Kubernetes node is scheduled for deletion. This metric indicates that a node has been marked for removal from the cluster and will be deleted at some point in the future. The timestamp represents the moment when the node's deletion was initiated, not necessarily when it will actually be removed from the cluster.\n\nThis metric can be used to monitor and alert on node deletions, ensuring that critical workloads are not impacted by unexpected node removals. It may also indicate issues with node management or scaling policies within the cluster.\n\nPotential use cases include:\n- Alerting on pending node deletions to prevent data loss or service disruptions.\n- Monitoring node deletion rates to identify potential scaling or resource allocation issues.\n- Correlating node deletions with other metrics, such as pod evictions or network errors, to diagnose underlying causes.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_info",
      "node": "*",
      "kernel_version": "*",
      "os_image": "*",
      "container_runtime_version": "*",
      "kubelet_version": "*",
      "kubeproxy_version": "*",
      "provider_id": "*",
      "pod_cidr": "*",
      "system_uuid": "*",
      "internal_ip": "*"
    },
    "description": "The **kube_node_info** metric in Prometheus provides static metadata about each Kubernetes node, including node name, kernel version, OS image, container runtime version, kubelet and kube-proxy versions, provider ID, pod CIDR, system UUID, and internal IP address. This metric is primarily used to identify and inventory nodes, track version consistency across the cluster, and correlate node-specific events or issues.\n\n**Purpose:**  \n- Enables SREs to verify node configurations and software versions for compliance and troubleshooting.  \n- Helps detect nodes running outdated or mismatched versions that may require upgrades or maintenance.  \n- Supports correlation of node metadata with dynamic metrics (e.g., node health, resource usage) for comprehensive monitoring.\n\n**Alerting Guidance:**  \nSince **kube_node_info** is a static metadata metric, it does not have numeric values to threshold directly. Instead, alerts should be based on the absence or unexpected changes in this metric, such as:  \n- Alert if a node\u2019s kubelet_version or container_runtime_version differs from the expected cluster baseline, indicating a potential drift or failed upgrade.  \n- Alert if a node\u2019s **kube_node_info** metric disappears, signaling node removal or failure.  \n- Alert if pod_cidr or provider_id changes unexpectedly, which may indicate misconfiguration.\n\n**Impact:**  \n- Missing or inconsistent node metadata can lead to blind spots in monitoring and complicate incident response.  \n- Version mismatches may cause compatibility issues, degraded performance, or security vulnerabilities.  \n- Incorrect pod CIDRs or provider IDs can disrupt networking and scheduling.\n\n**Example Usage:**  \n- **Dashboard:** Display a table listing all nodes with their kernel_version, kubelet_version, and container_runtime_version to quickly identify outliers or nodes pending upgrades.  \n- **Alert Rule Example:**  \n```yaml\nalert: NodeVersionMismatch  \nexpr: kube_node_info{job=\"kube-state-metrics\"} and on(node) (kube_node_info{job=\"kube-state-metrics\", kubelet_version!=\"v1.26.0\"})  \nfor: 10m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"Node {{ $labels.node }} is running kubelet version {{ $labels.kubelet_version }}, expected v1.26.0\"  \n  description: \"Node {{ $labels.node }} has a kubelet version different from the cluster baseline, which may cause compatibility issues.\"  \n```\n\nUse **kube_node_info** as a foundational metric to maintain cluster node consistency and to enrich dynamic monitoring data with node-specific context.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_labels"
    },
    "description": "The kube_node_labels metric represents a mapping of Kubernetes node labels to Prometheus labels. This metric is generated by scraping the Kubernetes API and converting the node's labels into a format that can be understood by Prometheus. The metric provides a way to monitor and track changes to node labels, which can be useful for identifying issues related to node configuration or resource allocation. Potential implications include using this metric in alerting rules to notify when a specific label is added or removed from a node, or in dashboards to visualize the current state of node labels across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_role"
    },
    "description": "The kube_node_role metric measures the role of a cluster node in a Kubernetes environment. This metric indicates whether a node is functioning as a master (control plane), worker (compute), or both. The possible values for this metric are 'master', 'worker', and 'both'. In monitoring and alerting, this metric can be used to ensure that the correct nodes are assigned to their respective roles, preventing potential issues such as misconfigured control planes or underutilized compute resources. It can also be used to detect node role changes, which may indicate a scaling event, maintenance activity, or other operational changes.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_spec_taint"
    },
    "description": "The kube_node_spec_taint metric measures the taints applied to a Kubernetes cluster node's specification. Taints are key-value pairs that can be used to mark nodes for specific purposes or to indicate their readiness for scheduling pods. This metric provides insight into the configuration and availability of cluster resources, which is crucial for ensuring efficient resource utilization and optimal workload placement.\n\nPotential implications or usage in monitoring or alerting include:\n- Detecting nodes with critical taints that may impact pod scheduling or node availability.\n- Identifying nodes with specific taints that require special handling or maintenance.\n- Monitoring the addition or removal of taints to track changes in cluster configuration and resource allocation.\n\nThis metric can be used in conjunction with other metrics, such as kube_node_status_condition, to gain a comprehensive understanding of node health and availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_spec_unschedulable",
      "node": "*"
    },
    "description": "This metric measures whether a Kubernetes node is schedulable for new pod deployments. A value of true indicates that the node cannot schedule new pods due to various reasons such as resource constraints, network issues, or other unschedulable conditions. This metric can be used in monitoring and alerting to identify nodes that are not available for new workload deployment, potentially leading to capacity planning and resource allocation decisions. It may also indicate underlying infrastructure problems that need attention.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_status_allocatable",
      "node": "*",
      "resource": "*",
      "unit": "*"
    },
    "description": "The kube_node_status_allocatable metric measures the allocatable resources for a Kubernetes node, which are the resources that are available for scheduling pods. This includes CPU and memory, as well as other resources such as ephemeral storage and huge pages. The metric provides a snapshot of the current allocatable resources on each node, allowing operators to monitor and troubleshoot resource utilization issues in their cluster. Potential implications or usage in monitoring or alerting include: detecting nodes with low allocatable resources, identifying resource bottlenecks that may impact pod scheduling, and triggering alerts when allocatable resources fall below a certain threshold. This metric is particularly useful for ensuring that nodes have sufficient resources to handle incoming workloads and preventing resource exhaustion.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_status_capacity",
      "node": "*",
      "resource": "*",
      "unit": "*"
    },
    "description": "The **kube_node_status_capacity** metric reports the total allocatable capacity of a specific resource (e.g., CPU cores, memory bytes, ephemeral storage bytes) on a given Kubernetes node. It reflects the maximum amount of that resource available for pod scheduling, expressed in the resource\u2019s native unit (e.g., cores for CPU, bytes for memory and storage). This metric helps cluster administrators monitor node resource availability and make informed decisions about pod placement and node provisioning.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_status_condition",
      "node": "*",
      "condition": "*",
      "status": "*"
    },
    "description": "The kube_node_status_condition metric measures the status conditions of a Kubernetes cluster node. This metric provides information about the health and operational state of each node in the cluster. The status conditions include details such as whether the node is ready, whether it has sufficient resources (e.g., CPU, memory), and whether it is experiencing any issues or errors. This metric can be used to monitor the overall health of the cluster, identify potential problems with individual nodes, and trigger alerts when a node's status changes. For example, if a node becomes not-ready due to resource constraints, this metric would indicate that the node is no longer available for scheduling pods, potentially impacting application availability. By monitoring this metric, operators can proactively address issues before they impact application performance or availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_node_status_addresses",
      "node": "*",
      "type": "*",
      "address": "*"
    },
    "description": "The kube_node_status_addresses metric measures the node address information for each Kubernetes node in the cluster. This includes the IP addresses and hostnames associated with each node. The metric provides a snapshot of the current network configuration for each node, which can be useful for monitoring and troubleshooting purposes.\n\nPotential implications or usage in monitoring or alerting include:\n- Identifying nodes that are not reachable due to incorrect or outdated IP address information.\n- Detecting changes in node addresses, such as when a node is reconfigured with a new hostname or IP address.\n- Correlating node address information with other metrics, such as network latency or packet loss, to diagnose issues related to node connectivity.\n\nThis metric can be used in conjunction with other Kubernetes metrics and tools to gain a more comprehensive understanding of the cluster's health and performance.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_labels"
    },
    "description": "The **kube_persistentvolumeclaim_labels** metric exposes the Kubernetes labels assigned to Persistent Volume Claims (PVCs) as Prometheus labels. It does not measure a numeric value but provides metadata in the form of key-value pairs for each PVC. This enables filtering, grouping, and alerting based on PVC labels within Prometheus. The metric helps track PVC organization and detect label changes or misconfigurations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_annotations"
    },
    "description": "The **kube_persistentvolumeclaim_annotations** metric exposes the annotations of Kubernetes Persistent Volume Claims (PVCs) as Prometheus labels. It does not measure a numeric value but represents the presence and content of annotation key-value pairs attached to each PVC. This enables querying and filtering PVCs based on their annotations for monitoring or troubleshooting purposes. The metric helps track metadata changes and correlate PVC annotations with other cluster metrics.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_info",
      "namespace": "*",
      "persistentvolumeclaim": "*",
      "storageclass": "*",
      "volumename": "*",
      "volumemode": "*"
    },
    "description": "The kube_persistentvolumeclaim_info metric measures information about persistent volume claims (PVCs) in a Kubernetes cluster. This includes details such as PVC name, namespace, status, and phase. The metric provides insights into the lifecycle of PVCs, including their creation, binding to a Persistent Volume (PV), and deletion. It can be used to monitor PVC usage, identify potential issues with PV availability or PVC configuration, and troubleshoot storage-related problems in the cluster. This information is essential for ensuring that applications have access to the required storage resources and for maintaining overall cluster health.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_status_phase",
      "namespace": "*",
      "persistentvolumeclaim": "*",
      "phase": "*"
    },
    "description": "The **kube_persistentvolumeclaim_status_phase** metric in Prometheus reports the current lifecycle phase of each Persistent Volume Claim (PVC) in a Kubernetes cluster, labeled by namespace, PVC name, and phase (e.g., 'Pending', 'Bound', 'Failed', 'Unknown'). Each PVC will have a value of 1 for its current phase and 0 for others, enabling precise phase tracking.\n\n**Purpose:**  \nThis metric helps SREs monitor PVC health and provisioning status. For example, PVCs stuck in the 'Pending' phase may indicate storage provisioning issues, while 'Failed' signals errors requiring immediate investigation. Tracking phase transitions over time aids capacity planning and troubleshooting.\n\n**Alerting thresholds:**  \n- Alert if any PVC remains in the 'Pending' phase for more than 10 minutes, indicating provisioning delays or failures.  \n- Alert immediately if any PVC enters the 'Failed' phase, signaling critical storage errors.  \n- Optionally, alert if a high number of PVCs are in 'Unknown' phase, which may indicate cluster or storage system instability.\n\n**Impact of values:**  \n- A high count of PVCs in 'Bound' is normal and indicates healthy volume attachment.  \n- Persistent or increasing PVCs in 'Pending' or 'Failed' phases can cause application downtime or degraded performance due to unavailable storage.  \n- Sudden spikes in 'Unknown' phase PVCs may reflect systemic issues requiring urgent attention.\n\n**Example alert rule:**  \n```yaml\nalert: PersistentVolumeClaimStuckPending\nexpr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\n  and on(namespace, persistentvolumeclaim)\n  kube_persistentvolumeclaim_status_phase{phase=\"Pending\"}[10m] == 1\nfor: 10m\nlabels:\n  severity: warning\nannotations:\n  summary: \"PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} stuck in Pending phase\"\n  description: \"PVC has been in Pending phase for over 10 minutes, indicating possible storage provisioning issues.\"\n```\n\n**Example dashboard usage:**  \nVisualize the count of PVCs by phase over time using a Prometheus query like:  \n```\nsum by (phase) (kube_persistentvolumeclaim_status_phase)\n```\nThis helps identify trends such as increasing 'Pending' PVCs or sudden 'Failed' states, enabling proactive capacity and incident management.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_resource_requests_storage_bytes",
      "namespace": "*",
      "persistentvolumeclaim": "*"
    },
    "description": "This metric measures the total storage capacity requested by a Persistent Volume Claim (PVC) in bytes. It represents the amount of storage space that the PVC is requesting from the Kubernetes cluster. This metric can be used to monitor and alert on potential issues related to storage capacity, such as running out of disk space or exceeding storage quotas. For example, it could trigger an alert when a PVC requests more storage than its allocated quota, indicating a potential misconfiguration or resource exhaustion issue.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_access_mode",
      "namespace": "*",
      "persistentvolumeclaim": "*",
      "access_mode": "*"
    },
    "description": "The kube_persistentvolumeclaim_access_mode metric measures the access mode(s) specified by a Persistent Volume Claim (PVC) in a Kubernetes cluster. The access mode determines how multiple pods can access a shared persistent volume. This metric can be used to monitor and troubleshoot issues related to PVCs, such as incorrect or inconsistent access modes that may lead to data corruption or unauthorized access. Potential implications include identifying misconfigured PVCs, detecting potential security risks due to overly permissive access modes, or optimizing storage resource utilization by ensuring the correct access mode is specified for each PVC.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_status_condition"
    },
    "description": "The kube_persistentvolumeclaim_status_condition metric measures the status of different conditions associated with a Persistent Volume Claim (PVC) in a Kubernetes cluster. This includes conditions such as 'Bound', 'Synced', and 'Unhealthy'. The metric provides information about the current state of each condition, allowing operators to monitor PVC health and identify potential issues. Potential implications or usage in monitoring or alerting include setting up alerts for when a PVC becomes unbound or unhealthy, or tracking the synchronization status of PVCs to ensure data consistency. This metric can be used to troubleshoot PVC-related problems, such as storage capacity issues or configuration errors.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_created",
      "namespace": "*",
      "persistentvolumeclaim": "*"
    },
    "description": "The **kube_persistentvolumeclaim_created** metric records the Unix timestamp when a Persistent Volume Claim (PVC) was created in a Kubernetes cluster. This metric helps SREs track the age and creation timing of PVCs, enabling monitoring of storage resource lifecycle and trends in PVC provisioning.\n\n**Purpose:**  \n- Understand when each PVC was created to correlate storage usage patterns with application behavior.  \n- Detect sudden spikes in PVC creation that may indicate abnormal workload changes or automation issues.  \n- Identify stale or orphaned PVCs by comparing creation times against current time.\n\n**Thresholds and Alerting:**  \n- Alert if a PVC is created unexpectedly in a critical namespace or outside of deployment windows (e.g., creation timestamp within the last 5 minutes when no deployments are expected).  \n- Alert if no new PVCs are created over a prolonged period in environments where PVC creation is expected regularly, indicating potential provisioning failures.  \n- Alert on rapid bursts of PVC creation (e.g., more than N PVCs created within M minutes), which may signal runaway jobs or misconfigurations.\n\n**Impact of Values:**  \n- **High values (recent timestamps):** Indicate newly created PVCs; a sudden increase may reflect scaling events or automation errors.  \n- **Low values (older timestamps):** Indicate older PVCs; persistent old PVCs might suggest resource leaks or unused storage consuming capacity.\n\n**Example Usage:**  \n- **Dashboard:** Display PVC age by subtracting `kube_persistentvolumeclaim_created` from the current time (`time() - kube_persistentvolumeclaim_created`) to visualize PVC lifecycle and identify stale claims.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: UnexpectedPVCsCreated\n  expr: count_over_time(kube_persistentvolumeclaim_created[5m]) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High number of PVCs created in the last 5 minutes\"\n    description: \"More than 10 PVCs were created in the last 5 minutes, which may indicate abnormal workload or automation issues.\"\n  ```\n\nThis metric is essential for proactive storage management and ensuring PVC lifecycle aligns with application and cluster expectations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolumeclaim_deletion_timestamp"
    },
    "description": "This metric measures the Unix timestamp when a Persistent Volume Claim (PVC) is deleted from a Kubernetes cluster. It indicates the point in time when the PVC was removed from the system. The metric can be used to monitor and track the deletion of PVCs, which can be useful for identifying potential issues with storage resources or pod deployments. For example, if a PVC is deleted unexpectedly, this metric can help identify the timestamp when the deletion occurred, allowing operators to investigate further. Additionally, this metric can be used in alerting rules to notify administrators when PVC deletions occur outside of expected maintenance windows.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_claim_ref",
      "persistentvolume": "*",
      "name": "*",
      "claim_namespace": "*"
    },
    "description": "The kube_persistentvolume_claim_ref metric measures information about the Persistent Volume Claim (PVC) references in a Kubernetes cluster. It provides details on the PVCs that are being referenced by various resources such as Deployments, StatefulSets, and DaemonSets. This metric can be used to monitor and troubleshoot issues related to PVCs, such as misconfigured PVCs, PVCs not being deleted after their corresponding resources have been deleted, or PVCs being referenced by multiple resources. Potential implications of this metric include identifying resource leaks, detecting configuration errors, and optimizing storage usage in the cluster. This information can be used in monitoring and alerting to ensure that PVC-related issues are promptly identified and addressed, thereby maintaining the health and stability of the Kubernetes cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_annotations"
    },
    "description": "The **kube_persistentvolume_annotations** metric exposes Kubernetes Persistent Volume (PV) annotations as Prometheus labels, enabling SREs to monitor and analyze metadata associated with PVs directly within Prometheus. This metric\u2019s primary purpose is to provide visibility into PV configurations, such as storage class assignments or custom settings, by translating annotation key-value pairs into queryable labels.\n\n**Alerting guidance:** Since this metric reflects metadata rather than numeric values, alerts should focus on the presence, absence, or unexpected values of specific annotation labels. For example, trigger an alert if a PV lacks a required annotation (e.g., `storageclass`), or if an annotation value changes unexpectedly, indicating potential misconfiguration or drift. Thresholds are therefore defined by annotation existence or matching expected patterns rather than numeric thresholds.\n\n**Impact of anomalies:**  \n- **Missing or incorrect annotations** can lead to PVs being provisioned with wrong storage classes, causing performance degradation or application failures.  \n- **Unexpected annotation changes** may signal unauthorized configuration changes or automation errors, impacting storage reliability or compliance.\n\n**Example usage:**  \n- **Dashboard:** Display a table or heatmap showing PVs grouped by their `storageclass` annotation to track adoption and distribution of storage classes across the cluster.  \n- **Alert rule example:**  \n```yaml\nalert: MissingStorageClassAnnotationOnPV\nexpr: kube_persistentvolume_annotations{annotation_storageclass=\"\"} == 1\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Persistent Volume missing storageclass annotation\"\n  description: \"PV {{ $labels.persistentvolume }} does not have a storageclass annotation, which may cause provisioning issues.\"\n```\n\nThis approach helps SREs proactively detect and respond to PV configuration issues by monitoring annotation presence and correctness through Prometheus queries and alerts.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_labels"
    },
    "description": "The **kube_persistentvolume_labels** metric exposes the Kubernetes labels assigned to each PersistentVolume (PV) in the cluster as Prometheus labels. It is a gauge metric with a constant value of 1 for each unique combination of PV and its associated labels, serving as a metadata indicator rather than a numeric measurement. The data is sourced directly from the Kubernetes API server, reflecting the current label set on each PersistentVolume object. This metric enables querying and filtering of PersistentVolumes based on their labels within Prometheus, facilitating correlation with other metrics or resource inventories. An unusual absence of expected labels or the sudden disappearance of this metric for existing PersistentVolumes may indicate issues such as misconfiguration, API server connectivity problems, or PV deletion, which could warrant alerting to ensure accurate resource tracking and cluster state awareness.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_status_phase",
      "persistentvolume": "*",
      "phase": "*"
    },
    "description": "The **kube_persistentvolume_status_phase** metric reports the current lifecycle phase of each Persistent Volume (PV) in the Kubernetes cluster, labeled by PV name and phase (Available, Bound, Released, Failed, or Unavailable). This metric enables SREs to track PV health and usage patterns by showing how many PVs are in each phase at any time.\n\n**Purpose:**  \nMonitor PV states to detect storage resource issues early. For example, a rising count of PVs in the \"Failed\" or \"Released\" phases may indicate storage backend failures or orphaned volumes, while a low number of \"Available\" PVs could signal capacity shortages.\n\n**Alert Thresholds:**  \n- Trigger an alert if the number of PVs in the **Failed** phase exceeds 0, indicating storage failures requiring immediate investigation.  \n- Alert if the count of **Available** PVs drops below a critical threshold (e.g., less than 10% of total PVs), suggesting insufficient free storage capacity.  \n- Optionally, alert if PVs remain in the **Released** phase for an extended period, as this may indicate cleanup issues.\n\n**Impact of Values:**  \n- **High \"Failed\" PV count:** Potential data loss or degraded application performance due to inaccessible storage.  \n- **Low \"Available\" PV count:** Risk of PVC provisioning failures and application downtime due to lack of free volumes.  \n- **High \"Released\" PV count:** Possible resource leaks and wasted storage capacity.\n\n**Example Usage:**  \n- **Dashboard:** Display a stacked bar chart showing the count of PVs per phase over time to visualize storage health trends.  \n- **Alert Rule (PromQL):**  \n  ```\n  sum by(phase) (kube_persistentvolume_status_phase == 1 and phase=\"Failed\") > 0\n  ```  \n  This alerts immediately if any PV enters the Failed phase.  \n- Another alert to warn on low availability:  \n  ```\n  sum(kube_persistentvolume_status_phase{phase=\"Available\"}) / sum(kube_persistentvolume_status_phase) < 0.1\n  ```  \n  This triggers if less than 10% of PVs are available.\n\nBy integrating this metric into monitoring and alerting, SREs can proactively manage storage health, prevent capacity exhaustion, and reduce application disruptions caused by persistent volume issues.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_info",
      "persistentvolume": "*",
      "storageclass": "*",
      "gce_persistent_disk_name": "*",
      "ebs_volume_id": "*",
      "azure_disk_name": "*",
      "fc_wwids": "*",
      "fc_lun": "*",
      "fc_target_wwns": "*",
      "iscsi_target_portal": "*",
      "iscsi_iqn": "*",
      "iscsi_lun": "*",
      "iscsi_initiator_name": "*",
      "nfs_server": "*",
      "nfs_path": "*",
      "csi_driver": "*",
      "csi_volume_handle": "*",
      "local_path": "*",
      "local_fs": "*",
      "host_path": "*",
      "host_path_type": "*"
    },
    "description": "The **kube_persistentvolume_info** metric in Prometheus provides detailed metadata about each PersistentVolume (PV) in your Kubernetes cluster, including identifiers such as volume name, storage class, and backend-specific details (e.g., GCE disk name, EBS volume ID, Azure disk name). This metric does not measure numeric values but exposes labels that describe the configuration and state of PVs, enabling correlation with other metrics that track usage, capacity, and health.\n\n**Purpose:**  \nUse this metric to inventory and verify persistent volume configurations, identify which storage backends are in use, and correlate PV metadata with operational metrics (e.g., capacity usage, binding status). It is foundational for building dashboards that show PV distribution by storage class or backend type and for alerting on PV lifecycle events when combined with other metrics.\n\n**Alerting Guidance:**  \nSince this metric is informational (non-numeric), alerts should be based on related metrics such as `kube_persistentvolume_status_phase` or capacity metrics. For example, alert if the number of PVs in the `Failed` or `Released` phase exceeds zero, or if PV capacity usage approaches limits. Use `kube_persistentvolume_info` labels to filter or group alerts by storage class or volume type.\n\n**Impact of Values:**  \n- Presence of PV info labels indicates the PV exists and is recognized by the cluster.  \n- Missing or unexpected labels may indicate misconfiguration or provisioning issues.  \n- Changes in labels (e.g., storage class or backend ID) can signal volume migration or reconfiguration.\n\n**Example Usage:**  \nTo create a dashboard panel showing the count of PVs by storage class:  \n```\ncount(kube_persistentvolume_info) by (storageclass)\n```\n\nTo alert on PVs stuck in a non-`Bound` phase for over 10 minutes (using related status metric):  \n```\nalert: PersistentVolumeNotBound\nexpr: kube_persistentvolume_status_phase{phase!=\"Bound\"} > 0\nfor: 10m\nlabels:\n  severity: warning\nannotations:\n  summary: \"PersistentVolume(s) not bound\"\n  description: \"One or more PersistentVolumes are not in Bound phase for over 10 minutes. Check storage provisioning.\"\n```\n\nIn summary, **kube_persistentvolume_info** is essential for identifying and grouping PVs by their metadata, enabling targeted monitoring and alerting when combined with status and usage metrics.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_capacity_bytes",
      "persistentvolume": "*"
    },
    "description": "This metric, **kube_persistentvolume_capacity_bytes**, reports the total storage capacity (in bytes) allocated to each Persistent Volume (PV) in the Kubernetes cluster. It helps SREs understand the maximum storage available per PV, which is critical for capacity planning and ensuring that workloads have sufficient disk space.\n\n**Purpose:**  \n- Monitor the allocated size of each PV to detect unexpected changes or misconfigurations.  \n- Track storage capacity trends over time to anticipate scaling needs or optimize resource usage.\n\n**Alerting Thresholds:**  \n- While this metric itself reflects total capacity (a static or slowly changing value), it is typically combined with usage metrics (e.g., volume usage) to trigger alerts.  \n- For example, alert if the **used storage** approaches or exceeds 90-95% of the capacity reported by this metric, indicating risk of running out of space.  \n- Sudden decreases in capacity may indicate PV resizing or deletion events that require investigation.\n\n**Impact of Values:**  \n- **High values:** Indicate large PVs, which may be appropriate for data-intensive workloads but could lead to inefficient resource allocation if underutilized.  \n- **Low values:** May signal insufficient storage for workloads, potentially causing application failures or degraded performance.\n\n**Example Usage:**  \n- **Dashboard:** Display a panel showing PV capacity alongside usage metrics (e.g., `kubelet_volume_stats_used_bytes`) to visualize storage utilization percentage per PV.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: PersistentVolumeStorageAlmostFull\n  expr: (kubelet_volume_stats_used_bytes{persistentvolume=~\".+\"} / kube_persistentvolume_capacity_bytes{persistentvolume=~\".+\"}) > 0.9\n  for: 10m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Persistent Volume {{ $labels.persistentvolume }} is over 90% full\"\n    description: \"The persistent volume {{ $labels.persistentvolume }} has used more than 90% of its capacity.\"\n  ```\n\nThis metric is foundational for understanding PV storage limits and should be used in conjunction with usage metrics to maintain healthy storage operations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_created",
      "persistentvolume": "*"
    },
    "description": "The **kube_persistentvolume_created** metric records the Unix timestamp when each Persistent Volume (PV) in the Kubernetes cluster was created. This metric helps SREs track the lifecycle and age of PVs, enabling visibility into storage resource provisioning and potential resource leaks.\n\n**Purpose:**  \n- Monitor when PVs are created to understand storage usage trends.  \n- Identify stale or orphaned PVs by comparing creation times with current usage.  \n- Correlate PV creation events with application deployments or upgrades for troubleshooting.\n\n**Alerting guidance:**  \n- Trigger an alert if a PV is created unexpectedly outside of normal provisioning windows, indicating potential unauthorized or automated resource creation.  \n- Alert if PVs remain unused for an extended period (e.g., PVs older than 30 days without any bound pods), suggesting resource waste or cleanup needs.\n\n**Impact of values:**  \n- **High timestamp values (recent creation):** Indicate new PV provisioning activity; a sudden spike may reflect scaling events or misconfigurations.  \n- **Low timestamp values (old PVs):** Suggest long-lived volumes that may be unused or orphaned, potentially consuming storage unnecessarily.\n\n**Example usage:**  \n- **Dashboard:** Display PV age by subtracting `kube_persistentvolume_created` from the current time (`time() - kube_persistentvolume_created`) to visualize volume lifetimes and identify stale volumes.  \n- **Alert rule example:**  \n  ```yaml\n  alert: OrphanedPersistentVolume\n  expr: (time() - kube_persistentvolume_created{persistentvolume=~\".*\"}) > 2592000 and ignoring(persistentvolume) kube_persistentvolume_status_phase{phase=\"Available\"} == 1\n  for: 1h\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Persistent Volume {{ $labels.persistentvolume }} is orphaned\"\n    description: \"Persistent Volume {{ $labels.persistentvolume }} has been in 'Available' state for over 30 days, indicating it may be unused and should be reviewed.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_deletion_timestamp"
    },
    "description": "This metric records the Unix timestamp (in seconds) when a Persistent Volume (PV) in a Kubernetes cluster is marked for deletion. It reflects the exact time the deletion process for the PV began. Monitoring this metric helps track PV deletions and ensures storage resources are properly managed. It can also alert administrators to unusual spikes in PV deletions that may indicate configuration or capacity issues.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_persistentvolume_volume_mode",
      "persistentvolume": "*",
      "volumemode": "*"
    },
    "description": "The **kube_persistentvolume_volume_mode** metric exposes the volume mode of each PersistentVolume (PV) in the Kubernetes cluster, labeled by PV name and volume mode type (\"Filesystem\" or \"Block\"). This metric helps SREs verify that PVs are provisioned with the correct volume mode expected by consuming Pods, preventing storage interface mismatches that can cause Pod failures or data corruption.\n\n**Purpose:**  \nMonitor and validate the volume mode configuration of PVs to ensure compatibility with Pod volume mounts. For example, Pods requiring filesystem access must use PVs with volume mode \"Filesystem,\" while certain workloads needing raw block devices require \"Block\" mode.\n\n**Alerting Thresholds:**  \nTrigger alerts when a PV\u2019s volume mode does not match the expected mode for its consuming Pods. For instance, alert if a PV with volume mode \"Block\" is bound to a Pod expecting \"Filesystem,\" or if a PV\u2019s volume mode unexpectedly changes. Since this metric is categorical, alerts are typically based on presence or absence of specific label combinations rather than numeric thresholds.\n\n**Impact:**  \n- **Incorrect volume mode:** Pods may fail to mount volumes, leading to application downtime or degraded performance.  \n- **Misconfigured PVs:** Can cause data access errors or require manual intervention to fix volume mode mismatches.  \n- **Proactive monitoring:** Helps prevent deployment failures and ensures storage consistency across the cluster.\n\n**Example Usage:**  \n- **Dashboard:** Display counts of PVs by volume mode to track distribution and detect anomalies.  \n- **Alert Rule:**  \n```yaml\nalert: PersistentVolumeVolumeModeMismatch  \nexpr: kube_persistentvolume_volume_mode{volumemode=\"Block\"} and on(persistentvolume) kube_pod_spec_volumes_persistentvolumeclaim_info{persistentvolumeclaim=~\".*\"} unless on(persistentvolume) kube_persistentvolume_volume_mode{volumemode=\"Filesystem\"}  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"PersistentVolume {{ $labels.persistentvolume }} has volume mode Block but is used by a Pod expecting Filesystem\"  \n  description: \"PersistentVolume {{ $labels.persistentvolume }} is configured with volume mode Block, which may be incompatible with Pods expecting Filesystem volumes. Verify volume mode and Pod specs to prevent mount failures.\"  \n```\nThis ensures SREs can detect and resolve volume mode mismatches before they impact workloads.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_annotations"
    },
    "description": "The **kube_poddisruptionbudget_annotations** metric exposes Kubernetes PodDisruptionBudget (PDB) annotations as Prometheus labels, enabling monitoring of PDB configurations directly within Prometheus. This metric itself does not have numeric values but provides metadata about PDBs, such as minAvailable or maxUnavailable settings, which define how many pods can be disrupted simultaneously during voluntary disruptions (e.g., node maintenance or scaling).\n\n**Purpose:**  \nIt helps SREs correlate PDB configurations with pod availability and disruption events by enriching other PDB-related metrics with annotation details. This is essential for understanding the intent behind disruption budgets and verifying that PDBs are configured as expected.\n\n**Alerting Guidance:**  \nSince this metric is annotation metadata, alert thresholds should be based on related metrics like **kube_poddisruptionbudget_status_current_healthy** and **kube_poddisruptionbudget_status_desired_healthy**. For example, alert if the current healthy pods fall below the minAvailable threshold defined in the PDB annotations, indicating a potential violation of the disruption budget.\n\n**Impact of Values:**  \n- Presence of expected annotations (e.g., minAvailable, maxUnavailable) indicates PDBs are configured to protect pod availability.  \n- Missing or incorrect annotations may lead to unintentional pod disruptions, risking application downtime.  \n- Changes in annotation values can signal configuration drift or intentional updates that should be monitored.\n\n**Example Usage:**  \nTo create an alert when the number of healthy pods falls below the PDB\u2019s minAvailable annotation, you can join this metric with the current healthy pod count:\n\n```yaml\nalert: PodDisruptionBudgetViolated\nexpr: kube_poddisruptionbudget_status_current_healthy < on(poddisruptionbudget) kube_poddisruptionbudget_annotations{annotation_pdb_minAvailable=~\".+\"}\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"PodDisruptionBudget violation for {{ $labels.poddisruptionbudget }}\"\n  description: \"Current healthy pods ({{ $value }}) are below the minAvailable threshold defined in PDB annotations.\"\n```\n\nIn dashboards, use **kube_poddisruptionbudget_annotations** to display PDB configuration details alongside pod health metrics, helping quickly identify misconfigurations or risks related to pod disruptions.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_labels"
    },
    "description": "The **kube_poddisruptionbudget_labels** metric exposes the Kubernetes labels assigned to each Pod Disruption Budget (PDB) as Prometheus labels. It provides a label mapping without a numerical value, enabling identification and correlation of PDBs with their associated pods and services. This metric helps monitor the configuration and presence of PDBs to ensure application availability during pod disruptions. It is primarily used for tracking PDB metadata rather than measuring quantitative data.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_created",
      "namespace": "*",
      "poddisruptionbudget": "*"
    },
    "description": "This metric records the Unix timestamp when a specific Pod Disruption Budget (PDB) object was created in a Kubernetes cluster. The timestamp is measured in seconds since the Unix epoch (January 1, 1970). A PDB defines the minimum number of pods that must remain available during voluntary disruptions to maintain application availability. Monitoring this metric helps track the creation time of PDBs for auditing and change detection purposes.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_status_current_healthy",
      "namespace": "*",
      "poddisruptionbudget": "*"
    },
    "description": "This metric measures the current number of healthy pods in a Kubernetes cluster, as reported by the PodDisruptionBudget (PDB) controller. A PDB is a resource that allows users to define the maximum number of pods that can be running at any given time for a particular deployment or replica set. The 'current_healthy' metric indicates the actual number of healthy pods currently running, which can be used to monitor and enforce the desired level of availability for the application. This metric is particularly useful in scenarios where applications have specific requirements for minimum available instances, such as high-availability databases or stateful services. By monitoring this metric, operators can detect potential issues with pod scaling, deployment rollouts, or resource constraints that may impact application availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_status_desired_healthy",
      "namespace": "*",
      "poddisruptionbudget": "*"
    },
    "description": "This metric, **kube_poddisruptionbudget_status_desired_healthy**, represents the minimum number of pods that must remain healthy and available according to the Pod Disruption Budget (PDB) for a given namespace and PDB name. It reflects the target count of pods that should be running and ready to serve traffic, ensuring application availability during voluntary disruptions such as node drains or upgrades.\n\n**Purpose:**  \nSREs use this metric in Prometheus to monitor whether the cluster maintains the required minimum healthy pods as defined by the PDB. It helps prevent excessive pod evictions that could degrade service availability.\n\n**Alert Threshold:**  \nAn alert should be triggered if the actual number of healthy pods (measured by a related metric like `kube_poddisruptionbudget_status_current_healthy`) falls below this desired healthy count (`kube_poddisruptionbudget_status_desired_healthy`). For example:  \n```\nkube_poddisruptionbudget_status_current_healthy < kube_poddisruptionbudget_status_desired_healthy\n```\nThis indicates that the PDB\u2019s availability guarantee is not being met, potentially impacting application reliability.\n\n**Impact of Values:**  \n- **High desired healthy value:** Indicates a strict availability requirement; fewer disruptions are allowed. If unmet, it signals a risk of service degradation.  \n- **Low desired healthy value:** Allows more pod disruptions; suitable for less critical workloads or during planned maintenance.\n\n**Example Usage:**  \nIn a dashboard, display both `kube_poddisruptionbudget_status_desired_healthy` and `kube_poddisruptionbudget_status_current_healthy` side-by-side to visualize if the current healthy pods meet the PDB target.  \n\n**Example alert rule:**  \n```yaml\nalert: PodDisruptionBudgetUnavailable\nexpr: kube_poddisruptionbudget_status_current_healthy < kube_poddisruptionbudget_status_desired_healthy\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Pod Disruption Budget not met for {{ $labels.poddisruptionbudget }} in namespace {{ $labels.namespace }}\"\n  description: \"Current healthy pods ({{ $value }}) are below the desired healthy pods ({{ $labels.desired_healthy }}) as defined by the PDB. This may impact application availability.\"\n```\n\nThis metric is essential for ensuring that voluntary disruptions do not reduce pod availability below acceptable levels, helping maintain application stability during cluster operations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_status_pod_disruptions_allowed",
      "namespace": "*",
      "poddisruptionbudget": "*"
    },
    "description": "This metric measures the number of pod disruptions that are currently allowed by a Pod Disruption Budget (PDB) in a Kubernetes cluster. A PDB is a policy defined for an application or service to limit the number of pods that can be evicted or terminated at any given time, ensuring high availability and minimizing downtime. The value of this metric indicates how many pod disruptions are permitted before the PDB is violated, triggering alerts or automated actions to mitigate potential issues. This information can be used in monitoring and alerting to detect when a PDB is approaching its limit, allowing operators to take proactive measures to prevent disruptions, such as scaling up resources or adjusting deployment strategies.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_status_expected_pods",
      "namespace": "*",
      "poddisruptionbudget": "*"
    },
    "description": "This metric measures the total number of pods that are expected to be running according to a Pod Disruption Budget (PDB) in a Kubernetes cluster. A PDB is a policy defined by an administrator to limit the number of pods of a replicated component that can be voluntarily terminated at any given time. This metric indicates the target number of pods that should be maintained for a specific deployment or replica set, taking into account factors such as scaling and resource constraints.\n\nIn monitoring and alerting, this metric can be used to detect potential issues related to pod availability, scaling, or resource utilization. For example, if the actual number of running pods deviates significantly from the expected value, it may indicate a problem with the deployment process, resource allocation, or even a misconfigured PDB.\n\nThis metric is particularly useful for operations teams to ensure that their applications are meeting service level agreements (SLAs) and to identify potential bottlenecks in their infrastructure. It can also be used as a threshold-based alerting mechanism to notify administrators of any discrepancies between expected and actual pod counts.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_poddisruptionbudget_status_observed_generation",
      "namespace": "*",
      "poddisruptionbudget": "*"
    },
    "description": "This metric measures the most recent generation observed when updating the status of a Kubernetes Pod Disruption Budget (PDB). A PDB is used to specify the maximum number of pods that can be evicted or terminated in a given time period. The observed generation is an important aspect of PDBs, as it helps ensure that the desired level of disruption is maintained. This metric can be used to monitor and troubleshoot issues related to PDB updates, such as delayed or failed updates, which may impact the overall availability and reliability of applications running on the cluster. Potential implications for monitoring and alerting include setting up alerts when the observed generation deviates significantly from the expected value, indicating potential issues with PDB updates.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_completion_time"
    },
    "description": "The **kube_pod_completion_time** metric records the Unix timestamp when a pod finishes its lifecycle and terminates in the Kubernetes cluster. It specifically measures the completion time of a pod from start to termination, not the duration of readiness or serving requests. This metric helps identify pods that take unusually long to complete, which may indicate issues such as resource constraints or scheduling delays. It is useful for monitoring pod lifecycle performance, setting alert thresholds, and optimizing cluster resource allocation.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_info",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*",
      "image_spec": "*",
      "image": "*",
      "image_id": "*",
      "container_id": "*"
    },
    "description": "The kube_pod_container_info metric provides a snapshot of information about each container within a pod in a Kubernetes cluster. This includes details such as the container's name, namespace, and labels, as well as its current state (e.g., running, terminated). The metric can be used to monitor the health and status of containers across the cluster, allowing for more targeted troubleshooting and issue resolution. It may also be leveraged in alerting scenarios to notify operators when a critical container is not running or has failed.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_resource_limits",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*",
      "node": "*",
      "resource": "*",
      "unit": "*"
    },
    "description": "The **kube_pod_container_resource_limits** metric reports the configured resource limits (such as CPU, memory, ephemeral storage, and others) for each container within a Kubernetes pod. It is collected from the Kubernetes API server and represents the maximum amount of each resource that a container is allowed to use. The values are provided in their native units (e.g., CPU cores, bytes of memory). For more precise resource allocation details, consider using the **kube_pod_resource_limits** metric from the kube-scheduler.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_resource_requests",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*",
      "node": "*",
      "resource": "*",
      "unit": "*"
    },
    "description": "The **kube_pod_container_resource_requests** metric in Prometheus reports the amount of CPU or memory resources that a container within a Kubernetes pod has explicitly requested in its configuration. It reflects the resource guarantees set by the pod spec, not the actual usage. This metric is critical for understanding the intended resource allocation per container, enabling capacity planning, resource balancing, and proactive alerting.\n\n### Purpose\n- Tracks the resource requests (CPU cores, memory bytes) defined for each container in a pod.\n- Helps SREs assess cluster resource demand based on declared requests rather than actual consumption.\n- Useful for identifying pods that may be over- or under-provisioned relative to their workload.\n\n### Thresholds and Alerting Guidance\n- **High resource requests:** Alert if total requested CPU or memory on a node approaches or exceeds a configurable percentage (e.g., 80-90%) of that node\u2019s allocatable resources. This indicates potential capacity saturation and risk of scheduling failures.\n- **Low resource requests:** Alert if a pod\u2019s resource requests are consistently very low compared to observed usage (requires correlating with usage metrics), which may cause throttling or OOM kills due to under-provisioning.\n- Example alert condition for CPU requests on a node:\n  ```\n  sum by(node) (kube_pod_container_resource_requests{resource=\"cpu\"}) > 0.9 * kube_node_status_allocatable_cpu_cores\n  ```\n  This triggers when requested CPU exceeds 90% of node allocatable CPU.\n\n### Impact of Values\n- **High values:** Indicate containers requesting large amounts of resources, which can reduce cluster scheduling flexibility and increase costs. May lead to resource contention if actual usage is also high.\n- **Low values:** May suggest under-requested resources, risking performance degradation or container restarts if actual usage exceeds requests.\n\n### Example Usage in Dashboard or Alert Rule\n- **Dashboard:** Visualize total CPU and memory requests per node alongside allocatable capacity and actual usage to identify resource pressure and overcommitment.\n- **Alert Rule Example:**\n  ```yaml\n  alert: HighNodeResourceRequests\n  expr: sum by(node) (kube_pod_container_resource_requests{resource=\"memory\"}) > 0.85 * kube_node_status_allocatable_memory_bytes\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High memory resource requests on node {{ $labels.node }}\"\n    description: \"Memory requests exceed 85% of allocatable memory on node {{ $labels.node }} for more than 5 minutes.\"\n  ```\n\n### Note\nWhile **kube_pod_container_resource_requests** provides the requested resources, it does not reflect actual usage. For more precise resource consumption insights, consider using metrics like **kube_pod_container_resource_usage** or those exposed by the Kubernetes scheduler.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_state_started",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "The **kube_pod_container_state_started** metric records the Unix timestamp when a specific container within a Kubernetes pod begins running. It measures the exact start time of the container's execution. This metric helps monitor container startup performance and detect delays or failures during initialization. It is useful for troubleshooting, alerting, and correlating startup times with other system events.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_last_terminated_reason"
    },
    "description": "This metric measures the last reason a container was terminated within a Kubernetes pod. It provides insight into the cause of container termination, which can be useful for troubleshooting and debugging purposes. The possible values for this metric include 'Error', 'CrashLoopBackOff', 'OOMKilled' (Out Of Memory Killed), 'ForceKill', 'Unreachable', 'Unknown', or 'ContainerCannotRun'. Potential implications or usage in monitoring or alerting include setting up alerts for specific termination reasons, such as when a container is terminated due to an error or crash. This can help operations teams quickly identify and address issues affecting pod availability and performance.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_last_terminated_exitcode"
    },
    "description": "This metric measures the exit code of the last container in a terminated state for each pod in the Kubernetes cluster. The exit code is an integer value that indicates the status of the container's execution. A non-zero exit code typically signifies an error or failure, while a zero exit code usually indicates successful termination. This metric can be used to monitor and troubleshoot issues related to container crashes, failed deployments, or other pod-related problems. It may also serve as a trigger for alerting mechanisms when a certain threshold of terminated containers with non-zero exit codes is exceeded, indicating potential infrastructure or application issues.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_last_terminated_timestamp"
    },
    "description": "**kube_pod_container_status_last_terminated_timestamp** records the Unix timestamp of the most recent termination event for a specific pod container. It reflects the exact time when the container last stopped, whether due to normal shutdown, error, or crash. This metric helps monitor container lifecycle events and diagnose issues related to frequent or prolonged terminations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_ready",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "The **kube_pod_container_status_ready** metric indicates whether a specific container within a Kubernetes pod is ready to serve traffic. It is a boolean value: 1 if the container has passed its readiness probe and is ready, or 0 if it is not ready. This metric reflects the container\u2019s readiness state as determined by Kubernetes readiness checks, not liveness probes. It is used to monitor container availability and readiness within a given namespace, pod, and container.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_restarts_total",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric measures the total number of restarts for each container across all pods in the Kubernetes cluster. It represents a count of how many times a container has been restarted due to various reasons such as crashes, failures, or manual restarts by the user. The metric is incremented every time a container restarts and provides insight into the reliability and stability of containers within the cluster. Potential implications for monitoring include identifying problematic containers that are frequently restarting, which may indicate underlying issues with the application, configuration, or infrastructure. This information can be used to trigger alerts for proactive maintenance, optimize resource allocation, or even automate corrective actions such as rolling updates or container restarts.",
    "metric_type": "counter",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_running",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric reports the running status of a specific container within a Kubernetes pod. It has a value of 1 if the container is currently running, and 0 if it is not. The metric is dimensioned by namespace, pod name, pod UID, and container name to uniquely identify each container instance. It is used to monitor container lifecycle states and detect containers that are not running as expected.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_terminated",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric, **kube_pod_container_status_terminated**, indicates whether a specific container within a Kubernetes pod has terminated its execution. It is a gauge with a value of 1 if the container is in a terminated state, and 0 otherwise. This metric helps SREs track container lifecycle events, particularly unexpected or repeated terminations that may signal application crashes, misconfigurations, or resource issues.\n\n**Purpose:**  \nIn Prometheus, this metric is used to identify containers that have stopped running, enabling monitoring of pod stability and container health over time.\n\n**Alert Threshold Guidance:**  \n- Trigger an alert if the metric equals 1 for a container repeatedly or for a sustained period (e.g., >5 minutes), indicating a container is stuck terminated or crashing frequently.  \n- A common threshold is to alert if more than a certain number (e.g., >5) of containers in a namespace or deployment are terminated simultaneously, which may indicate systemic issues.\n\n**Impact of Values:**  \n- **High values (1):** Indicate terminated containers. A high count of terminated containers can lead to degraded application availability or service disruption.  \n- **Low values (0):** Indicate containers are running or have not terminated, which is the expected healthy state.\n\n**Example Usage:**  \n- **Alert Rule:**  \n```yaml\nalert: ContainerTerminatedTooOften\nexpr: sum by (namespace) (kube_pod_container_status_terminated == 1) > 5\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"More than 5 containers terminated in namespace {{ $labels.namespace }}\"\n  description: \"Multiple containers have terminated in the last 5 minutes, indicating potential instability.\"\n```\n\n- **Dashboard Query:**  \n```promql\nsum by (namespace, pod, container) (kube_pod_container_status_terminated)\n```\nThis query shows the count of terminated containers grouped by namespace, pod, and container, helping visualize termination patterns and identify problematic workloads.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_terminated_reason"
    },
    "description": "**kube_pod_container_status_terminated_reason**  \nThis Prometheus metric indicates the specific reason why a container within a Kubernetes pod has terminated. It is a labeled gauge metric where each label corresponds to a termination reason (e.g., OOMKilled, Error, Completed). The metric\u2019s value is typically 1 when the container has terminated for that reason, and 0 otherwise.\n\n**Purpose:**  \nSREs use this metric to monitor container termination causes, helping to quickly identify and diagnose issues such as crashes, out-of-memory kills, or intentional shutdowns. Tracking termination reasons enables proactive detection of unstable workloads or infrastructure problems.\n\n**Alerting Thresholds:**  \n- Trigger an alert if the metric value for critical termination reasons (e.g., `Error`, `OOMKilled`) is greater than 0 for a sustained period (e.g., 5 minutes), indicating repeated or ongoing container failures.  \n- Example alert condition: `sum by (reason) (kube_pod_container_status_terminated_reason{reason=~\"Error|OOMKilled\"}) > 0` sustained over 5 minutes.\n\n**Impact of Values:**  \n- **High values** for error-related reasons signal frequent container crashes or resource issues, potentially causing service disruption or degraded performance. Immediate investigation is required.  \n- **Low or zero values** indicate stable container operation with no recent terminations for problematic reasons.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing counts of terminated containers grouped by reason to visualize trends and spikes in failures.  \n- **Alert Rule:**  \n  ```yaml\n  alert: ContainerTerminationErrors\n  expr: sum by (reason) (kube_pod_container_status_terminated_reason{reason=~\"Error|OOMKilled\"}) > 0\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"Container termination due to {{ $labels.reason }} detected\"\n    description: \"One or more containers have terminated with reason '{{ $labels.reason }}' in the last 5 minutes. Investigate pod logs and resource usage.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_waiting",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "The kube_pod_container_status_waiting metric measures whether a container within a Kubernetes pod is currently in a waiting state. This indicates that the container has not yet started or has been paused due to various reasons such as resource constraints, configuration issues, or errors during startup. The metric provides visibility into the health and status of containers within pods, enabling operators to identify potential issues and take corrective actions. Potential implications for monitoring and alerting include setting up alerts for high waiting container counts, investigating pod-level issues, and optimizing resource allocation to prevent waiting states.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_container_status_waiting_reason"
    },
    "description": "This metric measures the reason a container within a Kubernetes pod is currently in a waiting state. The waiting state indicates that the container has not yet started or has terminated and is awaiting further instructions from the scheduler. The kube_pod_container_status_waiting_reason metric provides insight into the specific cause of this waiting state, which can be due to various reasons such as 'ContainerCreating', 'ImagePullBackOff', 'CrashLoopBackOff', or other custom reasons defined by the application or infrastructure team. This information is crucial for identifying and resolving issues related to pod deployment, container startup, or resource constraints. It can be used in monitoring and alerting to detect potential problems, such as stuck pods, failed deployments, or resource exhaustion, allowing operators to take corrective actions to ensure smooth operation of their Kubernetes cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_created",
      "namespace": "*",
      "pod": "*",
      "uid": "*"
    },
    "description": "The **kube_pod_created** metric records the Unix timestamp when a Kubernetes pod was instantiated in the cluster, labeled by namespace, pod name, and UID. In Prometheus, this metric helps SREs track pod creation events over time, enabling analysis of deployment rates and lifecycle patterns. \n\n**Purpose:**  \n- Monitor pod creation frequency to detect sudden spikes or drops that may indicate deployment issues or workload changes.  \n- Correlate pod creation timing with resource usage or failures to troubleshoot cluster behavior.\n\n**Alert Threshold Guidance:**  \n- Alert if no new pods are created within an expected timeframe during active deployment periods (e.g., no pod creation events in 10 minutes when scaling up).  \n- Alert on an unusually high rate of pod creation (e.g., more than 50 pods created within 5 minutes), which may signal a deployment loop or instability.\n\n**Impact of Values:**  \n- **High values (recent timestamps for many pods):** Could indicate rapid scaling or potential pod churn, possibly stressing cluster resources.  \n- **Low or stale values (old timestamps, no recent pod creation):** May suggest stalled deployments, failed pod startups, or issues with the scheduler.\n\n**Example Usage:**  \n- **Dashboard:** Plot the rate of pod creation over time using `rate(kube_pod_created[5m])` grouped by namespace to visualize deployment activity spikes or lulls.  \n- **Alert Rule:**  \n  ```\n  alert: PodCreationStalled\n  expr: count_over_time(kube_pod_created[10m]) == 0\n  for: 10m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"No pods created in the last 10 minutes\"\n    description: \"Pod creation has stalled in namespace {{ $labels.namespace }} for over 10 minutes, indicating possible deployment or scheduler issues.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_deletion_timestamp"
    },
    "description": "The **kube_pod_deletion_timestamp** metric records the Unix timestamp of when a Kubernetes pod was marked for deletion. In Prometheus, this metric helps SREs track pod termination events and identify pods that are stuck in the deletion process or being deleted unexpectedly. \n\n**Purpose:**  \n- Monitor pod lifecycle by detecting when pods enter the deletion phase.  \n- Identify pods that have been pending deletion for an unusually long time, which may indicate issues such as stuck finalizers or resource cleanup failures.  \n- Correlate with other metrics like **kube_pod_status_phase** to diagnose pod termination problems or unexpected restarts.\n\n**Alerting Thresholds:**  \n- Trigger an alert if the current time minus **kube_pod_deletion_timestamp** exceeds a threshold (e.g., 10 minutes), indicating a pod has been stuck deleting for too long.  \n- Example alert condition: `(time() - kube_pod_deletion_timestamp) > 600` seconds.\n\n**Impact of Values:**  \n- **Low or recent values:** Indicate pods are being deleted normally and promptly.  \n- **High values:** Suggest pods are stuck in deletion, potentially causing resource leaks, blocking pod replacements, or impacting application availability.\n\n**Example Usage:**  \n- **Alert Rule:**  \n  ```yaml\n  alert: PodDeletionStuck\n  expr: (time() - kube_pod_deletion_timestamp) > 600\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Pod {{ $labels.pod }} stuck in deletion for over 10 minutes\"\n    description: \"Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been marked for deletion since {{ $value | humanizeTimestamp }} but has not terminated.\"\n  ```\n- **Dashboard Panel:**  \n  Display pods with deletion timestamps older than 10 minutes to quickly identify stuck deletions, using a query like:  \n  `kube_pod_deletion_timestamp and (time() - kube_pod_deletion_timestamp) > 600`  \n  This helps visualize pods that require manual intervention or further investigation.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_info",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "host_ip": "*",
      "pod_ip": "*",
      "node": "*",
      "created_by_kind": "*",
      "created_by_name": "*",
      "priority_class": "*",
      "host_network": "*"
    },
    "description": "The **kube_pod_info** metric in Prometheus provides detailed metadata about each pod running in a Kubernetes cluster, including identifiers (pod name, UID), network details (host IP, pod IP), scheduling info (node, priority class, host network), and ownership (created_by_kind, created_by_name). This metric itself is a static informational metric that does not change values over time but serves as a foundational label source to correlate pod-specific metrics such as CPU, memory usage, or pod phase.\n\n**Purpose:**  \nIt enables SREs to uniquely identify and filter pods across namespaces and nodes, facilitating precise monitoring and troubleshooting by linking pod metadata with dynamic metrics. For example, it helps distinguish pods by their creator (e.g., Deployment, StatefulSet), priority class, or network configuration.\n\n**Alerting Guidance:**  \nSince **kube_pod_info** is static metadata, it does not have numeric values or thresholds to alert on directly. Instead, it is used in conjunction with other metrics (e.g., `kube_pod_status_phase`, `container_cpu_usage_seconds_total`) to create meaningful alerts. For instance, an alert might trigger if a pod with a specific `created_by_kind` or `priority_class` remains in a non-Running phase for longer than a threshold duration.\n\n**Impact of Values:**  \n- Presence of this metric with expected labels confirms pod existence and metadata correctness.  \n- Missing or inconsistent labels (e.g., missing `created_by_name` or incorrect `priority_class`) can indicate misconfigurations or deployment issues.  \n- Changes in pod metadata (e.g., pod moving to a different node) can be tracked by correlating this metric with pod lifecycle events.\n\n**Example Usage:**  \nTo alert on pods created by a Deployment that are stuck in a non-Running phase for more than 5 minutes:\n\n```yaml\nalert: PodNotRunning\nexpr: kube_pod_info * on(pod, namespace) group_left() kube_pod_status_phase{phase!=\"Running\"} > 0\n  and ignoring() time() - kube_pod_status_start_time_seconds > 300\n  and kube_pod_info{created_by_kind=\"Deployment\"}\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not running\"\n  description: \"Pod created by Deployment {{ $labels.created_by_name }} has been in phase {{ $labels.phase }} for over 5 minutes.\"\n```\n\nIn dashboards, **kube_pod_info** labels enable filtering and grouping of pod metrics by attributes like node, priority class, or creator, improving visibility into pod distribution and behavior across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_ips",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "ip": "*",
      "ip_family": "*"
    },
    "description": "The **kube_pod_ips** metric reports the IP addresses assigned to each pod in a Kubernetes cluster. It provides the IP address (string) and IP family (e.g., IPv4 or IPv6) for each pod identified by namespace, pod name, and UID. This metric helps track pod network configurations and supports troubleshooting connectivity issues. It is useful for monitoring pod reachability and network topology within the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_info",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*",
      "image_spec": "*",
      "image": "*",
      "image_id": "*",
      "container_id": "*",
      "restart_policy": "*"
    },
    "description": "The kube_pod_init_container_info metric measures information about an init container in a Kubernetes pod. This includes details such as the name of the init container, its image, command, and arguments. Init containers are used to perform setup tasks before the main application containers start. They can be used for tasks like downloading dependencies, configuring environment variables, or even running database migrations. Monitoring this metric can help identify issues with pod initialization, such as failed container startups or incorrect configuration. Potential implications include delayed pod startup times, failed deployments due to init container failures, and resource utilization inefficiencies. This metric can be used in monitoring and alerting to detect anomalies in init container behavior, ensuring that pods are properly initialized before the main application containers start.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_resource_limits"
    },
    "description": "The kube_pod_init_container_resource_limits metric measures the number of requested limit resources (e.g., CPU and memory) by an init container within a Kubernetes pod. Init containers are responsible for setting up the environment for the main application containers to run in, and they often require specific resource allocations to function correctly.\n\nThis metric can be used to monitor and troubleshoot issues related to init container resource utilization, such as:\n\n* Identifying pods with high resource requests that may impact cluster performance or lead to resource exhaustion.\n* Detecting potential bottlenecks in the deployment process due to insufficient resources allocated to init containers.\n* Correlating resource limits with pod failures or crashes to determine if resource constraints are contributing factors.\n\nIn monitoring and alerting, this metric can be used to set thresholds for high resource requests, trigger alerts when pods exceed these thresholds, or create dashboards to visualize resource utilization across the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_resource_requests",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*",
      "node": "*",
      "resource": "*",
      "unit": "*"
    },
    "description": "This metric measures the number of requested resource requests made by an init container in a Kubernetes pod. An init container is a special type of container that runs to completion before the main application containers start. The requested resources include CPU and memory, which are specified as part of the container's configuration. This metric can be used to monitor and troubleshoot issues related to resource allocation and utilization within pods. For example, if the number of requested requests is consistently high or increasing over time, it may indicate a problem with pod scaling or resource availability. Additionally, this metric can be used in conjunction with other metrics, such as kube_pod_container_resource_requests, to gain a more comprehensive understanding of resource usage across all containers within a pod.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_last_terminated_reason"
    },
    "description": "This metric measures the last reason an init container within a Kubernetes pod was terminated. Init containers are used to perform setup tasks before the main application containers start. The termination of an init container can indicate issues with the initialization process or resource constraints. This metric provides insight into the health and stability of the pod's initialization phase, allowing operators to identify potential problems and take corrective action. Potential implications include: identifying misconfigured init containers, detecting resource exhaustion, or pinpointing specific errors in the initialization process.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_ready",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric measures the readiness status of init containers within a Kubernetes pod. An init container is a special type of container that runs to completion before the main application containers in a pod start. The kube_pod_init_container_status_ready metric indicates whether the readiness check for each init container has succeeded or failed. A value of 1 (or true) means the readiness check was successful, while a value of 0 (or false) indicates failure. This metric can be used to monitor and troubleshoot issues related to pod initialization, such as delayed or failing init containers, which may impact the overall health and availability of the application. It can also serve as a trigger for alerts when there are repeated failures in init container readiness checks, helping operations teams to identify and address potential problems before they escalate.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_restarts_total",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric measures the total number of restarts for the init container in a Kubernetes pod. The init container is responsible for setting up the environment and dependencies required by the main application containers. A high number of restarts may indicate issues with the init container's configuration, resource constraints, or errors during its execution. This metric can be used to monitor the reliability and stability of the init container, identify potential bottlenecks, and trigger alerts when thresholds are exceeded. It is essential to consider other related metrics, such as kube_pod_init_container_status_running and kube_pod_init_container_status_terminated, for a comprehensive understanding of the pod's lifecycle.",
    "metric_type": "counter",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_running",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric measures the running status of init containers within pods in a Kubernetes cluster. An init container is a special type of container that runs to completion before the main application containers are started. The metric indicates whether each init container has successfully transitioned into a 'running' state, which is a critical step in the pod's initialization process. A value of 1 (or true) for this metric suggests that the init container is currently running as expected, while a value of 0 (or false) may indicate issues such as failed startup or resource constraints. This information can be used to monitor and troubleshoot pod initialization failures, identify potential bottlenecks in the deployment process, and optimize resource allocation for improved application availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_terminated",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric measures the status of init containers within pods in a Kubernetes cluster. It indicates whether an init container has terminated, which can be due to successful completion or termination due to an error. The presence of this metric suggests that the pod's init container lifecycle is being monitored for potential issues. Potential implications include identifying failed deployments, detecting configuration errors, or pinpointing resource constraints affecting init containers. This metric can be used in monitoring and alerting to trigger notifications when a significant number of pods have terminated init containers, indicating broader cluster health problems.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_terminated_reason",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*",
      "reason": "*"
    },
    "description": "This metric measures the reason behind the termination of an init container within a Kubernetes pod. It provides insight into the status of the init container's lifecycle and can be used to identify potential issues or errors that may have caused its termination. The metric can help operators diagnose and troubleshoot problems related to init container failures, such as configuration errors, resource constraints, or unexpected behavior. By monitoring this metric, teams can proactively address issues before they impact the overall health of the pod or application.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_waiting",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "container": "*"
    },
    "description": "This metric, **kube_pod_init_container_status_waiting**, reports whether an init container within a Kubernetes pod is currently in a waiting state (value = 1) or not (value = 0). It is exposed by the kube-state-metrics component and scraped by Prometheus to provide real-time visibility into pod initialization progress.\n\n**Purpose:**  \nUse this metric to monitor the readiness and startup behavior of pods by tracking init containers that have not yet completed. Persistent waiting states often indicate issues such as image pull failures, misconfigurations, or resource constraints delaying pod startup.\n\n**Alerting Threshold:**  \nAn alert should be triggered if the number of pods with init containers stuck in the waiting state exceeds a small threshold for a sustained period. For example:  \n```\nsum(kube_pod_init_container_status_waiting == 1) > 5 for 5m\n```  \nThis means if more than 5 init containers are waiting continuously for 5 minutes, it likely signals a cluster-wide problem requiring investigation.\n\n**Impact of Values:**  \n- **High values:** Indicate multiple pods are delayed in initialization, potentially causing application downtime or degraded service availability. This can cascade into resource contention or deployment failures.  \n- **Low or zero values:** Indicate normal pod startup behavior with init containers completing promptly.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph showing the count of init containers in waiting state over time, segmented by namespace or deployment, to quickly identify problematic workloads.  \n- **Alert Rule:**  \n```yaml\nalert: InitContainersStuckWaiting  \nexpr: sum(kube_pod_init_container_status_waiting == 1) > 5  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"Multiple init containers stuck waiting in the cluster\"  \n  description: \"More than 5 init containers have been waiting for over 5 minutes, indicating potential pod startup issues.\"  \n```\n\nBy monitoring this metric, SREs can proactively detect and resolve pod initialization bottlenecks before they impact application availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_init_container_status_waiting_reason"
    },
    "description": "This metric measures the reason why an init container in a Kubernetes pod is currently in a waiting state. The init container is responsible for setting up the environment for the main containers to run. If it's stuck in a waiting state, it may indicate issues with the container's configuration, dependencies, or execution. This metric can be used to identify and troubleshoot problems related to init container failures, such as missing dependencies, incorrect configuration, or resource constraints. It can also help operators understand the root cause of pod initialization delays and take corrective actions to ensure smooth deployment and scaling of applications.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_annotations"
    },
    "description": "The **kube_pod_annotations** metric exposes Kubernetes pod annotations as Prometheus labels, enabling SREs to monitor metadata attached to pods for operational insights. This metric helps track the presence, absence, or changes of specific annotations that may be critical for pod behavior, compliance, or configuration management. \n\n**Purpose:** Use this metric to verify that required annotations (e.g., for security policies, deployment metadata, or monitoring configurations) are present and correctly set on pods. Missing or incorrect annotations can indicate misconfigurations or policy violations.\n\n**Alerting Thresholds:**  \n- Trigger an alert if the count of pods missing a required annotation exceeds a defined threshold (e.g., more than 5 pods without the annotation for 5 minutes).  \n- Alert if annotation values deviate from expected patterns or contain invalid data.\n\n**Impact:**  \n- **High values** of missing or invalid annotations may lead to policy enforcement failures, security risks, or monitoring blind spots.  \n- **Low or zero values** indicate compliance with annotation requirements, reducing operational risk.\n\n**Example Alert Rule:**  \n```yaml\nalert: MissingCriticalPodAnnotations  \nexpr: count(kube_pod_annotations{annotation_key=\"critical-label\"}) - count(kube_pod_annotations{annotation_key=\"critical-label\", annotation_value=~\".+\"}) > 5  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"More than 5 pods missing the 'critical-label' annotation\"  \n  description: \"Pods missing the required 'critical-label' annotation for over 5 minutes may indicate deployment or configuration issues.\"  \n```\n\n**Example Dashboard Use:**  \nVisualize the number of pods missing key annotations over time to quickly identify trends or sudden changes in pod metadata compliance, enabling proactive troubleshooting and audit readiness.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_labels"
    },
    "description": "The **kube_pod_labels** metric exposes the set of Kubernetes labels assigned to each pod as Prometheus labels, enabling you to filter, group, and aggregate pod-related metrics based on these labels. Its primary purpose in Prometheus is to provide metadata context for pods, allowing SREs to create more precise queries, dashboards, and alerts that reflect the operational state and role of pods within the cluster.\n\nSince **kube_pod_labels** itself is a metadata metric (typically a gauge with a constant value of 1 per label set), it does not have numeric thresholds that directly trigger alerts. Instead, it is used as a dimension to filter or identify pods with specific labels. For example, you might alert if no pods with a critical label (e.g., `app=frontend`) exist in a namespace, indicating a deployment or scheduling issue.\n\nHigh or low values are not applicable to this metric directly; rather, the presence or absence of specific label combinations is what matters. Missing expected labels or unexpected label values can indicate misconfigurations or deployment problems, which can impact monitoring accuracy and operational decisions.\n\n**Example usage in an alert rule:**\n\n```yaml\nalert: MissingFrontendPods\nexpr: count(kube_pod_labels{label_app=\"frontend\", namespace=\"production\"}) == 0\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"No frontend pods detected in production namespace\"\n  description: \"No pods with label app=frontend have been found in the production namespace for over 5 minutes, indicating a potential deployment or scheduling failure.\"\n```\n\n**Example usage in a dashboard query:**\n\n```promql\ncount by (namespace, label_app) (kube_pod_labels)\n```\n\nThis query shows the count of pods grouped by namespace and application label, helping you visualize pod distribution and identify missing or unexpected labels across your cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_overhead_cpu_cores"
    },
    "description": "The kube_pod_overhead_cpu_cores metric measures the additional CPU resources required by a Kubernetes pod to account for overheads such as scheduling, resource allocation, and other system-level tasks. This value represents the number of CPU cores allocated beyond what is requested by the container(s) running within the pod. It provides insight into the actual CPU resources consumed by the pod, which can be used to optimize resource utilization, prevent over-allocation, and ensure efficient use of cluster resources. In monitoring or alerting scenarios, this metric can be used to detect potential issues such as high overheads indicating inefficient resource allocation, or low overheads suggesting underutilized resources.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_overhead_memory_bytes"
    },
    "description": "The **kube_pod_overhead_memory_bytes** metric in Prometheus measures the additional memory consumed by the Kubernetes pod infrastructure beyond the container's own memory usage. This overhead includes memory used by the container runtime, pause containers, and other system components necessary to run the pod. Monitoring this metric helps SREs understand the true memory footprint of pods, enabling more accurate resource allocation and capacity planning.\n\n**Purpose:**  \n- To quantify the memory overhead per pod, separate from application memory usage.  \n- To identify pods with unexpectedly high overhead that may indicate misconfiguration or inefficiencies in the container runtime or pod setup.\n\n**Thresholds and Alerting:**  \n- Alert when **kube_pod_overhead_memory_bytes** exceeds a predefined threshold relative to the pod\u2019s requested memory (e.g., overhead > 10-15% of pod memory request), as this may indicate resource waste or potential memory pressure.  \n- Example alert condition:  \n  ```\n  kube_pod_overhead_memory_bytes / kube_pod_container_resource_requests_memory_bytes > 0.15\n  ```  \n- Adjust thresholds based on workload characteristics and cluster baseline overhead.\n\n**Impact of Values:**  \n- **High values:** Indicate excessive memory overhead, which can reduce available memory for application containers, potentially causing OOM kills or degraded performance. May also signal inefficient container runtimes or pod configurations.  \n- **Low values:** Suggest minimal overhead, which is generally desirable, but unusually low values could indicate missing or incomplete metric reporting.\n\n**Example Usage:**  \n- **Dashboard:** Display a graph showing **kube_pod_overhead_memory_bytes** alongside pod memory requests and usage to visualize overhead proportion and trends over time.  \n- **Alert Rule:**  \n  ```yaml\n  alert: HighPodMemoryOverhead\n  expr: kube_pod_overhead_memory_bytes / kube_pod_container_resource_requests_memory_bytes > 0.15\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Pod memory overhead exceeds 15% of requested memory\"\n    description: \"Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has memory overhead at {{ $value | humanize }} bytes, which is more than 15% of its requested memory.\"\n  ```\n\nBy monitoring **kube_pod_overhead_memory_bytes**, SREs can ensure pods are allocated sufficient memory, detect inefficiencies in pod runtime overhead, and optimize cluster resource utilization.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_owner",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "owner_kind": "*",
      "owner_name": "*",
      "owner_is_controller": "*"
    },
    "description": "The kube_pod_owner metric measures information about the owner of a Kubernetes Pod. This includes details such as the name and namespace of the owning resource (e.g., Deployment, ReplicaSet, etc.). The metric provides insight into the lifecycle and dependencies of Pods within a cluster, enabling operators to understand relationships between resources and troubleshoot issues more effectively.\n\nPotential implications for monitoring or alerting include:\n- Identifying orphaned Pods without an owner, which may indicate configuration errors or resource leaks.\n- Detecting changes in Pod ownership, such as when a Deployment is scaled up or down, allowing for proactive capacity planning.\n- Correlating Pod performance issues with the owning resources, facilitating root cause analysis and resolution.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_restart_policy",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "type": "*"
    },
    "description": "The kube_pod_restart_policy metric measures the restart policy in use by a Kubernetes pod. This policy determines how the pod is restarted or recreated when it fails or terminates unexpectedly. The possible values for this metric are 'Always', 'OnFailure', and 'Never'. A value of 'Always' indicates that the pod will always be restarted, regardless of its previous state. A value of 'OnFailure' means that the pod will only be restarted if it fails, while a value of 'Never' indicates that the pod will not be restarted under any circumstances. This metric can be used in monitoring and alerting to identify pods with potentially problematic restart policies, which may indicate issues with application reliability or resource utilization.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_runtimeclass_name_info"
    },
    "description": "The **kube_pod_runtimeclass_name_info** metric in Prometheus exposes the runtime class assigned to each pod in a Kubernetes cluster, indicating the container runtime environment (e.g., Docker, containerd, or custom runtimes) used to run the pod. This metric is a label-based informational metric rather than a numeric gauge, so it does not have values that increase or decrease; instead, it provides metadata to help identify which runtime class each pod is using.\n\n**Purpose:**  \nThis metric helps SREs verify that pods are running with the expected runtime classes, enabling validation of runtime configurations, compliance with security policies, and troubleshooting runtime-specific issues.\n\n**Alerting Guidance:**  \nSince this metric is informational, alerts should be based on the presence or absence of expected runtime classes rather than numeric thresholds. For example, an alert can trigger if pods are running with an unexpected or deprecated runtime class, or if no pods are running with a required runtime class.\n\n**Impact:**  \n- **Unexpected runtime classes:** May indicate misconfiguration, security risks, or unsupported environments that could affect pod stability or compliance.  \n- **Missing expected runtime classes:** Could signal deployment issues or incomplete rollout of runtime policies.\n\n**Example Alert Rule:**  \nAlert if any pod is running with a runtime class not in the approved list (e.g., only \"runc\" and \"kata-containers\" allowed):\n\n```yaml\nalert: UnexpectedPodRuntimeClass\nexpr: |\n  count by (runtimeclass) (\n    kube_pod_runtimeclass_name_info\n    unless on(runtimeclass) (label_replace(vector(\"runc\"), \"runtimeclass\", \"runc\", \"\", \"\") or label_replace(vector(\"kata-containers\"), \"runtimeclass\", \"kata-containers\", \"\", \"\"))\n  ) > 0\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Pod(s) running with unexpected runtime class\"\n  description: \"One or more pods are running with a runtime class not in the approved list (runc, kata-containers). Investigate pod runtime configurations.\"\n```\n\n**Example Dashboard Use:**  \nDisplay a table or pie chart grouping pods by their runtime class using `kube_pod_runtimeclass_name_info` labels to monitor distribution and detect unexpected runtime usage visually.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_spec_volumes_persistentvolumeclaims_info",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "volume": "*",
      "persistentvolumeclaim": "*"
    },
    "description": "This metric exposes metadata about PersistentVolumeClaims (PVCs) referenced by Kubernetes pods, including the pod namespace, pod name, volume name, and PVC name. It serves as a static informational metric indicating which PVCs are mounted by which pods, rather than a numeric measurement. Because it does not represent a value that changes over time, it is not suitable for threshold-based alerting by itself. Instead, it is primarily used to correlate pod-to-PVC relationships for troubleshooting storage issues or capacity planning.\n\n**Purpose:**  \n- Identify which pods are using which PVCs, enabling visibility into pod storage dependencies.  \n- Assist in diagnosing storage-related problems by linking pods to their PVCs.  \n- Support capacity planning by mapping PVC usage across the cluster.\n\n**Alerting Guidance:**  \n- Since this metric is informational and non-numeric, alerts should be based on related metrics such as PVC status conditions (e.g., `kube_persistentvolumeclaim_status_phase`), PVC capacity usage, or pod volume mount errors.  \n- For example, alert if a PVC used by a pod enters a `Failed` or `Lost` state, or if PVC capacity usage exceeds a defined threshold.\n\n**Impact of Values:**  \n- The presence of this metric with specific label combinations indicates a pod is using a particular PVC.  \n- A missing metric for an expected pod-PVC pair may indicate a misconfiguration or pod startup issue.  \n- Changes in the set of label values over time can reveal dynamic volume attachment/detachment events.\n\n**Example Usage:**  \n- Dashboard: Display a table showing pods and their associated PVCs by querying this metric, helping operators quickly identify storage dependencies.  \n- Alert Rule (example, based on related PVC status metric):  \n  ```yaml\n  alert: PersistentVolumeClaimNotBound  \n  expr: kube_persistentvolumeclaim_status_phase{phase!=\"Bound\"} > 0  \n  for: 5m  \n  labels:  \n    severity: warning  \n  annotations:  \n    summary: \"PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is not bound\"  \n    description: \"PVC {{ $labels.persistentvolumeclaim }} used by pod(s) may be unavailable or misconfigured.\"  \n  ```",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_spec_volumes_persistentvolumeclaims_readonly",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "volume": "*",
      "persistentvolumeclaim": "*"
    },
    "description": "This metric indicates whether a PersistentVolumeClaim (PVC) is mounted as read-only in a Kubernetes pod specification. It has a value of 1 if the PVC is set to read-only, and 0 if it is mounted with write access. This helps identify pods configured to prevent modifications to the PVC data. Monitoring this metric can assist in ensuring data protection and detecting potential misconfigurations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_start_time",
      "namespace": "*",
      "pod": "*",
      "uid": "*"
    },
    "description": "The **kube_pod_start_time** metric records the exact time a Kubernetes pod began running, expressed as a Unix timestamp (seconds since January 1, 1970 UTC). It reflects when the pod transitioned to the Running state for the first time. This metric helps track pod startup timing and can be used to calculate pod uptime or identify delays in pod initialization.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_phase",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "phase": "*"
    },
    "description": "The kube_pod_status_phase metric measures the current phase of a Kubernetes pod, indicating its lifecycle status. The possible phases include Pending, Running, Succeeded, Failed, and Unknown. This metric is crucial for monitoring pod health and detecting potential issues in the cluster. It can be used to trigger alerts when pods are stuck in a pending or failed state, indicating potential infrastructure or application problems. Additionally, it can help operators identify resource constraints or misconfigurations that may be causing pods to fail or remain in a pending state.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_qos_class",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "qos_class": "*"
    },
    "description": "The kube_pod_status_qos_class metric measures the Quality of Service (QoS) class assigned to a pod in a Kubernetes cluster. The QoS class is determined by the pod's resource requests and limits for CPU and memory. This metric can be used to monitor the QoS class of pods, which can indicate potential issues with resource allocation or utilization. For example, if a pod is assigned a 'BestEffort' QoS class, it may not receive guaranteed resources, potentially leading to performance issues or even node crashes. Monitoring this metric can help identify such issues and inform capacity planning or resource optimization efforts.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_ready",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "condition": "*"
    },
    "description": "The kube_pod_status_ready metric measures whether a Kubernetes pod is ready to serve requests based on its status. A pod is considered ready if all of its containers are running and at least one container is in the 'Ready' state. This metric can be used to monitor the health and availability of pods in a cluster, allowing operators to identify potential issues with pod deployment or scaling. It may also be used as a trigger for alerting when a pod becomes not ready, indicating a possible problem with the application or infrastructure.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_ready_time",
      "namespace": "*",
      "pod": "*",
      "uid": "*"
    },
    "description": "The **kube_pod_status_ready_time** metric records the Unix timestamp when a pod first transitions to the Ready state, indicating it has completed startup and is prepared to handle traffic. In Prometheus, this metric helps SREs track pod readiness timing to ensure application availability and performance.\n\n**Purpose:**  \n- Measure how quickly pods become ready after creation or restart.  \n- Identify pods that are slow to start, which may signal resource bottlenecks, configuration errors, or application issues.\n\n**Thresholds for Alerting:**  \n- Alert if the time difference between pod creation and readiness exceeds a defined threshold (e.g., 5 minutes).  \n- This threshold depends on your application\u2019s expected startup time; unusually high values suggest problems delaying readiness.\n\n**Impact of Values:**  \n- **High values:** Indicate pods are taking too long to become ready, potentially causing service degradation or downtime.  \n- **Low values:** Indicate healthy, fast pod startup and readiness.\n\n**Example Usage:**  \nTo alert on pods taking longer than 5 minutes to become ready, use a Prometheus alert rule comparing the pod creation time (`kube_pod_created`) and readiness time:\n\n```yaml\nalert: PodReadyDelayHigh\nexpr: (kube_pod_status_ready_time - kube_pod_created) > 300\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is slow to become ready\"\n  description: \"Pod {{ $labels.pod }} has taken more than 5 minutes to reach Ready state, indicating potential startup issues.\"\n```\n\nIn dashboards, visualize the readiness delay per pod by plotting `(kube_pod_status_ready_time - kube_pod_created)` over time to quickly identify pods with startup delays.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_initialized_time",
      "namespace": "*",
      "pod": "*",
      "uid": "*"
    },
    "description": "The **kube_pod_status_initialized_time** metric records the Unix timestamp when a Kubernetes pod transitions to the \"Initialized\" phase, marking the start of its operational lifecycle. In Prometheus, this metric helps SREs track pod initialization timing to ensure pods are starting promptly after creation.\n\n**Purpose:**  \n- Measure when each pod completes initialization.  \n- Identify pods with delayed startup, which may indicate issues such as resource contention, node pressure, or misconfigurations.\n\n**Alerting Threshold:**  \n- Alert if the time elapsed since pod creation to initialization exceeds a defined threshold (e.g., if a pod remains uninitialized for more than 5 minutes).  \n- This can be implemented by comparing the current time to the pod\u2019s initialization timestamp and triggering alerts when the delay is excessive.\n\n**Impact of Values:**  \n- **High values (older timestamps):** Indicate pods that have been initialized a long time ago, which is normal for running pods. However, if the initialization timestamp is missing or very recent but the pod is expected to be running, it may signal initialization delays or failures.  \n- **Low or missing values:** Suggest pods that have not yet initialized, potentially stuck in pending or creating states, which can impact application availability.\n\n**Example Usage:**  \n- **Dashboard:** Display the time difference between pod creation and initialization to visualize pod startup latency across namespaces or deployments.  \n- **Alert Rule (PromQL):**  \n  ```\n  time() - kube_pod_status_initialized_time{namespace=\"your-namespace\"} > 300\n  ```  \n  This rule fires if any pod in the specified namespace has been uninitialized for more than 5 minutes, indicating a potential startup issue requiring investigation.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_container_ready_time",
      "namespace": "*",
      "pod": "*",
      "uid": "*"
    },
    "description": "The **kube_pod_status_container_ready_time** metric records the Unix timestamp when a container within a Kubernetes pod first became ready to serve traffic. In Prometheus, this metric helps SREs track container startup latency and readiness state transitions, enabling detection of slow startups or readiness failures. \n\n**Purpose:**  \n- Measure the exact time a container achieves readiness, indicating it is healthy and able to handle requests.  \n- Identify containers that take unusually long to become ready, which may signal application issues, resource constraints, or configuration problems.\n\n**Alerting Thresholds:**  \n- Alert if the container readiness time exceeds a defined threshold relative to pod start time (e.g., container not ready within 2 minutes after pod creation).  \n- For example, trigger an alert if `(kube_pod_status_container_ready_time - kube_pod_start_time) > 120 seconds`, indicating delayed readiness.\n\n**Impact of Values:**  \n- **High values:** Indicate slow container startup or readiness delays, potentially causing increased latency, failed deployments, or degraded service availability.  \n- **Low values:** Reflect fast container readiness, contributing to quicker rollouts and stable service operation.\n\n**Example Usage:**  \n- **Dashboard:** Plot the difference between `kube_pod_status_container_ready_time` and `kube_pod_start_time` over time per pod to visualize startup latency trends.  \n- **Alert Rule (PromQL):**  \n  ```\n  (kube_pod_status_container_ready_time - kube_pod_start_time) > 120\n  ```  \n  This alert fires if any container takes longer than 2 minutes to become ready after pod start, prompting investigation into startup issues.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_reason",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "reason": "*"
    },
    "description": "The **kube_pod_status_reason** metric in Prometheus indicates the current status reason for each Kubernetes pod, labeled by namespace, pod name, UID, and reason (e.g., 'Pending', 'Running', 'Failed'). It reflects the operational state of pods by exposing discrete status reasons reported by the Kubernetes API, enabling SREs to monitor pod lifecycle events and diagnose issues.\n\n**Purpose:**  \nThis metric helps identify pods that are stuck in non-running states (such as 'Pending', 'ContainerCreating', or 'Failed'), which may indicate deployment problems, resource constraints, or application errors. It is essential for tracking pod health and availability across the cluster.\n\n**Alert Thresholds:**  \n- Trigger alerts when any pod remains in a non-terminal, non-running state (e.g., 'Pending' or 'ContainerCreating') for longer than a defined duration (e.g., 5 minutes), indicating potential scheduling or image pull issues.  \n- Alert immediately if any pod reports 'Failed' or 'Error' status reasons, signaling application crashes or critical failures.  \n- Optionally, alert if the count of pods in 'Unknown' status exceeds a threshold, which may indicate cluster communication problems.\n\n**Impact of Values:**  \n- High counts of pods in 'Pending' or 'ContainerCreating' suggest resource shortages or configuration errors delaying pod startup.  \n- Frequent or sustained 'Failed' or 'Error' statuses indicate application instability or infrastructure issues, potentially causing downtime.  \n- A low or zero count of pods in non-running states generally reflects a healthy cluster with stable workloads.\n\n**Example Usage:**  \nA Prometheus alert rule to detect pods stuck in 'Pending' for over 5 minutes:  \n```\nalert: PodPendingTooLong\nexpr: kube_pod_status_reason{reason=\"Pending\"} > 0 and on(pod) (time() - kube_pod_status_reason{reason=\"Pending\"} > 300)\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is Pending for over 5 minutes\"\n  description: \"Pod {{ $labels.pod }} has been stuck in Pending state for more than 5 minutes, indicating possible scheduling or resource issues.\"\n```\n\nIn dashboards, visualize the count of pods by status reason over time to quickly identify trends in pod health and detect anomalies such as spikes in 'Failed' or 'Pending' pods, enabling proactive troubleshooting and capacity planning.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_scheduled",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "condition": "*"
    },
    "description": "The kube_pod_status_scheduled metric measures the scheduling status of a pod in a Kubernetes cluster. It indicates whether the pod has been successfully scheduled on a node or not. A value of 'true' means the pod is scheduled and running, while a value of 'false' indicates that the pod is not scheduled due to various reasons such as resource unavailability, node failure, or scheduling conflicts. This metric can be used in monitoring and alerting to detect issues related to pod scheduling, such as node capacity problems, pod overcommitment, or Kubernetes configuration errors. It can also serve as a precursor to other metrics that measure pod performance and health.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_scheduled_time",
      "namespace": "*",
      "pod": "*",
      "uid": "*"
    },
    "description": "The kube_pod_status_scheduled_time metric measures the Unix timestamp when a pod in a Kubernetes cluster moved into a scheduled status. This indicates that the pod has been successfully assigned to a node for execution. The metric provides insight into the timing of pod scheduling, which can be useful for monitoring and troubleshooting purposes.\n\nPotential implications or usage in monitoring or alerting include:\n- Identifying delays in pod scheduling, which may indicate issues with cluster capacity, resource allocation, or network connectivity.\n- Monitoring the time it takes for pods to move from pending to scheduled status, which can help optimize cluster performance and reduce latency.\n- Triggering alerts when pods remain in a pending state for an extended period, indicating potential issues with pod creation or scheduling.\n\nThis metric is useful for operations teams to understand the timing of pod scheduling and identify potential bottlenecks in the cluster.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_status_unschedulable"
    },
    "description": "The kube_pod_status_unschedulable metric measures the number of pods in an unschedulable state within a Kubernetes cluster. A pod is considered unschedulable if it has been marked as such by its owner or due to resource constraints, preventing it from being scheduled for execution on any node. This metric can be used to identify potential issues with pod scheduling, resource allocation, or configuration errors that may impact the overall performance and reliability of the cluster. It might trigger alerts when a certain threshold of unschedulable pods is reached, indicating a need for manual intervention to resolve the issue.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_tolerations",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "key": "*",
      "operator": "*",
      "value": "*",
      "effect": "*",
      "toleration_seconds": "*"
    },
    "description": "The **kube_pod_tolerations** metric in Prometheus exposes detailed information about the tolerations configured on each pod within a Kubernetes cluster. Each data point includes the pod\u2019s namespace, name, unique identifier (uid), and the toleration attributes: key, operator, value, effect, and toleration_seconds. This metric helps SREs understand how pods tolerate node taints, which directly affects pod scheduling decisions.\n\n**Purpose:**  \nUse this metric to verify that pods have the correct tolerations to be scheduled on nodes with specific taints. It is essential for diagnosing scheduling failures where pods remain in a Pending state due to missing or incorrect tolerations.\n\n**Alerting Thresholds:**  \n- Alert if pods lack required tolerations for critical node taints, causing scheduling delays longer than a defined threshold (e.g., pods stuck Pending for more than 5 minutes).  \n- Alert if toleration_seconds values are unexpectedly low or missing for pods expected to tolerate transient taints, potentially causing premature eviction.\n\n**Impact of Values:**  \n- A **high count** of pods without matching tolerations for existing node taints can indicate widespread scheduling issues and potential service disruptions.  \n- A **low or zero value** for toleration_seconds on pods expected to tolerate taints temporarily may lead to pods being evicted too quickly, causing instability.  \n- Correctly set tolerations ensure pods are scheduled efficiently and remain stable on nodes with taints.\n\n**Example Usage:**  \n- **Dashboard:** Display pods grouped by toleration key and effect, highlighting pods missing tolerations for critical taints (e.g., `node.kubernetes.io/not-ready:NoExecute`).  \n- **Alert Rule:**  \n```yaml\nalert: PodSchedulingBlockedDueToTolerations\nexpr: kube_pod_status_phase{phase=\"Pending\"} > 0 and ignoring(pod) absent(kube_pod_tolerations{key=\"node.kubernetes.io/not-ready\", effect=\"NoExecute\"})\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is Pending without required tolerations\"\n  description: \"Pod {{ $labels.pod }} has been Pending for over 5 minutes and lacks tolerations for critical node taints, preventing scheduling.\"\n```\n\nBy monitoring **kube_pod_tolerations**, SREs can proactively detect and resolve pod scheduling issues related to taints and tolerations, ensuring cluster stability and workload availability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_service_account",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "service_account": "*"
    },
    "description": "The **kube_pod_service_account** metric records the service account name assigned to each pod in a Kubernetes cluster. It provides a label value (string) indicating which service account a pod uses for authentication and authorization. This metric helps track and verify service account assignments across pods. It is useful for detecting misconfigurations or security issues related to pod identity.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_pod_scheduler",
      "namespace": "*",
      "pod": "*",
      "uid": "*",
      "name": "*"
    },
    "description": "The **kube_pod_scheduler** metric counts the total number of times the Kubernetes scheduler has assigned a pod to a node. It is a cumulative counter measured in the number of scheduling events. This metric helps track pod scheduling activity and identify issues such as scheduling failures or delays. Monitoring it can reveal problems with node availability, resource constraints, or scheduler performance.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_created",
      "namespace": "*",
      "replicaset": "*"
    },
    "description": "The **kube_replicaset_created** metric reports the Unix timestamp (in seconds) representing the creation time of a specific Kubernetes ReplicaSet object. It is a gauge metric collected from the Kubernetes API server via the kube-state-metrics exporter. The metric is labeled by the ReplicaSet\u2019s namespace and name, allowing identification of individual ReplicaSets within the cluster. This metric\u2019s value remains constant after the ReplicaSet is created and reflects the exact time the ReplicaSet resource was instantiated. Anomalies such as a missing metric for an expected ReplicaSet, a sudden reset to zero, or an unexpectedly recent creation timestamp for a long-standing ReplicaSet may indicate issues like ReplicaSet deletion and recreation, misconfigurations, or potential deployment problems, and should trigger alerts for further investigation.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_status_replicas",
      "namespace": "*",
      "replicaset": "*"
    },
    "description": "The kube_replicaset_status_replicas metric measures the actual number of replicas that are currently running for each ReplicaSet in a Kubernetes cluster. This value is distinct from the desired replica count specified by the user and can be influenced by factors such as pod scheduling, resource availability, and deployment rollouts. Monitoring this metric can help identify issues with ReplicaSet scaling, pod failures, or resource constraints, allowing operators to take corrective action to maintain application availability and performance.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_status_fully_labeled_replicas",
      "namespace": "*",
      "replicaset": "*"
    },
    "description": "This metric measures the number of fully labeled replicas per ReplicaSet in a Kubernetes cluster. A replica is considered 'fully labeled' if it has all the labels required by its associated ReplicaSet. This metric can be used to monitor the health and consistency of ReplicaSets, which are essential components of Kubernetes deployments. Potential implications or usage in monitoring or alerting include: detecting inconsistencies between ReplicaSets and their desired state, identifying potential issues with label management, and triggering alerts when a significant number of replicas become fully labeled, indicating a successful deployment or scaling event.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_status_ready_replicas",
      "namespace": "*",
      "replicaset": "*"
    },
    "description": "This metric measures the number of ready replicas per ReplicaSet in a Kubernetes cluster. A replica is considered 'ready' if it has been successfully scheduled and is running on a node. This metric provides insight into the health and availability of ReplicaSets, which are essential components for ensuring application scalability and high availability. The metric can be used to monitor the readiness of ReplicaSets, identify potential issues with pod scheduling or deployment, and trigger alerts when the number of ready replicas falls below a certain threshold. It is also useful for capacity planning and resource allocation, as it indicates the actual number of replicas that are available to serve traffic.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_status_observed_generation",
      "namespace": "*",
      "replicaset": "*"
    },
    "description": "The **kube_replicaset_status_observed_generation** metric reports the most recent generation number of a ReplicaSet that the Kubernetes controller has processed. It is a unitless integer representing the ReplicaSet's observed configuration version. This metric helps track whether the controller has acknowledged and acted on the latest changes to the ReplicaSet's specification. Monitoring it can reveal if updates are pending or if the ReplicaSet is stuck during rollout.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_spec_replicas",
      "namespace": "*",
      "replicaset": "*"
    },
    "description": "This metric measures the number of desired pods for a ReplicaSet in a Kubernetes cluster. It represents the intended state of the ReplicaSet, indicating how many replicas should be running at any given time. This value is typically set by the user or automatically managed by the ReplicaSet controller based on the application's requirements.\n\nIn monitoring and alerting, this metric can be used to detect potential issues with pod scaling, such as when the desired number of replicas is not being met due to resource constraints or other factors. It can also serve as a baseline for evaluating the health and performance of ReplicaSets in the cluster.\n\nFor example, if the `kube_replicaset_spec_replicas` metric is consistently lower than expected, it may indicate that there are issues with pod creation or scaling, which could impact application availability and performance.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_metadata_generation",
      "namespace": "*",
      "replicaset": "*"
    },
    "description": "The kube_replicaset_metadata_generation metric measures the sequence number representing a specific generation of the desired state for a Kubernetes ReplicaSet. This metric indicates the version or iteration of the ReplicaSet's configuration that is currently being applied to the cluster. It can be used in monitoring and alerting to detect changes in the ReplicaSet's configuration, such as unexpected updates or rollbacks. For example, if this metric suddenly increases, it may indicate a manual update or a failed rollout, requiring further investigation. Conversely, if the value remains stable, it suggests that the ReplicaSet's configuration is being successfully applied and maintained.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_owner",
      "namespace": "*",
      "replicaset": "*",
      "owner_kind": "*",
      "owner_name": "*",
      "owner_is_controller": "*"
    },
    "description": "The kube_replicaset_owner metric measures information about the ReplicaSet's owner, which is typically a Deployment or another ReplicaSet. This metric provides insight into the ownership and management of ReplicaSets in a Kubernetes cluster. It can be used to monitor and troubleshoot issues related to ReplicaSet creation, scaling, and deletion. Potential implications include identifying orphaned ReplicaSets without an owner, detecting misconfigured Deployments that are not properly managing their ReplicaSets, or alerting on changes in ReplicaSet ownership due to unexpected updates or rollbacks.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_annotations"
    },
    "description": "The kube_replicaset_annotations metric measures the Kubernetes annotations associated with ReplicaSets in a cluster. Annotations are key-value pairs that provide additional metadata about an object without modifying its stored representation. In this context, the metric captures these annotations and converts them into Prometheus labels, making it possible to filter and analyze data based on specific annotation values. This information can be useful for monitoring and troubleshooting purposes, such as identifying ReplicaSets with specific configurations or dependencies. Potential implications include using this metric in alerting rules to notify about changes in ReplicaSet annotations, or in dashboards to visualize the distribution of annotation values across different ReplicaSets.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_replicaset_labels"
    },
    "description": "The **kube_replicaset_labels** metric exposes the Kubernetes labels assigned to ReplicaSets as Prometheus labels, enabling detailed filtering and correlation of ReplicaSets based on their metadata. This metric itself is a set of label mappings rather than a numeric value, so it does not have thresholds or direct alerting conditions. Instead, it is primarily used to:\n\n- Identify and verify the presence and correctness of expected labels on ReplicaSets.\n- Detect label mismatches or missing labels that could indicate misconfigurations affecting deployment behavior or service discovery.\n- Enable label-based aggregation and filtering in dashboards and alerting rules to monitor ReplicaSet health and scaling patterns.\n\n**Impact of label anomalies:**  \n- Missing or incorrect labels may cause ReplicaSets to be excluded from monitoring scopes or service selectors, potentially leading to unnoticed scaling issues or traffic routing failures.  \n- Consistent labeling supports accurate resource tracking and troubleshooting.\n\n**Example usage:**  \nTo alert on ReplicaSets missing a critical label (e.g., `app`), you can use a Prometheus query like:  \n```\ncount(kube_replicaset_labels{label_app=\"\"}) > 0\n```\nThis alert triggers if any ReplicaSet lacks the `app` label, indicating a potential misconfiguration.\n\nIn dashboards, use `kube_replicaset_labels` to group ReplicaSet metrics by labels such as `app`, `environment`, or `version` to analyze deployment patterns and resource usage effectively.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_secret_info",
      "namespace": "*",
      "secret": "*"
    },
    "description": "The **kube_secret_info** metric in Prometheus provides metadata about Kubernetes secrets, including their existence, creation timestamps, and update events, labeled by namespace and secret name. Its primary purpose is to track the lifecycle and presence of secrets within the cluster, enabling SREs to monitor configuration changes and detect potential issues such as missing or stale secrets that could disrupt application functionality.\n\nSince **kube_secret_info** is an informational metric (typically a gauge with a constant value of 1 per existing secret), alerting thresholds are based on the presence or absence of expected secrets rather than numeric values. For example, an alert should trigger if a critical secret is missing (metric absent) or if the secret has not been updated within an expected timeframe, indicating potential staleness or misconfiguration.\n\n**Impact of values:**\n- A value of 1 indicates the secret exists and is recognized by the cluster.\n- Absence of the metric for a critical secret signals deletion or failure to load, potentially causing application errors.\n- Lack of recent updates may indicate outdated secrets, risking security or functionality issues.\n\n**Example alert rule:**\n```yaml\nalert: MissingCriticalSecret\nexpr: absent(kube_secret_info{namespace=\"prod\", secret=\"db-credentials\"}) == 1\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"Critical secret 'db-credentials' is missing in namespace 'prod'\"\n  description: \"The secret 'db-credentials' has not been found for over 5 minutes, which may cause application failures.\"\n```\n\n**Example dashboard usage:**\nDisplay a table or list of all secrets per namespace with their creation timestamps, highlighting any missing or outdated secrets by comparing the current time to the last update time derived from related metrics or events, enabling quick identification of secret lifecycle anomalies.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_secret_type",
      "namespace": "*",
      "secret": "*",
      "type": "*"
    },
    "description": "The kube_secret_type metric measures the type of Kubernetes secret stored in a cluster. This metric can be used to monitor and track changes to sensitive data such as passwords, OAuth tokens, SSH keys, etc. It may also indicate potential security risks if an unauthorized or unencrypted secret is detected. In monitoring and alerting, this metric can be used to trigger notifications when a specific type of secret is created or updated, helping operators to identify and address potential security vulnerabilities in a timely manner.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_secret_annotations"
    },
    "description": "The **kube_secret_annotations** metric records the annotations of Kubernetes Secret objects as Prometheus labels. It captures metadata key-value pairs attached to each secret, providing contextual information without a specific unit of measurement. This metric helps track and monitor changes or inconsistencies in secret annotations across the cluster. It is useful for auditing secret metadata and detecting unexpected modifications.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_secret_labels"
    },
    "description": "The **kube_secret_labels** metric exposes Kubernetes secret labels as Prometheus labels, enabling monitoring of secret metadata across the cluster. Each time a secret is scraped, this metric maps its labels, allowing SREs to track the count and distribution of secrets by specific labels (e.g., environment, team, or application). \n\n**Purpose:**  \n- Monitor the presence and changes of secret labels to detect misconfigurations or unauthorized modifications.  \n- Correlate secret metadata with other metrics for targeted troubleshooting and compliance checks.\n\n**Alerting Guidance:**  \n- Trigger an alert if the count of secrets with a critical label (e.g., `environment=production`) drops below an expected threshold, indicating potential secret deletion or mislabeling.  \n- Alert if unexpected labels appear or if the number of secrets with sensitive labels (e.g., `type=database-credentials`) suddenly increases, which could signal a security risk.\n\n**Impact of Values:**  \n- **High values:** A sudden spike in secrets with a particular label may indicate secret sprawl or unauthorized secret creation, increasing attack surface and management complexity.  \n- **Low values:** A drop in secrets with essential labels might mean secrets were deleted or relabeled incorrectly, potentially causing application failures or security gaps.\n\n**Example Usage:**  \nTo alert if the number of production secrets falls below 10:  \n```yaml\nalert: LowProductionSecrets\nexpr: count(kube_secret_labels{environment=\"production\"}) < 10\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Low number of production secrets detected\"\n  description: \"The count of secrets labeled 'environment=production' is below 10 for more than 5 minutes, which may indicate secret deletion or mislabeling.\"\n```\n\nIn dashboards, use `count(kube_secret_labels{label_key=\"label_value\"})` to visualize secret distribution by label, helping identify trends or anomalies in secret management.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_secret_created",
      "namespace": "*",
      "secret": "*"
    },
    "description": "The kube_secret_created metric measures the Unix creation timestamp of a Kubernetes secret. This metric indicates when a secret was created within the cluster. It can be used to monitor and track changes to secrets over time, such as when new secrets are introduced or existing ones are updated. In operations, this metric might be useful for identifying potential security risks associated with outdated or compromised secrets. For example, an alert could be triggered if a secret older than a certain threshold is detected, indicating the need for rotation or replacement.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_secret_metadata_resource_version",
      "namespace": "*",
      "secret": "*"
    },
    "description": "The kube_secret_metadata_resource_version metric measures the resource version of a specific secret in a Kubernetes cluster. This metric represents a unique identifier for each version of the secret's metadata, allowing for tracking changes to the secret over time. It can be used to monitor and detect unauthorized modifications or updates to sensitive secrets, such as API keys or database credentials. Potential implications include triggering alerts when the resource version changes unexpectedly, indicating potential security breaches or configuration drift. This metric is particularly useful in environments where secrets are frequently updated or rotated.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_secret_owner",
      "namespace": "*",
      "secret": "*",
      "owner_kind": "*",
      "owner_name": "*",
      "owner_is_controller": "*"
    },
    "description": "The kube_secret_owner metric measures information about the owner of a Kubernetes Secret resource. This can be useful in monitoring and alerting scenarios where it's essential to track who is responsible for managing sensitive data stored as Secrets within the cluster. Potential implications include identifying orphaned or unmanaged Secrets, detecting unauthorized access or modifications to sensitive data, or ensuring that Secrets are properly cleaned up when their owner is deleted or updated. This metric can be used in conjunction with other metrics and alerts to provide a more comprehensive view of Kubernetes Secret management and security.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_service_info",
      "namespace": "*",
      "service": "*",
      "uid": "*",
      "cluster_ip": "*",
      "external_name": "*",
      "load_balancer_ip": "*"
    },
    "description": "The **kube_service_info** metric in Prometheus provides static metadata about Kubernetes services, including their namespace, service name, unique identifier (UID), and IP addresses (cluster IP, external name, load balancer IP). It does not represent a numeric value that changes over time but serves as a foundational label set to correlate with other dynamic metrics related to service health and performance.\n\n**Purpose:**  \nThis metric helps SREs identify and track all services running in the cluster along with their configuration details. It is essential for mapping service-related metrics (e.g., request rates, error rates) to specific services and for detecting configuration changes such as IP address updates or service renaming.\n\n**Alerting Guidance:**  \nSince **kube_service_info** itself is not a gauge or counter, it does not have thresholds or values to alert on directly. Instead, alerts should be based on related metrics that use this metric\u2019s labels, such as:  \n- Missing or disappearing services (e.g., a service present in previous scrapes but missing now).  \n- Unexpected changes in service IPs or external names indicating configuration drift.  \n- Absence of endpoints or pods backing a service, which can be detected by combining this metric with endpoint or pod metrics.\n\n**Impact of Changes:**  \n- A missing or removed service entry may indicate a service crash, deletion, or misconfiguration, potentially causing application downtime.  \n- Changes in IP addresses or external names without planned updates can lead to traffic routing failures or client connection issues.\n\n**Example Usage:**  \nTo monitor for services that have disappeared unexpectedly, an alert rule could compare the current set of services against a baseline or previous scrape, for example:\n\n```yaml\nalert: KubernetesServiceMissing\nexpr: count(kube_service_info) < count(kube_service_info offset 5m)\nfor: 5m\nlabels:\n  severity: critical\nannotations:\n  summary: \"Kubernetes service missing in namespace {{ $labels.namespace }}\"\n  description: \"Service {{ $labels.service }} in namespace {{ $labels.namespace }} was present 5 minutes ago but is now missing.\"\n```\n\nIn dashboards, **kube_service_info** labels can be used to filter and group service-related metrics, enabling visualization of service health, configuration, and endpoint status across namespaces and clusters.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_service_created",
      "namespace": "*",
      "service": "*",
      "uid": "*"
    },
    "description": "The **kube_service_created** metric records the Unix timestamp when a Kubernetes service was created, labeled by namespace, service name, and UID. In Prometheus, this metric helps SREs track the age of services, enabling monitoring of service deployment timelines and detecting unexpected service creations or deletions.\n\n**Purpose:**  \n- Identify newly created services and their creation times.  \n- Monitor service lifecycle events to detect anomalies such as sudden bursts of service creation that may indicate automation errors or security issues.  \n- Correlate service age with incidents or performance issues.\n\n**Alerting Guidance:**  \n- Alert if a service is created unexpectedly outside of deployment windows or if a large number of services are created within a short time frame (e.g., more than 5 new services within 10 minutes).  \n- Example threshold: Trigger an alert if `count_over_time(kube_service_created[10m]) > 5`, indicating potential abnormal service creation activity.\n\n**Impact of Values:**  \n- **High (recent) values:** Indicate newly created services; a sudden spike may signal deployment automation issues or potential security breaches.  \n- **Low (older) values:** Indicate stable, long-running services; unusually old services might suggest stale or orphaned resources needing cleanup.\n\n**Example Usage:**  \n- **Dashboard:** Display service age by calculating `time() - kube_service_created` to visualize how long each service has been running.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: HighServiceCreationRate\n  expr: count_over_time(kube_service_created[10m]) > 5\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of new Kubernetes service creations\"\n    description: \"More than 5 services have been created in the last 10 minutes, which may indicate abnormal deployment activity.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_service_spec_type",
      "namespace": "*",
      "service": "*",
      "uid": "*",
      "type": "*"
    },
    "description": "The **kube_service_spec_type** metric indicates the type of a Kubernetes Service resource, such as ClusterIP, NodePort, LoadBalancer, or ExternalName. It is a labeled metric where the value is typically set to 1 to signify the presence of the service type for the given service. This metric helps track and verify the configured service type within a specific namespace and service UID. It is useful for detecting unexpected changes or misconfigurations in service types that may affect application accessibility or behavior.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_service_annotations"
    },
    "description": "The kube_service_annotations metric measures the Kubernetes annotations associated with a service in a Prometheus-scraped Kubernetes cluster. Annotations are key-value pairs that can be attached to resources such as services, pods, and deployments. They provide additional metadata about the resource without modifying its underlying configuration. This metric converts these annotations into Prometheus labels, making them available for monitoring and analysis. The implications of this metric include being able to track changes in service configurations over time, identify potential issues related to annotation mismatches or inconsistencies, and use annotations as a dimension for aggregating metrics. For example, you could create alerts based on specific annotation values or use the annotations to filter metrics for services with certain characteristics.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_service_labels"
    },
    "description": "The **kube_service_labels** metric in Prometheus is a set of label mappings representing the key-value pairs assigned to Kubernetes Service objects. It is a metadata metric generated by the Kubernetes API server and collected via the kube-state-metrics component. This metric does not have a numeric value but instead exposes service labels as Prometheus labels attached to a constant gauge value of 1, effectively encoding the labels of each Kubernetes Service for use in Prometheus queries and aggregations. The metric\u2019s primary purpose is to enable filtering, grouping, and selection of metrics based on service labels within Prometheus, facilitating detailed monitoring and alerting tied to service metadata. Since it reflects the current state of service labels, an unusual value\u2014such as missing expected labels, unexpected label keys, or label values that deviate from defined conventions\u2014may indicate misconfiguration, deployment issues, or unauthorized changes. Alerting on such anomalies can help detect configuration drift, policy violations, or potential security concerns in the cluster\u2019s service definitions.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_service_spec_external_ip"
    },
    "description": "The kube_service_spec_external_ip metric measures the external IP addresses associated with each service in a Kubernetes cluster. This metric provides one series for each unique external IP address assigned to a service. It is used to monitor and track changes to service external IPs, which can be indicative of scaling events, load balancer reconfigurations, or other infrastructure updates. Potential implications include alerting on unexpected changes to service external IPs, tracking the number of services with external IPs, or analyzing the distribution of external IP addresses across different services.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_service_status_load_balancer_ingress",
      "namespace": "*",
      "service": "*",
      "uid": "*",
      "ip": "*",
      "hostname": "*"
    },
    "description": "This metric, **kube_service_status_load_balancer_ingress**, reports the current status of the load balancer ingress points associated with a Kubernetes Service. Each data point is labeled by namespace, service name, unique identifier (uid), IP address, and hostname of the ingress. It reflects whether the load balancer has successfully provisioned and is routing traffic to the service endpoints.\n\n**Purpose:**  \nUse this metric in Prometheus to monitor the health and availability of the load balancer ingress for Kubernetes Services. It helps detect issues such as failed provisioning, connectivity problems, or misconfigurations that prevent external traffic from reaching the service pods.\n\n**Thresholds and Alerting:**  \n- A value of **1** typically indicates a healthy ingress (load balancer is active and routing traffic).  \n- A value of **0** or absence of data indicates the ingress is not ready or has failed.  \n- Trigger alerts if the metric remains **0** or missing for more than 5 minutes, signaling that the load balancer is not routing traffic properly. For example:  \n  ```  \n  alert: LoadBalancerIngressUnavailable  \n  expr: kube_service_status_load_balancer_ingress == 0  \n  for: 5m  \n  labels: { severity = \"critical\" }  \n  annotations: { summary = \"Load balancer ingress is unavailable for service {{ $labels.service }} in namespace {{ $labels.namespace }}\" }  \n  ```\n\n**Impact of Values:**  \n- **High (1):** Load balancer ingress is healthy; external traffic can reach the service.  \n- **Low (0) or missing:** Load balancer ingress is down or not provisioned; service may be unreachable externally, causing potential downtime or degraded user experience.\n\n**Example Usage:**  \nIn a Grafana dashboard, plot `kube_service_status_load_balancer_ingress` over time filtered by namespace and service to visualize ingress availability trends. Combine with related metrics like pod readiness and endpoint counts to correlate ingress issues with backend service health. This enables rapid identification and troubleshooting of load balancer-related service disruptions.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_created",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "The **kube_statefulset_created** metric records the Unix timestamp when a Kubernetes StatefulSet was initially created in the cluster. In Prometheus, this metric helps SREs track the age and lifecycle of StatefulSets by providing a precise creation time. \n\n**Purpose:**  \n- Monitor the deployment timeline of StatefulSets.  \n- Detect unexpected recreations or rollbacks by observing changes in creation timestamps.  \n- Correlate StatefulSet age with stability or incident patterns.\n\n**Alert Threshold Guidance:**  \n- Alert if the creation timestamp suddenly resets to a recent time for a StatefulSet expected to be long-lived, indicating an unexpected deletion and recreation.  \n- For example, trigger an alert if `kube_statefulset_created` for a StatefulSet is less than 1 hour old but the StatefulSet is expected to be stable and older than 24 hours.\n\n**Impact of Values:**  \n- **High (older) values:** Indicate a stable, long-running StatefulSet. This is generally healthy unless the StatefulSet is outdated and requires an update.  \n- **Low (recent) values:** May indicate a recent deployment, update, or unexpected recreation, which could be normal during rollouts or problematic if unplanned.\n\n**Example Usage:**  \n- **Dashboard:** Display the age of each StatefulSet by subtracting `kube_statefulset_created` from the current time (`time() - kube_statefulset_created`), helping visualize which StatefulSets are newly created or long-standing.  \n- **Alert Rule Example:**  \n  ```\n  alert: UnexpectedStatefulSetRecreation\n  expr: time() - kube_statefulset_created{namespace=\"prod\", statefulset=\"my-app\"} < 3600\n  for: 10m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"StatefulSet 'my-app' was recreated within the last hour\"\n    description: \"The StatefulSet 'my-app' in namespace 'prod' has a creation timestamp less than 1 hour ago, which may indicate an unexpected recreation.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_replicas",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "This metric measures the number of replicas for each StatefulSet in a Kubernetes cluster. A replica is an instance of a pod that makes up a StatefulSet. The metric provides information on the current state of the replicas for each StatefulSet, which can be useful for monitoring and troubleshooting purposes.\n\nPotential implications or usage in monitoring or alerting include:\n- Identifying under or over-provisioned resources: If the number of replicas is not matching the expected value, it may indicate that there are insufficient resources available to support the desired level of replication.\n- Detecting pod failures or crashes: If the number of replicas for a StatefulSet is consistently lower than expected, it could be an indication that some pods have failed or crashed.\n- Monitoring scaling and deployment operations: This metric can also be used to monitor the progress of scaling or deployment operations, ensuring that the desired number of replicas is reached.\n\nNote: The accuracy of this metric depends on the Prometheus configuration and the Kubernetes cluster's state. It may not reflect real-time changes in the cluster's state due to factors like delayed scraping or caching.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_replicas_available",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "The kube_statefulset_status_replicas_available metric measures the number of available replicas for each StatefulSet in a Kubernetes cluster. A replica is considered available if it has been successfully created and is running. This metric provides insight into the deployment and scaling status of StatefulSets, which are used to manage stateful applications such as databases or message queues. High values indicate that the desired number of replicas have been successfully deployed, while low values may indicate issues with deployment or scaling. Potential implications for monitoring include setting up alerts for when the available replica count falls below a certain threshold, indicating potential issues with application availability or scalability. This metric can also be used to monitor the health and performance of StatefulSets over time, allowing operators to identify trends and patterns in deployment and scaling.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_replicas_current",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "This metric measures the current number of replicas for each StatefulSet in a Kubernetes cluster. A replica is an instance of a pod that makes up a StatefulSet. The metric provides real-time information on the actual number of running replicas, which can be compared to the desired number of replicas specified by the StatefulSet's 'replicas' field. This allows operators to monitor and troubleshoot issues related to StatefulSet scaling, deployment, or pod management. Potential implications include detecting under- or over-provisioning of resources, identifying failed deployments, or tracking changes in replica counts due to scaling events. Operators can use this metric to create alerts for unexpected changes in replica counts, ensuring that their applications remain available and meet service level agreements.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_replicas_ready",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "This metric measures the number of ready replicas for each StatefulSet in a Kubernetes cluster. A replica is considered 'ready' if its pod has been created and is running successfully. This metric provides insight into the deployment status of StatefulSets, which are used to manage stateful applications such as databases or message queues. It can be used to monitor the health and availability of these applications, identify scaling issues, or detect problems with pod creation or deployment. Potential implications include alerting on low replica counts, investigating pod failures, or optimizing StatefulSet configurations for better scalability.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_replicas_updated",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "This metric measures the number of updated replicas for each StatefulSet in a Kubernetes cluster. It indicates the number of replicas that have been successfully updated to match the desired state defined by the StatefulSet's specification. This can be used to monitor the health and consistency of StatefulSets, which are commonly used for applications requiring ordered, replicated, and self-healing pods. Potential implications include identifying issues with pod updates, detecting inconsistencies between desired and actual states, or triggering alerts when a significant number of replicas fail to update successfully.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_observed_generation",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "This metric measures the generation observed by the StatefulSet controller in a Kubernetes cluster. It represents the current state of the StatefulSet's observed generation, which is used to track changes made to the StatefulSet's configuration. A higher value indicates that the StatefulSet has been updated or modified since its last observed generation. This metric can be used to monitor the health and consistency of StatefulSets in a cluster, and potential implications include detecting configuration drift, identifying scaling issues, or triggering rollouts due to changes in the application's requirements.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_replicas",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "This metric measures the number of desired pods for a StatefulSet in a Kubernetes cluster. A StatefulSet is a workload management resource that ensures a specified number of replicas (pods) are running at any given time. This metric indicates the intended state of the StatefulSet, which can be used to monitor and troubleshoot issues related to pod scaling, deployment rollouts, or other operations that affect the desired replica count. Potential implications for monitoring include tracking changes in the desired replica count over time, identifying discrepancies between the desired and actual number of pods, and setting up alerts when the desired replica count is not met. This metric can be used in conjunction with other metrics, such as kube_statefulset_replicas_current, to gain a more comprehensive understanding of StatefulSet behavior.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_ordinals_start"
    },
    "description": "The **kube_statefulset_ordinals_start** metric reports the starting ordinal number (an integer) assigned to the first pod in a Kubernetes StatefulSet. This ordinal defines the initial index from which the StatefulSet\u2019s replicas are created and managed. It helps track the base replica number during StatefulSet scaling or updates. Monitoring this metric aids in detecting configuration errors or unexpected changes in pod ordinals.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_metadata_generation",
      "namespace": "*",
      "statefulset": "*"
    },
    "description": "The kube_statefulset_metadata_generation metric measures the sequence number representing a specific generation of the desired state for the StatefulSet in a Kubernetes cluster. This metric indicates the current version of the StatefulSet's metadata, which is used to track changes and ensure consistency between the desired and actual states of the deployment. A higher value of this metric signifies that there have been updates or modifications made to the StatefulSet's configuration, such as scaling, rolling updates, or other changes. This information can be useful in monitoring and alerting scenarios where it is essential to detect and respond to changes in the cluster's state. For instance, if the generation number increases unexpectedly, it may indicate a misconfiguration, deployment failure, or other issues that require immediate attention from operators.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_persistentvolumeclaim_retention_policy",
      "namespace": "*",
      "statefulset": "*",
      "when_deleted": "*",
      "when_scaled": "*"
    },
    "description": "This metric measures the count of retention policies for Persistent Volume Claims (PVCs) in StatefulSet templates. A retention policy determines how long a PVC is retained after its corresponding pod is deleted or recreated. This metric can be used to monitor and troubleshoot issues related to PVC retention, such as identifying when a PVC is not being properly cleaned up or when a retention policy is not being applied correctly. Potential implications include detecting potential storage leaks, optimizing resource utilization, and ensuring compliance with data retention policies. It may also be useful in alerting scenarios where PVC retention exceeds a certain threshold or when a PVC is retained for an unexpectedly long period.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_annotations"
    },
    "description": "The **kube_statefulset_annotations** metric exposes the annotations attached to Kubernetes StatefulSets as Prometheus labels, enabling SREs to monitor metadata changes and enforce configuration consistency across StatefulSets. Since annotations are arbitrary key-value pairs used for operational metadata, this metric helps track annotation presence and detect unexpected modifications that could impact StatefulSet behavior or cluster policies.\n\n**Purpose:**  \n- To provide visibility into StatefulSet annotations for auditing, compliance, or operational triggers.  \n- To detect annotation drift or missing critical annotations that may affect deployment strategies, backups, or custom controllers relying on these annotations.\n\n**Alerting Guidance:**  \n- Alerts should be based on the presence, absence, or unexpected changes of specific annotation keys or values critical to your environment.  \n- For example, trigger an alert if a required annotation (e.g., `backup/enabled=\"true\"`) is missing or altered unexpectedly.  \n- Thresholds are context-dependent; a common pattern is alerting on annotation mismatches or sudden changes rather than numeric thresholds.\n\n**Impact of Values:**  \n- A high cardinality of annotations (many unique key-value pairs) may indicate configuration sprawl or inconsistent metadata usage, potentially complicating automation or monitoring.  \n- Missing or incorrect annotations can lead to failed automation workflows, misconfigured StatefulSets, or policy violations.\n\n**Example Usage:**  \n- **Dashboard:** Display a table of StatefulSets with their critical annotations to quickly identify missing or inconsistent metadata.  \n- **Alert Rule Example:**  \n```yaml\nalert: MissingBackupAnnotationOnStatefulSet\nexpr: kube_statefulset_annotations{annotation_backup_enabled!=\"true\"} == 1\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"StatefulSet {{ $labels.statefulset }} is missing backup enabled annotation\"\n  description: \"The StatefulSet {{ $labels.statefulset }} in namespace {{ $labels.namespace }} does not have the annotation 'backup/enabled=true'. This may impact backup workflows.\"\n```\n\nThis metric is most effective when combined with knowledge of which annotations are operationally significant in your cluster. Collaborate with cluster administrators to define critical annotations and alerting criteria.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_labels"
    },
    "description": "The **kube_statefulset_labels** metric exposes the Kubernetes labels assigned to each StatefulSet resource in the cluster as Prometheus labels. Its primary purpose is to enable identification, filtering, and correlation of StatefulSets based on their metadata within Prometheus queries, dashboards, and alerting rules.\n\n**Purpose in Prometheus:**  \nThis metric does not represent a numeric value but rather metadata about StatefulSets, allowing SREs to group, filter, and track StatefulSets by labels such as environment (e.g., dev, prod), application, or version. It is essential for contextualizing other metrics related to StatefulSets and for detecting configuration changes via label updates.\n\n**Alerting Guidance:**  \nSince this metric reflects labels (metadata), alerting is typically based on detecting changes in the presence or values of labels rather than numeric thresholds. For example, an alert can be triggered if a critical label (e.g., `environment=prod`) is missing or unexpectedly changed on a StatefulSet, which might indicate a misconfiguration or deployment issue.\n\n**Impact of Changes:**  \n- **High cardinality or unexpected label changes** can complicate monitoring by increasing metric complexity or indicating drift in deployment standards.  \n- **Missing or incorrect labels** can lead to gaps in monitoring coverage or misrouted alerts, potentially causing delayed incident response.\n\n**Example Usage:**  \n- **Dashboard:** Use `kube_statefulset_labels` to filter metrics by label, e.g., show CPU usage only for StatefulSets labeled `environment=\"prod\"`:  \n  ```promql\n  sum(rate(container_cpu_usage_seconds_total{statefulset=~\".+\", environment=\"prod\"}[5m])) by (statefulset)\n  ```  \n- **Alert Rule:** Alert if any StatefulSet in the `prod` environment loses the `app` label:  \n  ```yaml\n  alert: MissingAppLabelOnProdStatefulSet\n  expr: count(kube_statefulset_labels{environment=\"prod\", label_app=\"\"}) > 0\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"StatefulSet missing 'app' label in production\"\n    description: \"One or more StatefulSets in the production environment are missing the 'app' label, which may indicate a misconfiguration.\"\n  ```\n\nBy leveraging **kube_statefulset_labels**, SREs can maintain accurate metadata tracking for StatefulSets, ensuring reliable grouping, alerting, and analysis of stateful workloads in Kubernetes.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_current_revision",
      "namespace": "*",
      "statefulset": "*",
      "revision": "*"
    },
    "description": "This metric measures the current revision of a StatefulSet in a Kubernetes cluster. It indicates the version of the StatefulSet used to generate Pods in the sequence from 0 to the current number of replicas. This metric can be useful for monitoring and alerting purposes, such as detecting changes in the StatefulSet configuration or identifying potential issues with Pod deployment. For example, if the revision number increases unexpectedly, it may indicate a change in the application's configuration or a problem with the StatefulSet's rollout process.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_statefulset_status_update_revision",
      "namespace": "*",
      "statefulset": "*",
      "revision": "*"
    },
    "description": "This metric measures the version of the StatefulSet used to generate Pods in a sequence. It indicates the revision number of the StatefulSet that has been updated and is currently being used by the system. The metric provides information about the current state of the StatefulSet, including the number of replicas that have been updated and the total number of replicas. This can be useful for monitoring and alerting purposes to detect any discrepancies or issues with the StatefulSet updates. For example, if the revision number is not incrementing as expected, it may indicate a problem with the deployment process or a configuration issue. Additionally, this metric can be used to track the progress of rolling updates and ensure that all replicas are updated successfully.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_storageclass_info",
      "storageclass": "*",
      "provisioner": "*",
      "reclaim_policy": "*",
      "volume_binding_mode": "*"
    },
    "description": "The **kube_storageclass_info** metric in Prometheus provides metadata about each StorageClass defined in the Kubernetes cluster, including its name (`storageclass`), provisioner type (`provisioner`), reclaim policy (`reclaim_policy`), and volume binding mode (`volume_binding_mode`). This metric itself is a static informational metric and does not have numeric values that increase or decrease; instead, it exposes labels that describe the configuration of storage classes currently available.\n\n**Purpose:**  \nThis metric helps SREs verify which StorageClasses exist and their configurations, enabling validation that the cluster\u2019s storage setup matches expected policies and provisioning strategies. It is essential for inventory, auditing, and ensuring compliance with storage standards.\n\n**Alerting Guidance:**  \nSince this metric is informational, alerting should focus on the presence or absence of expected StorageClasses or unexpected changes in their configuration labels. For example:  \n- Alert if a critical StorageClass (e.g., `fast-ssd`) is missing from the cluster.  \n- Alert if a StorageClass\u2019s `reclaim_policy` changes unexpectedly (e.g., from `Delete` to `Retain`), which could impact volume lifecycle management.  \n- Alert if no StorageClasses are available, indicating potential storage provisioning issues.\n\n**Impact of Changes:**  \n- Missing or misconfigured StorageClasses can cause PersistentVolumeClaims to remain unbound, leading to application failures or degraded performance.  \n- Incorrect `reclaim_policy` settings may cause volumes to persist longer than intended, leading to resource leaks or data retention issues.  \n- Misaligned `volume_binding_mode` can affect pod scheduling and volume attachment timing, impacting application availability.\n\n**Example Usage:**  \nTo alert if the `fast-ssd` StorageClass is missing:  \n```yaml\nalert: MissingFastSSDStorageClass  \nexpr: absent(kube_storageclass_info{storageclass=\"fast-ssd\"})  \nfor: 5m  \nlabels:  \n  severity: critical  \nannotations:  \n  summary: \"StorageClass 'fast-ssd' is missing\"  \n  description: \"The 'fast-ssd' StorageClass is not present in the cluster, which may prevent pods from provisioning volumes with expected performance.\"  \n```\n\nTo display StorageClass configurations in a Grafana dashboard:  \n```promql\nkube_storageclass_info\n```\nUse the labels (`storageclass`, `provisioner`, `reclaim_policy`, `volume_binding_mode`) as table columns to provide an overview of all StorageClasses and their settings.\n\n---\n\nThis approach ensures SREs can monitor StorageClass presence and configuration integrity, enabling proactive detection of storage provisioning issues before they impact workloads.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_storageclass_created",
      "storageclass": "*"
    },
    "description": "This metric records the Unix timestamp when a Kubernetes StorageClass object was created, labeled by the storageclass name. It helps SREs track the age and changes of storage classes in the cluster, enabling detection of newly added or unexpectedly recreated storage classes. \n\n**Purpose:**  \n- Monitor the creation time of each StorageClass to identify recent additions or modifications.  \n- Detect unauthorized or unexpected creation events that may indicate configuration drift or security issues.\n\n**Alerting guidance:**  \n- Trigger an alert if a StorageClass is created unexpectedly outside of planned deployment windows (e.g., creation timestamp within the last 5 minutes).  \n- Alert if a StorageClass is recreated frequently, which may signal instability or misconfiguration.\n\n**Impact of values:**  \n- A very recent (high) timestamp indicates a newly created StorageClass, which may be expected during deployments or unexpected if unplanned.  \n- Older (low) timestamps indicate stable, long-lived StorageClasses, typically expected in steady-state clusters.\n\n**Example alert rule:**  \n```\nalert: UnexpectedStorageClassCreation  \nexpr: time() - kube_storageclass_created > 0 and time() - kube_storageclass_created < 300  \nfor: 5m  \nlabels: { severity=\"warning\" }  \nannotations: { summary=\"StorageClass {{ $labels.storageclass }} was created within the last 5 minutes\" }  \n```\n\n**Example dashboard usage:**  \n- Display StorageClasses with their creation timestamps converted to human-readable dates to track cluster storage configuration changes over time.  \n- Use a table or graph to highlight StorageClasses created recently to quickly identify new or modified storage configurations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_storageclass_annotations"
    },
    "description": "The **kube_storageclass_annotations** metric exposes the annotations applied to Kubernetes StorageClass resources as Prometheus labels. Its primary purpose is to enable monitoring and validation of StorageClass configurations by tracking the presence and correctness of annotations that influence storage behavior, such as reclaim policies, provisioner-specific settings, or custom metadata. \n\n**Purpose:**  \n- Helps SREs verify that StorageClasses have the expected annotations, ensuring consistent storage provisioning and avoiding misconfigurations that could disrupt pod scheduling or data persistence.  \n- Facilitates detection of missing, unexpected, or incorrect annotations that may indicate configuration drift or deployment issues.\n\n**Thresholds and Alerting:**  \n- Since this metric is label-based and not numeric, alerts should focus on the presence or absence of expected annotations rather than numeric thresholds.  \n- Example alert condition: trigger if any StorageClass is missing a required annotation (e.g., `storageclass.kubernetes.io/is-default-class`) or has an annotation with an unexpected value.  \n- Example PromQL alert:  \n  ```\n  absent(kube_storageclass_annotations{annotation_storageclass_kubernetes_io_is_default_class=\"true\"})\n  ```  \n  This fires if no StorageClass is marked as the default, which can cause pods to fail provisioning volumes.\n\n**Impact of Values:**  \n- A high count of StorageClasses with missing or incorrect annotations may lead to failed volume provisioning, pod scheduling delays, or data persistence issues.  \n- Conversely, consistent and correct annotations across StorageClasses indicate healthy storage configuration and reduce risk of storage-related failures.\n\n**Example Usage:**  \n- **Dashboard:** Display a table listing StorageClasses with their annotations, highlighting those missing critical annotations or having unexpected values.  \n- **Alert Rule:**  \n  ```yaml\n  alert: MissingDefaultStorageClassAnnotation\n  expr: absent(kube_storageclass_annotations{annotation_storageclass_kubernetes_io_is_default_class=\"true\"})\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"No default StorageClass annotation found\"\n    description: \"No StorageClass is annotated as default. This may cause volume provisioning failures.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_storageclass_labels"
    },
    "description": "The **kube_storageclass_labels** metric exposes the set of labels assigned to each Kubernetes StorageClass as Prometheus labels. It enables SREs to monitor and query storage class configurations dynamically, helping ensure storage resources are correctly labeled and aligned with organizational policies. Since this metric reflects metadata rather than numeric values, it is primarily used for detecting the presence, absence, or unexpected changes of specific label key-value pairs rather than threshold-based alerts.\n\n**Purpose:**  \n- Track and verify StorageClass labels to detect misconfigurations or unauthorized changes.  \n- Facilitate filtering and grouping of storage classes by labels in dashboards and alerts.  \n- Support compliance and operational policies by ensuring required labels exist (e.g., environment, performance tier).\n\n**Alerting Guidance:**  \n- Trigger alerts on missing mandatory labels (e.g., absence of `environment` or `tier` labels).  \n- Alert on unexpected label values that may indicate misconfiguration (e.g., `tier=gold` on a StorageClass intended for low-cost storage).  \n- Detect sudden changes in label sets that could impact storage provisioning or performance.\n\n**Impact of High or Low Values:**  \n- Since this metric is label-based, \"high\" or \"low\" values correspond to the presence or absence of specific labels or label values.  \n- Missing critical labels can lead to improper storage provisioning, SLA violations, or security risks.  \n- Unexpected label changes may cause workloads to use incorrect storage classes, impacting performance or availability.\n\n**Example Usage:**  \n- **Dashboard:** Display a table of StorageClasses grouped by key labels (e.g., `environment`, `tier`) to quickly identify unlabeled or mis-labeled storage classes.  \n- **Alert Rule:**  \n```yaml\nalert: MissingStorageClassLabel  \nexpr: count(kube_storageclass_labels{label_environment=\"\"}) > 0  \nfor: 5m  \nlabels:  \n  severity: warning  \nannotations:  \n  summary: \"StorageClass missing 'environment' label\"  \n  description: \"One or more StorageClasses do not have the required 'environment' label.\"  \n```\n\nThis approach helps SREs maintain consistent storage class labeling, ensuring reliable storage provisioning and easier troubleshooting.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_validatingwebhookconfiguration_info",
      "namespace": "*",
      "validatingwebhookconfiguration": "*"
    },
    "description": "The kube_validatingwebhookconfiguration_info metric measures information about the ValidatingWebhookConfiguration resources in a Kubernetes cluster. This includes details such as the number of ValidatingWebhookConfigurations, their names, and possibly other metadata. The metric provides insights into the configuration and setup of webhook validation mechanisms within the cluster. Potential implications for monitoring or alerting include tracking changes to ValidatingWebhookConfigurations, detecting discrepancies between expected and actual configurations, or identifying potential security risks associated with misconfigured webhooks. This information can be used to ensure that webhook validation is properly set up and functioning as intended, which is crucial for maintaining the integrity of cluster operations.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_validatingwebhookconfiguration_created",
      "namespace": "*",
      "validatingwebhookconfiguration": "*"
    },
    "description": "This metric, **kube_validatingwebhookconfiguration_created**, records the Unix timestamp when each Kubernetes ValidatingWebhookConfiguration resource was created. It helps SREs track the lifecycle and age of webhook configurations within the cluster, which is critical for security and operational stability.\n\n**Purpose:**  \n- Monitor when webhook configurations are introduced to detect unexpected or unauthorized changes.  \n- Assess the age of webhook configurations to identify outdated or potentially vulnerable configurations that may require review or rotation.\n\n**Thresholds and Alerting Guidance:**  \n- **Alert on new creations:** Trigger an alert if a new webhook configuration appears unexpectedly, indicating a possible change in cluster behavior or security posture. For example, alert if a webhook configuration is created outside of a scheduled deployment window.  \n- **Alert on stale configurations:** Trigger an alert if a webhook configuration has not been updated for a defined period (e.g., 90 days), as stale configurations may pose security risks or indicate neglect.\n\n**Impact of Values:**  \n- **High timestamp values (recent creation):** Indicate new webhook configurations; sudden spikes may signal configuration changes or deployments.  \n- **Low timestamp values (old creation):** Indicate older configurations; if too old, they may be outdated or insecure.\n\n**Example Usage:**  \n- **Dashboard:** Display the age of each ValidatingWebhookConfiguration by subtracting the creation timestamp from the current time, highlighting configurations older than 90 days in red.  \n- **Alert Rule (Prometheus):**  \n```yaml\nalert: StaleValidatingWebhookConfiguration\nexpr: time() - kube_validatingwebhookconfiguration_created > 7776000  # 90 days in seconds\nfor: 1h\nlabels:\n  severity: warning\nannotations:\n  summary: \"ValidatingWebhookConfiguration '{{ $labels.validatingwebhookconfiguration }}' is stale\"\n  description: \"The webhook configuration '{{ $labels.validatingwebhookconfiguration }}' in namespace '{{ $labels.namespace }}' has not been updated for over 90 days.\"\n```\n\nThis approach enables proactive monitoring of webhook configurations to maintain cluster security and operational integrity.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_validatingwebhookconfiguration_metadata_resource_version",
      "namespace": "*",
      "validatingwebhookconfiguration": "*"
    },
    "description": "This metric represents the resource version of a specific ValidatingWebhookConfiguration in the Kubernetes cluster. It measures the current state of the webhook configuration's metadata, indicating whether it has been updated or modified since its last recorded version. The resource version is a unique identifier assigned to each object in the cluster, allowing for efficient tracking and management of changes. This metric can be used to monitor the integrity and consistency of webhook configurations across the cluster, enabling operators to detect potential issues or discrepancies that may arise from concurrent updates or misconfigurations. Potential implications include identifying stale or outdated webhook configurations, detecting unauthorized modifications, or triggering alerts when a configuration's resource version deviates significantly from its expected value.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_validatingwebhookconfiguration_webhook_clientconfig_service",
      "namespace": "*",
      "validatingwebhookconfiguration": "*",
      "webhook_name": "*",
      "service_name": "*",
      "service_namespace": "*"
    },
    "description": "This metric measures the service used by the Kubernetes API server to connect to a validating webhook configuration. It indicates the service endpoint that the API server uses to communicate with the validating webhook for admission control and validation of incoming requests. The metric can be used in monitoring and alerting to detect issues related to communication between the API server and the validating webhook, such as service unavailability or misconfiguration. Potential implications include delayed or failed pod deployments, incorrect resource creation, or security vulnerabilities due to invalidating webhooks. This metric is particularly useful for ensuring the integrity of the Kubernetes cluster's admission control mechanism.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_volumeattachment_labels",
      "volumeattachment": "*"
    },
    "description": "The kube_volumeattachment_labels metric represents a collection of key-value pairs associated with Kubernetes volume attachments. These labels are used to identify and categorize volume attachments based on their characteristics, such as namespace, name, or environment. This metric can be used in monitoring and alerting to track changes in these labels over time, which may indicate configuration drift, misconfigured resources, or other issues that require attention. For example, an operator might use this metric to create alerts when a specific label is added or removed from a volume attachment, indicating a potential security risk or compliance issue.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_volumeattachment_info",
      "volumeattachment": "*",
      "attacher": "*",
      "node": "*"
    },
    "description": "The kube_volumeattachment_info metric provides information about volume attachments in a Kubernetes cluster. It measures the status and details of each volume attachment, including its name, namespace, node, and phase. This metric can be used to monitor the health and availability of persistent volumes (PVs) and their corresponding pod attachments. Potential implications or usage in monitoring or alerting include: detecting issues with PV provisioning, identifying stuck or failed volume attachments, and ensuring that pods have access to required storage resources. Additionally, this metric can help operators troubleshoot volume attachment problems by providing a detailed view of the attachment's status and any errors encountered during the attachment process.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_volumeattachment_created",
      "volumeattachment": "*"
    },
    "description": "The **kube_volumeattachment_created** metric records the Unix timestamp (in seconds) when each volume attachment object was created in a Kubernetes cluster. It provides the exact creation time of volume attachments, allowing monitoring of their lifecycle. This metric helps track the frequency of new volume attachments and can assist in diagnosing issues related to volume provisioning or attachment delays. It can also be correlated with other cluster events such as pod scheduling or deployment updates.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_volumeattachment_spec_source_persistentvolume",
      "volumeattachment": "*",
      "volumename": "*"
    },
    "description": "This metric measures the PersistentVolume source reference for a VolumeAttachment in a Kubernetes cluster. It indicates the specific PersistentVolume that is being referenced by the VolumeAttachment. This information can be useful in monitoring and troubleshooting storage-related issues in the cluster. For example, if there are problems with data persistence or volume availability, this metric can help identify which PersistentVolumes are affected. Additionally, it can be used to monitor the consistency of PersistentVolume references across different VolumeAttachments, ensuring that they match the expected source PersistentVolume.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_volumeattachment_status_attached",
      "volumeattachment": "*"
    },
    "description": "The kube_volumeattachment_status_attached metric measures the status of volume attachments in a Kubernetes cluster. It indicates whether a volume attachment is currently attached to a node or not. This metric can be used to monitor and troubleshoot issues related to persistent storage in the cluster, such as failed attach operations or stuck volumes. Potential implications include identifying nodes with high numbers of detached volumes, which may indicate resource constraints or configuration issues. In monitoring or alerting, this metric could trigger notifications when a volume attachment status changes unexpectedly, helping operators to quickly respond to potential problems.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "kube_volumeattachment_status_attachment_metadata",
      "volumeattachment": "*",
      "metadata_device_path": "*"
    },
    "description": "This metric provides metadata details for each volume attachment in a Kubernetes cluster, identified by the volume attachment name and device path. It reports string labels describing the attachment's configuration, such as the device path on the node where the volume is attached. The metric itself is a gauge with a constant value of 1, indicating the presence of the specified metadata. It is used to monitor and correlate volume attachment information for troubleshooting and inventory purposes.",
    "metric_type": "gauge",
    "exporter": "KubeStateMetrics"
  },
  {
    "query": {
      "MetricName": "cadvisor_version_info",
      "cadvisorRevision": "*",
      "cadvisorVersion": "*",
      "dockerVersion": "*",
      "kernelVersion": "*",
      "osVersion": "*"
    },
    "description": "The cadvisor_version_info metric provides a snapshot of the system's version information, including kernel, OS, Docker, and cAdvisor versions, as well as the cAdvisor revision. This metric is useful for monitoring and tracking changes to the system configuration over time. It can be used to identify potential issues related to compatibility or updates, such as outdated kernel versions that may lead to security vulnerabilities or performance degradation. Additionally, this metric can aid in troubleshooting by providing a clear picture of the system's version landscape. In terms of usage, this metric could be utilized in alerting scenarios to notify administrators when critical components are out of date or when changes occur that may impact system stability.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_blkio_device_usage_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "major": "*",
      "minor": "*",
      "name": "*",
      "namespace": "*",
      "operation": "*",
      "pod": "*"
    },
    "description": "The container_blkio_device_usage_total metric measures the total bytes used by a container on a specific block I/O device. This includes both read and write operations. It provides insight into the storage usage of containers, helping to identify potential issues with disk space or performance. In monitoring or alerting, this metric can be used to detect when a container is consuming excessive storage resources, potentially leading to slow down or even crashes. It can also be used to set thresholds for average or peak usage, triggering alerts when these thresholds are exceeded.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_cpu_cfs_periods_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_cpu_cfs_periods_total metric measures the total number of enforcement period intervals that have elapsed for a given container's CPU usage. This metric is relevant to containers running on Linux systems and utilizes the Control Group (cgroup) scheduler's Completely Fair Scheduler (CFS). The CFS enforces a fair share of CPU resources among tasks within a cgroup, ensuring each task receives its allocated time slice before other tasks are scheduled. The enforcement period interval is the duration for which this allocation is enforced. This metric can be used to monitor and analyze container resource utilization, identify potential bottlenecks, or detect anomalies in CPU usage patterns. It may also serve as an input for alerting mechanisms when a threshold of elapsed periods exceeds a certain value, indicating that a container's CPU requirements are not being met.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_cpu_cfs_throttled_periods_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric counts the total number of scheduling periods during which a container's CPU usage was throttled by the Linux cgroup scheduler. Each throttled period represents a fixed time interval when the container exceeded its CPU quota and was limited from using more CPU. The value is a cumulative count of these throttled intervals since the container started. Monitoring this metric helps identify containers that frequently hit their CPU limits, indicating potential resource contention or the need for resource allocation adjustments.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_cpu_cfs_throttled_seconds_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the total time duration that a container has been throttled by the Linux Control Group (cgroup) scheduler due to CPU constraints. The cgroup scheduler enforces limits on resource usage for containers and processes within them. When a container exceeds its allocated CPU resources, it is throttled, causing its execution to slow down or be delayed. This metric provides insight into how often and for how long the container has been throttled, which can indicate potential performance bottlenecks or resource contention issues. It may be used in monitoring and alerting to detect containers that are consistently being throttled, indicating a need for increased resources or optimization of workload distribution.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_cpu_load_average_10s",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the average CPU load of a container over the last 10 seconds. It represents the average number of processes competing for CPU resources within the container during this time period. A higher value indicates increased CPU utilization and potential performance bottlenecks. This metric can be used to monitor container resource usage, detect CPU-intensive workloads, and trigger alerts when thresholds are exceeded. For example, it could be used to alert developers or operators when a container's CPU load exceeds 50% for an extended period, indicating the need for optimization or scaling.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_cpu_system_seconds_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the total cumulative CPU time, in seconds, that a container's processes have spent executing in kernel (system) mode. It includes time used for system calls, context switches, and interrupt handling performed by the operating system on behalf of the container. The value is a continuously increasing counter representing system CPU usage since the container started. Use this metric to monitor system-level CPU consumption and identify containers with high kernel CPU usage.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_cpu_usage_seconds_total",
      "container": "*",
      "cpu": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative CPU time consumed by a container in seconds. It represents the total amount of CPU time used by the container since it started running. This can be used to monitor and track the resource utilization of containers, identify potential performance bottlenecks, and optimize resource allocation. For example, if this value is consistently high for a particular container, it may indicate that the container is not optimized for its workload or that there are underlying issues with the system's CPU resources. This metric can be used in monitoring and alerting to detect anomalies in CPU usage, trigger scaling actions, or notify developers of potential performance issues.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_cpu_user_seconds_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative user CPU time consumed by a container in seconds. It represents the total amount of time spent executing user-level code within the container's processes. This can be used to monitor and analyze the resource utilization of containers, identify potential performance bottlenecks, and optimize resource allocation. For example, if this value is consistently high for a particular container, it may indicate that the application running inside the container is CPU-intensive or inefficiently coded. In monitoring or alerting scenarios, this metric can be used to trigger alerts when a container's user CPU time exceeds a certain threshold, indicating potential performance issues or resource exhaustion.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_file_descriptors",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_file_descriptors metric measures the number of open file descriptors for a container. File descriptors are system resources that represent open files or connections to external systems. High numbers of open file descriptors can indicate resource contention, potential memory leaks, or inefficient use of system resources. This metric is useful in monitoring and alerting scenarios where high file descriptor usage may lead to performance degradation, errors, or even crashes. It can be used to detect issues such as: (1) Resource exhaustion due to excessive file descriptor allocation; (2) Memory leaks caused by unclosed files or connections; (3) Inefficient use of system resources leading to slow performance. This metric should be monitored in conjunction with other metrics, such as CPU usage, memory consumption, and network I/O, to provide a comprehensive view of container resource utilization.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_inodes_free",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_inodes_free metric measures the number of available inodes on a container's file system. Inodes are the data structures that represent files and directories on a Linux file system. They contain metadata such as file permissions, ownership, and timestamps. This metric is crucial for monitoring disk space utilization and potential inode exhaustion issues. High values indicate sufficient free inodes, while low values may signal impending inode scarcity, which can lead to file creation failures or other issues. This metric can be used in alerting to notify administrators of impending inode exhaustion, allowing them to take proactive measures such as resizing the container's disk space or optimizing file system usage.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_inodes_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_fs_inodes_total** metric reports the total number of inodes available on a container's file system. Inodes are filesystem data structures that store metadata about files and directories. This metric is a count (unitless) representing the maximum number of files and directories the container's filesystem can hold. Monitoring this value helps detect inode exhaustion, which can prevent creating new files or directories within the container.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_io_current",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_io_current metric measures the number of I/O operations currently in progress within a container's file system. This includes reads and writes to disk, as well as other file system interactions such as metadata updates. The metric provides insight into the current workload on the container's storage resources, allowing operators to identify potential bottlenecks or performance issues. It can be used to monitor container resource utilization, detect anomalies in I/O patterns, and trigger alerts when excessive I/O activity is detected. This information can inform decisions around scaling, resource allocation, and optimization of containerized applications.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_io_time_seconds_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative count of seconds spent doing I/O operations within a container's file system. It represents the total time consumed by read and write operations to disk, which can be indicative of storage performance issues or resource bottlenecks. Potential implications include identifying slow-performing containers, detecting potential storage capacity issues, or optimizing application configuration for better resource utilization. This metric is particularly useful in monitoring environments where containerized applications are deployed, as it provides insight into the I/O efficiency and overall system responsiveness.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_io_time_weighted_seconds_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative weighted I/O time in seconds for a container's file system operations. It represents the total time spent on read and write operations to the container's file system, taking into account the weight of each operation (e.g., larger files or more frequent requests). The metric is useful for identifying performance bottlenecks, detecting potential storage issues, and optimizing container resource allocation. It can be used in monitoring and alerting to trigger notifications when I/O times exceed thresholds, indicating a need for intervention.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_limit_bytes",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_limit_bytes metric measures the maximum amount of storage space allocated to a container on a specific filesystem. This value is set by the Docker runtime and represents the total number of bytes that can be consumed by the container's file system, including any data stored in the container's writable layers. In other words, it indicates the maximum size of the container's disk usage. Monitoring this metric can help identify potential storage issues or resource bottlenecks within a containerized environment. For instance, if the container_fs_limit_bytes value is consistently approaching its limit, it may indicate that the container requires more storage space or that there are storage-related performance issues. This information can be used to optimize container resource allocation, prevent data loss due to disk full conditions, and ensure smooth operation of containerized applications.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_read_seconds_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative count of seconds spent reading from the file system by a container. It represents the total time spent on read operations, including disk I/O and network reads, if applicable. This metric can be used to monitor the performance of containers and identify potential issues with storage or network bottlenecks. For example, high values for this metric may indicate slow disk performance, which could lead to increased latency in container applications. It can also be used as a threshold-based alert to notify operations teams when a container's read time exceeds a certain threshold, indicating the need for further investigation and potential optimization of storage or network resources.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_reads_bytes_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative count of bytes read from the file system by a container, as reported by cAdvisor. It represents the total number of bytes that have been read from disk storage within the container's file system since it was started. This metric can be used to monitor and troubleshoot issues related to container I/O performance, such as slow or stalled containers due to excessive disk reads. It may also indicate potential storage capacity issues if the rate of reads is consistently high. In monitoring and alerting, this metric could be used to set thresholds for average or maximum read rates per container, or to trigger alerts when a container's read count exceeds a certain threshold.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_reads_merged_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_reads_merged_total metric measures the cumulative count of read operations that have been merged by the cAdvisor container's file system. This metric is a gauge that tracks the total number of reads merged since the container started, providing insight into the I/O performance and efficiency of the container's file system. A high value may indicate inefficient disk usage or potential bottlenecks in the container's read operations. This metric can be used to monitor and alert on issues related to container storage performance, such as slow disk access times or excessive read requests.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_reads_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_reads_total metric measures the cumulative count of read operations completed by a Docker container's file system. This includes reads from both the container's root filesystem and any mounted volumes. The metric is incremented each time a read operation is performed, providing a total count of reads since the container was started. This information can be useful for monitoring and troubleshooting purposes, such as identifying containers with high disk I/O activity or detecting potential issues with file system performance. It may also be used to set up alerts when a container's read operations exceed a certain threshold, indicating potential issues with data access or storage.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_sector_reads_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_sector_reads_total metric measures the cumulative count of sector reads completed by a container's file system. This metric is collected by cAdvisor and exported to Prometheus. It provides insight into the I/O performance of a container's file system, specifically the number of sectors read from disk storage. A high value for this metric may indicate issues with disk I/O, such as slow disk speeds or insufficient disk capacity. Potential implications include: (1) identifying containers experiencing high disk I/O activity, which could impact application performance; (2) detecting potential disk space issues before they cause service disruptions; and (3) optimizing container deployment strategies to minimize disk I/O bottlenecks. This metric can be used in monitoring and alerting to trigger notifications when a container's sector reads exceed a certain threshold or exhibit unusual patterns, enabling proactive troubleshooting and optimization of containerized applications.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_sector_writes_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_sector_writes_total metric measures the cumulative count of sector writes completed by a container's file system. This metric is collected by cAdvisor and exported to Prometheus. It provides insight into the disk I/O activity of containers, which can be useful for identifying potential storage bottlenecks or performance issues. For example, if this metric is consistently high, it may indicate that a container is experiencing frequent disk writes, potentially leading to slow performance or even data corruption. This metric can be used in monitoring and alerting to detect such issues and trigger further investigation or corrective action.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_usage_bytes",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_fs_usage_bytes** metric measures the total number of bytes used by a container on its filesystem. It reflects the actual disk space consumed by the container\u2019s files, including data, logs, and temporary files. This metric is expressed in bytes and helps monitor the container\u2019s storage usage. It is useful for detecting containers that are consuming excessive disk space or approaching their storage limits.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_write_seconds_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative count of seconds spent writing to the file system by a container. It represents the total time spent on write operations, including disk I/O and file system metadata updates. This metric can be used to monitor the performance and resource utilization of containers, particularly in scenarios where high write activity is expected or observed. Potential implications include identifying slow-performing containers, detecting potential storage bottlenecks, or optimizing container configurations for improved write efficiency.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_writes_bytes_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric, **container_fs_writes_bytes_total**, tracks the cumulative number of bytes written to the file system by a container since it started. It is essential for monitoring disk write activity at the container level in Prometheus, helping SREs identify abnormal or excessive write patterns that could impact performance or stability.\n\n**Purpose:**  \n- Measure total disk write volume per container over time.  \n- Detect containers with unusually high write throughput that may cause disk saturation, increased latency, or wear on storage devices.  \n- Correlate with other metrics like `container_fs_reads_bytes_total` and `container_fs_usage_percent` to assess overall file system health and performance.\n\n**Alerting Guidance:**  \n- Set thresholds based on baseline write rates for your workloads. For example, alert if a container writes more than 1 GB (1,073,741,824 bytes) within 5 minutes, or if the write rate increases by more than 3x the normal baseline.  \n- High sustained values or rapid spikes may indicate runaway processes, logging storms, or misconfigured applications causing excessive disk I/O.  \n- Low or zero writes over extended periods might be normal for some workloads but could also signal stalled or idle containers depending on context.\n\n**Impact of Values:**  \n- **High values:** Potential disk bottlenecks, increased latency, risk of disk wear, or application issues generating excessive writes. May degrade overall node or cluster performance.  \n- **Low values:** Typically normal for read-heavy or idle containers but should be verified against expected workload patterns.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-container write rate by calculating the derivative over time, e.g., `rate(container_fs_writes_bytes_total[5m])`, to visualize write throughput trends and identify spikes.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: HighContainerDiskWrite\n  expr: rate(container_fs_writes_bytes_total[5m]) > 1e9\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Container {{ $labels.container }} is writing >1GB in 5 minutes\"\n    description: \"Container {{ $labels.container }} in pod {{ $labels.pod }} is generating high disk write traffic, which may impact performance.\"\n  ```",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_writes_merged_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_writes_merged_total metric measures the cumulative count of write operations that have been merged by the cAdvisor container's file system. This metric is incremented each time a write operation is merged with another pending write operation, resulting in a single I/O request to the underlying storage device. The purpose of merging writes is to reduce the number of I/O requests and improve storage performance. High values for this metric may indicate inefficient write patterns or resource contention within the container's file system, potentially leading to storage bottlenecks or decreased application performance. This metric can be used in monitoring and alerting to detect issues related to containerized applications' storage usage and optimize their configuration accordingly.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_fs_writes_total",
      "container": "*",
      "device": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_fs_writes_total metric measures the cumulative count of write operations completed by a Docker container's file system. This includes writes to both the container's root filesystem and any mounted volumes. The metric is incremented each time data is written to disk, providing insight into the container's I/O activity. It can be used in monitoring and alerting to detect potential issues such as high disk usage, slow write performance, or even data corruption due to excessive writes. This metric is particularly useful for identifying resource-intensive containers that may require optimization or scaling.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_last_seen",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_last_seen** metric records the Unix timestamp (in seconds) of the most recent observation of a specific container by the cAdvisor exporter. It indicates the exact time when the container's status was last updated. This metric helps identify containers that have stopped reporting, which may signal crashes, restarts, or connectivity issues. Alerts can be configured if the timestamp is older than a defined threshold, indicating potential problems with the container or its host.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_cache",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_memory_cache** metric reports the amount of memory (in bytes) used by a container for page cache, which stores frequently accessed disk data to speed up I/O operations. In Prometheus, this metric helps SREs monitor how effectively a container leverages caching to improve performance and reduce disk reads.\n\n**Purpose:**  \n- Track container-level page cache usage to understand memory consumption patterns related to disk caching.  \n- Identify containers with unusually high cache usage that might indicate excessive memory consumption or inefficient cache eviction.  \n- Detect low cache usage that could suggest insufficient memory allocation or degraded performance due to frequent disk access.\n\n**Alert Thresholds:**  \n- Trigger a warning if **container_memory_cache** exceeds 80% of the container\u2019s memory limit for more than 5 minutes, indicating potential memory pressure or cache bloat.  \n- Trigger a warning if cache usage drops below 10% for an extended period, which may signal insufficient caching and possible performance degradation.\n\n**Impact:**  \n- **High values:** May cause memory pressure, leading to container or node-level OOM (Out Of Memory) events, increased latency, or eviction of other critical caches. Investigate if the container\u2019s memory limits need adjustment or if the application\u2019s caching behavior is suboptimal.  \n- **Low values:** Could indicate that the container is not effectively caching data, resulting in higher disk I/O and slower application response times.\n\n**Example Usage:**  \n- **Dashboard:** Plot `container_memory_cache` alongside `container_memory_usage_bytes` and `container_memory_limit_bytes` to visualize cache usage relative to total memory consumption and limits.  \n- **Alert Rule (PromQL):**  \n  ```\n  container_memory_cache / container_spec_memory_limit_bytes > 0.8\n  ```\n  This fires when cache usage exceeds 80% of the container\u2019s memory limit, sustained over a defined period (e.g., 5 minutes).\n\nBy monitoring **container_memory_cache**, SREs can proactively manage container memory resources, optimize caching strategies, and prevent performance degradation or outages related to memory pressure.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_failcnt",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_memory_failcnt metric measures the number of times a container's memory usage has exceeded its configured limits, triggering a failure condition. This count is incremented each time the container's memory usage exceeds its limit, regardless of whether the container is still running or not. The metric provides insight into how often containers are experiencing memory-related issues, which can be indicative of resource constraints, misconfigured containers, or other problems that may require attention from operators. It can be used to identify containers that are consistently exceeding their memory limits, allowing for proactive measures such as adjusting container configurations, adding more resources, or terminating problematic containers.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_failures_total",
      "container": "*",
      "failure_type": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*",
      "scope": "*"
    },
    "description": "The container_memory_failures_total metric measures the cumulative count of memory allocation failures within a container. This metric is collected by cAdvisor and exposed to Prometheus for monitoring purposes. It indicates the number of times the container's memory requests have been denied due to insufficient available memory, resulting in failed allocations. High values or increasing trends may indicate memory-related issues, such as resource starvation, inefficient memory usage, or even potential DoS attacks. This metric can be used to detect and alert on memory-related problems, allowing operators to investigate and address the root cause before it impacts container performance or availability.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_kernel_usage",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_memory_kernel_usage** metric tracks the amount of kernel memory (in bytes) currently allocated to a specific container. Kernel memory is used by the operating system for critical functions such as process scheduling, memory management, and I/O operations within the container\u2019s cgroup. Monitoring this metric helps identify containers that may be consuming excessive kernel memory, which can lead to system instability or resource contention.\n\n**Purpose:**  \nIn Prometheus, this metric enables SREs to monitor kernel memory usage per container, helping to detect abnormal increases that could indicate memory leaks, inefficient kernel resource usage, or misconfigured container workloads.\n\n**Thresholds and Alerting:**  \nA typical alert threshold might be set when kernel memory usage exceeds a certain percentage of the container\u2019s memory limit or a fixed byte value (e.g., > 100 MiB) sustained over a period (e.g., 5 minutes). For example:  \n```\ncontainer_memory_kernel_usage_bytes > 100 * 1024 * 1024\n```\nThis threshold should be adjusted based on workload characteristics and container memory limits.\n\n**Impact of Values:**  \n- **High values:** May indicate kernel memory leaks, excessive kernel resource consumption, or container misbehavior, potentially causing degraded container or node performance and risking OOM (Out Of Memory) kills.  \n- **Low values:** Generally expected and indicate normal kernel memory usage.\n\n**Example Usage:**  \n- **Dashboard:** Plot `container_memory_kernel_usage_bytes` over time per container to visualize trends and identify spikes.  \n- **Alert Rule Example:**  \n```yaml\nalert: HighKernelMemoryUsage\nexpr: container_memory_kernel_usage_bytes > 100 * 1024 * 1024\n  and on(container, pod, namespace) container_spec_memory_limit_bytes > 0\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High kernel memory usage detected in container {{ $labels.container }} (pod {{ $labels.pod }})\"\n  description: \"Kernel memory usage is above 100MiB for more than 5 minutes, which may impact container stability.\"\n```\nUse this metric alongside overall container memory usage metrics to get a complete picture of container memory health.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_mapped_file",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_memory_mapped_file** metric in Prometheus tracks the total size (in bytes) of memory-mapped files used by a specific container. Memory-mapped files allow applications to access file data directly in virtual memory, improving I/O efficiency by avoiding explicit reads into RAM. Monitoring this metric helps SREs understand how much container memory is tied up in mapped files versus other memory types.\n\n**Purpose:**  \nThis metric is critical for identifying containers that heavily rely on memory-mapped files, which can impact overall memory availability and performance. It complements metrics like `container_memory_usage_bytes` by revealing how much memory is reserved for file mappings rather than heap or stack usage.\n\n**Alert Thresholds:**  \n- Trigger an alert if `container_memory_mapped_file` exceeds a sustained threshold, e.g., 500MB (524,288,000 bytes), for more than 5 minutes, indicating potential inefficient memory use or memory leaks related to file mappings.  \n- Conversely, unusually low values in containers expected to use memory-mapped files (e.g., databases or file servers) might indicate misconfiguration or degraded functionality.\n\n**Impact of Values:**  \n- **High values:** May cause increased memory pressure, leading to container OOM kills or degraded performance due to excessive paging or cache thrashing. It can also signal memory leaks if the mapped files are not released properly.  \n- **Low values:** Could suggest insufficient memory allocation for workloads relying on memory-mapped files, potentially causing application errors or slower file access.\n\n**Example Alert Rule:**  \n```yaml\nalert: HighMemoryMappedFileUsage\nexpr: container_memory_mapped_file > 524288000\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High memory mapped file usage in container {{ $labels.container }}\"\n  description: \"Container {{ $labels.container }} in pod {{ $labels.pod }} has memory mapped files exceeding 500MB for over 5 minutes.\"\n```\n\n**Example Dashboard Query:**  \n```promql\ncontainer_memory_mapped_file{container!=\"\", namespace=\"production\"}\n```\nUse this query to visualize memory-mapped file usage over time per container, helping correlate spikes with application behavior or incidents. Combine with `container_memory_usage_bytes` and `container_memory_rss` for a holistic view of container memory utilization.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_max_usage_bytes",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric, **container_memory_max_usage_bytes**, tracks the highest amount of memory (in bytes) used by a container since it started or last restarted. It reflects the peak memory consumption, helping SREs understand the container\u2019s maximum memory demand over its lifetime. Monitoring this metric in Prometheus allows you to identify containers that may be under-provisioned or experiencing memory leaks.\n\n**Purpose:**  \nUse this metric to detect containers that approach or exceed their allocated memory limits, which can lead to OOM (Out Of Memory) kills or degraded performance. It is especially useful for capacity planning and tuning resource requests and limits in Kubernetes.\n\n**Alert Threshold Guidance:**  \nSet alert thresholds based on the container\u2019s memory limit or expected usage patterns. For example, trigger an alert if the maximum memory usage exceeds 90% of the container\u2019s memory limit for a sustained period (e.g., 5 minutes). This indicates the container is at risk of memory exhaustion and may require resource adjustment or investigation.\n\n**Impact of Values:**  \n- **High values:** Suggest the container is consuming a large amount of memory, potentially causing resource contention or OOM kills. Persistent high peaks may indicate memory leaks or inefficient memory usage.  \n- **Low values:** Indicate the container is using less memory than allocated, which may suggest over-provisioning or efficient memory usage.\n\n**Example Alert Rule:**  \n```yaml\nalert: ContainerHighMemoryUsage\nexpr: container_memory_max_usage_bytes{namespace=\"prod\"} > 0.9 * container_spec_memory_limit_bytes{namespace=\"prod\"}\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High memory usage detected in container {{ $labels.container }} (pod: {{ $labels.pod }})\"\n  description: \"Container memory usage has exceeded 90% of its memory limit for more than 5 minutes.\"\n```\n\n**Example Dashboard Query:**  \n```promql\ncontainer_memory_max_usage_bytes{namespace=\"prod\", pod=~\".*\"}\n```\nVisualize this over time to identify memory usage spikes and trends, correlating with container restarts or performance issues.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_rss",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_memory_rss** metric in Prometheus measures the Resident Set Size (RSS) of a container in bytes, representing the amount of physical RAM currently used by the container\u2019s processes without including swapped-out memory. This metric is critical for SREs to monitor real-time memory consumption at the container level, helping to identify memory pressure, leaks, or inefficient resource usage.\n\n**Purpose:**  \n- Track actual physical memory usage of containers to ensure they operate within allocated limits.  \n- Detect abnormal memory growth that could lead to out-of-memory (OOM) kills or degraded performance.  \n- Inform capacity planning and resource allocation by understanding memory demand patterns.\n\n**Alert Threshold Guidance:**  \n- Trigger alerts when **container_memory_rss** exceeds a defined percentage (commonly 80-90%) of the container\u2019s memory limit or node\u2019s available RAM, indicating potential memory exhaustion.  \n- Example threshold: Alert if `container_memory_rss > 0.9 * container_memory_limit_bytes` sustained for 5 minutes.\n\n**Impact of Values:**  \n- **High values:** May indicate memory leaks, inefficient memory usage, or insufficient memory allocation, potentially causing OOM kills or degraded container performance. Immediate investigation and remediation are advised.  \n- **Low values:** Generally indicate healthy memory usage but sudden drops could signal container restarts or process crashes.\n\n**Example Usage:**  \n- **Dashboard:** Visualize `container_memory_rss` over time per container or pod to spot trends and spikes in memory usage.  \n- **Alert Rule (PromQL):**  \n  ```promql\n  container_memory_rss{namespace=\"prod\"} > 0.9 * container_spec_memory_limit_bytes{namespace=\"prod\"}\n  ```  \n  This rule fires if a container in the \"prod\" namespace uses more than 90% of its memory limit for 5 minutes, prompting an alert to investigate potential memory pressure.\n\nBy monitoring **container_memory_rss** with these guidelines, SREs can proactively manage container memory health, prevent outages, and optimize resource utilization.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_swap",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_memory_swap** metric in Prometheus measures the amount of memory (in bytes) that a container has swapped out to disk. This metric helps SREs understand how much container memory usage exceeds available RAM, causing the system to use slower swap space. Elevated swap values often indicate memory pressure, inefficient resource allocation, or potential memory leaks, which can degrade container and application performance.\n\n**Alerting guidance:**  \nSet alert thresholds based on your environment\u2019s tolerance, for example:  \n- **Warning:** container_memory_swap > 100 MiB sustained for 5 minutes  \n- **Critical:** container_memory_swap > 500 MiB sustained for 5 minutes  \nThese thresholds should be adjusted according to container size and workload characteristics. Persistent or rapidly increasing swap usage warrants investigation and possible remediation, such as increasing memory limits or optimizing application memory usage.\n\n**Impact of values:**  \n- **High values:** Indicate that the container is using swap space due to insufficient RAM, leading to increased latency and degraded performance. This may also signal memory leaks or misconfigured resource limits.  \n- **Low or zero values:** Indicate healthy memory usage with no swapping, which is ideal for performance.\n\n**Example usage:**  \n- **Dashboard:** Plot container_memory_swap over time per container or pod to visualize swap trends and identify problematic workloads.  \n- **Alert rule example (PromQL):**  \n  ```\n  sum by (pod, namespace) (container_memory_swap) > 100 * 1024 * 1024\n  ```\n  triggers an alert if swap usage exceeds 100 MiB for any pod, indicating potential memory pressure.\n\nMonitoring this metric enables proactive management of container memory resources, helping maintain application stability and performance.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_usage_bytes",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_memory_usage_bytes metric measures the current memory usage in bytes for a given container, including all memory regardless of when it was accessed. This includes both used and unused memory, such as buffers, caches, and other allocated memory spaces. The metric provides a snapshot of the container's memory footprint at a specific point in time, allowing operators to monitor and troubleshoot memory-related issues. Potential implications for monitoring or alerting include detecting memory leaks, identifying containers with high memory usage, and triggering alerts when memory thresholds are exceeded. This metric can be used in conjunction with other metrics, such as CPU usage and network I/O, to gain a comprehensive understanding of container performance and resource utilization.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_memory_working_set_bytes",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_memory_working_set_bytes metric measures the current working set of a container in bytes, which represents the portion of memory that is currently being used by the container's processes and is not available for other containers to use. This value can fluctuate over time as the container's workload changes and it allocates or deallocates memory. In monitoring and alerting, this metric can be used to detect potential memory issues in a container, such as high memory usage that may lead to performance degradation or even crashes. It can also be used to identify containers that are consuming excessive amounts of memory, which may indicate inefficient resource utilization or even security vulnerabilities. By setting thresholds for this metric, operators can receive alerts when a container's working set exceeds a certain percentage of its allocated memory or when it approaches the system's available memory limits.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_receive_bytes_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric, **container_network_receive_bytes_total**, tracks the cumulative number of bytes received by a container's network interface since the container started. It is essential for monitoring network throughput and connectivity at the container level within Kubernetes environments. \n\n**Purpose:**  \n- Helps SREs understand network load and data ingress per container.  \n- Useful for detecting network bottlenecks, packet loss, or unusual traffic patterns that may indicate issues such as network congestion, misconfigurations, or security incidents.\n\n**Alerting Thresholds:**  \n- Alert if the rate of increase (bytes received per second) drops below a defined baseline for a sustained period (e.g., less than 10 KB/s for 5 minutes), which may indicate network connectivity loss or application failure.  \n- Alert if the rate spikes above expected capacity (e.g., exceeding 1 Gbps sustained for 5 minutes), which could signal a DDoS attack, misbehaving container, or resource saturation.\n\n**Impact of Values:**  \n- **High values or spikes:** May indicate heavy inbound traffic, potential overload, or abnormal activity requiring investigation.  \n- **Low or zero values:** Could signal network interface issues, container downtime, or application-level problems preventing data reception.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-container network receive rate by calculating the derivative of this metric over time (e.g., `rate(container_network_receive_bytes_total[5m])`) to visualize network traffic trends and identify anomalies.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: ContainerNetworkReceiveLow\n  expr: rate(container_network_receive_bytes_total[5m]) < 10000\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Low network receive rate on container {{ $labels.container }} in pod {{ $labels.pod }}\"\n    description: \"The container's network receive rate has been below 10 KB/s for more than 5 minutes, indicating possible network or application issues.\"\n  ```",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_receive_errors_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric, **container_network_receive_errors_total**, tracks the cumulative number of errors encountered by a container while receiving network traffic on its network interfaces. It reflects issues such as packet drops, corrupted packets, or other receive-side network failures that can degrade container communication reliability.\n\n**Purpose:**  \nIn Prometheus, this metric helps SREs monitor the health and stability of container network reception. A rising or high value indicates potential network problems affecting the container\u2019s ability to receive data correctly, which may impact application performance or availability.\n\n**Alerting Thresholds:**  \n- A common alert threshold is when the rate of increase in receive errors exceeds a small baseline (e.g., > 5 errors per minute) sustained over 5 minutes, indicating a persistent problem rather than transient spikes.  \n- Absolute values alone are less meaningful; focus on error rate trends and sudden increases.\n\n**Impact of Values:**  \n- **Low or zero values:** Normal operation, no receive errors detected.  \n- **Sustained high or increasing values:** Suggest network congestion, misconfigured firewall rules, driver/hardware issues, or container resource constraints causing packet loss or corruption. This can lead to degraded application responsiveness or failures in network-dependent services.\n\n**Example Usage:**  \n- **Dashboard:** Plot the per-container rate of `container_network_receive_errors_total` (e.g., `rate(container_network_receive_errors_total[5m])`) alongside `container_network_receive_bytes_total` to correlate error spikes with traffic volume.  \n- **Alert Rule Example:**  \n  ```\n  alert: ContainerNetworkReceiveErrorsHigh\n  expr: rate(container_network_receive_errors_total[5m]) > 0.083  # ~5 errors per minute\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High network receive errors on container {{ $labels.container }} in pod {{ $labels.pod }}\"\n    description: \"Container {{ $labels.container }} in pod {{ $labels.pod }} has experienced a sustained high rate of network receive errors (>5 errors/min over 5 minutes), indicating potential network issues.\"\n  ```",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_receive_packets_dropped_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative count of packets dropped while receiving in a container network. It indicates the number of packets that were rejected or discarded by the network stack due to various reasons such as buffer overflow, invalid checksum, or other errors. This metric can be used to monitor and troubleshoot issues related to packet loss, network congestion, or misconfigured network settings. Potential implications include identifying bottlenecks in container communication, detecting anomalies in network traffic patterns, or triggering alerts for potential security vulnerabilities. However, without further context or additional metrics, it is unclear what specific thresholds or baselines should be used to determine when this metric indicates a problem.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_receive_packets_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative count of packets received by a container's network interface. It represents the total number of packets that have been received since the container was started or since the last reset. This metric can be used to monitor and troubleshoot network connectivity issues, identify potential bottlenecks in data transfer, and detect anomalies in packet reception rates. For example, if this metric is consistently high, it may indicate a denial-of-service (DoS) attack on the container's network interface. Conversely, if the value is low or decreasing, it could suggest a problem with the underlying network infrastructure or a misconfigured firewall rule.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_transmit_bytes_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric, **container_network_transmit_bytes_total**, tracks the cumulative number of bytes transmitted by a container's network interface since the container started. It is exposed in Prometheus with labels identifying the container, pod, namespace, interface, and other metadata, enabling granular network egress monitoring per container.\n\n**Purpose:**  \nSREs use this metric to monitor outbound network traffic from containers, helping to identify abnormal network usage patterns, potential bottlenecks, or network saturation issues affecting application performance.\n\n**Thresholds and Alerts:**  \n- Set alert thresholds based on the rate of bytes transmitted per second (e.g., using `rate(container_network_transmit_bytes_total[5m])`).  \n- A high sustained transmit rate (e.g., exceeding 100 Mbps for a container not expected to generate such traffic) may indicate a misbehaving container, data exfiltration, or a network storm.  \n- A sudden drop to near zero in transmit bytes for a container that normally sends traffic could signal network connectivity problems or application failure.\n\n**Impact of Values:**  \n- **High values/rates:** May lead to network congestion, increased latency, or resource exhaustion on the host or network infrastructure. It can also indicate abnormal behavior such as data leaks or DDoS attacks originating from the container.  \n- **Low or zero values:** Could mean the container is idle, crashed, or experiencing network issues preventing outbound traffic.\n\n**Example Usage:**  \n- **Dashboard:** Plot `rate(container_network_transmit_bytes_total[5m])` per container to visualize outbound network throughput trends and identify spikes or drops.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: HighContainerNetworkTransmit\n  expr: rate(container_network_transmit_bytes_total[5m]) > 1e8\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High network transmit rate detected for container {{ $labels.container }}\"\n    description: \"Container {{ $labels.container }} in pod {{ $labels.pod }} is transmitting data at >100 Mbps for over 5 minutes.\"\n  ```\n\nThis guidance enables SREs to effectively monitor, alert on, and troubleshoot container network egress using this metric in Prometheus.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_transmit_errors_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric, **container_network_transmit_errors_total**, tracks the cumulative number of transmission errors encountered on a container's network interface since the container started or the metric was last reset. It helps SREs monitor network health at the container level by indicating issues such as packet loss, hardware faults, or driver problems affecting outbound network traffic.\n\n**Purpose in Prometheus:**  \nUse this metric to detect and quantify network transmission failures for individual containers, enabling early identification of connectivity problems that could degrade application performance or availability.\n\n**Alerting Threshold:**  \nA practical alert threshold is when the rate of increase in this metric exceeds 0.1 errors per second sustained over 5 minutes (e.g., `rate(container_network_transmit_errors_total[5m]) > 0.1`). This indicates a persistent transmission problem requiring investigation.\n\n**Impact of Values:**  \n- **Low or zero values:** Normal operation with no detected transmission errors.  \n- **Sustained high values or spikes:** Suggest network interface issues causing packet loss or failed transmissions, potentially leading to degraded application responsiveness or data loss.\n\n**Example Usage:**  \n- **Alert Rule:**  \n  ```promql\n  alert: ContainerNetworkTransmitErrorsHigh\n  expr: rate(container_network_transmit_errors_total[5m]) > 0.1\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High network transmit errors on container {{ $labels.container }} in pod {{ $labels.pod }}\"\n    description: \"Container {{ $labels.container }} in pod {{ $labels.pod }} has a high rate of network transmit errors (>0.1 errors/sec) over the last 5 minutes.\"\n  ```\n\n- **Dashboard Visualization:**  \n  Plot the `rate(container_network_transmit_errors_total[5m])` over time per container and interface to quickly identify containers experiencing network transmission issues, correlating with other metrics like `container_network_transmit_bytes_total` to assess overall network throughput versus error rates.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_transmit_packets_dropped_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric counts the total number of network packets dropped during transmission by a container's network interface. It is a cumulative counter representing packets lost due to issues like buffer overflow or network errors. The unit is the number of packets dropped. Monitoring this metric helps identify network problems affecting container communication.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_network_transmit_packets_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "interface": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the cumulative count of packets transmitted by a container network interface. It represents the total number of packets sent over the network from the container's perspective. This metric can be used to monitor and troubleshoot network connectivity issues within containers, such as packet loss or transmission errors. In monitoring, this metric can help identify potential bottlenecks in network communication between containers or with external services. For example, a sudden spike in transmitted packets could indicate a denial-of-service (DoS) attack or an unexpected increase in traffic. In alerting, thresholds can be set to trigger notifications when the packet transmission rate exceeds a certain threshold, indicating potential issues with container networking.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_oom_events_total",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_oom_events_total metric measures the total count of out-of-memory (OOM) events observed for a specific container. An OOM event occurs when the container's memory usage exceeds its allocated limit, causing the kernel to terminate the process and potentially leading to service disruptions or crashes. This metric can be used in monitoring and alerting to detect potential issues with container resource allocation, identify containers that are consistently experiencing memory pressure, or trigger notifications for proactive troubleshooting and optimization efforts.",
    "metric_type": "counter",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_processes",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_processes** metric counts the total number of active processes running inside a specific container at the time of measurement. This includes all processes initiated by the container\u2019s main application and any supporting system processes. The value is a simple integer representing the current process count. Monitoring this metric helps identify abnormal behavior, such as resource leaks or security issues, by tracking deviations from typical process counts.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_scrape_error"
    },
    "description": "The container_scrape_error metric measures the occurrence of errors during the scraping process for container metrics in cAdvisor. It is a binary metric (1 or 0) that indicates whether an error occurred while retrieving container metrics. A value of 1 signifies that there was an issue with collecting metrics from one or more containers, whereas a value of 0 indicates successful collection. This metric can be used to identify potential issues with the cAdvisor setup, network connectivity, or container health, and may trigger alerts for further investigation. It is essential to monitor this metric in conjunction with other relevant metrics to gain a comprehensive understanding of system performance and troubleshoot any errors that may arise.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_sockets",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_sockets metric measures the number of open sockets for a container, which represents the network connections established by the container to communicate with other services or hosts. This metric can be used to monitor and troubleshoot issues related to container networking, such as excessive socket creation, connection timeouts, or failed connections. It may also indicate potential security risks if an unusually high number of open sockets is detected, suggesting a possible attack or misconfiguration. In monitoring and alerting, this metric can be used to set thresholds for normal socket usage, triggering alerts when the count exceeds expected levels. Additionally, it can be correlated with other metrics, such as CPU, memory, or network I/O, to identify potential bottlenecks or performance issues related to container networking.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_spec_cpu_period",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_spec_cpu_period metric measures the CPU period of a container, which is a fundamental property of the container's resource allocation. This value represents the time interval (in nanoseconds) over which the container's CPU quota is calculated. In other words, it defines how often the container's CPU usage is checked and adjusted to ensure that it does not exceed its allocated share. A higher period indicates less frequent checks, while a lower period implies more frequent checks. This metric can be used in monitoring and alerting to detect potential issues with container resource allocation, such as over- or under-allocation of CPU resources. It may also be useful for capacity planning and optimization purposes.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_spec_cpu_quota",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_spec_cpu_quota metric measures the CPU quota assigned to a container by its runtime environment. This value represents the maximum amount of CPU time that the container can consume within a given time period, usually expressed in millicpu (mCPU) units. The CPU quota is a mechanism used by container runtimes like Docker to limit the CPU resources available to containers, preventing them from consuming excessive CPU and impacting other containers or the host system.\n\nIn monitoring and alerting, this metric can be useful for detecting potential issues related to under-provisioned or over-provisioned CPU resources. For example, if a container's CPU quota is consistently high, it may indicate that the application requires more CPU resources than initially allocated. Conversely, if the quota is low, it could suggest that the container is not utilizing its full CPU capacity.\n\nThis metric can also be used to correlate with other metrics, such as container CPU usage or request latency, to identify performance bottlenecks and optimize resource allocation.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_spec_cpu_shares",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_spec_cpu_shares** metric shows the CPU shares assigned to a container by the Linux kernel's CPU scheduler. It is a unitless value representing the relative weight of CPU time allocated to the container compared to others. A higher value means the container has a greater priority for CPU resources when contention occurs. This metric helps monitor CPU resource distribution among containers.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_spec_memory_limit_bytes",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_spec_memory_limit_bytes metric measures the maximum amount of memory that a container is allowed to consume as specified in its Docker configuration. This value represents the hard limit imposed on the container's memory usage and is used by the Docker runtime to enforce memory constraints. In monitoring or alerting, this metric can be useful for detecting containers that are approaching or exceeding their allocated memory limits, potentially leading to performance issues or even crashes. It may also indicate misconfigured containers or resource-intensive applications that require adjustments to their memory allocations.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_spec_memory_reservation_limit_bytes",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "This metric measures the memory reservation limit for a container in bytes. It represents the maximum amount of memory that can be reserved by the container's processes, as specified in the container's configuration. This value is typically set by the Docker runtime and can be used to ensure that containers do not consume excessive amounts of system memory. In monitoring or alerting, this metric can be used to detect potential issues such as memory exhaustion or misconfigured containers. For example, an alert could be triggered when a container exceeds its reserved memory limit, indicating a need for resource optimization or configuration adjustments.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_spec_memory_swap_limit_bytes",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_spec_memory_swap_limit_bytes metric measures the maximum amount of memory that a container is allowed to use before it starts swapping with disk space. This value is set by the container's specification and represents the upper limit for the container's memory usage. It does not necessarily reflect the actual memory usage or available swap space. In monitoring, this metric can be used to detect potential issues such as containers exceeding their allocated memory limits, which could lead to performance degradation or even crashes. It may also be useful in capacity planning and resource allocation decisions. However, it is essential to note that this metric does not account for other factors like disk I/O, network usage, or container scheduling policies.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_start_time_seconds",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_start_time_seconds metric measures the time in seconds since the Unix epoch (January 1, 1970) when a container was started. This metric provides insight into the lifecycle of containers within a cluster or deployment. It can be used to monitor and analyze container startup times, identify potential issues with container creation or launch processes, and optimize resource allocation based on container usage patterns.\n\nIn monitoring and alerting, this metric can be utilized to:\n- Set up alerts for slow-starting containers that may indicate underlying infrastructure or configuration issues.\n- Track changes in average container start times over time to identify trends or anomalies.\n- Correlate container start times with other metrics, such as CPU or memory usage, to understand the impact of container startup on system resources.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_tasks_state",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*",
      "state": "*"
    },
    "description": "The **container_tasks_state** metric in Prometheus tracks the number of tasks (processes or threads) within a specific container, identified by labels such as container, id, image, name, namespace, and pod, categorized by their current state (e.g., running, waiting, zombie). This metric helps SREs monitor container workload and health by revealing how many tasks are active, blocked, or defunct at any time.\n\n**Purpose:**  \n- Understand container task distribution and lifecycle states to detect abnormal behavior.  \n- Identify resource contention or scheduling delays when tasks accumulate in waiting or blocked states.  \n- Detect potential process leaks or crashes when zombie task counts rise.\n\n**Alert Thresholds:**  \n- Trigger alerts if the number of tasks in the **waiting** or **blocked** state exceeds a threshold (e.g., > 50 tasks) for a sustained period (e.g., 5 minutes), indicating possible resource starvation or deadlocks.  \n- Alert if the number of **zombie** tasks is greater than zero for more than 1 minute, signaling potential process cleanup issues.  \n- High counts of **running** tasks beyond expected limits may indicate CPU saturation or runaway processes.\n\n**Impact of Values:**  \n- **High running tasks:** May cause CPU contention, degraded performance, or instability.  \n- **High waiting tasks:** Suggests I/O bottlenecks, lock contention, or scheduling delays.  \n- **High zombie tasks:** Indicates improper process termination, risking resource leaks.\n\n**Example Usage:**  \n- **Dashboard:** Visualize `container_tasks_state` broken down by `state` and `container` to quickly spot spikes in waiting or zombie tasks.  \n- **Alert Rule (PromQL):**  \n  ```promql\n  sum by(container, namespace, pod) (container_tasks_state{state=\"waiting\"}) > 50\n  ```  \n  This alert fires if any container has more than 50 waiting tasks, indicating potential resource issues.\n\nBy monitoring **container_tasks_state**, SREs can proactively detect and respond to container-level process anomalies, improving reliability and performance.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_threads",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The container_threads metric measures the number of threads running inside a container at any given time. This metric is collected by cAdvisor and exposed to Prometheus for monitoring purposes. A high or increasing number of threads may indicate resource contention, potential performance bottlenecks, or even signs of a memory leak within the container. Conversely, a low thread count could suggest underutilization of available resources. This metric can be used in conjunction with other metrics, such as CPU usage and memory consumption, to gain a more comprehensive understanding of container health and resource allocation. It may also serve as a trigger for alerts when thresholds are exceeded or when sudden changes occur in the thread count.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_threads_max",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*"
    },
    "description": "The **container_threads_max** metric reports the configured maximum number of threads allowed to run concurrently inside a container. This limit is set by the container runtime or cgroup configuration and controls thread concurrency to prevent resource exhaustion. A value of zero means no explicit thread limit is enforced, allowing unlimited threads.\n\n**Purpose:**  \nIn Prometheus, this metric helps SREs monitor container thread limits to ensure applications are not constrained by overly low thread caps or at risk of instability due to unbounded thread creation.\n\n**Thresholds and Alerts:**  \n- Alert if **container_threads_max** is set too low (e.g., below 100 threads) for workloads known to require high concurrency, as this may cause thread starvation and degraded performance.  \n- Alert if the metric is zero (unlimited) in environments where thread limits are required for resource control, indicating a potential misconfiguration.  \n- Combine with actual thread usage metrics to detect when thread counts approach the limit (e.g., >80% of container_threads_max), signaling risk of thread exhaustion.\n\n**Impact of Values:**  \n- **Low values:** May restrict application concurrency, causing slowdowns or deadlocks if the workload demands more threads than allowed.  \n- **High or unlimited values:** Allow greater concurrency but risk resource exhaustion (CPU, memory) if threads grow unchecked, potentially destabilizing the container or host.\n\n**Example Usage:**  \n- **Dashboard:** Display container_threads_max alongside current thread usage to visualize headroom and detect bottlenecks.  \n- **Alert Rule (PromQL):**  \n  ```\n  container_threads_max > 0 and (container_threads_current / container_threads_max) > 0.8\n  ```  \n  This fires when thread usage exceeds 80% of the configured max, prompting investigation before hitting the limit.\n\nBy monitoring **container_threads_max** with these guidelines, SREs can proactively manage thread concurrency limits to maintain container performance and stability.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "container_ulimits_soft",
      "container": "*",
      "id": "*",
      "image": "*",
      "name": "*",
      "namespace": "*",
      "pod": "*",
      "ulimit": "*"
    },
    "description": "The container_ulimits_soft metric measures the soft ulimit values for the root process of a container. Ulimits are resource limits imposed on processes to prevent them from consuming excessive system resources. Soft ulimits are the default limits that can be temporarily exceeded, but will trigger warnings and may cause the process to terminate if not adjusted. This metric provides insight into the resource constraints of the container's root process, helping operators understand potential bottlenecks or resource exhaustion risks. It can be used in monitoring and alerting to detect containers approaching ulimit limits, allowing for proactive adjustments to prevent service disruptions.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_cpu_cores",
      "boot_id": "*",
      "machine_id": "*",
      "system_uuid": "*"
    },
    "description": "The machine_cpu_cores metric measures the total number of logical CPU cores available on a container or host machine. This value represents the maximum number of concurrent execution threads that can run on the system's processor(s). It is an essential metric for understanding the processing power and capacity of a machine, which can impact application performance, resource utilization, and scalability.\n\nIn monitoring and alerting, this metric can be used to:\n- Identify under-provisioned or over-provisioned resources, ensuring optimal allocation of CPU resources.\n- Detect potential bottlenecks in system performance, allowing for proactive capacity planning and optimization.\n- Set thresholds for CPU-intensive workloads, triggering alerts when actual usage exceeds expected levels.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_cpu_physical_cores",
      "boot_id": "*",
      "machine_id": "*",
      "system_uuid": "*"
    },
    "description": "The **machine_cpu_physical_cores** metric reports the total number of physical CPU cores available on a container host or server, reflecting the system's raw processing capacity. In Prometheus, this metric helps SREs understand the hardware limits for scheduling workloads and assessing resource availability. \n\n**Purpose:** It serves as a baseline for evaluating CPU utilization and capacity planning. Since it is a static hardware attribute, it does not change frequently unless the machine is replaced or reconfigured.\n\n**Alert Thresholds:** While this metric itself does not fluctuate, it is critical to correlate it with CPU usage metrics (e.g., `node_cpu_seconds_total`) to detect saturation. An alert should be triggered if CPU usage approaches or exceeds 90% of the total physical cores consistently over a defined period, indicating potential CPU bottlenecks.\n\n**Impact of Values:**  \n- **High value:** Indicates a system with many physical cores, capable of handling more concurrent workloads.  \n- **Low value:** Suggests limited CPU capacity, requiring careful workload distribution to avoid overload.\n\n**Example Usage:**  \n- **Dashboard:** Display `machine_cpu_physical_cores` alongside CPU utilization percentages to visualize how much of the available CPU capacity is in use.  \n- **Alert Rule Example:**  \n```yaml\nalert: HighCpuUtilization\nexpr: sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[5m])) by (instance) > 0.9 * machine_cpu_physical_cores{instance=~\".*\"}\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"High CPU utilization on {{ $labels.instance }}\"\n  description: \"CPU usage is above 90% of physical cores for more than 5 minutes.\"\n```\nThis alert helps ensure that CPU usage remains within the physical core limits, preventing performance degradation due to CPU saturation.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_cpu_sockets",
      "boot_id": "*",
      "machine_id": "*",
      "system_uuid": "*"
    },
    "description": "The **machine_cpu_sockets** metric counts the total number of physical CPU sockets installed on a host machine or container. Each socket can hold one or more CPU cores. This metric is reported as an integer representing the number of sockets. It helps monitor hardware configuration and can be used to assess CPU resource allocation and detect changes in system hardware.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_memory_bytes",
      "boot_id": "*",
      "machine_id": "*",
      "system_uuid": "*"
    },
    "description": "The **machine_memory_bytes** metric reports the total physical RAM installed on a machine, measured in bytes. It reflects only volatile memory (RAM) and does not include non-volatile memory such as ROM. This value is collected by cAdvisor and stored in Prometheus for monitoring purposes. It helps track available system memory capacity and supports resource management and capacity planning.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_nvm_avg_power_budget_watts",
      "boot_id": "*",
      "machine_id": "*",
      "system_uuid": "*"
    },
    "description": "The machine_nvm_avg_power_budget_watts metric measures the average power budget allocated to Non-Volatile Memory (NVM) components within a containerized environment, as reported by cAdvisor. This metric is indicative of the maximum power consumption allowed for NVM devices such as SSDs or NVMe drives. It can be used in monitoring and alerting to detect potential issues related to power management, thermal throttling, or hardware limitations. For instance, if this value consistently exceeds a certain threshold, it may indicate that the system is experiencing power-related bottlenecks, necessitating further investigation into resource allocation, cooling infrastructure, or hardware upgrades.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_nvm_capacity",
      "boot_id": "*",
      "machine_id": "*",
      "mode": "*",
      "system_uuid": "*"
    },
    "description": "The **machine_nvm_capacity** metric reports the total Non-Volatile Memory (NVM) capacity available on a containerized system, segmented by its operating mode (e.g., memory mode, app direct mode). This metric helps SREs monitor the allocation and availability of NVM resources critical for performance-sensitive workloads. \n\n**Purpose:**  \nIn Prometheus, this metric enables tracking of NVM resource capacity per machine and mode, assisting in capacity planning and anomaly detection related to NVM usage.\n\n**Alerting Thresholds:**  \nAn alert should be triggered if the available NVM capacity falls below a predefined threshold, such as 10% of the expected total capacity, indicating potential resource exhaustion or misconfiguration. For example:  \n`machine_nvm_capacity < 0.1 * expected_total_capacity`\n\n**Impact of Values:**  \n- **High values:** Indicate sufficient NVM capacity is available, supporting optimal application performance and stability.  \n- **Low values:** Suggest limited NVM resources, which can lead to degraded application performance, increased latency, or failures in workloads relying on persistent memory.\n\n**Example Usage:**  \n- **Dashboard:** Display a time series graph of `machine_nvm_capacity` grouped by `mode` and `machine_id` to visualize capacity trends and detect sudden drops.  \n- **Alert Rule:**  \n```yaml\nalert: LowNvmCapacity\nexpr: machine_nvm_capacity{mode=\"app_direct\"} < 10000000000  # less than 10GB\nfor: 5m\nlabels:\n  severity: warning\nannotations:\n  summary: \"Low NVM capacity detected on {{ $labels.machine_id }} (mode={{ $labels.mode }})\"\n  description: \"NVM capacity is below 10GB for more than 5 minutes, which may impact container performance.\"\n```\n\nThis guidance enables proactive monitoring and timely response to NVM capacity issues in containerized environments.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_scrape_error"
    },
    "description": "The machine_scrape_error metric measures the occurrence of errors during the scraping process for machine metrics in cAdvisor. It indicates whether there was an issue while retrieving machine-level data, such as CPU usage, memory consumption, or disk I/O statistics. A value of 1 signifies that an error occurred, whereas a value of 0 indicates successful retrieval of machine metrics. This metric can be used to identify potential issues with the cAdvisor setup, network connectivity, or resource availability on the monitored machines. It may trigger alerts for operations teams to investigate and resolve errors affecting data collection, ensuring continuous monitoring and informed decision-making.",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  },
  {
    "query": {
      "MetricName": "machine_swap_bytes",
      "boot_id": "*",
      "machine_id": "*",
      "system_uuid": "*"
    },
    "description": "The **machine_swap_bytes** metric represents the total size, in bytes, of the swap space configured on a machine. Swap space is disk-based virtual memory used when physical RAM is fully utilized, allowing the system to offload inactive pages to disk to free up RAM. In Prometheus, this metric helps SREs monitor the capacity of swap configured on a host, which is critical for understanding memory management and potential performance bottlenecks.\n\n**Purpose:**  \n- Track the total swap space available on a machine to ensure it is sufficient for workload demands.  \n- Detect changes in swap configuration across reboots or system updates.\n\n**Thresholds and Alerts:**  \n- While **machine_swap_bytes** itself measures total swap size (not usage), a low or zero value indicates no swap configured, which may increase risk of out-of-memory (OOM) errors under high memory pressure.  \n- Alert if **machine_swap_bytes** is zero or below a minimum expected threshold (e.g., < 1 GB) on machines that require swap for stability.  \n- Combine with swap usage metrics (e.g., `machine_swap_used_bytes`) to alert on high swap usage relative to total swap size.\n\n**Impact of Values:**  \n- **High value:** Indicates a large swap space configured, which can help prevent OOM but may degrade performance if heavily used due to slower disk access.  \n- **Low or zero value:** No or insufficient swap configured, increasing risk of application crashes or system instability under memory pressure.\n\n**Example Usage:**  \n- **Dashboard:** Display `machine_swap_bytes` alongside `machine_swap_used_bytes` and `machine_memory_bytes` to visualize total swap, used swap, and physical memory, helping identify memory pressure trends.  \n- **Alert Rule Example:**  \n  ```yaml\n  alert: SwapNotConfigured\n  expr: machine_swap_bytes < 1073741824  # less than 1 GB\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"Swap space is less than 1GB on {{ $labels.machine_id }}\"\n    description: \"The machine {{ $labels.machine_id }} has less than 1GB of swap configured, which may lead to OOM errors under high memory load.\"\n  ```",
    "metric_type": "gauge",
    "exporter": "Cadvisor"
  }
]
